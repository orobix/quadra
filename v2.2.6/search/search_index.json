{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"getting_started.html","title":"Getting Started","text":"<p> Docs \u2022   Tutorials \u2022   Configurations </p> <p><code>Quadra</code> aims to simplify deep learning experimenting process, allowing researchers or developers to compare, monitor, and share their experiments quickly. It provides a simple and flexible way to train and deploy deep learning models using YAML configuration files and open-source tools such as Hydra, Lightning framework, and Pytorch. It lets you compose your experiment configurations from single command line interface, so you can conduct multiple experiments with different settings and hyperparameters. Every experiment can be logged using integrations provided by  Lightning framework such Mlflow.</p>"},{"location":"getting_started.html#quick-start-guide","title":"Quick Start Guide","text":"<p>If you use pip to manage your packages, you can install <code>quadra</code> from PyPi by running the following command: <pre><code>pip install quadra\n</code></pre></p> <p>If instead you prefer to use poetry, you can install <code>quadra</code> from PyPi by running the following command: <pre><code>poetry add quadra\n</code></pre></p> <p>If you don't have virtual environment ready, Let's set up our environment for using the <code>quadra</code> library. We have two parts in this guide: Common setup and Environment-specific setup.</p>"},{"location":"getting_started.html#using-conda","title":"Using Conda","text":"<p>Create and activate a new <code>Conda</code> environment. </p> <pre><code>conda create -n myenv python=3.10\nconda activate myenv\n</code></pre>"},{"location":"getting_started.html#using-python-virtualenv","title":"Using Python Virtualenv","text":"<p>Create and activate a new virtual environment.</p> <pre><code># replace `myenv` with the name of your virtual environment\npython3 -m venv myenv\nsource myenv/bin/activate\n</code></pre>"},{"location":"getting_started.html#common-setup","title":"Common Setup","text":"<ol> <li> <p>Check your git version:    Make sure you have git version 2.10 or higher, to avoid any installation failures.   <pre><code>git --version\n</code></pre></p> </li> <li> <p>Upgrade pip:   <pre><code>pip install --upgrade pip\n</code></pre></p> </li> <li> <p>Install the package</p> <ul> <li> <p>Install the <code>quadra</code> package with pip:   <pre><code>pip install quadra\n</code></pre></p> </li> <li> <p>Install the <code>quadra</code> package with poetry:   <pre><code>curl -sSL https://install.python-poetry.org | python3 -\npoetry add quadra\n</code></pre></p> </li> </ul> </li> <li> <p>Run from CLI:   Run the following command to check if the installation was successful:   <pre><code>quadra experiment=default\n</code></pre></p> </li> </ol>"},{"location":"getting_started.html#setup-mlflow-optional","title":"Setup <code>Mlflow</code> (Optional)","text":"<p>To use Mlflow and leverage its functionalities such as saving models, logging metrics, saving artifacts, and visualizing results, you need to ensure that the Mlflow server is running. You can find more information about Mlflow here.</p> <p>By default, the logger configuration is set to Mlflow, and experiments expect the <code>MLFLOW_TRACKING_URI</code> environment variable to be set to the address of the Mlflow server. There are two ways to set this variable:</p> <p>Using the command line:</p> <p><pre><code>export MLFLOW_TRACKING_URI=http://localhost:5000\n</code></pre> This command sets the <code>MLFLOW_TRACKING_URI</code> variable to <code>http://localhost:5000</code>. Replace this with the actual address of your Mlflow server if it's running on a different host or port.</p> <p>Adding it to your environment file:</p> <p><code>Quadra</code> uses environment variables to store credentials and other sensitive information. Thanks to <code>python-dotenv</code> library, you can create a <code>.env</code> file in the main folder of your project and store the credentials there. During the runtime, the library will automatically load the environment variables from the <code>.env</code> file. You can also add the <code>MLFLOW_TRACKING_URI</code> variable to your environment file (e.g., <code>.env</code>). Open the file in a text editor and add the following line:</p> <p><pre><code>MLFLOW_TRACKING_URI=http://localhost:5000\n</code></pre> Again, modify the address if your Mlflow server is running on a different host or port.</p> <p>By setting the <code>MLFLOW_TRACKING_URI</code> variable using either method, you configure the logger to connect to the Mlflow server, enabling you to utilize its features effectively.</p> <p>The <code>export</code> command is specific to Unix-based systems like Linux or macOS. If you are using a different operating system, refer to the appropriate method for setting environment variables.</p>"},{"location":"getting_started.html#run-example-experiments","title":"Run Example Experiments","text":"<p><code>quadra</code> provides a set of example experiments that can be used to test the installation and to provide some example configuration files.</p> <p>By default all the experiments will run on the <code>GPU 0</code>, to run it on the <code>CPU</code> you can specify a different <code>trainer</code> configuration parameter:</p> <pre><code>quadra &lt;configurations&gt; trainer=lightning_cpu\n</code></pre>"},{"location":"getting_started.html#classification-training","title":"Classification Training","text":"<p>To run a simple classification training on the Imagenette dataset with a Resnet18 architecture run the following command:</p> <pre><code>quadra experiment=generic/imagenette/classification/default logger=csv\n</code></pre> <p>This will train the model for 20 epochs and log the metrics in a csv file, at the end of the training a <code>torchscript</code> model will be saved for inference alongside some output images.</p> <p>By default the experiment will run on the GPU 0, to run it on the CPU you can specify a different <code>trainer</code> configuration parameter.</p>"},{"location":"getting_started.html#segmentation-training","title":"Segmentation Training","text":"<p>To run a simple segmentation training on the Oxford pet dataset with a Unet architecture run the following command:</p> <pre><code>quadra experiment=generic/oxford_pet/segmentation/smp logger=csv\n</code></pre> <p>This will make use of the segmentation models pytorch library to train the model for 10 epochs, logging the metrics to a csv file. At the end of the training a torchscript model will be saved for inference alongside some output images.</p>"},{"location":"getting_started.html#ssl-self-supervised-learning-training","title":"(SSL) Self-supervised Learning Training","text":"<p>On the same dataset we can run a simple SSL training using the BYOL algorithm with the following command:</p> <pre><code>quadra experiment=generic/imagenette/ssl/byol logger=csv\n</code></pre> <p>BYOL is not the only SSL algorithm available, you can find a list of all the available algorithms under <code>quadra/configs/experiment/generic/imagenette/ssl</code> folder.</p>"},{"location":"getting_started.html#anomaly-detection-training","title":"Anomaly Detection Training","text":"<p>To run a simple anomaly detection training on the MNIST dataset using the PADIM algorithm run the following command:</p> <pre><code>quadra experiment=generic/mnist/anomaly/padim logger=csv\n</code></pre> <p>This will run an anomaly detection considering on of the classes as good (default is the number 9) and the rest as anomalies.</p> <p>This will make use of the anomalib library to train the model. Many different algorithms are available, you can find them under <code>quadra/configs/experiment/generic/mnist/anomaly</code> folder.</p>"},{"location":"getting_started.html#running-with-custom-datasets","title":"Running with Custom Datasets","text":"<p>Each task comes with a default configuration file that can be customized for your needs. Each example experiment we have seen so far uses a default configuration file that can be found under <code>quadra/configs/experiment/base/&lt;task&gt;/&lt;config_name&gt;.yaml</code>. </p> <p>Let's see how we can customize the configuration file to run the classification experiment on a custom dataset.</p> <p>Structure your dataset in the following way:</p> <pre><code>dataset/\n\u251c\u2500\u2500 class_1\n\u2502   \u251c\u2500\u2500 abc.xyz\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 class_2\n\u2502   \u251c\u2500\u2500 abc.xyz\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 class_3 \n\u2502   \u251c\u2500\u2500 abc.xyz\n\u2502   \u2514\u2500\u2500 ...\n</code></pre> <p>Create a new experiment configuration file under <code>quadra/configs/experiment/custom/&lt;config_name&gt;.yaml</code> with the following content:</p> <pre><code># @package _global_\ndefaults:\n- base/classification/classification # extends the default classification configuration\n\ncore:\nname: &lt;your-custom-experiment-name&gt; # name of the experiment\n\nmodel:\nnum_classes: &lt;number-of_classes-you-have&gt; # number of classes in your dataset\n</code></pre> <p>Run the experiment with the following command:</p> <pre><code>quadra experiment=custom/&lt;config_name&gt; logger=csv\n</code></pre> <p>It will run the experiment using the configuration file you have just created and it will apply the default parameters from the classification configuration file. Furthermore, it will log the metrics to a csv file. You can add or customize the parameters in the configuration file to fit your needs.</p> <p>For more information about advanced usage, please check tutorials and task specific examples.</p>"},{"location":"getting_started.html#development","title":"Development","text":"<p>First clone the repository from Github</p> <p>First clone the repository from <code>Github</code>, then we need to install the package with optional dependencies (generally in editable mode) and enable the pre-commit hooks.</p> <ol> <li><code>git clone https://github.com/orobix/quadra.git &amp;&amp; cd quadra</code> </li> <li>Install poetry <code>curl -sSL https://install.python-poetry.org | python3 -</code></li> <li>Install the required poetry plugins   <pre><code>poetry self add poetry-bumpversion\npoetry self add poetry-dotenv-plugin\n</code></pre></li> <li>Install <code>quadra</code> package in editable mode <code>poetry install --with test,dev,docs --all-extras</code></li> <li>Install pre-commit hooks <code>pre-commit install</code></li> <li>(Optional) Eventually build documentation by calling required commands (see below).</li> </ol> <p>Now you can start developing and the pre-commit hooks will run automatically to prevent you from committing code that does not pass the linting and formatting checks.</p> <p>We rely on a combination of <code>Pylint</code>, <code>Mypy</code> and <code>Ruff</code> to enforce code quality.</p>"},{"location":"getting_started.html#building-documentations","title":"Building Documentations","text":"<ol> <li>Activate your virtual environment.</li> <li>Install the <code>quadra</code> package with at least <code>doc</code> version.</li> <li>To run the webserver for real-time rendering and editing run <code>mkdocs serve</code> and visit <code>http://localhost:8000/</code>.</li> <li>If you want to export the static website to a specific folder  <code>mkdocs build -d &lt;Destination Folder&gt;</code></li> </ol>"},{"location":"getting_started.html#acknowledgements","title":"Acknowledgements","text":"<p>This project is based on many open-source libraries and frameworks, we would like to thank all the contributors for their work. Here is a list of the main libraries and frameworks we use:</p> <ul> <li>Pytorch and Pytorch Lightning for training and deploying deep learning models. These two libraries are core part of training and testing tasks that allow us to run experiments on different devices in agile way.</li> <li>Pretrained models are usually loaded from Pytorch Hub or Pytorch-image-models (or called as <code>timm</code>).</li> <li>Each specific task may rely on different libraries. For example, <code>segmentation</code> task uses Segmentation_models.pytorch for loading backbones. The <code>anomaly detection</code> task uses a fork of Anomalib maintained by Orobix on this repository. We use light-weight ML models from scikit-learn. We have also implementation of some SOTA models inside our library. </li> <li>Data processing and augmentation are done using Albumentations and OpenCV.</li> <li>Hydra for composing configurations and running experiments. Hydra is a powerful framework that allows us to compose configurations from command line interface and run multiple experiments with different settings and hyperparameters. We have followed suggestions from <code>Configuring Experiments</code> section of Hydra documentation and lightning-hydra-template repository.</li> <li>Documentation website is using Material for MkDocs and MkDocs. For code documentation we are using Mkdocstrings. For releasing software versions we combine Bumpver and Mike.</li> <li>Models can be exported in different ways (<code>torchscript</code> or <code>torch</code> file). We have also added ONNX support for some models.</li> <li>Testing framework is based on Pytest and related plug-ins.</li> <li>Code quality is ensured by pre-commit hooks. We are using Ruff for linting, enforcing code quality and formatting, Pylint for in depth linting and Mypy for type checking.</li> </ul>"},{"location":"getting_started.html#faq","title":"FAQ","text":"<p>How can I fix errors related to <code>GL</code> when I install full <code>opencv</code> package?</p> <p>If you are running on a remote server without a display and you are using <code>opencv-python</code> instead of <code>opencv-python-headless</code> you can run the following command to fix the issue:</p> <p>It can be solved by correctly linking the <code>libGL.so.1</code> library:</p> <pre><code># Check where the library is located\nfind /usr -name libGL.so.1\n</code></pre> <pre><code># if the library is located in /usr/lib/x86_64-linux-gnu\n# link it to /usr/lib\nsudo ln -s /usr/lib/x86_64-linux-gnu/libGL.so.1 /usr/lib/libGL.so\n</code></pre> <p>or installing following packages (depending on your OS this may vary):</p> <pre><code>sudo apt-get install libgl1-mesa-glx\n</code></pre> <p>How can I run multiple experiments with single command?</p> <p>You can run multiple experiments with a single command by passing <code>--multirun</code> flag:</p> <pre><code>quadra &lt;configurations&gt; --multirun\n</code></pre> <p>For example if you want to run same experiment with different seeds you can run:</p> <pre><code>quadra experiment=generic/imagenette/classification/default trainer=lightning_cpu logger=csv core.seed=1,2,3 --multirun </code></pre>"},{"location":"reference/CHANGELOG.html","title":"Changelog","text":""},{"location":"reference/CHANGELOG.html#226","title":"[2.2.6]","text":""},{"location":"reference/CHANGELOG.html#updated","title":"Updated","text":"<ul> <li>Remove gradients logging callback from default configs to avoid slowing down the training process</li> <li>Add automatic batch size calculator as default in <code>all</code> callback configuration</li> </ul>"},{"location":"reference/CHANGELOG.html#225","title":"[2.2.5]","text":""},{"location":"reference/CHANGELOG.html#updated_1","title":"Updated","text":"<ul> <li>Update anomalib to v0.7.0.dev143 to fix a bug introduced in the previous version that caused the training to fail if the dataset size was smaller than the batch size.</li> </ul>"},{"location":"reference/CHANGELOG.html#224","title":"[2.2.4]","text":""},{"location":"reference/CHANGELOG.html#updated_2","title":"Updated","text":"<ul> <li>Update anomalib to v0.7.0.dev142 that improves the memory footprint of patchcore model training.</li> </ul>"},{"location":"reference/CHANGELOG.html#223","title":"[2.2.3]","text":""},{"location":"reference/CHANGELOG.html#updated_3","title":"Updated","text":"<ul> <li>Keep only opencv-python-headless as a dependency for quadra to avoid conflicts with the non-headless version</li> </ul>"},{"location":"reference/CHANGELOG.html#222","title":"[2.2.2]","text":""},{"location":"reference/CHANGELOG.html#updated_4","title":"Updated","text":"<ul> <li>Terminate quadra with error if it's not possible to export an ONNX model with automatic mixed precision instead of falling back to full precision</li> </ul>"},{"location":"reference/CHANGELOG.html#fixed","title":"Fixed","text":"<ul> <li>Fix default quadra installation requiring extra dependencies incorrectly</li> <li>Fix matplotlib using interactive backend by default</li> <li>Fix documentation errors</li> </ul>"},{"location":"reference/CHANGELOG.html#221","title":"[2.2.1]","text":""},{"location":"reference/CHANGELOG.html#updated_5","title":"Updated","text":"<ul> <li>Update anomalib version, improve release workflow</li> </ul>"},{"location":"reference/CHANGELOG.html#220","title":"[2.2.0]","text":""},{"location":"reference/CHANGELOG.html#updated_6","title":"Updated","text":"<ul> <li>Update dependencies to support publishing Quadra to PyPI</li> </ul>"},{"location":"reference/CHANGELOG.html#2113","title":"[2.1.13]","text":""},{"location":"reference/CHANGELOG.html#updated_7","title":"Updated","text":"<ul> <li>Improve safe batch size computation for sklearn based classification tasks</li> </ul>"},{"location":"reference/CHANGELOG.html#2112","title":"[2.1.12]","text":""},{"location":"reference/CHANGELOG.html#fixed_1","title":"Fixed","text":"<ul> <li>Fix wrong dtype used when evaluating finetuning or anomaly models trained in fp16 precision</li> </ul>"},{"location":"reference/CHANGELOG.html#2111","title":"[2.1.11]","text":""},{"location":"reference/CHANGELOG.html#fixed_2","title":"Fixed","text":"<ul> <li>Fix sklearn automatic batch finder not working properly with ONNX backbones</li> </ul>"},{"location":"reference/CHANGELOG.html#2110","title":"[2.1.10]","text":""},{"location":"reference/CHANGELOG.html#fixed_3","title":"Fixed","text":"<ul> <li>Fix anomaly visualizer callback showing wrong heatmaps after anomaly score refactoring</li> </ul>"},{"location":"reference/CHANGELOG.html#219","title":"[2.1.9]","text":""},{"location":"reference/CHANGELOG.html#updated_8","title":"Updated","text":"<ul> <li>Update anomalib to v0.7.0+obx.1.3.3</li> <li>Update network builders to support loading model checkpoints from disk</li> </ul>"},{"location":"reference/CHANGELOG.html#218","title":"[2.1.8]","text":""},{"location":"reference/CHANGELOG.html#added","title":"Added","text":"<ul> <li>Add onnxconverter-common to the dependencies in order to allow exporting onnx models in mixed precision if issues are encountered exporting the model entirely in half precision.</li> </ul>"},{"location":"reference/CHANGELOG.html#217","title":"[2.1.7]","text":""},{"location":"reference/CHANGELOG.html#fixed_4","title":"Fixed","text":"<ul> <li>Fix lightning implementation of batch size finder not working properly when the initial batch size is bigger than the dataset length, now also the code checks that the last iteration works properly.</li> </ul>"},{"location":"reference/CHANGELOG.html#216","title":"[2.1.6]","text":""},{"location":"reference/CHANGELOG.html#updated_9","title":"Updated","text":"<ul> <li>Remove poetry dependencies from quadra toml</li> <li>Update readme to explain how to use poetry properly</li> </ul>"},{"location":"reference/CHANGELOG.html#215","title":"[2.1.5]","text":""},{"location":"reference/CHANGELOG.html#fixed_5","title":"Fixed","text":"<ul> <li>Fix classification val_dataloader shuffling data when it shouldn't</li> </ul>"},{"location":"reference/CHANGELOG.html#214","title":"[2.1.4]","text":""},{"location":"reference/CHANGELOG.html#updated_10","title":"Updated","text":"<ul> <li>Remove black from pre-commit hooks</li> <li>Use ruff as the main formatter and linting tool</li> <li>Upgrade mypy version</li> <li>Upgrade mlflow version</li> <li>Apply new pre-commits to all tests</li> <li>Update most of typing to py310 style</li> </ul>"},{"location":"reference/CHANGELOG.html#213","title":"[2.1.3]","text":""},{"location":"reference/CHANGELOG.html#updated_11","title":"Updated","text":"<ul> <li>Update anomalib to v0.7.0+obx.1.3.2</li> </ul>"},{"location":"reference/CHANGELOG.html#212","title":"[2.1.2]","text":""},{"location":"reference/CHANGELOG.html#updated_12","title":"Updated","text":"<ul> <li>Update anomalib to v0.7.0+obx.1.3.1</li> <li>The optimal anomaly threshold is now computed as the average between the max good and min bad score when the F1 is 1</li> </ul>"},{"location":"reference/CHANGELOG.html#211","title":"[2.1.1]","text":""},{"location":"reference/CHANGELOG.html#updated_13","title":"Updated","text":"<ul> <li>Anomaly test task now exports results based on the normalized anomaly scores instead of the raw scores. The normalized anomaly scores and the optimal threshold are computed based on the training threshold of the model.</li> </ul>"},{"location":"reference/CHANGELOG.html#210","title":"[2.1.0]","text":""},{"location":"reference/CHANGELOG.html#updated_14","title":"Updated","text":"<ul> <li>Change the way anomaly scores are normalized by default, instead of using a [0-1] range with a 0.5 threshold, the scores are now normalized to a [0-1000] range with a threshold of 100, the new score represents the distance from the selected threshold, for example, a score of 200 means that the anomaly score is 100% of the threshold above the threshold itself, a score of 50 means that the anomaly score is 50% of the threshold below. </li> <li>Change the default normalization config name for anomaly from <code>min_max_normalization</code> to <code>score_normalization</code>.</li> </ul>"},{"location":"reference/CHANGELOG.html#fixed_6","title":"Fixed","text":"<ul> <li>Fix the output heatmaps and preditions of anomaly inference tasks not being saved properly when images belonged to different classes but had the same name.</li> </ul>"},{"location":"reference/CHANGELOG.html#204","title":"[2.0.4]","text":""},{"location":"reference/CHANGELOG.html#fixed_7","title":"Fixed","text":"<ul> <li>Fix segmentation num_data_train sorting</li> </ul>"},{"location":"reference/CHANGELOG.html#added_1","title":"Added","text":"<ul> <li>Add default presorting to segmentation samples</li> </ul>"},{"location":"reference/CHANGELOG.html#203","title":"[2.0.3]","text":""},{"location":"reference/CHANGELOG.html#fixed_8","title":"Fixed","text":"<ul> <li>Fix anomaly visualizer callback not working properly after lightning upgrade</li> </ul>"},{"location":"reference/CHANGELOG.html#202","title":"[2.0.2]","text":""},{"location":"reference/CHANGELOG.html#fixed_9","title":"Fixed","text":"<ul> <li>Fix deepcopy removing model signature wrapper from the model for classification</li> </ul>"},{"location":"reference/CHANGELOG.html#201","title":"[2.0.1]","text":""},{"location":"reference/CHANGELOG.html#fixed_10","title":"Fixed","text":"<ul> <li>Fix pytorch model mlflow upload. Not supported.</li> </ul>"},{"location":"reference/CHANGELOG.html#200","title":"[2.0.0]","text":""},{"location":"reference/CHANGELOG.html#updated_15","title":"Updated","text":"<ul> <li>Update torch to 2.1.2 with CUDA 12 support</li> <li>Update pytorch lightning to 2.1.*</li> </ul>"},{"location":"reference/CHANGELOG.html#changed","title":"Changed","text":"<ul> <li>Refactor hydra plugins to use optional dev groups intend of extras to avoid dragging local packages around in external installations</li> <li>Refactor extra dev dependencies to use poetry groups instead</li> <li>Improve trainer configs to avoid wrong overrides when calling different trainer overrides</li> </ul>"},{"location":"reference/CHANGELOG.html#158","title":"[1.5.8]","text":""},{"location":"reference/CHANGELOG.html#fix","title":"Fix","text":"<ul> <li>Fix ONNX import call for the utilities when ONNX is not installed.</li> </ul>"},{"location":"reference/CHANGELOG.html#157","title":"[1.5.7]","text":""},{"location":"reference/CHANGELOG.html#added_2","title":"Added","text":"<ul> <li>Add upload_models to core config</li> </ul>"},{"location":"reference/CHANGELOG.html#refactored","title":"Refactored","text":"<ul> <li>infer_signature_torch_model refactored to infer_signature_model</li> </ul>"},{"location":"reference/CHANGELOG.html#156","title":"[1.5.6]","text":""},{"location":"reference/CHANGELOG.html#added_3","title":"Added","text":"<ul> <li>Add support for half precision training and inference for sklearn based tasks</li> <li>Add gradcam export for sklearn training</li> </ul>"},{"location":"reference/CHANGELOG.html#155","title":"[1.5.5]","text":""},{"location":"reference/CHANGELOG.html#fixed_11","title":"Fixed","text":"<ul> <li>Fix poetry version not updating init file properly</li> <li>Fix model_summary.txt not being saved correctly </li> </ul>"},{"location":"reference/CHANGELOG.html#154","title":"[1.5.4]","text":""},{"location":"reference/CHANGELOG.html#added_4","title":"Added","text":"<ul> <li>Add test for half precision export</li> </ul>"},{"location":"reference/CHANGELOG.html#fixed_12","title":"Fixed","text":"<ul> <li>Fix half precision export not working properly with onnx iobindings</li> <li>Change full precision tolerance to avoid test failures</li> </ul>"},{"location":"reference/CHANGELOG.html#153","title":"[1.5.3]","text":""},{"location":"reference/CHANGELOG.html#fixed_13","title":"Fixed","text":"<ul> <li>Fix multiclass segmentation analysis report.</li> </ul>"},{"location":"reference/CHANGELOG.html#152","title":"[1.5.2]","text":""},{"location":"reference/CHANGELOG.html#fixed_14","title":"Fixed","text":"<ul> <li>Fix hydra plugin not working properly when the library is installed from external sources.</li> </ul>"},{"location":"reference/CHANGELOG.html#151","title":"[1.5.1]","text":""},{"location":"reference/CHANGELOG.html#fixed_15","title":"Fixed","text":"<ul> <li>Fix hydra plugin not working properly.</li> </ul>"},{"location":"reference/CHANGELOG.html#150","title":"[1.5.0]","text":""},{"location":"reference/CHANGELOG.html#changed_1","title":"Changed","text":"<ul> <li>Change default build system from <code>setup.py</code> to <code>poetry</code> for better dependency management.</li> </ul>"},{"location":"reference/CHANGELOG.html#141","title":"[1.4.1]","text":""},{"location":"reference/CHANGELOG.html#changed_2","title":"Changed","text":"<ul> <li>Change weights of resnet18 and wideresnet50 to old ones in anomaly model configs</li> </ul>"},{"location":"reference/CHANGELOG.html#updated_16","title":"Updated","text":"<ul> <li>Update anomalib to [v0.7.0+obx.1.2.9] (added default padim n_features for resnets' old weights)</li> </ul>"},{"location":"reference/CHANGELOG.html#140","title":"[1.4.0]","text":""},{"location":"reference/CHANGELOG.html#added_5","title":"Added","text":"<ul> <li>Add new backbones for classification</li> <li>Add parameter to save a model summary for sklearn based classification tasks</li> <li>Add results csv file for anomaly detection task</li> <li>Add a way to freeze backbone layers by index for the finetuning task</li> </ul>"},{"location":"reference/CHANGELOG.html#updated_17","title":"Updated","text":"<ul> <li>Update timm requirements to 0.9.12</li> </ul>"},{"location":"reference/CHANGELOG.html#fixed_16","title":"Fixed","text":"<ul> <li>Fix ModelSignatureWrapper not returing the correct instance when cpu, to and half functions are called</li> <li>Fix failure in model logging on mlflow whe half precision is used</li> </ul>"},{"location":"reference/CHANGELOG.html#138","title":"[1.3.8]","text":""},{"location":"reference/CHANGELOG.html#updated_18","title":"Updated","text":"<ul> <li>Update anomalib to [v0.7.0+obx.1.2.7] (Efficient_ad pre_padding and smaller memory footprint during training)</li> </ul>"},{"location":"reference/CHANGELOG.html#137","title":"[1.3.7]","text":""},{"location":"reference/CHANGELOG.html#fixed_17","title":"Fixed","text":"<ul> <li>Fix BatchSizeFinder calling wrong super functions</li> <li>Fix ModelManager get_latest_version calling an hardcoded model</li> </ul>"},{"location":"reference/CHANGELOG.html#136","title":"[1.3.6]","text":""},{"location":"reference/CHANGELOG.html#fixed_18","title":"Fixed","text":"<ul> <li>Changed matplotlib backend in anomaly visualizer to solve slowdown on some devices</li> </ul>"},{"location":"reference/CHANGELOG.html#updated_19","title":"Updated","text":"<ul> <li>Update anomalib to [v0.7.0+obx.1.2.6] (Efficient_ad now keeps maps always on gpu during forward)</li> </ul>"},{"location":"reference/CHANGELOG.html#135","title":"[1.3.5]","text":""},{"location":"reference/CHANGELOG.html#fixed_19","title":"Fixed","text":"<ul> <li>Anomaly Dataset samples are initially ordered to strngthen reproducibility.</li> </ul>"},{"location":"reference/CHANGELOG.html#134","title":"[1.3.4]","text":""},{"location":"reference/CHANGELOG.html#updated_20","title":"Updated","text":"<ul> <li>Update anomalib to [v0.7.0+obx.1.2.5] (logical anomaly is now compatible with trainer.deterministic=True)</li> </ul>"},{"location":"reference/CHANGELOG.html#133","title":"[1.3.3]","text":""},{"location":"reference/CHANGELOG.html#updated_21","title":"Updated","text":"<ul> <li>Relax pytest version requirement to 7.x</li> <li>Add pytest env variables and pytest-env requirement</li> </ul>"},{"location":"reference/CHANGELOG.html#132","title":"[1.3.2]","text":""},{"location":"reference/CHANGELOG.html#updated_22","title":"Updated","text":"<ul> <li>Update <code>mlflow</code> requirements for <code>mlflow-skinny</code> package to align with the same version of main <code>mlflow</code> package.</li> </ul>"},{"location":"reference/CHANGELOG.html#131","title":"[1.3.1]","text":""},{"location":"reference/CHANGELOG.html#updated_23","title":"Updated","text":"<ul> <li>Update pandas requirements to use a more recent version and avoid slow build time when python 3.10 is used.</li> </ul>"},{"location":"reference/CHANGELOG.html#130","title":"[1.3.0]","text":""},{"location":"reference/CHANGELOG.html#added_6","title":"Added","text":"<ul> <li>Add batch_size_finder callback for lightning based models (disabled by default).</li> <li>Add automatic_batch_size parameter to sklearn based training tasks (disabled by default).</li> <li>Add automatic_batch_size decorator to automatically fix the batch size of test functions for evaluation tasks if any out of memory error occurs.</li> <li>Add --mock-training flag for tests to skip running the actual training and just run the test.</li> </ul>"},{"location":"reference/CHANGELOG.html#fixed_20","title":"Fixed","text":"<ul> <li>Fix lightning based tasks not working properly when no checkpoint was provided.</li> <li>Fix list and dict config not handled properly as input_shapes parameter.</li> </ul>"},{"location":"reference/CHANGELOG.html#updated_24","title":"Updated","text":"<ul> <li>Greatly reduce the dimension of test datasets to improve testing speed.</li> </ul>"},{"location":"reference/CHANGELOG.html#updated_25","title":"Updated","text":"<ul> <li>Make <code>disable</code> a quadra reserved keyword for all callbacks, to disable a callback just set it to <code>disable: true</code> in the configuration file.</li> </ul>"},{"location":"reference/CHANGELOG.html#127","title":"[1.2.7]","text":""},{"location":"reference/CHANGELOG.html#fixed_21","title":"Fixed","text":"<ul> <li>Fix test classification task crash when only images with no labels are used.</li> </ul>"},{"location":"reference/CHANGELOG.html#126","title":"[1.2.6]","text":""},{"location":"reference/CHANGELOG.html#added_7","title":"Added","text":"<ul> <li>Add optional <code>training_threshold_type</code> for anomaly detection inference task.</li> </ul>"},{"location":"reference/CHANGELOG.html#changed_3","title":"Changed","text":"<ul> <li>Compute results using the training image threshold instead of zero when running anomaly inference with no labelled data.</li> </ul>"},{"location":"reference/CHANGELOG.html#125","title":"[1.2.5]","text":""},{"location":"reference/CHANGELOG.html#fixed_22","title":"Fixed","text":"<ul> <li>Fix generic classification experiment crashing due to missing class to index configuration.</li> </ul>"},{"location":"reference/CHANGELOG.html#124","title":"[1.2.4]","text":""},{"location":"reference/CHANGELOG.html#added_8","title":"Added","text":"<ul> <li>Return also probabilities in Classification's module predict step and add them to <code>self.res</code>.</li> </ul>"},{"location":"reference/CHANGELOG.html#123","title":"[1.2.3]","text":""},{"location":"reference/CHANGELOG.html#fixed_23","title":"Fixed","text":"<ul> <li>Fix patch datamodule error when only a single image is available for validation or test.</li> </ul>"},{"location":"reference/CHANGELOG.html#122","title":"[1.2.2]","text":""},{"location":"reference/CHANGELOG.html#added_9","title":"Added","text":"<ul> <li>Add tests for efficient ad model export.</li> </ul>"},{"location":"reference/CHANGELOG.html#updated_26","title":"Updated","text":"<ul> <li>Update <code>anomalib</code> library from version 0.7.0+obx.1.2.0 to 0.7.0+obx.1.2.1</li> <li>Update default imagenette dir for efficient ad</li> </ul>"},{"location":"reference/CHANGELOG.html#121","title":"[1.2.1]","text":""},{"location":"reference/CHANGELOG.html#added_10","title":"Added","text":"<ul> <li>Add automatic num_classes computation in Classification Task.</li> </ul>"},{"location":"reference/CHANGELOG.html#changed_4","title":"Changed","text":"<ul> <li>Align Classification <code>test_results</code> format to the SklearnClassification one (dataframe).</li> </ul>"},{"location":"reference/CHANGELOG.html#120","title":"[1.2.0]","text":""},{"location":"reference/CHANGELOG.html#added_11","title":"Added","text":"<ul> <li>Add plot_raw_outputs feature to class VisualizerCallback in anomaly detection, to save the raw images of the segmentation and heatmap output.</li> <li>Add support for onnx exportation of trained models.</li> <li>Add support for onnx model import in all evaluation tasks.</li> <li>Add <code>export</code> configuration group to regulate exportation parameters.</li> <li>Add <code>inference</code> configuration group to regulate inference parameters.</li> <li>Add EfficientAD configuration for anomaly detection.</li> <li>Add <code>acknowledgements</code> section to <code>README.md</code> file.</li> <li>Add hashing parameters to datamodule configurations.</li> </ul>"},{"location":"reference/CHANGELOG.html#updated_27","title":"Updated","text":"<ul> <li>Update anomalib library from version 0.4.0 to 0.7.0</li> <li>Update mkdocs library from version 1.4.3 to 1.5.2</li> <li>Update mkdocs-material library from version 9.1.18 to 9.2.8</li> <li>Update mkdocstrings library by fixing the version to 0.23.0</li> <li>Update mkdocs-material-extensions library by fixing the version to 1.1.1</li> <li>Update mkdocs-autorefs library by fixing the version to 0.5.0</li> <li>Update mkdocs-section-index library from version 0.3.5 to 0.3.6</li> <li>Update mkdocstrings-python library from version 1.2.0 to 1.6.2</li> <li>Update datamodule documentation for hashing.</li> </ul>"},{"location":"reference/CHANGELOG.html#changed_5","title":"Changed","text":"<ul> <li>Move <code>export_types</code> parameter from <code>task</code> configuration group to <code>export</code> configuration group under <code>types</code> parameter.</li> <li>Refactor export model function to be more generic and be availble from the base task class.</li> <li>Remove <code>save_backbone</code> parameter for scikit-learn based tasks.</li> </ul>"},{"location":"reference/CHANGELOG.html#fixed_24","title":"Fixed","text":"<ul> <li>Fix failures when trying to override <code>hydra</code> configuration groups due to wrong override order.</li> <li>Fix certain anomalib models not loaded on the correct device.</li> <li>Fix quadra crash when launching an experiment inside a git repository not fully initialized (e.g. without a single commit).</li> <li>Fix documentation build failing due to wrong <code>mkdocstring</code> version.</li> <li>Fix SSL docstrings </li> <li>Fix reference page URL to segmentation page in module management tutorial.</li> <li>Fix <code>Makefile</code> command.</li> </ul>"},{"location":"reference/CHANGELOG.html#114","title":"[1.1.4]","text":""},{"location":"reference/CHANGELOG.html#fixed_25","title":"Fixed","text":"<ul> <li>Fix input shape not extracted properly in ModelSignatureWrapper when wrapping around a torchscript model.</li> </ul>"},{"location":"reference/CHANGELOG.html#113","title":"[1.1.3]","text":""},{"location":"reference/CHANGELOG.html#fixed_26","title":"Fixed","text":"<ul> <li>Fix penultimate patch extracted containing replicated data, now the penultimate patch contain the right data. Fix also the patch size and step computation to avoid generating more than one extra patch.</li> </ul>"},{"location":"reference/CHANGELOG.html#112","title":"[1.1.2]","text":""},{"location":"reference/CHANGELOG.html#fixed_27","title":"Fixed","text":"<ul> <li>Fix best checkpoint not used for testing when available in <code>LightningTask</code> class.</li> </ul>"},{"location":"reference/CHANGELOG.html#111","title":"[1.1.1]","text":""},{"location":"reference/CHANGELOG.html#fixed_28","title":"Fixed","text":"<ul> <li>Fix deprecated link for tutorials in <code>README.md</code> file.</li> </ul>"},{"location":"reference/CHANGELOG.html#110","title":"[1.1.0]","text":""},{"location":"reference/CHANGELOG.html#added_12","title":"Added","text":"<ul> <li>Add ModelManager class to manage model deployments on model tracking platforms (e.g. MLFlow).</li> <li>Add automatic storage of file hashes in BaseDataModule class for better experiment reproducibility and tracking.</li> <li>Automatically load transforms parameters from model info file in base Evaluation task.</li> <li>Add support for vit explainability using Attention Gradient Rollout method.</li> <li>Add export of pytorch model for Classification and SklearnClassification tasks.</li> <li>Add automatical detection of model input shapes for better exportation capabilities. Add support for custom input shapes like models with multiple inputs.</li> <li>Add documentation landing page, improve colore themes and logo.</li> <li>Add github actions to automatically build and deploy documentation of main and dev PRs.</li> </ul>"},{"location":"reference/CHANGELOG.html#changed_6","title":"Changed","text":"<ul> <li>Refactor evaluation task to be more generic and improve inheritance capabilities.</li> <li>Refactor export_types parameter to export for better configurability of export parameters.</li> <li>Change input_size model info parameter from HXW to a list of actual model parameters for inference (e.g [(3, 224, 224)]).</li> </ul>"},{"location":"reference/CHANGELOG.html#fixed_29","title":"Fixed","text":"<ul> <li>Fix gradcam not working properly with non rectangular images.</li> <li>Fix logistic regression wrapper not working properly with 2 classes for torch classification.</li> <li>Fix wrong typings in NetworkBuilder's init method.</li> <li>Fix broken links in documentation.</li> <li>Fix minor documentations issues.</li> </ul>"},{"location":"reference/CHANGELOG.html#102","title":"[1.0.2]","text":""},{"location":"reference/CHANGELOG.html#fixed_30","title":"Fixed","text":"<ul> <li>Fix anomaly detection training not working when images with non lowercase allowed extension are used (e.g. .BMP).</li> </ul>"},{"location":"reference/CHANGELOG.html#added_13","title":"Added","text":"<ul> <li>Increase the number of available extensions for classification and anomaly detection training.</li> </ul>"},{"location":"reference/CHANGELOG.html#101","title":"[1.0.1]","text":""},{"location":"reference/CHANGELOG.html#fixed_31","title":"Fixed","text":"<ul> <li>Fix training dataset used instead of validation dataset in segmentation datamodules.</li> </ul>"},{"location":"reference/CHANGELOG.html#100","title":"[1.0.0]","text":""},{"location":"reference/CHANGELOG.html#added_14","title":"Added","text":"<ul> <li>All required files for the first release.</li> </ul>"},{"location":"reference/SUMMARY.html","title":"Package Summary","text":"<ul> <li>quadra<ul> <li>callbacks<ul> <li>anomalib</li> <li>lightning</li> <li>mlflow</li> <li>scheduler</li> </ul> </li> <li>datamodules<ul> <li>anomaly</li> <li>base</li> <li>classification</li> <li>generic<ul> <li>imagenette</li> <li>mnist</li> <li>mvtec</li> <li>oxford_pet</li> </ul> </li> <li>patch</li> <li>segmentation</li> <li>ssl</li> </ul> </li> <li>datasets<ul> <li>anomaly</li> <li>classification</li> <li>patch</li> <li>segmentation</li> <li>ssl</li> </ul> </li> <li>losses<ul> <li>classification<ul> <li>asl</li> <li>focal</li> <li>prototypical</li> </ul> </li> <li>ssl<ul> <li>barlowtwins</li> <li>byol</li> <li>dino</li> <li>hyperspherical</li> <li>idmm</li> <li>simclr</li> <li>simsiam</li> <li>vicreg</li> </ul> </li> </ul> </li> <li>main</li> <li>metrics<ul> <li>segmentation</li> </ul> </li> <li>models<ul> <li>base</li> <li>classification<ul> <li>backbones</li> <li>base</li> </ul> </li> <li>evaluation</li> </ul> </li> <li>modules<ul> <li>backbone</li> <li>base</li> <li>classification<ul> <li>base</li> </ul> </li> <li>ssl<ul> <li>barlowtwins</li> <li>byol</li> <li>common</li> <li>dino</li> <li>hyperspherical</li> <li>idmm</li> <li>simclr</li> <li>simsiam</li> <li>vicreg</li> </ul> </li> </ul> </li> <li>optimizers<ul> <li>lars</li> <li>sam</li> </ul> </li> <li>schedulers<ul> <li>base</li> <li>warmup</li> </ul> </li> <li>tasks<ul> <li>anomaly</li> <li>base</li> <li>classification</li> <li>patch</li> <li>segmentation</li> <li>ssl</li> </ul> </li> <li>trainers<ul> <li>classification</li> </ul> </li> <li>utils<ul> <li>anomaly</li> <li>classification</li> <li>deprecation</li> <li>evaluation</li> <li>export</li> <li>imaging</li> <li>logger</li> <li>mlflow</li> <li>model_manager</li> <li>models</li> <li>patch<ul> <li>dataset</li> <li>metrics</li> <li>model</li> <li>visualization</li> </ul> </li> <li>resolver</li> <li>segmentation</li> <li>tests<ul> <li>fixtures<ul> <li>dataset<ul> <li>anomaly</li> <li>classification</li> <li>imagenette</li> <li>segmentation</li> </ul> </li> <li>models<ul> <li>anomaly</li> <li>classification</li> <li>segmentation</li> </ul> </li> </ul> </li> <li>helpers</li> <li>models</li> </ul> </li> <li>utils</li> <li>validator</li> <li>visualization</li> <li>vit_explainability</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/quadra/index.html","title":"quadra","text":""},{"location":"reference/quadra/index.html#submodules","title":"Submodules","text":"<ul> <li>optimizers</li> <li>models</li> <li>modules</li> <li>tasks</li> <li>metrics</li> <li>datasets</li> <li>datamodules</li> <li>losses</li> <li>callbacks</li> <li>utils</li> <li>trainers</li> <li>schedulers</li> </ul>"},{"location":"reference/quadra/index.html#python-files","title":"Python Files","text":"<ul> <li>main.py </li> </ul>"},{"location":"reference/quadra/index.html#quadra.get_version","title":"<code>get_version()</code>","text":"<p>Returns the version of the package.</p> Source code in <code>quadra/__init__.py</code> <pre><code>def get_version():\n\"\"\"Returns the version of the package.\"\"\"\n    return __version__\n</code></pre>"},{"location":"reference/quadra/main.html","title":"main","text":""},{"location":"reference/quadra/main.html#quadra.main.main","title":"<code>main(config)</code>","text":"<p>Main entry function for any of the tasks.</p> Source code in <code>quadra/main.py</code> <pre><code>@hydra.main(config_path=\"configs/\", config_name=\"config.yaml\", version_base=\"1.3.0\")\ndef main(config: DictConfig):\n\"\"\"Main entry function for any of the tasks.\"\"\"\n    if config.validate:\n        start = time.time()\n        validate_config(config)\n        stop = time.time()\n        log.info(\"Config validation took %f seconds\", stop - start)\n\n    from quadra.utils import utils  # pylint: disable=import-outside-toplevel\n\n    utils.extras(config)\n\n    # Prints the resolved configuration to the console\n    if config.get(\"print_config\"):\n        utils.print_config(config, resolve=True)\n\n    # Set seed for random number generators in pytorch, numpy and python.random\n    seed_everything(config.core.seed, workers=True)\n    setup_opencv()\n\n    # Run specified task using the configuration composition\n    task: Task = hydra.utils.instantiate(config.task, config, _recursive_=False)\n    task.execute()\n</code></pre>"},{"location":"reference/quadra/callbacks/index.html","title":"callbacks","text":""},{"location":"reference/quadra/callbacks/index.html#python-files","title":"Python Files","text":"<ul> <li>mlflow.py</li> <li>anomalib.py</li> <li>lightning.py</li> <li>scheduler.py </li> </ul>"},{"location":"reference/quadra/callbacks/anomalib.html","title":"anomalib","text":""},{"location":"reference/quadra/callbacks/anomalib.html#quadra.callbacks.anomalib.Visualizer","title":"<code>Visualizer()</code>","text":"<p>Anomaly Visualization.</p> <p>The visualizer object is responsible for collating all the images passed to it into a single image. This can then either be logged by accessing the <code>figure</code> attribute or can be saved directly by calling <code>save()</code> method.</p> Example <p>visualizer = Visualizer() visualizer.add_image(image=image, title=\"Image\") visualizer.close()</p> Source code in <code>quadra/callbacks/anomalib.py</code> <pre><code>def __init__(self) -&gt; None:\n    self.images: list[dict] = []\n\n    self.figure: matplotlib.figure.Figure\n    self.axis: np.ndarray\n</code></pre>"},{"location":"reference/quadra/callbacks/anomalib.html#quadra.callbacks.anomalib.Visualizer.add_image","title":"<code>add_image(image, title, color_map=None)</code>","text":"<p>Add image to figure.</p> <p>Parameters:</p> <ul> <li> image             (<code>ndarray</code>)         \u2013          <p>Image which should be added to the figure.</p> </li> <li> title             (<code>str</code>)         \u2013          <p>Image title shown on the plot.</p> </li> <li> color_map             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>Name of matplotlib color map used to map scalar data to colours. Defaults to None.</p> </li> </ul> Source code in <code>quadra/callbacks/anomalib.py</code> <pre><code>def add_image(self, image: np.ndarray, title: str, color_map: str | None = None):\n\"\"\"Add image to figure.\n\n    Args:\n      image: Image which should be added to the figure.\n      title: Image title shown on the plot.\n      color_map: Name of matplotlib color map used to map scalar data to colours. Defaults to None.\n    \"\"\"\n    image_data = {\"image\": image, \"title\": title, \"color_map\": color_map}\n    self.images.append(image_data)\n</code></pre>"},{"location":"reference/quadra/callbacks/anomalib.html#quadra.callbacks.anomalib.Visualizer.close","title":"<code>close()</code>","text":"<p>Close figure.</p> Source code in <code>quadra/callbacks/anomalib.py</code> <pre><code>def close(self):\n\"\"\"Close figure.\"\"\"\n    plt.close(self.figure)\n</code></pre>"},{"location":"reference/quadra/callbacks/anomalib.html#quadra.callbacks.anomalib.Visualizer.generate","title":"<code>generate()</code>","text":"<p>Generate the image.</p> Source code in <code>quadra/callbacks/anomalib.py</code> <pre><code>def generate(self):\n\"\"\"Generate the image.\"\"\"\n    default_plt_backend = plt.get_backend()\n    plt.switch_backend(\"Agg\")\n    num_cols = len(self.images)\n    figure_size = (num_cols * 3, 3)\n    self.figure, self.axis = plt.subplots(1, num_cols, figsize=figure_size)\n    self.figure.subplots_adjust(right=0.9)\n\n    axes = self.axis if len(self.images) &gt; 1 else [self.axis]\n    for axis, image_dict in zip(axes, self.images):\n        axis.axes.xaxis.set_visible(False)\n        axis.axes.yaxis.set_visible(False)\n        axis.imshow(image_dict[\"image\"], image_dict[\"color_map\"], vmin=0, vmax=255)\n        axis.title.set_text(image_dict[\"title\"])\n    plt.switch_backend(default_plt_backend)\n</code></pre>"},{"location":"reference/quadra/callbacks/anomalib.html#quadra.callbacks.anomalib.Visualizer.save","title":"<code>save(filename)</code>","text":"<p>Save image.</p> <p>Parameters:</p> <ul> <li> filename             (<code>Path</code>)         \u2013          <p>Filename to save image</p> </li> </ul> Source code in <code>quadra/callbacks/anomalib.py</code> <pre><code>def save(self, filename: Path):\n\"\"\"Save image.\n\n    Args:\n      filename: Filename to save image\n    \"\"\"\n    filename.parent.mkdir(parents=True, exist_ok=True)\n    self.figure.savefig(filename, dpi=100)\n</code></pre>"},{"location":"reference/quadra/callbacks/anomalib.html#quadra.callbacks.anomalib.Visualizer.show","title":"<code>show()</code>","text":"<p>Show image on a matplotlib figure.</p> Source code in <code>quadra/callbacks/anomalib.py</code> <pre><code>def show(self):\n\"\"\"Show image on a matplotlib figure.\"\"\"\n    self.figure.show()\n</code></pre>"},{"location":"reference/quadra/callbacks/anomalib.html#quadra.callbacks.anomalib.VisualizerCallback","title":"<code>VisualizerCallback(task='segmentation', output_path='anomaly_output', inputs_are_normalized=True, threshold_type='pixel', disable=False, plot_only_wrong=False, plot_raw_outputs=False)</code>","text":"<p>             Bases: <code>Callback</code></p> <p>Callback that visualizes the inference results of a model.</p> <p>The callback generates a figure showing the original image, the ground truth segmentation mask, the predicted error heat map, and the predicted segmentation mask. To save the images to the filesystem, add the 'local' keyword to the <code>project.log_images_to</code> parameter in the config.yaml file.</p> <p>Parameters:</p> <ul> <li> task             (<code>str</code>, default:                 <code>'segmentation'</code> )         \u2013          <p>either 'segmentation' or 'classification'</p> </li> <li> output_path             (<code>str</code>, default:                 <code>'anomaly_output'</code> )         \u2013          <p>location where the images will be saved.</p> </li> <li> inputs_are_normalized             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>whether the input images are normalized (like when using MinMax or Treshold callback).</p> </li> <li> threshold_type             (<code>str</code>, default:                 <code>'pixel'</code> )         \u2013          <p>Either 'pixel' or 'image'. If 'pixel', the threshold is computed on the pixel-level.</p> </li> <li> disable             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>whether to disable the callback.</p> </li> <li> plot_only_wrong             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>whether to plot only the images that are not correctly predicted.</p> </li> <li> plot_raw_outputs             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Saves the raw images of the segmentation and heatmap output.</p> </li> </ul> Source code in <code>quadra/callbacks/anomalib.py</code> <pre><code>def __init__(\n    self,\n    task: str = \"segmentation\",\n    output_path: str = \"anomaly_output\",\n    inputs_are_normalized: bool = True,\n    threshold_type: str = \"pixel\",\n    disable: bool = False,\n    plot_only_wrong: bool = False,\n    plot_raw_outputs: bool = False,\n) -&gt; None:\n    self.inputs_are_normalized = inputs_are_normalized\n    self.output_path = output_path\n    self.threshold_type = threshold_type\n    self.disable = disable\n    self.task = task\n    self.plot_only_wrong = plot_only_wrong\n    self.plot_raw_outputs = plot_raw_outputs\n</code></pre>"},{"location":"reference/quadra/callbacks/anomalib.html#quadra.callbacks.anomalib.VisualizerCallback.on_test_batch_end","title":"<code>on_test_batch_end(trainer, pl_module, outputs, batch, batch_idx, dataloader_idx=0)</code>","text":"<p>Log images at the end of every batch.</p> <p>Parameters:</p> <ul> <li> trainer             (<code>Trainer</code>)         \u2013          <p>Pytorch lightning trainer object (unused).</p> </li> <li> pl_module             (<code>AnomalyModule</code>)         \u2013          <p>Lightning modules derived from BaseAnomalyLightning object as currently only they support logging images.</p> </li> <li> outputs             (<code>STEP_OUTPUT | None</code>)         \u2013          <p>Outputs of the current test step.</p> </li> <li> batch             (<code>Any</code>)         \u2013          <p>Input batch of the current test step (unused).</p> </li> <li> batch_idx             (<code>int</code>)         \u2013          <p>Index of the current test batch (unused).</p> </li> <li> dataloader_idx             (<code>int</code>, default:                 <code>0</code> )         \u2013          <p>Index of the dataloader that yielded the current batch (unused).</p> </li> </ul> Source code in <code>quadra/callbacks/anomalib.py</code> <pre><code>def on_test_batch_end(\n    self,\n    trainer: pl.Trainer,\n    pl_module: AnomalyModule,\n    outputs: STEP_OUTPUT | None,\n    batch: Any,\n    batch_idx: int,\n    dataloader_idx: int = 0,\n) -&gt; None:\n\"\"\"Log images at the end of every batch.\n\n    Args:\n        trainer: Pytorch lightning trainer object (unused).\n        pl_module: Lightning modules derived from BaseAnomalyLightning object as\n            currently only they support logging images.\n        outputs: Outputs of the current test step.\n        batch: Input batch of the current test step (unused).\n        batch_idx: Index of the current test batch (unused).\n        dataloader_idx: Index of the dataloader that yielded the current batch (unused).\n    \"\"\"\n    if self.disable:\n        return\n\n    assert outputs is not None and isinstance(outputs, dict)\n\n    if any(x not in outputs for x in [\"image_path\", \"image\", \"mask\", \"anomaly_maps\", \"label\"]):\n        # I'm probably in the classification scenario so I can't use the visualizer\n        return\n\n    if self.threshold_type == \"pixel\":\n        if hasattr(pl_module.pixel_metrics.F1Score, \"threshold\"):\n            threshold = pl_module.pixel_metrics.F1Score.threshold\n        else:\n            raise AttributeError(\"Metric has no threshold attribute\")\n    elif hasattr(pl_module.image_metrics.F1Score, \"threshold\"):\n        threshold = pl_module.image_metrics.F1Score.threshold\n    else:\n        raise AttributeError(\"Metric has no threshold attribute\")\n\n    for (\n        filename,\n        image,\n        true_mask,\n        anomaly_map,\n        gt_label,\n        pred_label,\n        anomaly_score,\n    ) in tqdm(\n        zip(\n            outputs[\"image_path\"],\n            outputs[\"image\"],\n            outputs[\"mask\"],\n            outputs[\"anomaly_maps\"],\n            outputs[\"label\"],\n            outputs[\"pred_labels\"],\n            outputs[\"pred_scores\"],\n        )\n    ):\n        denormalized_image = Denormalize()(image.cpu())\n        current_true_mask = true_mask.cpu().numpy()\n        current_anomaly_map = anomaly_map.cpu().numpy()\n        # Normalize the map and rescale it to 0-1 range\n        # In this case we are saying that the anomaly map is in the range [normalized_th - 50, normalized_th + 50]\n        # This allow to have a stronger color for the anomalies and a lighter one for really normal regions\n        # It's also independent from the max or min anomaly score!\n        normalized_map: MapOrValue = (current_anomaly_map - (threshold - 50)) / 100\n        normalized_map = np.clip(normalized_map, 0, 1)\n\n        output_label_folder = \"ok\" if pred_label == gt_label else \"wrong\"\n\n        if self.plot_only_wrong and output_label_folder == \"ok\":\n            continue\n\n        heatmap = superimpose_anomaly_map(\n            normalized_map, denormalized_image, normalize=not self.inputs_are_normalized\n        )\n\n        if isinstance(threshold, float):\n            pred_mask = compute_mask(current_anomaly_map, threshold)\n        else:\n            raise TypeError(\"Threshold should be float\")\n        vis_img = mark_boundaries(denormalized_image, pred_mask, color=(1, 0, 0), mode=\"thick\")\n        visualizer = Visualizer()\n\n        if self.task == \"segmentation\":\n            visualizer.add_image(image=denormalized_image, title=\"Image\")\n            if \"mask\" in outputs:\n                current_true_mask = current_true_mask * 255\n                visualizer.add_image(image=current_true_mask, color_map=\"gray\", title=\"Ground Truth\")\n            visualizer.add_image(image=heatmap, title=\"Predicted Heat Map\")\n            visualizer.add_image(image=pred_mask, color_map=\"gray\", title=\"Predicted Mask\")\n            visualizer.add_image(image=vis_img, title=\"Segmentation Result\")\n        elif self.task == \"classification\":\n            gt_im = add_anomalous_label(denormalized_image) if gt_label else add_normal_label(denormalized_image)\n            visualizer.add_image(gt_im, title=\"Image/True label\")\n            if anomaly_score &gt;= threshold:\n                image_classified = add_anomalous_label(heatmap, anomaly_score)\n            else:\n                image_classified = add_normal_label(heatmap, 1 - anomaly_score)\n            visualizer.add_image(image=image_classified, title=\"Prediction\")\n\n        visualizer.generate()\n        visualizer.figure.suptitle(\n            f\"F1 threshold: {threshold}, Mask_max: {current_anomaly_map.max():.3f}, \"\n            f\"Anomaly_score: {anomaly_score:.3f}\"\n        )\n        path_filename = Path(filename)\n        self._add_images(visualizer, path_filename, output_label_folder)\n        visualizer.close()\n\n        if self.plot_raw_outputs:\n            for raw_output, raw_name in zip([heatmap, vis_img], [\"heatmap\", \"segmentation\"]):\n                current_raw_output = raw_output\n                if raw_name == \"segmentation\":\n                    current_raw_output = (raw_output * 255).astype(np.uint8)\n                current_raw_output = cv2.cvtColor(current_raw_output, cv2.COLOR_RGB2BGR)\n                raw_filename = (\n                    Path(self.output_path)\n                    / \"images\"\n                    / output_label_folder\n                    / path_filename.parent.name\n                    / \"raw_outputs\"\n                    / Path(path_filename.stem + f\"_{raw_name}.png\")\n                )\n                raw_filename.parent.mkdir(parents=True, exist_ok=True)\n                cv2.imwrite(str(raw_filename), current_raw_output)\n</code></pre>"},{"location":"reference/quadra/callbacks/anomalib.html#quadra.callbacks.anomalib.VisualizerCallback.on_test_end","title":"<code>on_test_end(_trainer, pl_module)</code>","text":"<p>Sync logs.</p> <p>Currently only <code>AnomalibWandbLogger</code> is called from this method. This is because logging as a single batch ensures that all images appear as part of the same step.</p> <p>Parameters:</p> <ul> <li> _trainer             (<code>Trainer</code>)         \u2013          <p>Pytorch Lightning trainer (unused)</p> </li> <li> pl_module             (<code>LightningModule</code>)         \u2013          <p>Anomaly module</p> </li> </ul> Source code in <code>quadra/callbacks/anomalib.py</code> <pre><code>def on_test_end(self, _trainer: pl.Trainer, pl_module: pl.LightningModule) -&gt; None:\n\"\"\"Sync logs.\n\n    Currently only ``AnomalibWandbLogger`` is called from this method. This is because logging as a single batch\n    ensures that all images appear as part of the same step.\n\n    Args:\n        _trainer: Pytorch Lightning trainer (unused)\n        pl_module: Anomaly module\n    \"\"\"\n    if self.disable:\n        return\n\n    if pl_module.logger is not None and isinstance(pl_module.logger, AnomalibWandbLogger):\n        pl_module.logger.save()\n</code></pre>"},{"location":"reference/quadra/callbacks/lightning.html","title":"lightning","text":""},{"location":"reference/quadra/callbacks/lightning.html#quadra.callbacks.lightning.BatchSizeFinder","title":"<code>BatchSizeFinder(find_train_batch_size=True, find_validation_batch_size=False, find_test_batch_size=False, find_predict_batch_size=False, mode='power', steps_per_trial=3, init_val=2, max_trials=25, batch_arg_name='batch_size')</code>","text":"<p>             Bases: <code>BatchSizeFinder</code></p> <p>Batch size finder setting the proper model training status as the current one from lightning seems bugged. It also allows to skip some batch size finding steps.</p> <p>Parameters:</p> <ul> <li> find_train_batch_size             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Whether to find the training batch size.</p> </li> <li> find_validation_batch_size             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to find the validation batch size.</p> </li> <li> find_test_batch_size             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to find the test batch size.</p> </li> <li> find_predict_batch_size             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to find the predict batch size.</p> </li> <li> mode             (<code>str</code>, default:                 <code>'power'</code> )         \u2013          <p>The mode to use for batch size finding. See <code>pytorch_lightning.callbacks.BatchSizeFinder</code> for more details.</p> </li> <li> steps_per_trial             (<code>int</code>, default:                 <code>3</code> )         \u2013          <p>The number of steps per trial. See <code>pytorch_lightning.callbacks.BatchSizeFinder</code> for more details.</p> </li> <li> init_val             (<code>int</code>, default:                 <code>2</code> )         \u2013          <p>The initial value for batch size. See <code>pytorch_lightning.callbacks.BatchSizeFinder</code> for more details.</p> </li> <li> max_trials             (<code>int</code>, default:                 <code>25</code> )         \u2013          <p>The maximum number of trials. See <code>pytorch_lightning.callbacks.BatchSizeFinder</code> for more details.</p> </li> <li> batch_arg_name             (<code>str</code>, default:                 <code>'batch_size'</code> )         \u2013          <p>The name of the batch size argument. See <code>pytorch_lightning.callbacks.BatchSizeFinder</code> for more details.</p> </li> </ul> Source code in <code>quadra/callbacks/lightning.py</code> <pre><code>def __init__(\n    self,\n    find_train_batch_size: bool = True,\n    find_validation_batch_size: bool = False,\n    find_test_batch_size: bool = False,\n    find_predict_batch_size: bool = False,\n    mode: str = \"power\",\n    steps_per_trial: int = 3,\n    init_val: int = 2,\n    max_trials: int = 25,\n    batch_arg_name: str = \"batch_size\",\n) -&gt; None:\n    super().__init__(mode, steps_per_trial, init_val, max_trials, batch_arg_name)\n    self.find_train_batch_size = find_train_batch_size\n    self.find_validation_batch_size = find_validation_batch_size\n    self.find_test_batch_size = find_test_batch_size\n    self.find_predict_batch_size = find_predict_batch_size\n</code></pre>"},{"location":"reference/quadra/callbacks/lightning.html#quadra.callbacks.lightning.BatchSizeFinder.scale_batch_size","title":"<code>scale_batch_size(trainer, pl_module)</code>","text":"<p>Scale the batch size.</p> Source code in <code>quadra/callbacks/lightning.py</code> <pre><code>def scale_batch_size(self, trainer: pl.Trainer, pl_module: pl.LightningModule) -&gt; None:\n\"\"\"Scale the batch size.\"\"\"\n    new_size = _scale_batch_size(\n        trainer,\n        self._mode,\n        self._steps_per_trial,\n        self._init_val,\n        self._max_trials,\n        self._batch_arg_name,\n    )\n\n    self.optimal_batch_size = new_size\n    if self._early_exit:\n        raise _TunerExitException()\n</code></pre>"},{"location":"reference/quadra/callbacks/lightning.html#quadra.callbacks.lightning.LightningTrainerBaseSetup","title":"<code>LightningTrainerBaseSetup(log_every_n_steps=1)</code>","text":"<p>             Bases: <code>Callback</code></p> <p>Custom callback used to setup a lightning trainer with default options.</p> <p>Parameters:</p> <ul> <li> log_every_n_steps             (<code>int</code>, default:                 <code>1</code> )         \u2013          <p>Default value for trainer.log_every_n_steps if the dataloader is too small.</p> </li> </ul> Source code in <code>quadra/callbacks/lightning.py</code> <pre><code>def __init__(self, log_every_n_steps: int = 1) -&gt; None:\n    self.log_every_n_steps = log_every_n_steps\n</code></pre>"},{"location":"reference/quadra/callbacks/lightning.html#quadra.callbacks.lightning.LightningTrainerBaseSetup.on_fit_start","title":"<code>on_fit_start(trainer, pl_module)</code>","text":"<p>Called on every stage.</p> Source code in <code>quadra/callbacks/lightning.py</code> <pre><code>@rank_zero_only\ndef on_fit_start(self, trainer: pl.Trainer, pl_module: pl.LightningModule) -&gt; None:\n\"\"\"Called on every stage.\"\"\"\n    if not hasattr(trainer, \"datamodule\") or not hasattr(trainer, \"log_every_n_steps\"):\n        raise ValueError(\"Trainer must have a datamodule and log_every_n_steps attribute.\")\n\n    len_train_dataloader = len(trainer.datamodule.train_dataloader())\n    if len_train_dataloader &lt;= trainer.log_every_n_steps:\n        if len_train_dataloader &gt; self.log_every_n_steps:\n            trainer.log_every_n_steps = self.log_every_n_steps\n            log.info(\"`trainer.log_every_n_steps` is too high, setting it to %d\", self.log_every_n_steps)\n        else:\n            trainer.log_every_n_steps = 1\n            log.warning(\n                \"The default log_every_n_steps %d is too high given the datamodule lenght %d, fallback to 1\",\n                self.log_every_n_steps,\n                len_train_dataloader,\n            )\n</code></pre>"},{"location":"reference/quadra/callbacks/lightning.html#quadra.callbacks.lightning.__scale_batch_dump_params","title":"<code>__scale_batch_dump_params(trainer)</code>","text":"<p>Dump the parameters that need to be reset after the batch size finder..</p> Source code in <code>quadra/callbacks/lightning.py</code> <pre><code>def __scale_batch_dump_params(trainer: pl.Trainer) -&gt; dict[str, Any]:\n\"\"\"Dump the parameters that need to be reset after the batch size finder..\"\"\"\n    dumped_params = {\n        \"loggers\": trainer.loggers,\n        \"callbacks\": trainer.callbacks,  # type: ignore[attr-defined]\n    }\n    loop = trainer._active_loop\n    assert loop is not None\n    if isinstance(loop, pl.loops._FitLoop):\n        dumped_params[\"max_steps\"] = trainer.max_steps\n        dumped_params[\"limit_train_batches\"] = trainer.limit_train_batches\n        dumped_params[\"limit_val_batches\"] = trainer.limit_val_batches\n    elif isinstance(loop, pl.loops._EvaluationLoop):\n        stage = trainer.state.stage\n        assert stage is not None\n        dumped_params[\"limit_eval_batches\"] = getattr(trainer, f\"limit_{stage.dataloader_prefix}_batches\")\n        dumped_params[\"loop_verbose\"] = loop.verbose\n\n    dumped_params[\"loop_state_dict\"] = deepcopy(loop.state_dict())\n    return dumped_params\n</code></pre>"},{"location":"reference/quadra/callbacks/lightning.html#quadra.callbacks.lightning.__scale_batch_reset_params","title":"<code>__scale_batch_reset_params(trainer, steps_per_trial)</code>","text":"<p>Reset the parameters that need to be reset after the batch size finder.</p> Source code in <code>quadra/callbacks/lightning.py</code> <pre><code>def __scale_batch_reset_params(trainer: pl.Trainer, steps_per_trial: int) -&gt; None:\n\"\"\"Reset the parameters that need to be reset after the batch size finder.\"\"\"\n    from pytorch_lightning.loggers.logger import DummyLogger  # pylint: disable=import-outside-toplevel\n\n    trainer.logger = DummyLogger() if trainer.logger is not None else None\n    trainer.callbacks = []  # type: ignore[attr-defined]\n\n    loop = trainer._active_loop\n    assert loop is not None\n    if isinstance(loop, pl.loops._FitLoop):\n        trainer.limit_train_batches = 1.0\n        trainer.limit_val_batches = steps_per_trial\n        trainer.fit_loop.epoch_loop.max_steps = steps_per_trial\n    elif isinstance(loop, pl.loops._EvaluationLoop):\n        stage = trainer.state.stage\n        assert stage is not None\n        setattr(trainer, f\"limit_{stage.dataloader_prefix}_batches\", steps_per_trial)\n        loop.verbose = False\n</code></pre>"},{"location":"reference/quadra/callbacks/lightning.html#quadra.callbacks.lightning.__scale_batch_restore_params","title":"<code>__scale_batch_restore_params(trainer, params)</code>","text":"<p>Restore the parameters that need to be reset after the batch size finder.</p> Source code in <code>quadra/callbacks/lightning.py</code> <pre><code>def __scale_batch_restore_params(trainer: pl.Trainer, params: dict[str, Any]) -&gt; None:\n\"\"\"Restore the parameters that need to be reset after the batch size finder.\"\"\"\n    # TODO: There are more states that needs to be reset (#4512 and #4870)\n    trainer.loggers = params[\"loggers\"]\n    trainer.callbacks = params[\"callbacks\"]  # type: ignore[attr-defined]\n\n    loop = trainer._active_loop\n    assert loop is not None\n    if isinstance(loop, pl.loops._FitLoop):\n        loop.epoch_loop.max_steps = params[\"max_steps\"]\n        trainer.limit_train_batches = params[\"limit_train_batches\"]\n        trainer.limit_val_batches = params[\"limit_val_batches\"]\n    elif isinstance(loop, pl.loops._EvaluationLoop):\n        stage = trainer.state.stage\n        assert stage is not None\n        setattr(trainer, f\"limit_{stage.dataloader_prefix}_batches\", params[\"limit_eval_batches\"])\n\n    loop.load_state_dict(deepcopy(params[\"loop_state_dict\"]))\n    loop.restarting = False\n    if isinstance(loop, pl.loops._EvaluationLoop) and \"loop_verbose\" in params:\n        loop.verbose = params[\"loop_verbose\"]\n\n    # make sure the loop's state is reset\n    _reset_dataloaders(trainer)\n    loop.reset()\n</code></pre>"},{"location":"reference/quadra/callbacks/mlflow.html","title":"mlflow","text":""},{"location":"reference/quadra/callbacks/mlflow.html#quadra.callbacks.mlflow.LogGradients","title":"<code>LogGradients(norm=2, tag=None, sep='/', round_to=3, log_all_grads=False)</code>","text":"<p>             Bases: <code>Callback</code></p> <p>Callback used to logs of the model at the end of the of each training step.</p> <p>Parameters:</p> <ul> <li> norm             (<code>int</code>, default:                 <code>2</code> )         \u2013          <p>Norm to use for the gradient. Default is L2 norm.</p> </li> <li> tag             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>Tag to add to the gradients. If None, no tag will be added.</p> </li> <li> sep             (<code>str</code>, default:                 <code>'/'</code> )         \u2013          <p>Separator to use in the log.</p> </li> <li> round_to             (<code>int</code>, default:                 <code>3</code> )         \u2013          <p>Number of decimals to round the gradients to.</p> </li> <li> log_all_grads             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>If True, log all gradients, not just the total norm.</p> </li> </ul> Source code in <code>quadra/callbacks/mlflow.py</code> <pre><code>def __init__(\n    self,\n    norm: int = 2,\n    tag: str | None = None,\n    sep: str = \"/\",\n    round_to: int = 3,\n    log_all_grads: bool = False,\n):\n    self.norm = norm\n    self.tag = tag\n    self.sep = sep\n    self.round_to = round_to\n    self.log_all_grads = log_all_grads\n</code></pre>"},{"location":"reference/quadra/callbacks/mlflow.html#quadra.callbacks.mlflow.LogGradients.on_train_batch_end","title":"<code>on_train_batch_end(trainer, pl_module, outputs, batch, batch_idx, unused=0)</code>","text":"<p>Method called at the end of the train batch Args:     trainer: pl.trainer     pl_module: lightning module     outputs: outputs     batch: batch     batch_idx: index     unused: dl index.</p> <p>Returns:</p> <ul> <li> <code>None</code>         \u2013          <p>None</p> </li> </ul> Source code in <code>quadra/callbacks/mlflow.py</code> <pre><code>@rank_zero_only\ndef on_train_batch_end(\n    self,\n    trainer: Trainer,\n    pl_module: LightningModule,\n    outputs: STEP_OUTPUT,\n    batch: Any,\n    batch_idx: int,\n    unused: int | None = 0,\n) -&gt; None:\n\"\"\"Method called at the end of the train batch\n    Args:\n        trainer: pl.trainer\n        pl_module: lightning module\n        outputs: outputs\n        batch: batch\n        batch_idx: index\n        unused: dl index.\n\n\n    Returns:\n        None\n    \"\"\"\n    # pylint: disable=unused-argument\n    logger = get_mlflow_logger(trainer=trainer)\n\n    if logger is None:\n        return\n\n    named_params = pl_module.named_parameters()\n    grads = self._grad_norm(named_params)\n    logger.log_metrics(grads)\n</code></pre>"},{"location":"reference/quadra/callbacks/mlflow.html#quadra.callbacks.mlflow.LogLearningRate","title":"<code>LogLearningRate(logging_interval=None, log_momentum=False)</code>","text":"<p>             Bases: <code>LearningRateMonitor</code></p> <p>Learning rate logger at the end of the training step/epoch.</p> <p>Parameters:</p> <ul> <li> logging_interval             (<code>Literal['step', 'epoch'] | None</code>, default:                 <code>None</code> )         \u2013          <p>Logging interval.</p> </li> <li> log_momentum             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>If True, log momentum as well.</p> </li> </ul> Source code in <code>quadra/callbacks/mlflow.py</code> <pre><code>def __init__(self, logging_interval: Literal[\"step\", \"epoch\"] | None = None, log_momentum: bool = False):\n    super().__init__(logging_interval=logging_interval, log_momentum=log_momentum)\n</code></pre>"},{"location":"reference/quadra/callbacks/mlflow.html#quadra.callbacks.mlflow.LogLearningRate.on_train_batch_start","title":"<code>on_train_batch_start(trainer, *args, **kwargs)</code>","text":"<p>Log learning rate at the beginning of the training step if logging interval is set to step.</p> Source code in <code>quadra/callbacks/mlflow.py</code> <pre><code>def on_train_batch_start(self, trainer, *args, **kwargs):\n\"\"\"Log learning rate at the beginning of the training step if logging interval is set to step.\"\"\"\n    if not trainer.logger_connector.should_update_logs:\n        return\n    if self.logging_interval != \"epoch\":\n        logger = get_mlflow_logger(trainer=trainer)\n\n        if logger is None:\n            return\n\n        interval = \"step\" if self.logging_interval is None else \"any\"\n        latest_stat = self._extract_stats(trainer, interval)\n\n        if latest_stat:\n            logger.log_metrics(latest_stat, step=trainer.global_step)\n</code></pre>"},{"location":"reference/quadra/callbacks/mlflow.html#quadra.callbacks.mlflow.LogLearningRate.on_train_epoch_start","title":"<code>on_train_epoch_start(trainer, *args, **kwargs)</code>","text":"<p>Log learning rate at the beginning of the epoch if logging interval is set to epoch.</p> Source code in <code>quadra/callbacks/mlflow.py</code> <pre><code>def on_train_epoch_start(self, trainer, *args, **kwargs):\n\"\"\"Log learning rate at the beginning of the epoch if logging interval is set to epoch.\"\"\"\n    if self.logging_interval != \"step\":\n        interval = \"epoch\" if self.logging_interval is None else \"any\"\n        latest_stat = self._extract_stats(trainer, interval)\n        logger = get_mlflow_logger(trainer=trainer)\n\n        if logger is None:\n            return\n\n        if latest_stat:\n            logger.log_metrics(latest_stat, step=trainer.global_step)\n</code></pre>"},{"location":"reference/quadra/callbacks/mlflow.html#quadra.callbacks.mlflow.UploadCheckpointsAsArtifact","title":"<code>UploadCheckpointsAsArtifact(ckpt_dir='checkpoints/', ckpt_ext='ckpt', upload_best_only=False, delete_after_upload=True, upload=True)</code>","text":"<p>             Bases: <code>Callback</code></p> <p>Callback used to upload checkpoints as artifacts.</p> <p>Parameters:</p> <ul> <li> ckpt_dir             (<code>str</code>, default:                 <code>'checkpoints/'</code> )         \u2013          <p>Folder where all the checkpoints are stored in artifact folder.</p> </li> <li> ckpt_ext             (<code>str</code>, default:                 <code>'ckpt'</code> )         \u2013          <p>Extension of checkpoint files (default: ckpt).</p> </li> <li> upload_best_only             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Only upload best checkpoint (default: False)</p> </li> <li> delete_after_upload             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Delete the checkpoint from local storage after uploading (default: True)</p> </li> <li> upload             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>If True, upload the checkpoints. If False, only save them on local machine.</p> </li> </ul> Source code in <code>quadra/callbacks/mlflow.py</code> <pre><code>def __init__(\n    self,\n    ckpt_dir: str = \"checkpoints/\",\n    ckpt_ext: str = \"ckpt\",\n    upload_best_only: bool = False,\n    delete_after_upload: bool = True,\n    upload: bool = True,\n):\n    self.ckpt_dir = ckpt_dir\n    self.upload_best_only = upload_best_only\n    self.ckpt_ext = ckpt_ext\n    self.delete_after_upload = delete_after_upload\n    self.upload = upload\n</code></pre>"},{"location":"reference/quadra/callbacks/mlflow.html#quadra.callbacks.mlflow.UploadCheckpointsAsArtifact.on_test_end","title":"<code>on_test_end(trainer, pl_module)</code>","text":"<p>Triggered at the end of test. Uploads all model checkpoints to mlflow as an artifact.</p> <p>Parameters:</p> <ul> <li> trainer             (<code>Trainer</code>)         \u2013          <p>Pytorch Lightning trainer.</p> </li> <li> pl_module             (<code>LightningModule</code>)         \u2013          <p>Pytorch Lightning module.</p> </li> </ul> Source code in <code>quadra/callbacks/mlflow.py</code> <pre><code>@rank_zero_only\ndef on_test_end(self, trainer: Trainer, pl_module: LightningModule):\n\"\"\"Triggered at the end of test. Uploads all model checkpoints to mlflow as an artifact.\n\n    Args:\n        trainer: Pytorch Lightning trainer.\n        pl_module: Pytorch Lightning module.\n    \"\"\"\n    logger = get_mlflow_logger(trainer=trainer)\n\n    if logger is None:\n        return\n\n    experiment = logger.experiment\n\n    if (\n        trainer.checkpoint_callback\n        and self.upload_best_only\n        and hasattr(trainer.checkpoint_callback, \"best_model_path\")\n    ):\n        if self.upload:\n            experiment.log_artifact(\n                run_id=logger.run_id,\n                local_path=trainer.checkpoint_callback.best_model_path,\n                artifact_path=\"checkpoints\",\n            )\n    else:\n        for path in glob.glob(os.path.join(self.ckpt_dir, f\"**/*.{self.ckpt_ext}\"), recursive=True):\n            if self.upload:\n                experiment.log_artifact(\n                    run_id=logger.run_id,\n                    local_path=path,\n                    artifact_path=\"checkpoints\",\n                )\n    if self.delete_after_upload:\n        for path in glob.glob(os.path.join(self.ckpt_dir, f\"**/*.{self.ckpt_ext}\"), recursive=True):\n            os.remove(path)\n</code></pre>"},{"location":"reference/quadra/callbacks/mlflow.html#quadra.callbacks.mlflow.UploadCodeAsArtifact","title":"<code>UploadCodeAsArtifact(source_dir)</code>","text":"<p>             Bases: <code>Callback</code></p> <p>Callback used to upload Code as artifact.</p> <p>Uploads all *.py files to mlflow as an artifact, at the beginning of the run but     after initializing the trainer. It creates project-source folder under mlflow     artifacts and other necessary subfolders.</p> <p>Parameters:</p> <ul> <li> source_dir             (<code>str</code>)         \u2013          <p>Folder where all the source files are stored.</p> </li> </ul> Source code in <code>quadra/callbacks/mlflow.py</code> <pre><code>def __init__(self, source_dir: str):\n    self.source_dir = source_dir\n</code></pre>"},{"location":"reference/quadra/callbacks/mlflow.html#quadra.callbacks.mlflow.UploadCodeAsArtifact.on_test_end","title":"<code>on_test_end(trainer, pl_module)</code>","text":"<p>Triggered at the end of test. Uploads all *.py files to mlflow as an artifact.</p> <p>Parameters:</p> <ul> <li> trainer             (<code>Trainer</code>)         \u2013          <p>Pytorch Lightning trainer.</p> </li> <li> pl_module             (<code>LightningModule</code>)         \u2013          <p>Pytorch Lightning module.</p> </li> </ul> Source code in <code>quadra/callbacks/mlflow.py</code> <pre><code>@rank_zero_only\ndef on_test_end(self, trainer: Trainer, pl_module: LightningModule):\n\"\"\"Triggered at the end of test. Uploads all *.py files to mlflow as an artifact.\n\n    Args:\n        trainer: Pytorch Lightning trainer.\n        pl_module: Pytorch Lightning module.\n    \"\"\"\n    logger = get_mlflow_logger(trainer=trainer)\n\n    if logger is None:\n        return\n\n    experiment = logger.experiment\n\n    for path in glob.glob(os.path.join(self.source_dir, \"**/*.py\"), recursive=True):\n        stripped_path = path.replace(self.source_dir, \"\")\n        if len(stripped_path.split(\"/\")) &gt; 1:\n            file_path_tree = \"/\" + \"/\".join(stripped_path.split(\"/\")[:-1])\n        else:\n            file_path_tree = \"\"\n        experiment.log_artifact(\n            run_id=logger.run_id,\n            local_path=path,\n            artifact_path=f\"project-source{file_path_tree}\",\n        )\n</code></pre>"},{"location":"reference/quadra/callbacks/mlflow.html#quadra.callbacks.mlflow.check_file_server_dependencies","title":"<code>check_file_server_dependencies()</code>","text":"<p>Check file dependencies as boto3.</p> <p>Returns:</p> <ul> <li> <code>None</code>         \u2013          <p>None</p> </li> </ul> Source code in <code>quadra/callbacks/mlflow.py</code> <pre><code>def check_file_server_dependencies() -&gt; None:\n\"\"\"Check file dependencies as boto3.\n\n    Returns:\n        None\n    \"\"\"\n    try:\n        # pylint: disable=unused-import,import-outside-toplevel\n        import boto3  # noqa\n        import minio  # noqa\n    except ImportError as e:\n        raise ImportError(\n            \"You are trying to upload mlflow artifacts, but boto3 and minio are not installed. Please install them by\"\n            \" calling pip install minio boto3.\"\n        ) from e\n</code></pre>"},{"location":"reference/quadra/callbacks/mlflow.html#quadra.callbacks.mlflow.check_minio_credentials","title":"<code>check_minio_credentials()</code>","text":"<p>Check minio credentials for aws based storage such as minio.</p> <p>Returns:</p> <ul> <li> <code>None</code>         \u2013          <p>None</p> </li> </ul> Source code in <code>quadra/callbacks/mlflow.py</code> <pre><code>def check_minio_credentials() -&gt; None:\n\"\"\"Check minio credentials for aws based storage such as minio.\n\n    Returns:\n        None\n    \"\"\"\n    check = os.environ.get(\"AWS_ACCESS_KEY_ID\") is not None and os.environ.get(\"AWS_SECRET_ACCESS_KEY\") is not None\n    if not check:\n        raise ValueError(\n            \"You are trying to upload mlflow artifacts, but minio credentials are not set. Please set them in your\"\n            \" environment variables.\"\n        )\n</code></pre>"},{"location":"reference/quadra/callbacks/mlflow.html#quadra.callbacks.mlflow.validate_artifact_storage","title":"<code>validate_artifact_storage(logger)</code>","text":"<p>Validate artifact storage.</p> <p>Parameters:</p> <ul> <li> logger             (<code>MLFlowLogger</code>)         \u2013          <p>Mlflow logger from pytorch lightning.</p> </li> </ul> Source code in <code>quadra/callbacks/mlflow.py</code> <pre><code>def validate_artifact_storage(logger: MLFlowLogger):\n\"\"\"Validate artifact storage.\n\n    Args:\n        logger: Mlflow logger from pytorch lightning.\n\n    \"\"\"\n    from quadra.utils.utils import get_logger  # pylint: disable=[import-outside-toplevel]\n\n    log = get_logger(__name__)\n\n    client = logger.experiment\n    # TODO: we have to access the internal api to get the artifact uri, however there could be a better way\n    artifact_uri = client._tracking_client._get_artifact_repo(  # pylint: disable=protected-access\n        logger.run_id\n    ).artifact_uri\n    if artifact_uri.startswith(\"s3://\"):\n        check_minio_credentials()\n        check_file_server_dependencies()\n        log.info(\"Mlflow artifact storage is AWS/S3 basedand credentials and dependencies are satisfied.\")\n    else:\n        log.info(\"Mlflow artifact storage uri is %s. Validation checks are not implemented.\", artifact_uri)\n</code></pre>"},{"location":"reference/quadra/callbacks/scheduler.html","title":"scheduler","text":""},{"location":"reference/quadra/callbacks/scheduler.html#quadra.callbacks.scheduler.WarmupInit","title":"<code>WarmupInit(scheduler_config)</code>","text":"<p>             Bases: <code>Callback</code></p> <p>Custom callback used to setup a warmup scheduler.</p> <p>Parameters:</p> <ul> <li> scheduler_config             (<code>DictConfig</code>)         \u2013          <p>scheduler configuration.</p> </li> </ul> Source code in <code>quadra/callbacks/scheduler.py</code> <pre><code>def __init__(\n    self,\n    scheduler_config: DictConfig,\n) -&gt; None:\n    self.scheduler_config = scheduler_config\n</code></pre>"},{"location":"reference/quadra/callbacks/scheduler.html#quadra.callbacks.scheduler.WarmupInit.on_fit_start","title":"<code>on_fit_start(trainer, pl_module)</code>","text":"<p>Called when fit begins.</p> Source code in <code>quadra/callbacks/scheduler.py</code> <pre><code>@rank_zero_only\ndef on_fit_start(self, trainer: pl.Trainer, pl_module: pl.LightningModule) -&gt; None:\n\"\"\"Called when fit begins.\"\"\"\n    if not hasattr(trainer, \"datamodule\"):\n        raise ValueError(\"Trainer must have a datamodule attribute.\")\n\n    if not any(isinstance(s.scheduler, CosineAnnealingWithLinearWarmUp) for s in trainer.lr_scheduler_configs):\n        return\n\n    log.info(\"Using warmup scheduler, forcing optimizer learning rate to zero.\")\n    for i, _ in enumerate(trainer.optimizers):\n        for param_group in trainer.optimizers[i].param_groups:\n            param_group[\"lr\"] = 0.0\n        trainer.optimizers[i].defaults[\"lr\"] = 0.0\n\n    batch_size = trainer.datamodule.batch_size\n    train_dataloader = trainer.datamodule.train_dataloader()\n    len_train_dataloader = len(train_dataloader)\n    if isinstance(trainer.device_ids, list) and pl_module.device.type == \"cuda\":\n        num_gpus = len(trainer.device_ids)\n        len_train_dataloader = len_train_dataloader // num_gpus\n        if not train_dataloader.drop_last:\n            len_train_dataloader += int((len_train_dataloader % num_gpus) != 0)\n\n    if len_train_dataloader == 1:\n        log.warning(\n            \"From this dataset size, we can only generate single batch. The batch size will be set as lenght of\"\n            \" the dataset \"\n        )\n        batch_size = len(train_dataloader.dataset)\n\n    if isinstance(trainer.device_ids, list) and pl_module.device.type == \"cuda\":\n        batch_size = batch_size * len(trainer.device_ids)\n\n    scheduler = hydra.utils.instantiate(\n        self.scheduler_config,\n        optimizer=pl_module.optimizer,\n        batch_size=batch_size,\n        len_loader=len_train_dataloader,\n    )\n\n    for i, s in enumerate(trainer.lr_scheduler_configs):\n        if isinstance(s.scheduler, CosineAnnealingWithLinearWarmUp):\n            trainer.lr_scheduler_configs[i].scheduler = scheduler\n</code></pre>"},{"location":"reference/quadra/datamodules/index.html","title":"datamodules","text":""},{"location":"reference/quadra/datamodules/index.html#quadra.datamodules.AnomalyDataModule","title":"<code>AnomalyDataModule(data_path, category=None, image_size=None, train_batch_size=32, test_batch_size=32, num_workers=8, train_transform=None, val_transform=None, test_transform=None, seed=0, task='segmentation', mask_suffix=None, create_test_set_if_empty=True, phase='train', name='anomaly_datamodule', valid_area_mask=None, crop_area=None, **kwargs)</code>","text":"<p>             Bases: <code>BaseDataModule</code></p> <p>Anomalib-like Lightning Data Module.</p> <p>Parameters:</p> <ul> <li> data_path             (<code>str</code>)         \u2013          <p>Path to the dataset</p> </li> <li> category             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>Name of the sub category to use.</p> </li> <li> image_size             (<code>int | tuple[int, int] | None</code>, default:                 <code>None</code> )         \u2013          <p>Variable to which image is resized.</p> </li> <li> train_batch_size             (<code>int</code>, default:                 <code>32</code> )         \u2013          <p>Training batch size.</p> </li> <li> test_batch_size             (<code>int</code>, default:                 <code>32</code> )         \u2013          <p>Testing batch size.</p> </li> <li> train_transform             (<code>Compose | None</code>, default:                 <code>None</code> )         \u2013          <p>transformations for training. Defaults to None.</p> </li> <li> val_transform             (<code>Compose | None</code>, default:                 <code>None</code> )         \u2013          <p>transformations for validation. Defaults to None.</p> </li> <li> test_transform             (<code>Compose | None</code>, default:                 <code>None</code> )         \u2013          <p>transformations for testing. Defaults to None.</p> </li> <li> num_workers             (<code>int</code>, default:                 <code>8</code> )         \u2013          <p>Number of workers.</p> </li> <li> seed             (<code>int</code>, default:                 <code>0</code> )         \u2013          <p>seed used for the random subset splitting</p> </li> <li> task             (<code>str</code>, default:                 <code>'segmentation'</code> )         \u2013          <p>Whether we are interested in segmenting the anomalies (segmentation) or not (classification)</p> </li> <li> mask_suffix             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>String to append to the base filename to get the mask name, by default for MVTec dataset masks are saved as imagename_mask.png in this case the parameter should be filled with \"_mask\"</p> </li> <li> create_test_set_if_empty             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>If True, the test set is created from good images if it is empty.</p> </li> <li> phase             (<code>str</code>, default:                 <code>'train'</code> )         \u2013          <p>Either train or test.</p> </li> <li> name             (<code>str</code>, default:                 <code>'anomaly_datamodule'</code> )         \u2013          <p>Name of the data module.</p> </li> <li> valid_area_mask             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>Optional path to the mask to use to filter out the valid area of the image. If None, the whole image is considered valid. The mask should match the image size even if the image is cropped.</p> </li> <li> crop_area             (<code>tuple[int, int, int, int] | None</code>, default:                 <code>None</code> )         \u2013          <p>Optional tuple of 4 integers (x1, y1, x2, y2) to crop the image to the specified area. If None, the whole image is considered valid.</p> </li> </ul> Source code in <code>quadra/datamodules/anomaly.py</code> <pre><code>def __init__(\n    self,\n    data_path: str,\n    category: str | None = None,\n    image_size: int | tuple[int, int] | None = None,\n    train_batch_size: int = 32,\n    test_batch_size: int = 32,\n    num_workers: int = 8,\n    train_transform: albumentations.Compose | None = None,\n    val_transform: albumentations.Compose | None = None,\n    test_transform: albumentations.Compose | None = None,\n    seed: int = 0,\n    task: str = \"segmentation\",\n    mask_suffix: str | None = None,\n    create_test_set_if_empty: bool = True,\n    phase: str = \"train\",\n    name: str = \"anomaly_datamodule\",\n    valid_area_mask: str | None = None,\n    crop_area: tuple[int, int, int, int] | None = None,\n    **kwargs,\n) -&gt; None:\n    super().__init__(\n        data_path=data_path,\n        name=name,\n        seed=seed,\n        train_transform=train_transform,\n        val_transform=val_transform,\n        test_transform=test_transform,\n        num_workers=num_workers,\n        **kwargs,\n    )\n\n    self.root = data_path\n    self.category = category\n    self.data_path = os.path.join(self.root, self.category) if self.category is not None else self.root\n    self.image_size = image_size\n\n    self.train_batch_size = train_batch_size\n    self.test_batch_size = test_batch_size\n    self.task = task\n\n    self.train_dataset: AnomalyDataset\n    self.test_dataset: AnomalyDataset\n    self.val_dataset: AnomalyDataset\n    self.mask_suffix = mask_suffix\n    self.create_test_set_if_empty = create_test_set_if_empty\n    self.phase = phase\n    self.valid_area_mask = valid_area_mask\n    self.crop_area = crop_area\n</code></pre>"},{"location":"reference/quadra/datamodules/index.html#quadra.datamodules.AnomalyDataModule.val_data","title":"<code>val_data: pd.DataFrame</code>  <code>property</code>","text":"<p>Get validation data.</p>"},{"location":"reference/quadra/datamodules/index.html#quadra.datamodules.AnomalyDataModule.predict_dataloader","title":"<code>predict_dataloader()</code>","text":"<p>Returns a dataloader used for predictions.</p> Source code in <code>quadra/datamodules/anomaly.py</code> <pre><code>def predict_dataloader(self) -&gt; DataLoader:\n\"\"\"Returns a dataloader used for predictions.\"\"\"\n    return DataLoader(\n        self.test_dataset,\n        shuffle=False,\n        batch_size=self.test_batch_size,\n        num_workers=self.num_workers,\n        pin_memory=True,\n    )\n</code></pre>"},{"location":"reference/quadra/datamodules/index.html#quadra.datamodules.AnomalyDataModule.setup","title":"<code>setup(stage=None)</code>","text":"<p>Setup data module based on stages of training.</p> Source code in <code>quadra/datamodules/anomaly.py</code> <pre><code>def setup(self, stage: str | None = None) -&gt; None:\n\"\"\"Setup data module based on stages of training.\"\"\"\n    if stage == \"fit\" and self.phase == \"train\":\n        self.train_dataset = AnomalyDataset(\n            transform=self.train_transform,\n            task=self.task,\n            samples=self.train_data,\n            valid_area_mask=self.valid_area_mask,\n            crop_area=self.crop_area,\n        )\n\n        if len(self.val_data) == 0:\n            log.info(\"Validation dataset is empty, using test set instead\")\n\n        self.val_dataset = AnomalyDataset(\n            transform=self.test_transform,\n            task=self.task,\n            samples=self.val_data if len(self.val_data) &gt; 0 else self.data,\n            valid_area_mask=self.valid_area_mask,\n            crop_area=self.crop_area,\n        )\n    if stage == \"test\" or self.phase == \"test\":\n        self.test_dataset = AnomalyDataset(\n            transform=self.test_transform,\n            task=self.task,\n            samples=self.test_data,\n            valid_area_mask=self.valid_area_mask,\n            crop_area=self.crop_area,\n        )\n</code></pre>"},{"location":"reference/quadra/datamodules/index.html#quadra.datamodules.AnomalyDataModule.test_dataloader","title":"<code>test_dataloader()</code>","text":"<p>Get test dataloader.</p> Source code in <code>quadra/datamodules/anomaly.py</code> <pre><code>def test_dataloader(self) -&gt; DataLoader:\n\"\"\"Get test dataloader.\"\"\"\n    return DataLoader(\n        self.test_dataset,\n        shuffle=False,\n        batch_size=self.test_batch_size,\n        num_workers=self.num_workers,\n        pin_memory=True,\n    )\n</code></pre>"},{"location":"reference/quadra/datamodules/index.html#quadra.datamodules.AnomalyDataModule.train_dataloader","title":"<code>train_dataloader()</code>","text":"<p>Get train dataloader.</p> Source code in <code>quadra/datamodules/anomaly.py</code> <pre><code>def train_dataloader(self) -&gt; DataLoader:\n\"\"\"Get train dataloader.\"\"\"\n    return DataLoader(\n        self.train_dataset,\n        shuffle=True,\n        batch_size=self.train_batch_size,\n        num_workers=self.num_workers,\n        pin_memory=True,\n    )\n</code></pre>"},{"location":"reference/quadra/datamodules/index.html#quadra.datamodules.AnomalyDataModule.val_dataloader","title":"<code>val_dataloader()</code>","text":"<p>Get validation dataloader.</p> Source code in <code>quadra/datamodules/anomaly.py</code> <pre><code>def val_dataloader(self) -&gt; DataLoader:\n\"\"\"Get validation dataloader.\"\"\"\n    return DataLoader(\n        dataset=self.val_dataset,\n        shuffle=False,\n        batch_size=self.test_batch_size,\n        num_workers=self.num_workers,\n        pin_memory=True,\n    )\n</code></pre>"},{"location":"reference/quadra/datamodules/index.html#quadra.datamodules.ClassificationDataModule","title":"<code>ClassificationDataModule(data_path, dataset=ImageClassificationListDataset, name='classification_datamodule', num_workers=8, batch_size=32, seed=42, val_size=0.2, test_size=0.2, num_data_class=None, exclude_filter=None, include_filter=None, label_map=None, load_aug_images=False, aug_name=None, n_aug_to_take=4, replace_str_from=None, replace_str_to=None, train_transform=None, val_transform=None, test_transform=None, train_split_file=None, test_split_file=None, val_split_file=None, class_to_idx=None, **kwargs)</code>","text":"<p>             Bases: <code>BaseDataModule</code></p> <p>Base class single folder based classification datamodules. If there is no nested folders, use this class.</p> <p>Parameters:</p> <ul> <li> data_path             (<code>str</code>)         \u2013          <p>Path to the data main folder.</p> </li> <li> name             (<code>str</code>, default:                 <code>'classification_datamodule'</code> )         \u2013          <p>The name for the data module. Defaults to \"classification_datamodule\".</p> </li> <li> num_workers             (<code>int</code>, default:                 <code>8</code> )         \u2013          <p>Number of workers for dataloaders. Defaults to 16.</p> </li> <li> batch_size             (<code>int</code>, default:                 <code>32</code> )         \u2013          <p>Batch size. Defaults to 32.</p> </li> <li> seed             (<code>int</code>, default:                 <code>42</code> )         \u2013          <p>Random generator seed. Defaults to 42.</p> </li> <li> dataset             (<code>type[ImageClassificationListDataset]</code>, default:                 <code>ImageClassificationListDataset</code> )         \u2013          <p>Dataset class.</p> </li> <li> val_size             (<code>float | None</code>, default:                 <code>0.2</code> )         \u2013          <p>The validation split. Defaults to 0.2.</p> </li> <li> test_size             (<code>float</code>, default:                 <code>0.2</code> )         \u2013          <p>The test split. Defaults to 0.2.</p> </li> <li> exclude_filter             (<code>list[str] | None</code>, default:                 <code>None</code> )         \u2013          <p>The filter for excluding folders. Defaults to None.</p> </li> <li> include_filter             (<code>list[str] | None</code>, default:                 <code>None</code> )         \u2013          <p>The filter for including folders. Defaults to None.</p> </li> <li> label_map             (<code>dict[str, Any] | None</code>, default:                 <code>None</code> )         \u2013          <p>The mapping for labels. Defaults to None.</p> </li> <li> num_data_class             (<code>int | None</code>, default:                 <code>None</code> )         \u2013          <p>The number of samples per class. Defaults to None.</p> </li> <li> train_transform             (<code>Compose | None</code>, default:                 <code>None</code> )         \u2013          <p>Transformations for train dataset. Defaults to None.</p> </li> <li> val_transform             (<code>Compose | None</code>, default:                 <code>None</code> )         \u2013          <p>Transformations for validation dataset. Defaults to None.</p> </li> <li> test_transform             (<code>Compose | None</code>, default:                 <code>None</code> )         \u2013          <p>Transformations for test dataset. Defaults to None.</p> </li> <li> train_split_file             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>The file with train split. Defaults to None.</p> </li> <li> val_split_file             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>The file with validation split. Defaults to None.</p> </li> <li> test_split_file             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>The file with test split. Defaults to None.</p> </li> <li> class_to_idx             (<code>dict[str, int] | None</code>, default:                 <code>None</code> )         \u2013          <p>The mapping from class name to index. Defaults to None.</p> </li> <li> **kwargs             (<code>Any</code>, default:                 <code>{}</code> )         \u2013          <p>Additional arguments for BaseDataModule.</p> </li> </ul> Source code in <code>quadra/datamodules/classification.py</code> <pre><code>def __init__(\n    self,\n    data_path: str,\n    dataset: type[ImageClassificationListDataset] = ImageClassificationListDataset,\n    name: str = \"classification_datamodule\",\n    num_workers: int = 8,\n    batch_size: int = 32,\n    seed: int = 42,\n    val_size: float | None = 0.2,\n    test_size: float = 0.2,\n    num_data_class: int | None = None,\n    exclude_filter: list[str] | None = None,\n    include_filter: list[str] | None = None,\n    label_map: dict[str, Any] | None = None,\n    load_aug_images: bool = False,\n    aug_name: str | None = None,\n    n_aug_to_take: int | None = 4,\n    replace_str_from: str | None = None,\n    replace_str_to: str | None = None,\n    train_transform: albumentations.Compose | None = None,\n    val_transform: albumentations.Compose | None = None,\n    test_transform: albumentations.Compose | None = None,\n    train_split_file: str | None = None,\n    test_split_file: str | None = None,\n    val_split_file: str | None = None,\n    class_to_idx: dict[str, int] | None = None,\n    **kwargs: Any,\n):\n    super().__init__(\n        data_path=data_path,\n        name=name,\n        seed=seed,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        train_transform=train_transform,\n        val_transform=val_transform,\n        test_transform=test_transform,\n        load_aug_images=load_aug_images,\n        aug_name=aug_name,\n        n_aug_to_take=n_aug_to_take,\n        replace_str_from=replace_str_from,\n        replace_str_to=replace_str_to,\n        **kwargs,\n    )\n    self.replace_str = None\n    self.exclude_filter = exclude_filter\n    self.include_filter = include_filter\n    self.val_size = val_size\n    self.test_size = test_size\n    self.label_map = label_map\n    self.num_data_class = num_data_class\n    self.dataset = dataset\n    self.train_split_file = train_split_file\n    self.test_split_file = test_split_file\n    self.val_split_file = val_split_file\n    self.class_to_idx: dict[str, int] | None\n\n    if class_to_idx is not None:\n        self.class_to_idx = class_to_idx\n        self.num_classes = len(self.class_to_idx)\n    else:\n        self.class_to_idx = self._find_classes_from_data_path(self.data_path)\n        if self.class_to_idx is None:\n            log.warning(\"Could not build a class_to_idx from the data_path subdirectories\")\n            self.num_classes = 0\n        else:\n            self.num_classes = len(self.class_to_idx)\n</code></pre>"},{"location":"reference/quadra/datamodules/index.html#quadra.datamodules.ClassificationDataModule.predict_dataloader","title":"<code>predict_dataloader()</code>","text":"<p>Returns a dataloader used for predictions.</p> Source code in <code>quadra/datamodules/classification.py</code> <pre><code>def predict_dataloader(self) -&gt; DataLoader:\n\"\"\"Returns a dataloader used for predictions.\"\"\"\n    return self.test_dataloader()\n</code></pre>"},{"location":"reference/quadra/datamodules/index.html#quadra.datamodules.ClassificationDataModule.setup","title":"<code>setup(stage=None)</code>","text":"<p>Setup data module based on stages of training.</p> Source code in <code>quadra/datamodules/classification.py</code> <pre><code>def setup(self, stage: str | None = None) -&gt; None:\n\"\"\"Setup data module based on stages of training.\"\"\"\n    if stage in [\"train\", \"fit\"]:\n        self.train_dataset = self.dataset(\n            samples=self.data[self.data[\"split\"] == \"train\"][\"samples\"].tolist(),\n            targets=self.data[self.data[\"split\"] == \"train\"][\"targets\"].tolist(),\n            transform=self.train_transform,\n            class_to_idx=self.class_to_idx,\n        )\n        self.val_dataset = self.dataset(\n            samples=self.data[self.data[\"split\"] == \"val\"][\"samples\"].tolist(),\n            targets=self.data[self.data[\"split\"] == \"val\"][\"targets\"].tolist(),\n            transform=self.val_transform,\n            class_to_idx=self.class_to_idx,\n        )\n    if stage in [\"test\", \"predict\"]:\n        self.test_dataset = self.dataset(\n            samples=self.data[self.data[\"split\"] == \"test\"][\"samples\"].tolist(),\n            targets=self.data[self.data[\"split\"] == \"test\"][\"targets\"].tolist(),\n            transform=self.test_transform,\n            class_to_idx=self.class_to_idx,\n        )\n</code></pre>"},{"location":"reference/quadra/datamodules/index.html#quadra.datamodules.ClassificationDataModule.test_dataloader","title":"<code>test_dataloader()</code>","text":"<p>Returns the test dataloader.</p> <p>Raises:</p> <ul> <li> <code>ValueError</code>           \u2013          <p>If test dataset is not initialized.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataLoader</code>         \u2013          <p>test dataloader.</p> </li> </ul> Source code in <code>quadra/datamodules/classification.py</code> <pre><code>def test_dataloader(self) -&gt; DataLoader:\n\"\"\"Returns the test dataloader.\n\n    Raises:\n        ValueError: If test dataset is not initialized.\n\n\n    Returns:\n        test dataloader.\n    \"\"\"\n    if not self.test_dataset_available:\n        raise ValueError(\"Test dataset is not initialized\")\n\n    loader = DataLoader(\n        self.test_dataset,\n        batch_size=self.batch_size,\n        shuffle=False,\n        num_workers=self.num_workers,\n        drop_last=False,\n        pin_memory=True,\n        persistent_workers=self.num_workers &gt; 0,\n    )\n    return loader\n</code></pre>"},{"location":"reference/quadra/datamodules/index.html#quadra.datamodules.ClassificationDataModule.train_dataloader","title":"<code>train_dataloader()</code>","text":"<p>Returns the train dataloader.</p> <p>Raises:</p> <ul> <li> <code>ValueError</code>           \u2013          <p>If train dataset is not initialized.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataLoader</code>         \u2013          <p>Train dataloader.</p> </li> </ul> Source code in <code>quadra/datamodules/classification.py</code> <pre><code>def train_dataloader(self) -&gt; DataLoader:\n\"\"\"Returns the train dataloader.\n\n    Raises:\n        ValueError: If train dataset is not initialized.\n\n    Returns:\n        Train dataloader.\n    \"\"\"\n    if not self.train_dataset_available:\n        raise ValueError(\"Train dataset is not initialized\")\n    if not isinstance(self.train_dataset, torch.utils.data.Dataset):\n        raise ValueError(\"Train dataset has to be single `torch.utils.data.Dataset` instance.\")\n    return DataLoader(\n        self.train_dataset,\n        batch_size=self.batch_size,\n        shuffle=True,\n        num_workers=self.num_workers,\n        drop_last=False,\n        pin_memory=True,\n        persistent_workers=self.num_workers &gt; 0,\n    )\n</code></pre>"},{"location":"reference/quadra/datamodules/index.html#quadra.datamodules.ClassificationDataModule.val_dataloader","title":"<code>val_dataloader()</code>","text":"<p>Returns the validation dataloader.</p> <p>Raises:</p> <ul> <li> <code>ValueError</code>           \u2013          <p>If validation dataset is not initialized.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataLoader</code>         \u2013          <p>val dataloader.</p> </li> </ul> Source code in <code>quadra/datamodules/classification.py</code> <pre><code>def val_dataloader(self) -&gt; DataLoader:\n\"\"\"Returns the validation dataloader.\n\n    Raises:\n        ValueError: If validation dataset is not initialized.\n\n    Returns:\n        val dataloader.\n    \"\"\"\n    if not self.val_dataset_available:\n        raise ValueError(\"Validation dataset is not initialized\")\n    if not isinstance(self.val_dataset, torch.utils.data.Dataset):\n        raise ValueError(\"Validation dataset has to be single `torch.utils.data.Dataset` instance.\")\n    return DataLoader(\n        self.val_dataset,\n        batch_size=self.batch_size,\n        shuffle=False,\n        num_workers=self.num_workers,\n        drop_last=False,\n        pin_memory=True,\n        persistent_workers=self.num_workers &gt; 0,\n    )\n</code></pre>"},{"location":"reference/quadra/datamodules/index.html#quadra.datamodules.MultilabelClassificationDataModule","title":"<code>MultilabelClassificationDataModule(data_path, images_and_labels_file=None, train_split_file=None, test_split_file=None, val_split_file=None, name='multilabel_datamodule', dataset=MultilabelClassificationDataset, num_classes=None, num_workers=16, batch_size=64, test_batch_size=64, seed=42, val_size=0.2, test_size=0.2, train_transform=None, val_transform=None, test_transform=None, class_to_idx=None, **kwargs)</code>","text":"<p>             Bases: <code>BaseDataModule</code></p> <p>Base class for all multi-label modules.</p> <p>Parameters:</p> <ul> <li> data_path             (<code>str</code>)         \u2013          <p>Path to the data main folder.</p> </li> <li> images_and_labels_file             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>a path to a txt file containing the relative (to <code>data_path</code>) path of images with their relative labels, in a comma-separated way. E.g.:</p> <ul> <li>path1,l1,l2,l3</li> <li>path2,l4,l5</li> <li>...</li> </ul> <p>One of <code>images_and_label</code> and both <code>train_split_file</code> and <code>test_split_file</code> must be set. Defaults to None.</p> </li> <li> name             (<code>str</code>, default:                 <code>'multilabel_datamodule'</code> )         \u2013          <p>The name for the data module. Defaults to \"multilabel_datamodule\".</p> </li> <li> dataset             (<code>Callable</code>, default:                 <code>MultilabelClassificationDataset</code> )         \u2013          <p>a callable returning a torch.utils.data.Dataset class.</p> </li> <li> num_classes             (<code>int | None</code>, default:                 <code>None</code> )         \u2013          <p>the number of classes in the dataset. This is used to create one-hot encoded targets. Defaults to None.</p> </li> <li> num_workers             (<code>int</code>, default:                 <code>16</code> )         \u2013          <p>Number of workers for dataloaders. Defaults to 16.</p> </li> <li> batch_size             (<code>int</code>, default:                 <code>64</code> )         \u2013          <p>Training batch size. Defaults to 64.</p> </li> <li> test_batch_size             (<code>int</code>, default:                 <code>64</code> )         \u2013          <p>Testing batch size. Defaults to 64.</p> </li> <li> seed             (<code>int</code>, default:                 <code>42</code> )         \u2013          <p>Random generator seed. Defaults to SegmentationEvalua2.</p> </li> <li> val_size             (<code>float | None</code>, default:                 <code>0.2</code> )         \u2013          <p>The validation split. Defaults to 0.2.</p> </li> <li> test_size             (<code>float | None</code>, default:                 <code>0.2</code> )         \u2013          <p>The test split. Defaults to 0.2.</p> </li> <li> train_transform             (<code>Compose | None</code>, default:                 <code>None</code> )         \u2013          <p>Transformations for train dataset. Defaults to None.</p> </li> <li> val_transform             (<code>Compose | None</code>, default:                 <code>None</code> )         \u2013          <p>Transformations for validation dataset. Defaults to None.</p> </li> <li> test_transform             (<code>Compose | None</code>, default:                 <code>None</code> )         \u2013          <p>Transformations for test dataset. Defaults to None.</p> </li> <li> train_split_file             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>The file with train split. Defaults to None.</p> </li> <li> val_split_file             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>The file with validation split. Defaults to None.</p> </li> <li> test_split_file             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>The file with test split. Defaults to None.</p> </li> <li> class_to_idx             (<code>dict[str, int] | None</code>, default:                 <code>None</code> )         \u2013          <p>a clss to idx dictionary. Defaults to None.</p> </li> </ul> Source code in <code>quadra/datamodules/classification.py</code> <pre><code>def __init__(\n    self,\n    data_path: str,\n    images_and_labels_file: str | None = None,\n    train_split_file: str | None = None,\n    test_split_file: str | None = None,\n    val_split_file: str | None = None,\n    name: str = \"multilabel_datamodule\",\n    dataset: Callable = MultilabelClassificationDataset,\n    num_classes: int | None = None,\n    num_workers: int = 16,\n    batch_size: int = 64,\n    test_batch_size: int = 64,\n    seed: int = 42,\n    val_size: float | None = 0.2,\n    test_size: float | None = 0.2,\n    train_transform: albumentations.Compose | None = None,\n    val_transform: albumentations.Compose | None = None,\n    test_transform: albumentations.Compose | None = None,\n    class_to_idx: dict[str, int] | None = None,\n    **kwargs,\n):\n    super().__init__(\n        data_path=data_path,\n        name=name,\n        num_workers=num_workers,\n        batch_size=batch_size,\n        seed=seed,\n        train_transform=train_transform,\n        val_transform=val_transform,\n        test_transform=test_transform,\n        **kwargs,\n    )\n    if not (images_and_labels_file is not None or (train_split_file is not None and test_split_file is not None)):\n        raise ValueError(\n            \"Either `images_and_labels_file` or both `train_split_file` and `test_split_file` must be set\"\n        )\n    self.images_and_labels_file = images_and_labels_file\n    self.dataset = dataset\n    self.num_classes = num_classes\n    self.train_batch_size = batch_size\n    self.test_batch_size = test_batch_size\n    self.val_size = val_size\n    self.test_size = test_size\n    self.train_split_file = train_split_file\n    self.test_split_file = test_split_file\n    self.val_split_file = val_split_file\n    self.class_to_idx = class_to_idx\n    self.train_dataset: MultilabelClassificationDataset\n    self.val_dataset: MultilabelClassificationDataset\n    self.test_dataset: MultilabelClassificationDataset\n</code></pre>"},{"location":"reference/quadra/datamodules/index.html#quadra.datamodules.MultilabelClassificationDataModule.predict_dataloader","title":"<code>predict_dataloader()</code>","text":"<p>Returns a dataloader used for predictions.</p> Source code in <code>quadra/datamodules/classification.py</code> <pre><code>def predict_dataloader(self) -&gt; DataLoader:\n\"\"\"Returns a dataloader used for predictions.\"\"\"\n    return self.test_dataloader()\n</code></pre>"},{"location":"reference/quadra/datamodules/index.html#quadra.datamodules.MultilabelClassificationDataModule.setup","title":"<code>setup(stage=None)</code>","text":"<p>Setup data module based on stages of training.</p> Source code in <code>quadra/datamodules/classification.py</code> <pre><code>def setup(self, stage: str | None = None) -&gt; None:\n\"\"\"Setup data module based on stages of training.\"\"\"\n    if stage in [\"train\", \"fit\"]:\n        train_samples = self.data[self.data[\"split\"] == \"train\"][\"samples\"].tolist()\n        train_targets = self.data[self.data[\"split\"] == \"train\"][\"targets\"].tolist()\n        val_samples = self.data[self.data[\"split\"] == \"val\"][\"samples\"].tolist()\n        val_targets = self.data[self.data[\"split\"] == \"val\"][\"targets\"].tolist()\n        self.train_dataset = self.dataset(\n            samples=train_samples,\n            targets=train_targets,\n            transform=self.train_transform,\n            class_to_idx=self.class_to_idx,\n        )\n        self.val_dataset = self.dataset(\n            samples=val_samples,\n            targets=val_targets,\n            transform=self.val_transform,\n            class_to_idx=self.class_to_idx,\n        )\n    if stage == \"test\":\n        test_samples = self.data[self.data[\"split\"] == \"test\"][\"samples\"].tolist()\n        test_targets = self.data[self.data[\"split\"] == \"test\"][\"targets\"].tolist()\n        self.test_dataset = self.dataset(\n            samples=test_samples,\n            targets=test_targets,\n            transform=self.test_transform,\n            class_to_idx=self.class_to_idx,\n        )\n</code></pre>"},{"location":"reference/quadra/datamodules/index.html#quadra.datamodules.MultilabelClassificationDataModule.test_dataloader","title":"<code>test_dataloader()</code>","text":"<p>Returns the test dataloader.</p> <p>Raises:</p> <ul> <li> <code>ValueError</code>           \u2013          <p>If test dataset is not initialized.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataLoader</code>         \u2013          <p>test dataloader.</p> </li> </ul> Source code in <code>quadra/datamodules/classification.py</code> <pre><code>def test_dataloader(self) -&gt; DataLoader:\n\"\"\"Returns the test dataloader.\n\n    Raises:\n        ValueError: If test dataset is not initialized.\n\n\n    Returns:\n        test dataloader.\n    \"\"\"\n    if not self.test_dataset_available:\n        raise ValueError(\"Test dataset is not initialized\")\n\n    loader = DataLoader(\n        self.test_dataset,\n        batch_size=self.batch_size,\n        shuffle=False,\n        num_workers=self.num_workers,\n        drop_last=False,\n        pin_memory=True,\n        persistent_workers=self.num_workers &gt; 0,\n    )\n    return loader\n</code></pre>"},{"location":"reference/quadra/datamodules/index.html#quadra.datamodules.MultilabelClassificationDataModule.train_dataloader","title":"<code>train_dataloader()</code>","text":"<p>Returns the train dataloader.</p> <p>Raises:</p> <ul> <li> <code>ValueError</code>           \u2013          <p>If train dataset is not initialized.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataLoader</code>         \u2013          <p>Train dataloader.</p> </li> </ul> Source code in <code>quadra/datamodules/classification.py</code> <pre><code>def train_dataloader(self) -&gt; DataLoader:\n\"\"\"Returns the train dataloader.\n\n    Raises:\n        ValueError: If train dataset is not initialized.\n\n    Returns:\n        Train dataloader.\n    \"\"\"\n    if not self.train_dataset_available:\n        raise ValueError(\"Train dataset is not initialized\")\n    return DataLoader(\n        self.train_dataset,\n        batch_size=self.batch_size,\n        shuffle=True,\n        num_workers=self.num_workers,\n        drop_last=False,\n        pin_memory=True,\n        persistent_workers=self.num_workers &gt; 0,\n    )\n</code></pre>"},{"location":"reference/quadra/datamodules/index.html#quadra.datamodules.MultilabelClassificationDataModule.val_dataloader","title":"<code>val_dataloader()</code>","text":"<p>Returns the validation dataloader.</p> <p>Raises:</p> <ul> <li> <code>ValueError</code>           \u2013          <p>If validation dataset is not initialized.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataLoader</code>         \u2013          <p>val dataloader.</p> </li> </ul> Source code in <code>quadra/datamodules/classification.py</code> <pre><code>def val_dataloader(self) -&gt; DataLoader:\n\"\"\"Returns the validation dataloader.\n\n    Raises:\n        ValueError: If validation dataset is not initialized.\n\n    Returns:\n        val dataloader.\n    \"\"\"\n    if not self.val_dataset_available:\n        raise ValueError(\"Validation dataset is not initialized\")\n    return DataLoader(\n        self.val_dataset,\n        batch_size=self.batch_size,\n        shuffle=False,\n        num_workers=self.num_workers,\n        drop_last=False,\n        pin_memory=True,\n        persistent_workers=self.num_workers &gt; 0,\n    )\n</code></pre>"},{"location":"reference/quadra/datamodules/index.html#quadra.datamodules.PatchSklearnClassificationDataModule","title":"<code>PatchSklearnClassificationDataModule(data_path, class_to_idx, name='patch_classification_datamodule', train_filename='dataset.txt', exclude_filter=None, include_filter=None, seed=42, batch_size=32, num_workers=6, train_transform=None, val_transform=None, test_transform=None, balance_classes=False, class_to_skip_training=None, **kwargs)</code>","text":"<p>             Bases: <code>BaseDataModule</code></p> <p>DataModule for patch classification.</p> <p>Parameters:</p> <ul> <li> data_path             (<code>str</code>)         \u2013          <p>Location of the dataset</p> </li> <li> name             (<code>str</code>, default:                 <code>'patch_classification_datamodule'</code> )         \u2013          <p>Name of the datamodule</p> </li> <li> train_filename             (<code>str</code>, default:                 <code>'dataset.txt'</code> )         \u2013          <p>Name of the file containing the list of training samples</p> </li> <li> exclude_filter             (<code>list[str] | None</code>, default:                 <code>None</code> )         \u2013          <p>Filter to exclude samples from the dataset</p> </li> <li> include_filter             (<code>list[str] | None</code>, default:                 <code>None</code> )         \u2013          <p>Filter to include samples from the dataset</p> </li> <li> class_to_idx             (<code>dict</code>)         \u2013          <p>Dictionary mapping class names to indices</p> </li> <li> seed             (<code>int</code>, default:                 <code>42</code> )         \u2013          <p>Random seed</p> </li> <li> batch_size             (<code>int</code>, default:                 <code>32</code> )         \u2013          <p>Batch size</p> </li> <li> num_workers             (<code>int</code>, default:                 <code>6</code> )         \u2013          <p>Number of workers</p> </li> <li> train_transform             (<code>Compose | None</code>, default:                 <code>None</code> )         \u2013          <p>Transform to apply to the training samples</p> </li> <li> val_transform             (<code>Compose | None</code>, default:                 <code>None</code> )         \u2013          <p>Transform to apply to the validation samples</p> </li> <li> test_transform             (<code>Compose | None</code>, default:                 <code>None</code> )         \u2013          <p>Transform to apply to the test samples</p> </li> <li> balance_classes             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>If True repeat low represented classes</p> </li> <li> class_to_skip_training             (<code>list | None</code>, default:                 <code>None</code> )         \u2013          <p>List of classes skipped during training.</p> </li> </ul> Source code in <code>quadra/datamodules/patch.py</code> <pre><code>def __init__(\n    self,\n    data_path: str,\n    class_to_idx: dict,\n    name: str = \"patch_classification_datamodule\",\n    train_filename: str = \"dataset.txt\",\n    exclude_filter: list[str] | None = None,\n    include_filter: list[str] | None = None,\n    seed: int = 42,\n    batch_size: int = 32,\n    num_workers: int = 6,\n    train_transform: albumentations.Compose | None = None,\n    val_transform: albumentations.Compose | None = None,\n    test_transform: albumentations.Compose | None = None,\n    balance_classes: bool = False,\n    class_to_skip_training: list | None = None,\n    **kwargs,\n):\n    super().__init__(\n        data_path=data_path,\n        name=name,\n        seed=seed,\n        num_workers=num_workers,\n        batch_size=batch_size,\n        train_transform=train_transform,\n        val_transform=val_transform,\n        test_transform=test_transform,\n        **kwargs,\n    )\n    self.class_to_idx = class_to_idx\n    self.balance_classes = balance_classes\n    self.train_filename = train_filename\n    self.include_filter = include_filter\n    self.exclude_filter = exclude_filter\n    self.class_to_skip_training = class_to_skip_training\n\n    self.train_folder = os.path.join(self.data_path, \"train\")\n    self.val_folder = os.path.join(self.data_path, \"val\")\n    self.test_folder = os.path.join(self.data_path, \"test\")\n    self.info: PatchDatasetInfo\n    self.train_dataset: PatchSklearnClassificationTrainDataset\n    self.val_dataset: ImageClassificationListDataset\n    self.test_dataset: ImageClassificationListDataset\n</code></pre>"},{"location":"reference/quadra/datamodules/index.html#quadra.datamodules.PatchSklearnClassificationDataModule.setup","title":"<code>setup(stage=None)</code>","text":"<p>Setup function.</p> Source code in <code>quadra/datamodules/patch.py</code> <pre><code>def setup(self, stage: str | None = None) -&gt; None:\n\"\"\"Setup function.\"\"\"\n    if stage == \"fit\":\n        self.train_dataset = PatchSklearnClassificationTrainDataset(\n            data_path=self.data_path,\n            class_to_idx=self.class_to_idx,\n            samples=self.data[self.data[\"split\"] == \"train\"][\"samples\"].tolist(),\n            targets=self.data[self.data[\"split\"] == \"train\"][\"targets\"].tolist(),\n            transform=self.train_transform,\n            balance_classes=self.balance_classes,\n        )\n\n        self.val_dataset = ImageClassificationListDataset(\n            class_to_idx=self.class_to_idx,\n            samples=self.data[self.data[\"split\"] == \"val\"][\"samples\"].tolist(),\n            targets=self.data[self.data[\"split\"] == \"val\"][\"targets\"].tolist(),\n            transform=self.val_transform,\n            allow_missing_label=False,\n        )\n\n    elif stage in [\"test\", \"predict\"]:\n        self.test_dataset = ImageClassificationListDataset(\n            class_to_idx=self.class_to_idx,\n            samples=self.data[self.data[\"split\"] == \"test\"][\"samples\"].tolist(),\n            targets=self.data[self.data[\"split\"] == \"test\"][\"targets\"].tolist(),\n            transform=self.test_transform,\n            allow_missing_label=True,\n        )\n</code></pre>"},{"location":"reference/quadra/datamodules/index.html#quadra.datamodules.PatchSklearnClassificationDataModule.test_dataloader","title":"<code>test_dataloader()</code>","text":"<p>Return the test dataloader.</p> Source code in <code>quadra/datamodules/patch.py</code> <pre><code>def test_dataloader(self) -&gt; DataLoader:\n\"\"\"Return the test dataloader.\"\"\"\n    if not self.test_dataset_available:\n        raise ValueError(\"No test dataset is available\")\n\n    return DataLoader(\n        self.test_dataset,\n        batch_size=self.batch_size,\n        shuffle=False,\n        num_workers=self.num_workers,\n        drop_last=False,\n        pin_memory=True,\n    )\n</code></pre>"},{"location":"reference/quadra/datamodules/index.html#quadra.datamodules.PatchSklearnClassificationDataModule.train_dataloader","title":"<code>train_dataloader()</code>","text":"<p>Return the train dataloader.</p> Source code in <code>quadra/datamodules/patch.py</code> <pre><code>def train_dataloader(self) -&gt; DataLoader:\n\"\"\"Return the train dataloader.\"\"\"\n    if not self.train_dataset_available:\n        raise ValueError(\"No training sample is available\")\n    return DataLoader(\n        self.train_dataset,\n        batch_size=self.batch_size,\n        shuffle=True,\n        num_workers=self.num_workers,\n        drop_last=False,\n        pin_memory=True,\n    )\n</code></pre>"},{"location":"reference/quadra/datamodules/index.html#quadra.datamodules.PatchSklearnClassificationDataModule.val_dataloader","title":"<code>val_dataloader()</code>","text":"<p>Return the validation dataloader.</p> Source code in <code>quadra/datamodules/patch.py</code> <pre><code>def val_dataloader(self) -&gt; DataLoader:\n\"\"\"Return the validation dataloader.\"\"\"\n    if not self.val_dataset_available:\n        raise ValueError(\"No validation dataset is available\")\n    return DataLoader(\n        self.val_dataset,\n        batch_size=self.batch_size,\n        shuffle=False,\n        num_workers=self.num_workers,\n        drop_last=False,\n        pin_memory=True,\n    )\n</code></pre>"},{"location":"reference/quadra/datamodules/index.html#quadra.datamodules.SSLDataModule","title":"<code>SSLDataModule(data_path, augmentation_dataset, name='ssl_datamodule', split_validation=True, **kwargs)</code>","text":"<p>             Bases: <code>ClassificationDataModule</code></p> <p>Base class for all data modules for self supervised learning data modules.</p> <p>Parameters:</p> <ul> <li> data_path             (<code>str</code>)         \u2013          <p>Path to the data main folder.</p> </li> <li> augmentation_dataset             (<code>TwoAugmentationDataset | TwoSetAugmentationDataset</code>)         \u2013          <p>Augmentation dataset for training dataset.</p> </li> <li> name             (<code>str</code>, default:                 <code>'ssl_datamodule'</code> )         \u2013          <p>The name for the data module. Defaults to  \"ssl_datamodule\".</p> </li> <li> split_validation             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Whether to split the validation set if . Defaults to True.</p> </li> <li> **kwargs             (<code>Any</code>, default:                 <code>{}</code> )         \u2013          <p>The keyword arguments for the classification data module. Defaults to None.</p> </li> </ul> Source code in <code>quadra/datamodules/ssl.py</code> <pre><code>def __init__(\n    self,\n    data_path: str,\n    augmentation_dataset: TwoAugmentationDataset | TwoSetAugmentationDataset,\n    name: str = \"ssl_datamodule\",\n    split_validation: bool = True,\n    **kwargs: Any,\n):\n    super().__init__(\n        data_path=data_path,\n        name=name,\n        **kwargs,\n    )\n    self.augmentation_dataset = augmentation_dataset\n    self.classifier_train_dataset: torch.utils.data.Dataset | None = None\n    self.split_validation = split_validation\n</code></pre>"},{"location":"reference/quadra/datamodules/index.html#quadra.datamodules.SSLDataModule.classifier_train_dataloader","title":"<code>classifier_train_dataloader()</code>","text":"<p>Returns classifier train dataloader.</p> Source code in <code>quadra/datamodules/ssl.py</code> <pre><code>def classifier_train_dataloader(self) -&gt; DataLoader:\n\"\"\"Returns classifier train dataloader.\"\"\"\n    if self.classifier_train_dataset is None:\n        raise ValueError(\"Classifier train dataset is not initialized\")\n\n    loader = DataLoader(\n        self.classifier_train_dataset,\n        batch_size=self.batch_size,\n        shuffle=True,\n        num_workers=self.num_workers,\n        drop_last=False,\n        pin_memory=True,\n        persistent_workers=self.num_workers &gt; 0,\n    )\n    return loader\n</code></pre>"},{"location":"reference/quadra/datamodules/index.html#quadra.datamodules.SSLDataModule.setup","title":"<code>setup(stage=None)</code>","text":"<p>Setup data module based on stages of training.</p> Source code in <code>quadra/datamodules/ssl.py</code> <pre><code>def setup(self, stage: str | None = None) -&gt; None:\n\"\"\"Setup data module based on stages of training.\"\"\"\n    if stage == \"fit\":\n        self.train_dataset = self.dataset(\n            samples=self.train_data[\"samples\"].tolist(),\n            targets=self.train_data[\"targets\"].tolist(),\n            transform=self.train_transform,\n        )\n\n        if np.unique(self.train_data[\"targets\"]).shape[0] &gt; 1 and not self.split_validation:\n            self.classifier_train_dataset = self.dataset(\n                samples=self.train_data[\"samples\"].tolist(),\n                targets=self.train_data[\"targets\"].tolist(),\n                transform=self.val_transform,\n            )\n            self.val_dataset = self.dataset(\n                samples=self.val_data[\"samples\"].tolist(),\n                targets=self.val_data[\"targets\"].tolist(),\n                transform=self.val_transform,\n            )\n        else:\n            train_classifier_samples, val_samples, train_classifier_targets, val_targets = train_test_split(\n                self.val_data[\"samples\"],\n                self.val_data[\"targets\"],\n                test_size=0.3,\n                random_state=self.seed,\n                stratify=self.val_data[\"targets\"],\n            )\n\n            self.classifier_train_dataset = self.dataset(\n                samples=train_classifier_samples,\n                targets=train_classifier_targets,\n                transform=self.test_transform,\n            )\n\n            self.val_dataset = self.dataset(\n                samples=val_samples,\n                targets=val_targets,\n                transform=self.val_transform,\n            )\n\n            log.warning(\n                \"The training set contains only one class and cannot be used to train a classifier. To overcome \"\n                \"this issue 70% of the validation set is used to train the classifier. The remaining will be used \"\n                \"as standard validation. To disable this behaviour set the `split_validation` parameter to False.\"\n            )\n            self._check_train_dataset_config()\n    if stage == \"test\":\n        self.test_dataset = self.dataset(\n            samples=self.test_data[\"samples\"].tolist(),\n            targets=self.test_data[\"targets\"].tolist(),\n            transform=self.test_transform,\n        )\n</code></pre>"},{"location":"reference/quadra/datamodules/index.html#quadra.datamodules.SSLDataModule.train_dataloader","title":"<code>train_dataloader()</code>","text":"<p>Returns train dataloader.</p> Source code in <code>quadra/datamodules/ssl.py</code> <pre><code>def train_dataloader(self) -&gt; DataLoader:\n\"\"\"Returns train dataloader.\"\"\"\n    if not isinstance(self.train_dataset, torch.utils.data.Dataset):\n        raise ValueError(\"Train dataset is not a subclass of `torch.utils.data.Dataset`\")\n    self.augmentation_dataset.dataset = self.train_dataset\n    loader = DataLoader(\n        self.augmentation_dataset,\n        batch_size=self.batch_size,\n        shuffle=True,\n        num_workers=self.num_workers,\n        drop_last=False,\n        pin_memory=True,\n        persistent_workers=self.num_workers &gt; 0,\n    )\n    return loader\n</code></pre>"},{"location":"reference/quadra/datamodules/index.html#quadra.datamodules.SegmentationDataModule","title":"<code>SegmentationDataModule(data_path, name='segmentation_datamodule', test_size=0.3, val_size=0.3, seed=42, dataset=SegmentationDataset, batch_size=32, num_workers=6, train_transform=None, test_transform=None, val_transform=None, train_split_file=None, test_split_file=None, val_split_file=None, num_data_class=None, exclude_good=False, **kwargs)</code>","text":"<p>             Bases: <code>BaseDataModule</code></p> <p>Base class for segmentation datasets.</p> <p>Parameters:</p> <ul> <li> data_path             (<code>str</code>)         \u2013          <p>Path to the data main folder.</p> </li> <li> name             (<code>str</code>, default:                 <code>'segmentation_datamodule'</code> )         \u2013          <p>The name for the data module. Defaults to \"segmentation_datamodule\".</p> </li> <li> val_size             (<code>float</code>, default:                 <code>0.3</code> )         \u2013          <p>The validation split. Defaults to 0.2.</p> </li> <li> test_size             (<code>float</code>, default:                 <code>0.3</code> )         \u2013          <p>The test split. Defaults to 0.2.</p> </li> <li> seed             (<code>int</code>, default:                 <code>42</code> )         \u2013          <p>Random generator seed. Defaults to 42.</p> </li> <li> dataset             (<code>type[SegmentationDataset]</code>, default:                 <code>SegmentationDataset</code> )         \u2013          <p>Dataset class.</p> </li> <li> batch_size             (<code>int</code>, default:                 <code>32</code> )         \u2013          <p>Batch size. Defaults to 32.</p> </li> <li> num_workers             (<code>int</code>, default:                 <code>6</code> )         \u2013          <p>Number of workers for dataloaders. Defaults to 16.</p> </li> <li> train_transform             (<code>Compose | None</code>, default:                 <code>None</code> )         \u2013          <p>Transformations for train dataset. Defaults to None.</p> </li> <li> val_transform             (<code>Compose | None</code>, default:                 <code>None</code> )         \u2013          <p>Transformations for validation dataset. Defaults to None.</p> </li> <li> test_transform             (<code>Compose | None</code>, default:                 <code>None</code> )         \u2013          <p>Transformations for test dataset. Defaults to None.</p> </li> <li> num_data_class             (<code>int | None</code>, default:                 <code>None</code> )         \u2013          <p>The number of samples per class. Defaults to None.</p> </li> <li> exclude_good             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>If True, exclude good samples from the dataset. Defaults to False.</p> </li> </ul> Source code in <code>quadra/datamodules/segmentation.py</code> <pre><code>def __init__(\n    self,\n    data_path: str,\n    name: str = \"segmentation_datamodule\",\n    test_size: float = 0.3,\n    val_size: float = 0.3,\n    seed: int = 42,\n    dataset: type[SegmentationDataset] = SegmentationDataset,\n    batch_size: int = 32,\n    num_workers: int = 6,\n    train_transform: albumentations.Compose | None = None,\n    test_transform: albumentations.Compose | None = None,\n    val_transform: albumentations.Compose | None = None,\n    train_split_file: str | None = None,\n    test_split_file: str | None = None,\n    val_split_file: str | None = None,\n    num_data_class: int | None = None,\n    exclude_good: bool = False,\n    **kwargs: Any,\n):\n    super().__init__(\n        data_path=data_path,\n        name=name,\n        seed=seed,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        train_transform=train_transform,\n        val_transform=val_transform,\n        test_transform=test_transform,\n        **kwargs,\n    )\n    self.test_size = test_size\n    self.val_size = val_size\n    self.num_data_class = num_data_class\n    self.exclude_good = exclude_good\n    self.train_split_file = train_split_file\n    self.test_split_file = test_split_file\n    self.val_split_file = val_split_file\n    self.dataset = dataset\n    self.train_dataset: SegmentationDataset\n    self.val_dataset: SegmentationDataset\n    self.test_dataset: SegmentationDataset\n</code></pre>"},{"location":"reference/quadra/datamodules/index.html#quadra.datamodules.SegmentationDataModule.predict_dataloader","title":"<code>predict_dataloader()</code>","text":"<p>Returns a dataloader used for predictions.</p> Source code in <code>quadra/datamodules/segmentation.py</code> <pre><code>def predict_dataloader(self) -&gt; DataLoader:\n\"\"\"Returns a dataloader used for predictions.\"\"\"\n    return self.test_dataloader()\n</code></pre>"},{"location":"reference/quadra/datamodules/index.html#quadra.datamodules.SegmentationDataModule.setup","title":"<code>setup(stage=None)</code>","text":"<p>Setup data module based on stages of training.</p> Source code in <code>quadra/datamodules/segmentation.py</code> <pre><code>def setup(self, stage=None):\n\"\"\"Setup data module based on stages of training.\"\"\"\n    if stage in [\"fit\", \"train\"]:\n        self.train_dataset = self.dataset(\n            image_paths=self.data[self.data[\"split\"] == \"train\"][\"samples\"].tolist(),\n            mask_paths=self.data[self.data[\"split\"] == \"train\"][\"masks\"].tolist(),\n            mask_preprocess=self._preprocess_mask,\n            labels=self.data[self.data[\"split\"] == \"train\"][\"targets\"].tolist(),\n            object_masks=None,\n            transform=self.train_transform,\n            batch_size=None,\n            defect_transform=None,\n            resize=None,\n        )\n        self.val_dataset = self.dataset(\n            image_paths=self.data[self.data[\"split\"] == \"val\"][\"samples\"].tolist(),\n            mask_paths=self.data[self.data[\"split\"] == \"val\"][\"masks\"].tolist(),\n            defect_transform=None,\n            labels=self.data[self.data[\"split\"] == \"val\"][\"targets\"].tolist(),\n            object_masks=None,\n            batch_size=None,\n            mask_preprocess=self._preprocess_mask,\n            transform=self.test_transform,\n            resize=None,\n        )\n    elif stage == \"test\":\n        self.test_dataset = self.dataset(\n            image_paths=self.data[self.data[\"split\"] == \"test\"][\"samples\"].tolist(),\n            mask_paths=self.data[self.data[\"split\"] == \"test\"][\"masks\"].tolist(),\n            labels=self.data[self.data[\"split\"] == \"test\"][\"targets\"].tolist(),\n            object_masks=None,\n            batch_size=None,\n            mask_preprocess=self._preprocess_mask,\n            transform=self.test_transform,\n            resize=None,\n        )\n    elif stage == \"predict\":\n        pass\n    else:\n        raise ValueError(f\"Unknown stage {stage}\")\n</code></pre>"},{"location":"reference/quadra/datamodules/index.html#quadra.datamodules.SegmentationDataModule.test_dataloader","title":"<code>test_dataloader()</code>","text":"<p>Returns the test dataloader.</p> <p>Raises:</p> <ul> <li> <code>ValueError</code>           \u2013          <p>If test dataset is not initialized.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataLoader</code>         \u2013          <p>test dataloader.</p> </li> </ul> Source code in <code>quadra/datamodules/segmentation.py</code> <pre><code>def test_dataloader(self) -&gt; DataLoader:\n\"\"\"Returns the test dataloader.\n\n    Raises:\n        ValueError: If test dataset is not initialized.\n\n\n    Returns:\n        test dataloader.\n    \"\"\"\n    if not self.test_dataset_available:\n        raise ValueError(\"Test dataset is not initialized\")\n\n    loader = DataLoader(\n        self.test_dataset,\n        batch_size=self.batch_size,\n        shuffle=False,\n        num_workers=self.num_workers,\n        drop_last=False,\n        pin_memory=True,\n        persistent_workers=self.num_workers &gt; 0,\n    )\n    return loader\n</code></pre>"},{"location":"reference/quadra/datamodules/index.html#quadra.datamodules.SegmentationDataModule.train_dataloader","title":"<code>train_dataloader()</code>","text":"<p>Returns the train dataloader.</p> <p>Raises:</p> <ul> <li> <code>ValueError</code>           \u2013          <p>If train dataset is not initialized.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataLoader</code>         \u2013          <p>Train dataloader.</p> </li> </ul> Source code in <code>quadra/datamodules/segmentation.py</code> <pre><code>def train_dataloader(self) -&gt; DataLoader:\n\"\"\"Returns the train dataloader.\n\n    Raises:\n        ValueError: If train dataset is not initialized.\n\n    Returns:\n        Train dataloader.\n    \"\"\"\n    if not self.train_dataset_available:\n        raise ValueError(\"Train dataset is not initialized\")\n\n    return DataLoader(\n        self.train_dataset,\n        batch_size=self.batch_size,\n        shuffle=True,\n        num_workers=self.num_workers,\n        drop_last=False,\n        pin_memory=True,\n        persistent_workers=self.num_workers &gt; 0,\n    )\n</code></pre>"},{"location":"reference/quadra/datamodules/index.html#quadra.datamodules.SegmentationDataModule.val_dataloader","title":"<code>val_dataloader()</code>","text":"<p>Returns the validation dataloader.</p> <p>Raises:</p> <ul> <li> <code>ValueError</code>           \u2013          <p>If validation dataset is not initialized.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataLoader</code>         \u2013          <p>val dataloader.</p> </li> </ul> Source code in <code>quadra/datamodules/segmentation.py</code> <pre><code>def val_dataloader(self) -&gt; DataLoader:\n\"\"\"Returns the validation dataloader.\n\n    Raises:\n        ValueError: If validation dataset is not initialized.\n\n    Returns:\n        val dataloader.\n    \"\"\"\n    if not self.val_dataset_available:\n        raise ValueError(\"Validation dataset is not initialized\")\n\n    return DataLoader(\n        self.val_dataset,\n        batch_size=self.batch_size,\n        shuffle=False,\n        num_workers=self.num_workers,\n        drop_last=False,\n        pin_memory=True,\n        persistent_workers=self.num_workers &gt; 0,\n    )\n</code></pre>"},{"location":"reference/quadra/datamodules/index.html#quadra.datamodules.SegmentationMulticlassDataModule","title":"<code>SegmentationMulticlassDataModule(data_path, idx_to_class, name='multiclass_segmentation_datamodule', dataset=SegmentationDatasetMulticlass, batch_size=32, test_size=0.3, val_size=0.3, seed=42, num_workers=6, train_transform=None, test_transform=None, val_transform=None, train_split_file=None, test_split_file=None, val_split_file=None, exclude_good=False, num_data_train=None, one_hot_encoding=False, **kwargs)</code>","text":"<p>             Bases: <code>BaseDataModule</code></p> <p>Base class for segmentation datasets with multiple classes.</p> <p>Parameters:</p> <ul> <li> data_path         \u2013          <p>Path to the data main folder.</p> </li> <li> idx_to_class             (<code>dict</code>)         \u2013          <p>dict with corrispondence btw mask index and classes: {1: class_1, 2: class_2, ..., N: class_N} except background class which is 0.</p> </li> <li> name         \u2013          <p>The name for the data module. Defaults to \"multiclass_segmentation_datamodule\".</p> </li> <li> dataset             (<code>type[SegmentationDatasetMulticlass]</code>, default:                 <code>SegmentationDatasetMulticlass</code> )         \u2013          <p>Dataset class.</p> </li> <li> batch_size         \u2013          <p>Batch size. Defaults to 32.</p> </li> <li> val_size         \u2013          <p>The validation split. Defaults to 0.3.</p> </li> <li> test_size         \u2013          <p>The test split. Defaults to 0.3.</p> </li> <li> seed         \u2013          <p>Random generator seed. Defaults to 42.</p> </li> <li> num_workers             (<code>int</code>, default:                 <code>6</code> )         \u2013          <p>Number of workers for dataloaders. Defaults to 6.</p> </li> <li> train_transform             (<code>Compose | None</code>, default:                 <code>None</code> )         \u2013          <p>Transformations for train dataset. Defaults to None.</p> </li> <li> val_transform         \u2013          <p>Transformations for validation dataset. Defaults to None.</p> </li> <li> test_transform         \u2013          <p>Transformations for test dataset. Defaults to None.</p> </li> <li> train_split_file             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>path to txt file with training samples list</p> </li> <li> val_split_file             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>path to txt file with validation samples list</p> </li> <li> test_split_file             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>path to txt file with test samples list</p> </li> <li> exclude_good         \u2013          <p>If True, exclude good samples from the dataset. Defaults to False.</p> </li> <li> num_data_train             (<code>int | None</code>, default:                 <code>None</code> )         \u2013          <p>number of samples to use in the train split (shuffle the samples and pick the first num_data_train)</p> </li> <li> one_hot_encoding             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>if True, the labels are one-hot encoded to N channels, where N is the number of classes. If False, masks are single channel that contains values as class indexes. Defaults to True.</p> </li> </ul> Source code in <code>quadra/datamodules/segmentation.py</code> <pre><code>def __init__(\n    self,\n    data_path: str,\n    idx_to_class: dict,\n    name: str = \"multiclass_segmentation_datamodule\",\n    dataset: type[SegmentationDatasetMulticlass] = SegmentationDatasetMulticlass,\n    batch_size: int = 32,\n    test_size: float = 0.3,\n    val_size: float = 0.3,\n    seed: int = 42,\n    num_workers: int = 6,\n    train_transform: albumentations.Compose | None = None,\n    test_transform: albumentations.Compose | None = None,\n    val_transform: albumentations.Compose | None = None,\n    train_split_file: str | None = None,\n    test_split_file: str | None = None,\n    val_split_file: str | None = None,\n    exclude_good: bool = False,\n    num_data_train: int | None = None,\n    one_hot_encoding: bool = False,\n    **kwargs: Any,\n):\n    super().__init__(\n        data_path=data_path,\n        name=name,\n        seed=seed,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        train_transform=train_transform,\n        val_transform=val_transform,\n        test_transform=test_transform,\n        **kwargs,\n    )\n    self.test_size = test_size\n    self.val_size = val_size\n    self.exclude_good = exclude_good\n    self.train_split_file = train_split_file\n    self.test_split_file = test_split_file\n    self.val_split_file = val_split_file\n    self.dataset = dataset\n    self.idx_to_class = idx_to_class\n    self.num_data_train = num_data_train\n    self.one_hot_encoding = one_hot_encoding\n    self.train_dataset: SegmentationDataset\n    self.val_dataset: SegmentationDataset\n    self.test_dataset: SegmentationDataset\n</code></pre>"},{"location":"reference/quadra/datamodules/index.html#quadra.datamodules.SegmentationMulticlassDataModule.predict_dataloader","title":"<code>predict_dataloader()</code>","text":"<p>Returns a dataloader used for predictions.</p> Source code in <code>quadra/datamodules/segmentation.py</code> <pre><code>def predict_dataloader(self) -&gt; DataLoader:\n\"\"\"Returns a dataloader used for predictions.\"\"\"\n    return self.test_dataloader()\n</code></pre>"},{"location":"reference/quadra/datamodules/index.html#quadra.datamodules.SegmentationMulticlassDataModule.setup","title":"<code>setup(stage=None)</code>","text":"<p>Setup data module based on stages of training.</p> Source code in <code>quadra/datamodules/segmentation.py</code> <pre><code>def setup(self, stage=None):\n\"\"\"Setup data module based on stages of training.\"\"\"\n    if stage in [\"fit\", \"train\"]:\n        train_data = self.data[self.data[\"split\"] == \"train\"]\n        val_data = self.data[self.data[\"split\"] == \"val\"]\n\n        self.train_dataset = self.dataset(\n            image_paths=train_data[\"samples\"].tolist(),\n            mask_paths=train_data[\"masks\"].tolist(),\n            idx_to_class=self.idx_to_class,\n            transform=self.train_transform,\n            one_hot=self.one_hot_encoding,\n        )\n        self.val_dataset = self.dataset(\n            image_paths=val_data[\"samples\"].tolist(),\n            mask_paths=val_data[\"masks\"].tolist(),\n            transform=self.val_transform,\n            idx_to_class=self.idx_to_class,\n            one_hot=self.one_hot_encoding,\n        )\n    elif stage == \"test\":\n        self.test_dataset = self.dataset(\n            image_paths=self.data[self.data[\"split\"] == \"test\"][\"samples\"].tolist(),\n            mask_paths=self.data[self.data[\"split\"] == \"test\"][\"masks\"].tolist(),\n            transform=self.test_transform,\n            idx_to_class=self.idx_to_class,\n            one_hot=self.one_hot_encoding,\n        )\n    elif stage == \"predict\":\n        pass\n    else:\n        raise ValueError(f\"Unknown stage {stage}\")\n</code></pre>"},{"location":"reference/quadra/datamodules/index.html#quadra.datamodules.SegmentationMulticlassDataModule.test_dataloader","title":"<code>test_dataloader()</code>","text":"<p>Returns the test dataloader.</p> <p>Raises:</p> <ul> <li> <code>ValueError</code>           \u2013          <p>If test dataset is not initialized.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataLoader</code>         \u2013          <p>test dataloader.</p> </li> </ul> Source code in <code>quadra/datamodules/segmentation.py</code> <pre><code>def test_dataloader(self) -&gt; DataLoader:\n\"\"\"Returns the test dataloader.\n\n    Raises:\n        ValueError: If test dataset is not initialized.\n\n\n    Returns:\n        test dataloader.\n    \"\"\"\n    if not self.test_dataset_available:\n        raise ValueError(\"Test dataset is not initialized\")\n\n    loader = DataLoader(\n        self.test_dataset,\n        batch_size=self.batch_size,\n        shuffle=False,\n        num_workers=self.num_workers,\n        drop_last=False,\n        pin_memory=True,\n        persistent_workers=self.num_workers &gt; 0,\n    )\n    return loader\n</code></pre>"},{"location":"reference/quadra/datamodules/index.html#quadra.datamodules.SegmentationMulticlassDataModule.train_dataloader","title":"<code>train_dataloader()</code>","text":"<p>Returns the train dataloader.</p> <p>Raises:</p> <ul> <li> <code>ValueError</code>           \u2013          <p>If train dataset is not initialized.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataLoader</code>         \u2013          <p>Train dataloader.</p> </li> </ul> Source code in <code>quadra/datamodules/segmentation.py</code> <pre><code>def train_dataloader(self) -&gt; DataLoader:\n\"\"\"Returns the train dataloader.\n\n    Raises:\n        ValueError: If train dataset is not initialized.\n\n    Returns:\n        Train dataloader.\n    \"\"\"\n    if not self.train_dataset_available:\n        raise ValueError(\"Train dataset is not initialized\")\n\n    return DataLoader(\n        self.train_dataset,\n        batch_size=self.batch_size,\n        shuffle=True,\n        num_workers=self.num_workers,\n        drop_last=False,\n        pin_memory=True,\n        persistent_workers=self.num_workers &gt; 0,\n    )\n</code></pre>"},{"location":"reference/quadra/datamodules/index.html#quadra.datamodules.SegmentationMulticlassDataModule.val_dataloader","title":"<code>val_dataloader()</code>","text":"<p>Returns the validation dataloader.</p> <p>Raises:</p> <ul> <li> <code>ValueError</code>           \u2013          <p>If validation dataset is not initialized.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataLoader</code>         \u2013          <p>val dataloader.</p> </li> </ul> Source code in <code>quadra/datamodules/segmentation.py</code> <pre><code>def val_dataloader(self) -&gt; DataLoader:\n\"\"\"Returns the validation dataloader.\n\n    Raises:\n        ValueError: If validation dataset is not initialized.\n\n    Returns:\n        val dataloader.\n    \"\"\"\n    if not self.val_dataset_available:\n        raise ValueError(\"Validation dataset is not initialized\")\n\n    return DataLoader(\n        self.val_dataset,\n        batch_size=self.batch_size,\n        shuffle=False,\n        num_workers=self.num_workers,\n        drop_last=False,\n        pin_memory=True,\n        persistent_workers=self.num_workers &gt; 0,\n    )\n</code></pre>"},{"location":"reference/quadra/datamodules/index.html#quadra.datamodules.SklearnClassificationDataModule","title":"<code>SklearnClassificationDataModule(data_path, exclude_filter=None, include_filter=None, val_size=0.2, class_to_idx=None, label_map=None, seed=42, batch_size=32, num_workers=6, train_transform=None, val_transform=None, test_transform=None, roi=None, n_splits=1, phase='train', cache=False, limit_training_data=None, train_split_file=None, test_split_file=None, name='sklearn_classification_datamodule', dataset=ImageClassificationListDataset, **kwargs)</code>","text":"<p>             Bases: <code>BaseDataModule</code></p> <p>A generic Data Module for classification with frozen torch backbone and sklearn classifier.</p> <p>It can also handle k-fold cross validation.</p> <p>Parameters:</p> <ul> <li> name             (<code>str</code>, default:                 <code>'sklearn_classification_datamodule'</code> )         \u2013          <p>The name for the data module. Defaults to \"sklearn_classification_datamodule\".</p> </li> <li> data_path             (<code>str</code>)         \u2013          <p>Path to images main folder</p> </li> <li> exclude_filter             (<code>list[str] | None</code>, default:                 <code>None</code> )         \u2013          <p>List of string filter to be used to exclude images. If None no filter will be applied.</p> </li> <li> include_filter             (<code>list[str] | None</code>, default:                 <code>None</code> )         \u2013          <p>List of string filter to be used to include images. Only images that satisfied at list one of             the filter will be included.</p> </li> <li> val_size             (<code>float</code>, default:                 <code>0.2</code> )         \u2013          <p>The validation split. Defaults to 0.2.</p> </li> <li> class_to_idx             (<code>dict[str, int] | None</code>, default:                 <code>None</code> )         \u2013          <p>Dictionary of conversion btw folder name and index. Only file whose label is in dictionary key list will be considered. If None all files will be considered and a custom conversion is created.</p> </li> <li> seed             (<code>int</code>, default:                 <code>42</code> )         \u2013          <p>Fixed seed for random operations</p> </li> <li> batch_size             (<code>int</code>, default:                 <code>32</code> )         \u2013          <p>Dimension of batches for dataloader</p> </li> <li> num_workers             (<code>int</code>, default:                 <code>6</code> )         \u2013          <p>Number of workers for dataloader</p> </li> <li> train_transform             (<code>Compose | None</code>, default:                 <code>None</code> )         \u2013          <p>Albumentation transformations for training set</p> </li> <li> val_transform             (<code>Compose | None</code>, default:                 <code>None</code> )         \u2013          <p>Albumentation transformations for validation set</p> </li> <li> test_transform             (<code>Compose | None</code>, default:                 <code>None</code> )         \u2013          <p>Albumentation transformations for test set</p> </li> <li> roi             (<code>tuple[int, int, int, int] | None</code>, default:                 <code>None</code> )         \u2013          <p>Optional cropping region</p> </li> <li> n_splits             (<code>int</code>, default:                 <code>1</code> )         \u2013          <p>Number of dataset subdivision (default 1 -&gt; train/test). Use a value &gt;= 2 for cross validation.</p> </li> <li> phase             (<code>str</code>, default:                 <code>'train'</code> )         \u2013          <p>Either train or test</p> </li> <li> cache             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>If true disable shuffling in all dataloader to enable feature caching</p> </li> <li> limit_training_data             (<code>int | None</code>, default:                 <code>None</code> )         \u2013          <p>if defined, each class will be donwsampled to this number. It must be &gt;= 2 to allow splitting</p> </li> <li> label_map             (<code>dict[str, Any] | None</code>, default:                 <code>None</code> )         \u2013          <p>Dictionary of conversion btw folder name and label.</p> </li> <li> train_split_file             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>Optional path to a csv file containing the train split samples.</p> </li> <li> test_split_file             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>Optional path to a csv file containing the test split samples.</p> </li> <li> **kwargs             (<code>Any</code>, default:                 <code>{}</code> )         \u2013          <p>Additional arguments for BaseDataModule</p> </li> </ul> Source code in <code>quadra/datamodules/classification.py</code> <pre><code>def __init__(\n    self,\n    data_path: str,\n    exclude_filter: list[str] | None = None,\n    include_filter: list[str] | None = None,\n    val_size: float = 0.2,\n    class_to_idx: dict[str, int] | None = None,\n    label_map: dict[str, Any] | None = None,\n    seed: int = 42,\n    batch_size: int = 32,\n    num_workers: int = 6,\n    train_transform: albumentations.Compose | None = None,\n    val_transform: albumentations.Compose | None = None,\n    test_transform: albumentations.Compose | None = None,\n    roi: tuple[int, int, int, int] | None = None,\n    n_splits: int = 1,\n    phase: str = \"train\",\n    cache: bool = False,\n    limit_training_data: int | None = None,\n    train_split_file: str | None = None,\n    test_split_file: str | None = None,\n    name: str = \"sklearn_classification_datamodule\",\n    dataset: type[ImageClassificationListDataset] = ImageClassificationListDataset,\n    **kwargs: Any,\n):\n    super().__init__(\n        data_path=data_path,\n        name=name,\n        seed=seed,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        train_transform=train_transform,\n        val_transform=val_transform,\n        test_transform=test_transform,\n        **kwargs,\n    )\n\n    self.class_to_idx = class_to_idx\n    self.roi = roi\n    self.cache = cache\n    self.limit_training_data = limit_training_data\n\n    self.dataset = dataset\n    self.phase = phase\n    self.n_splits = n_splits\n    self.train_split_file = train_split_file\n    self.test_split_file = test_split_file\n    self.exclude_filter = exclude_filter\n    self.include_filter = include_filter\n    self.val_size = val_size\n    self.label_map = label_map\n    self.full_dataset: ImageClassificationListDataset\n    self.train_dataset: list[ImageClassificationListDataset]\n    self.val_dataset: list[ImageClassificationListDataset]\n</code></pre>"},{"location":"reference/quadra/datamodules/index.html#quadra.datamodules.SklearnClassificationDataModule.full_dataloader","title":"<code>full_dataloader()</code>","text":"<p>Return a dataloader to perform training on the entire dataset.</p> <p>Returns:</p> <ul> <li> <code>DataLoader</code>         \u2013          <p>dataloader to perform training on the entire dataset after evaluation. This is useful</p> </li> <li> <code>DataLoader</code>         \u2013          <p>to perform a final training on the entire dataset after the evaluation phase.</p> </li> </ul> Source code in <code>quadra/datamodules/classification.py</code> <pre><code>def full_dataloader(self) -&gt; DataLoader:\n\"\"\"Return a dataloader to perform training on the entire dataset.\n\n    Returns:\n        dataloader to perform training on the entire dataset after evaluation. This is useful\n        to perform a final training on the entire dataset after the evaluation phase.\n\n    \"\"\"\n    if self.full_dataset is None:\n        raise ValueError(\"Full dataset is not initialized\")\n\n    return DataLoader(\n        self.full_dataset,\n        batch_size=self.batch_size,\n        shuffle=not self.cache,\n        num_workers=self.num_workers,\n        drop_last=False,\n        pin_memory=True,\n    )\n</code></pre>"},{"location":"reference/quadra/datamodules/index.html#quadra.datamodules.SklearnClassificationDataModule.predict_dataloader","title":"<code>predict_dataloader()</code>","text":"<p>Returns a dataloader used for predictions.</p> Source code in <code>quadra/datamodules/classification.py</code> <pre><code>def predict_dataloader(self) -&gt; DataLoader:\n\"\"\"Returns a dataloader used for predictions.\"\"\"\n    return self.test_dataloader()\n</code></pre>"},{"location":"reference/quadra/datamodules/index.html#quadra.datamodules.SklearnClassificationDataModule.setup","title":"<code>setup(stage)</code>","text":"<p>Setup data module based on stages of training.</p> Source code in <code>quadra/datamodules/classification.py</code> <pre><code>def setup(self, stage: str) -&gt; None:\n\"\"\"Setup data module based on stages of training.\"\"\"\n    if stage == \"fit\":\n        self.train_dataset = []\n        self.val_dataset = []\n\n        for cv_idx in range(self.n_splits):\n            cv_df = self.data[self.data[\"cv\"] == cv_idx]\n            train_samples = cv_df[cv_df[\"split\"] == \"train\"][\"samples\"].tolist()\n            train_targets = cv_df[cv_df[\"split\"] == \"train\"][\"targets\"].tolist()\n            val_samples = cv_df[cv_df[\"split\"] == \"val\"][\"samples\"].tolist()\n            val_targets = cv_df[cv_df[\"split\"] == \"val\"][\"targets\"].tolist()\n            self.train_dataset.append(\n                self.dataset(\n                    class_to_idx=self.class_to_idx,\n                    samples=train_samples,\n                    targets=train_targets,\n                    transform=self.train_transform,\n                    roi=self.roi,\n                )\n            )\n            self.val_dataset.append(\n                self.dataset(\n                    class_to_idx=self.class_to_idx,\n                    samples=val_samples,\n                    targets=val_targets,\n                    transform=self.val_transform,\n                    roi=self.roi,\n                )\n            )\n        all_samples = self.data[self.data[\"cv\"] == 0][\"samples\"].tolist()\n        all_targets = self.data[self.data[\"cv\"] == 0][\"targets\"].tolist()\n        self.full_dataset = self.dataset(\n            class_to_idx=self.class_to_idx,\n            samples=all_samples,\n            targets=all_targets,\n            transform=self.train_transform,\n            roi=self.roi,\n        )\n    if stage == \"test\":\n        test_samples = self.data[self.data[\"split\"] == \"test\"][\"samples\"].tolist()\n        test_targets = self.data[self.data[\"split\"] == \"test\"][\"targets\"]\n        self.test_dataset = self.dataset(\n            class_to_idx=self.class_to_idx,\n            samples=test_samples,\n            targets=test_targets.tolist(),\n            transform=self.test_transform,\n            roi=self.roi,\n            allow_missing_label=True,\n        )\n</code></pre>"},{"location":"reference/quadra/datamodules/index.html#quadra.datamodules.SklearnClassificationDataModule.test_dataloader","title":"<code>test_dataloader()</code>","text":"<p>Returns the test dataloader.</p> <p>Raises:</p> <ul> <li> <code>ValueError</code>           \u2013          <p>If test dataset is not initialized.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataLoader</code>         \u2013          <p>test dataloader.</p> </li> </ul> Source code in <code>quadra/datamodules/classification.py</code> <pre><code>def test_dataloader(self) -&gt; DataLoader:\n\"\"\"Returns the test dataloader.\n\n    Raises:\n        ValueError: If test dataset is not initialized.\n\n\n    Returns:\n        test dataloader.\n    \"\"\"\n    if not self.test_dataset_available:\n        raise ValueError(\"Test dataset is not initialized\")\n\n    loader = DataLoader(\n        self.test_dataset,\n        batch_size=self.batch_size,\n        shuffle=False,\n        num_workers=self.num_workers,\n        drop_last=False,\n        pin_memory=True,\n        persistent_workers=self.num_workers &gt; 0,\n    )\n    return loader\n</code></pre>"},{"location":"reference/quadra/datamodules/index.html#quadra.datamodules.SklearnClassificationDataModule.train_dataloader","title":"<code>train_dataloader()</code>","text":"<p>Returns a list of train dataloader.</p> <p>Raises:</p> <ul> <li> <code>ValueError</code>           \u2013          <p>If train dataset is not initialized.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[DataLoader]</code>         \u2013          <p>list of train dataloader.</p> </li> </ul> Source code in <code>quadra/datamodules/classification.py</code> <pre><code>def train_dataloader(self) -&gt; list[DataLoader]:\n\"\"\"Returns a list of train dataloader.\n\n    Raises:\n        ValueError: If train dataset is not initialized.\n\n    Returns:\n        list of train dataloader.\n    \"\"\"\n    if not self.train_dataset_available:\n        raise ValueError(\"Train dataset is not initialized\")\n\n    loader = []\n    for dataset in self.train_dataset:\n        loader.append(\n            DataLoader(\n                dataset,\n                batch_size=self.batch_size,\n                shuffle=not self.cache,\n                num_workers=self.num_workers,\n                drop_last=False,\n                pin_memory=True,\n            )\n        )\n    return loader\n</code></pre>"},{"location":"reference/quadra/datamodules/index.html#quadra.datamodules.SklearnClassificationDataModule.val_dataloader","title":"<code>val_dataloader()</code>","text":"<p>Returns a list of validation dataloader.</p> <p>Raises:</p> <ul> <li> <code>ValueError</code>           \u2013          <p>If validation dataset is not initialized.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[DataLoader]</code>         \u2013          <p>List of validation dataloader.</p> </li> </ul> Source code in <code>quadra/datamodules/classification.py</code> <pre><code>def val_dataloader(self) -&gt; list[DataLoader]:\n\"\"\"Returns a list of validation dataloader.\n\n    Raises:\n        ValueError: If validation dataset is not initialized.\n\n    Returns:\n        List of validation dataloader.\n    \"\"\"\n    if not self.val_dataset_available:\n        raise ValueError(\"Validation dataset is not initialized\")\n\n    loader = []\n    for dataset in self.val_dataset:\n        loader.append(\n            DataLoader(\n                dataset,\n                batch_size=self.batch_size,\n                shuffle=False,\n                num_workers=self.num_workers,\n                drop_last=False,\n                pin_memory=True,\n            )\n        )\n\n    return loader\n</code></pre>"},{"location":"reference/quadra/datamodules/anomaly.html","title":"anomaly","text":""},{"location":"reference/quadra/datamodules/anomaly.html#quadra.datamodules.anomaly.AnomalyDataModule","title":"<code>AnomalyDataModule(data_path, category=None, image_size=None, train_batch_size=32, test_batch_size=32, num_workers=8, train_transform=None, val_transform=None, test_transform=None, seed=0, task='segmentation', mask_suffix=None, create_test_set_if_empty=True, phase='train', name='anomaly_datamodule', valid_area_mask=None, crop_area=None, **kwargs)</code>","text":"<p>             Bases: <code>BaseDataModule</code></p> <p>Anomalib-like Lightning Data Module.</p> <p>Parameters:</p> <ul> <li> data_path             (<code>str</code>)         \u2013          <p>Path to the dataset</p> </li> <li> category             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>Name of the sub category to use.</p> </li> <li> image_size             (<code>int | tuple[int, int] | None</code>, default:                 <code>None</code> )         \u2013          <p>Variable to which image is resized.</p> </li> <li> train_batch_size             (<code>int</code>, default:                 <code>32</code> )         \u2013          <p>Training batch size.</p> </li> <li> test_batch_size             (<code>int</code>, default:                 <code>32</code> )         \u2013          <p>Testing batch size.</p> </li> <li> train_transform             (<code>Compose | None</code>, default:                 <code>None</code> )         \u2013          <p>transformations for training. Defaults to None.</p> </li> <li> val_transform             (<code>Compose | None</code>, default:                 <code>None</code> )         \u2013          <p>transformations for validation. Defaults to None.</p> </li> <li> test_transform             (<code>Compose | None</code>, default:                 <code>None</code> )         \u2013          <p>transformations for testing. Defaults to None.</p> </li> <li> num_workers             (<code>int</code>, default:                 <code>8</code> )         \u2013          <p>Number of workers.</p> </li> <li> seed             (<code>int</code>, default:                 <code>0</code> )         \u2013          <p>seed used for the random subset splitting</p> </li> <li> task             (<code>str</code>, default:                 <code>'segmentation'</code> )         \u2013          <p>Whether we are interested in segmenting the anomalies (segmentation) or not (classification)</p> </li> <li> mask_suffix             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>String to append to the base filename to get the mask name, by default for MVTec dataset masks are saved as imagename_mask.png in this case the parameter should be filled with \"_mask\"</p> </li> <li> create_test_set_if_empty             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>If True, the test set is created from good images if it is empty.</p> </li> <li> phase             (<code>str</code>, default:                 <code>'train'</code> )         \u2013          <p>Either train or test.</p> </li> <li> name             (<code>str</code>, default:                 <code>'anomaly_datamodule'</code> )         \u2013          <p>Name of the data module.</p> </li> <li> valid_area_mask             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>Optional path to the mask to use to filter out the valid area of the image. If None, the whole image is considered valid. The mask should match the image size even if the image is cropped.</p> </li> <li> crop_area             (<code>tuple[int, int, int, int] | None</code>, default:                 <code>None</code> )         \u2013          <p>Optional tuple of 4 integers (x1, y1, x2, y2) to crop the image to the specified area. If None, the whole image is considered valid.</p> </li> </ul> Source code in <code>quadra/datamodules/anomaly.py</code> <pre><code>def __init__(\n    self,\n    data_path: str,\n    category: str | None = None,\n    image_size: int | tuple[int, int] | None = None,\n    train_batch_size: int = 32,\n    test_batch_size: int = 32,\n    num_workers: int = 8,\n    train_transform: albumentations.Compose | None = None,\n    val_transform: albumentations.Compose | None = None,\n    test_transform: albumentations.Compose | None = None,\n    seed: int = 0,\n    task: str = \"segmentation\",\n    mask_suffix: str | None = None,\n    create_test_set_if_empty: bool = True,\n    phase: str = \"train\",\n    name: str = \"anomaly_datamodule\",\n    valid_area_mask: str | None = None,\n    crop_area: tuple[int, int, int, int] | None = None,\n    **kwargs,\n) -&gt; None:\n    super().__init__(\n        data_path=data_path,\n        name=name,\n        seed=seed,\n        train_transform=train_transform,\n        val_transform=val_transform,\n        test_transform=test_transform,\n        num_workers=num_workers,\n        **kwargs,\n    )\n\n    self.root = data_path\n    self.category = category\n    self.data_path = os.path.join(self.root, self.category) if self.category is not None else self.root\n    self.image_size = image_size\n\n    self.train_batch_size = train_batch_size\n    self.test_batch_size = test_batch_size\n    self.task = task\n\n    self.train_dataset: AnomalyDataset\n    self.test_dataset: AnomalyDataset\n    self.val_dataset: AnomalyDataset\n    self.mask_suffix = mask_suffix\n    self.create_test_set_if_empty = create_test_set_if_empty\n    self.phase = phase\n    self.valid_area_mask = valid_area_mask\n    self.crop_area = crop_area\n</code></pre>"},{"location":"reference/quadra/datamodules/anomaly.html#quadra.datamodules.anomaly.AnomalyDataModule.val_data","title":"<code>val_data: pd.DataFrame</code>  <code>property</code>","text":"<p>Get validation data.</p>"},{"location":"reference/quadra/datamodules/anomaly.html#quadra.datamodules.anomaly.AnomalyDataModule.predict_dataloader","title":"<code>predict_dataloader()</code>","text":"<p>Returns a dataloader used for predictions.</p> Source code in <code>quadra/datamodules/anomaly.py</code> <pre><code>def predict_dataloader(self) -&gt; DataLoader:\n\"\"\"Returns a dataloader used for predictions.\"\"\"\n    return DataLoader(\n        self.test_dataset,\n        shuffle=False,\n        batch_size=self.test_batch_size,\n        num_workers=self.num_workers,\n        pin_memory=True,\n    )\n</code></pre>"},{"location":"reference/quadra/datamodules/anomaly.html#quadra.datamodules.anomaly.AnomalyDataModule.setup","title":"<code>setup(stage=None)</code>","text":"<p>Setup data module based on stages of training.</p> Source code in <code>quadra/datamodules/anomaly.py</code> <pre><code>def setup(self, stage: str | None = None) -&gt; None:\n\"\"\"Setup data module based on stages of training.\"\"\"\n    if stage == \"fit\" and self.phase == \"train\":\n        self.train_dataset = AnomalyDataset(\n            transform=self.train_transform,\n            task=self.task,\n            samples=self.train_data,\n            valid_area_mask=self.valid_area_mask,\n            crop_area=self.crop_area,\n        )\n\n        if len(self.val_data) == 0:\n            log.info(\"Validation dataset is empty, using test set instead\")\n\n        self.val_dataset = AnomalyDataset(\n            transform=self.test_transform,\n            task=self.task,\n            samples=self.val_data if len(self.val_data) &gt; 0 else self.data,\n            valid_area_mask=self.valid_area_mask,\n            crop_area=self.crop_area,\n        )\n    if stage == \"test\" or self.phase == \"test\":\n        self.test_dataset = AnomalyDataset(\n            transform=self.test_transform,\n            task=self.task,\n            samples=self.test_data,\n            valid_area_mask=self.valid_area_mask,\n            crop_area=self.crop_area,\n        )\n</code></pre>"},{"location":"reference/quadra/datamodules/anomaly.html#quadra.datamodules.anomaly.AnomalyDataModule.test_dataloader","title":"<code>test_dataloader()</code>","text":"<p>Get test dataloader.</p> Source code in <code>quadra/datamodules/anomaly.py</code> <pre><code>def test_dataloader(self) -&gt; DataLoader:\n\"\"\"Get test dataloader.\"\"\"\n    return DataLoader(\n        self.test_dataset,\n        shuffle=False,\n        batch_size=self.test_batch_size,\n        num_workers=self.num_workers,\n        pin_memory=True,\n    )\n</code></pre>"},{"location":"reference/quadra/datamodules/anomaly.html#quadra.datamodules.anomaly.AnomalyDataModule.train_dataloader","title":"<code>train_dataloader()</code>","text":"<p>Get train dataloader.</p> Source code in <code>quadra/datamodules/anomaly.py</code> <pre><code>def train_dataloader(self) -&gt; DataLoader:\n\"\"\"Get train dataloader.\"\"\"\n    return DataLoader(\n        self.train_dataset,\n        shuffle=True,\n        batch_size=self.train_batch_size,\n        num_workers=self.num_workers,\n        pin_memory=True,\n    )\n</code></pre>"},{"location":"reference/quadra/datamodules/anomaly.html#quadra.datamodules.anomaly.AnomalyDataModule.val_dataloader","title":"<code>val_dataloader()</code>","text":"<p>Get validation dataloader.</p> Source code in <code>quadra/datamodules/anomaly.py</code> <pre><code>def val_dataloader(self) -&gt; DataLoader:\n\"\"\"Get validation dataloader.\"\"\"\n    return DataLoader(\n        dataset=self.val_dataset,\n        shuffle=False,\n        batch_size=self.test_batch_size,\n        num_workers=self.num_workers,\n        pin_memory=True,\n    )\n</code></pre>"},{"location":"reference/quadra/datamodules/base.html","title":"base","text":""},{"location":"reference/quadra/datamodules/base.html#quadra.datamodules.base.BaseDataModule","title":"<code>BaseDataModule(data_path, name='base_datamodule', num_workers=16, batch_size=32, seed=42, load_aug_images=False, aug_name=None, n_aug_to_take=None, replace_str_from=None, replace_str_to=None, train_transform=None, val_transform=None, test_transform=None, enable_hashing=True, hash_size=64, hash_type='content')</code>","text":"<p>             Bases: <code>LightningDataModule</code></p> <p>Base class for all data modules.</p> <p>Parameters:</p> <ul> <li> data_path             (<code>str</code>)         \u2013          <p>Path to the data main folder.</p> </li> <li> name             (<code>str</code>, default:                 <code>'base_datamodule'</code> )         \u2013          <p>The name for the data module. Defaults to \"base_datamodule\".</p> </li> <li> num_workers             (<code>int</code>, default:                 <code>16</code> )         \u2013          <p>Number of workers for dataloaders. Defaults to 16.</p> </li> <li> batch_size             (<code>int</code>, default:                 <code>32</code> )         \u2013          <p>Batch size. Defaults to 32.</p> </li> <li> seed             (<code>int</code>, default:                 <code>42</code> )         \u2013          <p>Random generator seed. Defaults to 42.</p> </li> <li> train_transform             (<code>Compose | None</code>, default:                 <code>None</code> )         \u2013          <p>Transformations for train dataset. Defaults to None.</p> </li> <li> val_transform             (<code>Compose | None</code>, default:                 <code>None</code> )         \u2013          <p>Transformations for validation dataset. Defaults to None.</p> </li> <li> test_transform             (<code>Compose | None</code>, default:                 <code>None</code> )         \u2013          <p>Transformations for test dataset. Defaults to None.</p> </li> <li> enable_hashing             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Whether to enable hashing of images. Defaults to True.</p> </li> <li> hash_size             (<code>Literal[32, 64, 128]</code>, default:                 <code>64</code> )         \u2013          <p>Size of the hash. Must be one of [32, 64, 128]. Defaults to 64.</p> </li> <li> hash_type             (<code>Literal['content', 'size']</code>, default:                 <code>'content'</code> )         \u2013          <p>Type of hash to use, if content hash is used, the hash is computed on the file content, otherwise the hash is computed on the file size which is faster but less safe. Defaults to \"content\".</p> </li> </ul> Source code in <code>quadra/datamodules/base.py</code> <pre><code>def __init__(\n    self,\n    data_path: str,\n    name: str = \"base_datamodule\",\n    num_workers: int = 16,\n    batch_size: int = 32,\n    seed: int = 42,\n    load_aug_images: bool = False,\n    aug_name: str | None = None,\n    n_aug_to_take: int | None = None,\n    replace_str_from: str | None = None,\n    replace_str_to: str | None = None,\n    train_transform: albumentations.Compose | None = None,\n    val_transform: albumentations.Compose | None = None,\n    test_transform: albumentations.Compose | None = None,\n    enable_hashing: bool = True,\n    hash_size: Literal[32, 64, 128] = 64,\n    hash_type: Literal[\"content\", \"size\"] = \"content\",\n):\n    super().__init__()\n    self.num_workers = num_workers\n    self.batch_size = batch_size\n    self.seed = seed\n    self.data_path = data_path\n    self.name = name\n    self.train_transform = train_transform\n    self.val_transform = val_transform\n    self.test_transform = test_transform\n    self.enable_hashing = enable_hashing\n    self.hash_size = hash_size\n    self.hash_type = hash_type\n\n    if self.hash_size not in [32, 64, 128]:\n        raise ValueError(f\"Invalid hash size {self.hash_size}. Must be one of [32, 64, 128].\")\n\n    self.load_aug_images = load_aug_images\n    self.aug_name = aug_name\n    self.n_aug_to_take = n_aug_to_take\n    self.replace_str_from = replace_str_from\n    self.replace_str_to = replace_str_to\n    self.extra_args: dict[str, Any] = {}\n    self.train_dataset: TrainDataset\n    self.val_dataset: ValDataset\n    self.test_dataset: TestDataset\n    self.data: pd.DataFrame\n    self.data_folder = \"data\"\n    os.makedirs(self.data_folder, exist_ok=True)\n    self.datamodule_checkpoint_file = os.path.join(self.data_folder, \"datamodule.pkl\")\n    self.dataset_file = os.path.join(self.data_folder, \"dataset.csv\")\n</code></pre>"},{"location":"reference/quadra/datamodules/base.html#quadra.datamodules.base.BaseDataModule.test_data","title":"<code>test_data: pd.DataFrame</code>  <code>property</code>","text":"<p>Get test data.</p>"},{"location":"reference/quadra/datamodules/base.html#quadra.datamodules.base.BaseDataModule.test_dataset_available","title":"<code>test_dataset_available: bool</code>  <code>property</code>","text":"<p>Checks if the test dataset is available.</p>"},{"location":"reference/quadra/datamodules/base.html#quadra.datamodules.base.BaseDataModule.train_data","title":"<code>train_data: pd.DataFrame</code>  <code>property</code>","text":"<p>Get train data.</p>"},{"location":"reference/quadra/datamodules/base.html#quadra.datamodules.base.BaseDataModule.train_dataset_available","title":"<code>train_dataset_available: bool</code>  <code>property</code>","text":"<p>Checks if the train dataset is available.</p>"},{"location":"reference/quadra/datamodules/base.html#quadra.datamodules.base.BaseDataModule.val_data","title":"<code>val_data: pd.DataFrame</code>  <code>property</code>","text":"<p>Get validation data.</p>"},{"location":"reference/quadra/datamodules/base.html#quadra.datamodules.base.BaseDataModule.val_dataset_available","title":"<code>val_dataset_available: bool</code>  <code>property</code>","text":"<p>Checks if the validation dataset is available.</p>"},{"location":"reference/quadra/datamodules/base.html#quadra.datamodules.base.BaseDataModule.__getstate__","title":"<code>__getstate__()</code>","text":"<p>This method is called when pickling the object. It's useful to remove attributes that shouldn't be pickled.</p> Source code in <code>quadra/datamodules/base.py</code> <pre><code>def __getstate__(self) -&gt; dict[str, Any]:\n\"\"\"This method is called when pickling the object.\n    It's useful to remove attributes that shouldn't be pickled.\n    \"\"\"\n    state = self.__dict__.copy()\n    if \"trainer\" in state:\n        # Lightning injects the trainer in the datamodule, we don't want to pickle it.\n        del state[\"trainer\"]\n\n    return state\n</code></pre>"},{"location":"reference/quadra/datamodules/base.html#quadra.datamodules.base.BaseDataModule.hash_data","title":"<code>hash_data()</code>","text":"<p>Computes the hash of the files inside the datasets.</p> Source code in <code>quadra/datamodules/base.py</code> <pre><code>def hash_data(self) -&gt; None:\n\"\"\"Computes the hash of the files inside the datasets.\"\"\"\n    if not self.enable_hashing:\n        return\n\n    # TODO: We need to find a way to annotate the columns of data.\n    paths_and_hash_length = zip(self.data[\"samples\"], [self.hash_size] * len(self.data))\n\n    with mp.Pool(min(8, mp.cpu_count() - 1)) as pool:\n        self.data[\"hash\"] = list(\n            tqdm(\n                pool.istarmap(  # type: ignore[attr-defined]\n                    compute_file_content_hash if self.hash_type == \"content\" else compute_file_size_hash,\n                    paths_and_hash_length,\n                ),\n                total=len(self.data),\n                desc=\"Computing hashes\",\n            )\n        )\n\n    self.data[\"hash_type\"] = self.hash_type\n</code></pre>"},{"location":"reference/quadra/datamodules/base.html#quadra.datamodules.base.BaseDataModule.load_augmented_samples","title":"<code>load_augmented_samples(samples, targets, replace_str_from=None, replace_str_to=None, shuffle=False)</code>","text":"<p>Loads augmented samples.</p> Source code in <code>quadra/datamodules/base.py</code> <pre><code>def load_augmented_samples(\n    self,\n    samples: list[str],\n    targets: list[Any],\n    replace_str_from: str | None = None,\n    replace_str_to: str | None = None,\n    shuffle: bool = False,\n) -&gt; tuple[list[str], list[str]]:\n\"\"\"Loads augmented samples.\"\"\"\n    if self.n_aug_to_take is None:\n        raise ValueError(\"`n_aug_to_take` is not set. Cannot load augmented samples.\")\n    aug_samples = []\n    aug_labels = []\n    for sample, label in zip(samples, targets):\n        aug_samples.append(sample)\n        aug_labels.append(label)\n        final_sample = sample\n        if replace_str_from is not None and replace_str_to is not None:\n            final_sample = final_sample.replace(replace_str_from, replace_str_to)\n        base, ext = os.path.splitext(final_sample)\n        for k in range(self.n_aug_to_take):\n            aug_samples.append(base + \"_\" + str(k + 1) + ext)\n            aug_labels.append(label)\n    samples = aug_samples\n    targets = aug_labels\n    if shuffle:\n        idexs = np.arange(len(aug_samples))\n        np.random.shuffle(idexs)\n        samples = np.array(samples)[idexs].tolist()\n        targets = np.array(targets)[idexs].tolist()\n    return samples, targets\n</code></pre>"},{"location":"reference/quadra/datamodules/base.html#quadra.datamodules.base.BaseDataModule.prepare_data","title":"<code>prepare_data()</code>","text":"<p>Prepares the data, should be overridden by subclasses.</p> Source code in <code>quadra/datamodules/base.py</code> <pre><code>def prepare_data(self) -&gt; None:\n\"\"\"Prepares the data, should be overridden by subclasses.\"\"\"\n    if hasattr(self, \"data\"):\n        return\n\n    self._prepare_data()\n    self.hash_data()\n    self.save_checkpoint()\n</code></pre>"},{"location":"reference/quadra/datamodules/base.html#quadra.datamodules.base.BaseDataModule.restore_checkpoint","title":"<code>restore_checkpoint()</code>","text":"<p>Loads the data from disk, utility function that should be called from setup.</p> Source code in <code>quadra/datamodules/base.py</code> <pre><code>def restore_checkpoint(self) -&gt; None:\n\"\"\"Loads the data from disk, utility function that should be called from setup.\"\"\"\n    if hasattr(self, \"data\"):\n        return\n\n    if not os.path.isfile(self.datamodule_checkpoint_file):\n        raise ValueError(f\"Dataset file {self.datamodule_checkpoint_file} does not exist.\")\n\n    with open(self.datamodule_checkpoint_file, \"rb\") as f:\n        checkpoint_datamodule = pkl.load(f)\n        for key, value in checkpoint_datamodule.__dict__.items():\n            setattr(self, key, value)\n</code></pre>"},{"location":"reference/quadra/datamodules/base.html#quadra.datamodules.base.BaseDataModule.save_checkpoint","title":"<code>save_checkpoint()</code>","text":"<p>Saves the datamodule to disk, utility function that is called from prepare_data. We are required to save datamodule to disk because we can't assign attributes to the datamodule in prepare_data when working with multiple gpus.</p> Source code in <code>quadra/datamodules/base.py</code> <pre><code>def save_checkpoint(self) -&gt; None:\n\"\"\"Saves the datamodule to disk, utility function that is called from prepare_data. We are required to save\n    datamodule to disk because we can't assign attributes to the datamodule in prepare_data when working with\n    multiple gpus.\n    \"\"\"\n    if not os.path.exists(self.datamodule_checkpoint_file) and not os.path.exists(self.dataset_file):\n        with open(self.datamodule_checkpoint_file, \"wb\") as f:\n            pkl.dump(self, f)\n\n        self.data.to_csv(self.dataset_file, index=False)\n        log.info(\"Datamodule checkpoint saved to disk.\")\n\n    if \"targets\" in self.data:\n        if isinstance(self.data[\"targets\"].iloc[0], np.ndarray):\n            # If we find a numpy array target it's very likely one hot encoded,\n            # in that case we just print the number of train/val/test samples\n            grouping = [\"split\"]\n        else:\n            grouping = [\"split\", \"targets\"]\n        log.info(\"Dataset Info:\")\n        split_order = {\"train\": 0, \"val\": 1, \"test\": 2}\n        log.info(\n            \"\\n%s\",\n            self.data.groupby(grouping)\n            .size()\n            .to_frame()\n            .reset_index()\n            .sort_values(by=[\"split\"], key=lambda x: x.map(split_order))\n            .rename(columns={0: \"count\"})\n            .to_string(index=False),\n        )\n</code></pre>"},{"location":"reference/quadra/datamodules/base.html#quadra.datamodules.base.DecorateParentMethod","title":"<code>DecorateParentMethod</code>","text":"<p>             Bases: <code>type</code></p> <p>Metaclass to decorate methods of subclasses.</p>"},{"location":"reference/quadra/datamodules/base.html#quadra.datamodules.base.DecorateParentMethod.__new__","title":"<code>__new__(name, bases, dct)</code>","text":"<p>Create new  decorator for parent class methods.</p> Source code in <code>quadra/datamodules/base.py</code> <pre><code>def __new__(cls, name, bases, dct):\n\"\"\"Create new  decorator for parent class methods.\"\"\"\n    method_decorator_mapper = {\n        \"setup\": load_data_from_disk_dec,\n    }\n    for method_name, decorator in method_decorator_mapper.items():\n        if method_name in dct:\n            dct[method_name] = decorator(dct[method_name])\n\n    return super().__new__(cls, name, bases, dct)\n</code></pre>"},{"location":"reference/quadra/datamodules/base.html#quadra.datamodules.base.compute_file_content_hash","title":"<code>compute_file_content_hash(path, hash_size=64)</code>","text":"<p>Get hash of a file based on its content.</p> <p>Parameters:</p> <ul> <li> path             (<code>str</code>)         \u2013          <p>Path to the file.</p> </li> <li> hash_size             (<code>Literal[32, 64, 128]</code>, default:                 <code>64</code> )         \u2013          <p>Size of the hash. Must be one of [32, 64, 128].</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>         \u2013          <p>The hash of the file.</p> </li> </ul> Source code in <code>quadra/datamodules/base.py</code> <pre><code>def compute_file_content_hash(path: str, hash_size: Literal[32, 64, 128] = 64) -&gt; str:\n\"\"\"Get hash of a file based on its content.\n\n    Args:\n        path: Path to the file.\n        hash_size: Size of the hash. Must be one of [32, 64, 128].\n\n    Returns:\n        The hash of the file.\n    \"\"\"\n    with open(path, \"rb\") as f:\n        data = f.read()\n\n        if hash_size == 32:\n            file_hash = xxhash.xxh32(data, seed=42).hexdigest()\n        elif hash_size == 64:\n            file_hash = xxhash.xxh64(data, seed=42).hexdigest()\n        elif hash_size == 128:\n            file_hash = xxhash.xxh128(data, seed=42).hexdigest()\n        else:\n            raise ValueError(f\"Invalid hash size {hash_size}. Must be one of [32, 64, 128].\")\n\n    return file_hash\n</code></pre>"},{"location":"reference/quadra/datamodules/base.html#quadra.datamodules.base.compute_file_size_hash","title":"<code>compute_file_size_hash(path, hash_size=64)</code>","text":"<p>Get hash of a file based on its size.</p> <p>Parameters:</p> <ul> <li> path             (<code>str</code>)         \u2013          <p>Path to the file.</p> </li> <li> hash_size             (<code>Literal[32, 64, 128]</code>, default:                 <code>64</code> )         \u2013          <p>Size of the hash. Must be one of [32, 64, 128].</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>         \u2013          <p>The hash of the file.</p> </li> </ul> Source code in <code>quadra/datamodules/base.py</code> <pre><code>def compute_file_size_hash(path: str, hash_size: Literal[32, 64, 128] = 64) -&gt; str:\n\"\"\"Get hash of a file based on its size.\n\n    Args:\n        path: Path to the file.\n        hash_size: Size of the hash. Must be one of [32, 64, 128].\n\n    Returns:\n        The hash of the file.\n    \"\"\"\n    data = str(os.path.getsize(path))\n\n    if hash_size == 32:\n        file_hash = xxhash.xxh32(data, seed=42).hexdigest()\n    elif hash_size == 64:\n        file_hash = xxhash.xxh64(data, seed=42).hexdigest()\n    elif hash_size == 128:\n        file_hash = xxhash.xxh128(data, seed=42).hexdigest()\n    else:\n        raise ValueError(f\"Invalid hash size {hash_size}. Must be one of [32, 64, 128].\")\n\n    return file_hash\n</code></pre>"},{"location":"reference/quadra/datamodules/base.html#quadra.datamodules.base.istarmap","title":"<code>istarmap(self, func, iterable, chunksize=1)</code>","text":"<p>Starmap-version of imap.</p> Source code in <code>quadra/datamodules/base.py</code> <pre><code>@typing.no_type_check\ndef istarmap(self, func: Callable, iterable: Iterable, chunksize: int = 1):\n    # pylint: disable=all\n\"\"\"Starmap-version of imap.\"\"\"\n    self._check_running()\n    if chunksize &lt; 1:\n        raise ValueError(f\"Chunksize must be 1+, not {chunksize:n}\")\n\n    task_batches = mpp.Pool._get_tasks(func, iterable, chunksize)\n    result = mpp.IMapIterator(self)\n    self._taskqueue.put((self._guarded_task_generation(result._job, mpp.starmapstar, task_batches), result._set_length))\n    return (item for chunk in result for item in chunk)\n</code></pre>"},{"location":"reference/quadra/datamodules/base.html#quadra.datamodules.base.load_data_from_disk_dec","title":"<code>load_data_from_disk_dec(func)</code>","text":"<p>Load data from disk if it exists.</p> Source code in <code>quadra/datamodules/base.py</code> <pre><code>def load_data_from_disk_dec(func):\n\"\"\"Load data from disk if it exists.\"\"\"\n\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n\"\"\"Wrapper function to load data from disk if it exists.\"\"\"\n        self = cast(BaseDataModule, args[0])\n        self.restore_checkpoint()\n        return func(*args, **kwargs)\n\n    return wrapper\n</code></pre>"},{"location":"reference/quadra/datamodules/classification.html","title":"classification","text":""},{"location":"reference/quadra/datamodules/classification.html#quadra.datamodules.classification.ClassificationDataModule","title":"<code>ClassificationDataModule(data_path, dataset=ImageClassificationListDataset, name='classification_datamodule', num_workers=8, batch_size=32, seed=42, val_size=0.2, test_size=0.2, num_data_class=None, exclude_filter=None, include_filter=None, label_map=None, load_aug_images=False, aug_name=None, n_aug_to_take=4, replace_str_from=None, replace_str_to=None, train_transform=None, val_transform=None, test_transform=None, train_split_file=None, test_split_file=None, val_split_file=None, class_to_idx=None, **kwargs)</code>","text":"<p>             Bases: <code>BaseDataModule</code></p> <p>Base class single folder based classification datamodules. If there is no nested folders, use this class.</p> <p>Parameters:</p> <ul> <li> data_path             (<code>str</code>)         \u2013          <p>Path to the data main folder.</p> </li> <li> name             (<code>str</code>, default:                 <code>'classification_datamodule'</code> )         \u2013          <p>The name for the data module. Defaults to \"classification_datamodule\".</p> </li> <li> num_workers             (<code>int</code>, default:                 <code>8</code> )         \u2013          <p>Number of workers for dataloaders. Defaults to 16.</p> </li> <li> batch_size             (<code>int</code>, default:                 <code>32</code> )         \u2013          <p>Batch size. Defaults to 32.</p> </li> <li> seed             (<code>int</code>, default:                 <code>42</code> )         \u2013          <p>Random generator seed. Defaults to 42.</p> </li> <li> dataset             (<code>type[ImageClassificationListDataset]</code>, default:                 <code>ImageClassificationListDataset</code> )         \u2013          <p>Dataset class.</p> </li> <li> val_size             (<code>float | None</code>, default:                 <code>0.2</code> )         \u2013          <p>The validation split. Defaults to 0.2.</p> </li> <li> test_size             (<code>float</code>, default:                 <code>0.2</code> )         \u2013          <p>The test split. Defaults to 0.2.</p> </li> <li> exclude_filter             (<code>list[str] | None</code>, default:                 <code>None</code> )         \u2013          <p>The filter for excluding folders. Defaults to None.</p> </li> <li> include_filter             (<code>list[str] | None</code>, default:                 <code>None</code> )         \u2013          <p>The filter for including folders. Defaults to None.</p> </li> <li> label_map             (<code>dict[str, Any] | None</code>, default:                 <code>None</code> )         \u2013          <p>The mapping for labels. Defaults to None.</p> </li> <li> num_data_class             (<code>int | None</code>, default:                 <code>None</code> )         \u2013          <p>The number of samples per class. Defaults to None.</p> </li> <li> train_transform             (<code>Compose | None</code>, default:                 <code>None</code> )         \u2013          <p>Transformations for train dataset. Defaults to None.</p> </li> <li> val_transform             (<code>Compose | None</code>, default:                 <code>None</code> )         \u2013          <p>Transformations for validation dataset. Defaults to None.</p> </li> <li> test_transform             (<code>Compose | None</code>, default:                 <code>None</code> )         \u2013          <p>Transformations for test dataset. Defaults to None.</p> </li> <li> train_split_file             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>The file with train split. Defaults to None.</p> </li> <li> val_split_file             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>The file with validation split. Defaults to None.</p> </li> <li> test_split_file             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>The file with test split. Defaults to None.</p> </li> <li> class_to_idx             (<code>dict[str, int] | None</code>, default:                 <code>None</code> )         \u2013          <p>The mapping from class name to index. Defaults to None.</p> </li> <li> **kwargs             (<code>Any</code>, default:                 <code>{}</code> )         \u2013          <p>Additional arguments for BaseDataModule.</p> </li> </ul> Source code in <code>quadra/datamodules/classification.py</code> <pre><code>def __init__(\n    self,\n    data_path: str,\n    dataset: type[ImageClassificationListDataset] = ImageClassificationListDataset,\n    name: str = \"classification_datamodule\",\n    num_workers: int = 8,\n    batch_size: int = 32,\n    seed: int = 42,\n    val_size: float | None = 0.2,\n    test_size: float = 0.2,\n    num_data_class: int | None = None,\n    exclude_filter: list[str] | None = None,\n    include_filter: list[str] | None = None,\n    label_map: dict[str, Any] | None = None,\n    load_aug_images: bool = False,\n    aug_name: str | None = None,\n    n_aug_to_take: int | None = 4,\n    replace_str_from: str | None = None,\n    replace_str_to: str | None = None,\n    train_transform: albumentations.Compose | None = None,\n    val_transform: albumentations.Compose | None = None,\n    test_transform: albumentations.Compose | None = None,\n    train_split_file: str | None = None,\n    test_split_file: str | None = None,\n    val_split_file: str | None = None,\n    class_to_idx: dict[str, int] | None = None,\n    **kwargs: Any,\n):\n    super().__init__(\n        data_path=data_path,\n        name=name,\n        seed=seed,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        train_transform=train_transform,\n        val_transform=val_transform,\n        test_transform=test_transform,\n        load_aug_images=load_aug_images,\n        aug_name=aug_name,\n        n_aug_to_take=n_aug_to_take,\n        replace_str_from=replace_str_from,\n        replace_str_to=replace_str_to,\n        **kwargs,\n    )\n    self.replace_str = None\n    self.exclude_filter = exclude_filter\n    self.include_filter = include_filter\n    self.val_size = val_size\n    self.test_size = test_size\n    self.label_map = label_map\n    self.num_data_class = num_data_class\n    self.dataset = dataset\n    self.train_split_file = train_split_file\n    self.test_split_file = test_split_file\n    self.val_split_file = val_split_file\n    self.class_to_idx: dict[str, int] | None\n\n    if class_to_idx is not None:\n        self.class_to_idx = class_to_idx\n        self.num_classes = len(self.class_to_idx)\n    else:\n        self.class_to_idx = self._find_classes_from_data_path(self.data_path)\n        if self.class_to_idx is None:\n            log.warning(\"Could not build a class_to_idx from the data_path subdirectories\")\n            self.num_classes = 0\n        else:\n            self.num_classes = len(self.class_to_idx)\n</code></pre>"},{"location":"reference/quadra/datamodules/classification.html#quadra.datamodules.classification.ClassificationDataModule.predict_dataloader","title":"<code>predict_dataloader()</code>","text":"<p>Returns a dataloader used for predictions.</p> Source code in <code>quadra/datamodules/classification.py</code> <pre><code>def predict_dataloader(self) -&gt; DataLoader:\n\"\"\"Returns a dataloader used for predictions.\"\"\"\n    return self.test_dataloader()\n</code></pre>"},{"location":"reference/quadra/datamodules/classification.html#quadra.datamodules.classification.ClassificationDataModule.setup","title":"<code>setup(stage=None)</code>","text":"<p>Setup data module based on stages of training.</p> Source code in <code>quadra/datamodules/classification.py</code> <pre><code>def setup(self, stage: str | None = None) -&gt; None:\n\"\"\"Setup data module based on stages of training.\"\"\"\n    if stage in [\"train\", \"fit\"]:\n        self.train_dataset = self.dataset(\n            samples=self.data[self.data[\"split\"] == \"train\"][\"samples\"].tolist(),\n            targets=self.data[self.data[\"split\"] == \"train\"][\"targets\"].tolist(),\n            transform=self.train_transform,\n            class_to_idx=self.class_to_idx,\n        )\n        self.val_dataset = self.dataset(\n            samples=self.data[self.data[\"split\"] == \"val\"][\"samples\"].tolist(),\n            targets=self.data[self.data[\"split\"] == \"val\"][\"targets\"].tolist(),\n            transform=self.val_transform,\n            class_to_idx=self.class_to_idx,\n        )\n    if stage in [\"test\", \"predict\"]:\n        self.test_dataset = self.dataset(\n            samples=self.data[self.data[\"split\"] == \"test\"][\"samples\"].tolist(),\n            targets=self.data[self.data[\"split\"] == \"test\"][\"targets\"].tolist(),\n            transform=self.test_transform,\n            class_to_idx=self.class_to_idx,\n        )\n</code></pre>"},{"location":"reference/quadra/datamodules/classification.html#quadra.datamodules.classification.ClassificationDataModule.test_dataloader","title":"<code>test_dataloader()</code>","text":"<p>Returns the test dataloader.</p> <p>Raises:</p> <ul> <li> <code>ValueError</code>           \u2013          <p>If test dataset is not initialized.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataLoader</code>         \u2013          <p>test dataloader.</p> </li> </ul> Source code in <code>quadra/datamodules/classification.py</code> <pre><code>def test_dataloader(self) -&gt; DataLoader:\n\"\"\"Returns the test dataloader.\n\n    Raises:\n        ValueError: If test dataset is not initialized.\n\n\n    Returns:\n        test dataloader.\n    \"\"\"\n    if not self.test_dataset_available:\n        raise ValueError(\"Test dataset is not initialized\")\n\n    loader = DataLoader(\n        self.test_dataset,\n        batch_size=self.batch_size,\n        shuffle=False,\n        num_workers=self.num_workers,\n        drop_last=False,\n        pin_memory=True,\n        persistent_workers=self.num_workers &gt; 0,\n    )\n    return loader\n</code></pre>"},{"location":"reference/quadra/datamodules/classification.html#quadra.datamodules.classification.ClassificationDataModule.train_dataloader","title":"<code>train_dataloader()</code>","text":"<p>Returns the train dataloader.</p> <p>Raises:</p> <ul> <li> <code>ValueError</code>           \u2013          <p>If train dataset is not initialized.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataLoader</code>         \u2013          <p>Train dataloader.</p> </li> </ul> Source code in <code>quadra/datamodules/classification.py</code> <pre><code>def train_dataloader(self) -&gt; DataLoader:\n\"\"\"Returns the train dataloader.\n\n    Raises:\n        ValueError: If train dataset is not initialized.\n\n    Returns:\n        Train dataloader.\n    \"\"\"\n    if not self.train_dataset_available:\n        raise ValueError(\"Train dataset is not initialized\")\n    if not isinstance(self.train_dataset, torch.utils.data.Dataset):\n        raise ValueError(\"Train dataset has to be single `torch.utils.data.Dataset` instance.\")\n    return DataLoader(\n        self.train_dataset,\n        batch_size=self.batch_size,\n        shuffle=True,\n        num_workers=self.num_workers,\n        drop_last=False,\n        pin_memory=True,\n        persistent_workers=self.num_workers &gt; 0,\n    )\n</code></pre>"},{"location":"reference/quadra/datamodules/classification.html#quadra.datamodules.classification.ClassificationDataModule.val_dataloader","title":"<code>val_dataloader()</code>","text":"<p>Returns the validation dataloader.</p> <p>Raises:</p> <ul> <li> <code>ValueError</code>           \u2013          <p>If validation dataset is not initialized.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataLoader</code>         \u2013          <p>val dataloader.</p> </li> </ul> Source code in <code>quadra/datamodules/classification.py</code> <pre><code>def val_dataloader(self) -&gt; DataLoader:\n\"\"\"Returns the validation dataloader.\n\n    Raises:\n        ValueError: If validation dataset is not initialized.\n\n    Returns:\n        val dataloader.\n    \"\"\"\n    if not self.val_dataset_available:\n        raise ValueError(\"Validation dataset is not initialized\")\n    if not isinstance(self.val_dataset, torch.utils.data.Dataset):\n        raise ValueError(\"Validation dataset has to be single `torch.utils.data.Dataset` instance.\")\n    return DataLoader(\n        self.val_dataset,\n        batch_size=self.batch_size,\n        shuffle=False,\n        num_workers=self.num_workers,\n        drop_last=False,\n        pin_memory=True,\n        persistent_workers=self.num_workers &gt; 0,\n    )\n</code></pre>"},{"location":"reference/quadra/datamodules/classification.html#quadra.datamodules.classification.MultilabelClassificationDataModule","title":"<code>MultilabelClassificationDataModule(data_path, images_and_labels_file=None, train_split_file=None, test_split_file=None, val_split_file=None, name='multilabel_datamodule', dataset=MultilabelClassificationDataset, num_classes=None, num_workers=16, batch_size=64, test_batch_size=64, seed=42, val_size=0.2, test_size=0.2, train_transform=None, val_transform=None, test_transform=None, class_to_idx=None, **kwargs)</code>","text":"<p>             Bases: <code>BaseDataModule</code></p> <p>Base class for all multi-label modules.</p> <p>Parameters:</p> <ul> <li> data_path             (<code>str</code>)         \u2013          <p>Path to the data main folder.</p> </li> <li> images_and_labels_file             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>a path to a txt file containing the relative (to <code>data_path</code>) path of images with their relative labels, in a comma-separated way. E.g.:</p> <ul> <li>path1,l1,l2,l3</li> <li>path2,l4,l5</li> <li>...</li> </ul> <p>One of <code>images_and_label</code> and both <code>train_split_file</code> and <code>test_split_file</code> must be set. Defaults to None.</p> </li> <li> name             (<code>str</code>, default:                 <code>'multilabel_datamodule'</code> )         \u2013          <p>The name for the data module. Defaults to \"multilabel_datamodule\".</p> </li> <li> dataset             (<code>Callable</code>, default:                 <code>MultilabelClassificationDataset</code> )         \u2013          <p>a callable returning a torch.utils.data.Dataset class.</p> </li> <li> num_classes             (<code>int | None</code>, default:                 <code>None</code> )         \u2013          <p>the number of classes in the dataset. This is used to create one-hot encoded targets. Defaults to None.</p> </li> <li> num_workers             (<code>int</code>, default:                 <code>16</code> )         \u2013          <p>Number of workers for dataloaders. Defaults to 16.</p> </li> <li> batch_size             (<code>int</code>, default:                 <code>64</code> )         \u2013          <p>Training batch size. Defaults to 64.</p> </li> <li> test_batch_size             (<code>int</code>, default:                 <code>64</code> )         \u2013          <p>Testing batch size. Defaults to 64.</p> </li> <li> seed             (<code>int</code>, default:                 <code>42</code> )         \u2013          <p>Random generator seed. Defaults to SegmentationEvalua2.</p> </li> <li> val_size             (<code>float | None</code>, default:                 <code>0.2</code> )         \u2013          <p>The validation split. Defaults to 0.2.</p> </li> <li> test_size             (<code>float | None</code>, default:                 <code>0.2</code> )         \u2013          <p>The test split. Defaults to 0.2.</p> </li> <li> train_transform             (<code>Compose | None</code>, default:                 <code>None</code> )         \u2013          <p>Transformations for train dataset. Defaults to None.</p> </li> <li> val_transform             (<code>Compose | None</code>, default:                 <code>None</code> )         \u2013          <p>Transformations for validation dataset. Defaults to None.</p> </li> <li> test_transform             (<code>Compose | None</code>, default:                 <code>None</code> )         \u2013          <p>Transformations for test dataset. Defaults to None.</p> </li> <li> train_split_file             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>The file with train split. Defaults to None.</p> </li> <li> val_split_file             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>The file with validation split. Defaults to None.</p> </li> <li> test_split_file             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>The file with test split. Defaults to None.</p> </li> <li> class_to_idx             (<code>dict[str, int] | None</code>, default:                 <code>None</code> )         \u2013          <p>a clss to idx dictionary. Defaults to None.</p> </li> </ul> Source code in <code>quadra/datamodules/classification.py</code> <pre><code>def __init__(\n    self,\n    data_path: str,\n    images_and_labels_file: str | None = None,\n    train_split_file: str | None = None,\n    test_split_file: str | None = None,\n    val_split_file: str | None = None,\n    name: str = \"multilabel_datamodule\",\n    dataset: Callable = MultilabelClassificationDataset,\n    num_classes: int | None = None,\n    num_workers: int = 16,\n    batch_size: int = 64,\n    test_batch_size: int = 64,\n    seed: int = 42,\n    val_size: float | None = 0.2,\n    test_size: float | None = 0.2,\n    train_transform: albumentations.Compose | None = None,\n    val_transform: albumentations.Compose | None = None,\n    test_transform: albumentations.Compose | None = None,\n    class_to_idx: dict[str, int] | None = None,\n    **kwargs,\n):\n    super().__init__(\n        data_path=data_path,\n        name=name,\n        num_workers=num_workers,\n        batch_size=batch_size,\n        seed=seed,\n        train_transform=train_transform,\n        val_transform=val_transform,\n        test_transform=test_transform,\n        **kwargs,\n    )\n    if not (images_and_labels_file is not None or (train_split_file is not None and test_split_file is not None)):\n        raise ValueError(\n            \"Either `images_and_labels_file` or both `train_split_file` and `test_split_file` must be set\"\n        )\n    self.images_and_labels_file = images_and_labels_file\n    self.dataset = dataset\n    self.num_classes = num_classes\n    self.train_batch_size = batch_size\n    self.test_batch_size = test_batch_size\n    self.val_size = val_size\n    self.test_size = test_size\n    self.train_split_file = train_split_file\n    self.test_split_file = test_split_file\n    self.val_split_file = val_split_file\n    self.class_to_idx = class_to_idx\n    self.train_dataset: MultilabelClassificationDataset\n    self.val_dataset: MultilabelClassificationDataset\n    self.test_dataset: MultilabelClassificationDataset\n</code></pre>"},{"location":"reference/quadra/datamodules/classification.html#quadra.datamodules.classification.MultilabelClassificationDataModule.predict_dataloader","title":"<code>predict_dataloader()</code>","text":"<p>Returns a dataloader used for predictions.</p> Source code in <code>quadra/datamodules/classification.py</code> <pre><code>def predict_dataloader(self) -&gt; DataLoader:\n\"\"\"Returns a dataloader used for predictions.\"\"\"\n    return self.test_dataloader()\n</code></pre>"},{"location":"reference/quadra/datamodules/classification.html#quadra.datamodules.classification.MultilabelClassificationDataModule.setup","title":"<code>setup(stage=None)</code>","text":"<p>Setup data module based on stages of training.</p> Source code in <code>quadra/datamodules/classification.py</code> <pre><code>def setup(self, stage: str | None = None) -&gt; None:\n\"\"\"Setup data module based on stages of training.\"\"\"\n    if stage in [\"train\", \"fit\"]:\n        train_samples = self.data[self.data[\"split\"] == \"train\"][\"samples\"].tolist()\n        train_targets = self.data[self.data[\"split\"] == \"train\"][\"targets\"].tolist()\n        val_samples = self.data[self.data[\"split\"] == \"val\"][\"samples\"].tolist()\n        val_targets = self.data[self.data[\"split\"] == \"val\"][\"targets\"].tolist()\n        self.train_dataset = self.dataset(\n            samples=train_samples,\n            targets=train_targets,\n            transform=self.train_transform,\n            class_to_idx=self.class_to_idx,\n        )\n        self.val_dataset = self.dataset(\n            samples=val_samples,\n            targets=val_targets,\n            transform=self.val_transform,\n            class_to_idx=self.class_to_idx,\n        )\n    if stage == \"test\":\n        test_samples = self.data[self.data[\"split\"] == \"test\"][\"samples\"].tolist()\n        test_targets = self.data[self.data[\"split\"] == \"test\"][\"targets\"].tolist()\n        self.test_dataset = self.dataset(\n            samples=test_samples,\n            targets=test_targets,\n            transform=self.test_transform,\n            class_to_idx=self.class_to_idx,\n        )\n</code></pre>"},{"location":"reference/quadra/datamodules/classification.html#quadra.datamodules.classification.MultilabelClassificationDataModule.test_dataloader","title":"<code>test_dataloader()</code>","text":"<p>Returns the test dataloader.</p> <p>Raises:</p> <ul> <li> <code>ValueError</code>           \u2013          <p>If test dataset is not initialized.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataLoader</code>         \u2013          <p>test dataloader.</p> </li> </ul> Source code in <code>quadra/datamodules/classification.py</code> <pre><code>def test_dataloader(self) -&gt; DataLoader:\n\"\"\"Returns the test dataloader.\n\n    Raises:\n        ValueError: If test dataset is not initialized.\n\n\n    Returns:\n        test dataloader.\n    \"\"\"\n    if not self.test_dataset_available:\n        raise ValueError(\"Test dataset is not initialized\")\n\n    loader = DataLoader(\n        self.test_dataset,\n        batch_size=self.batch_size,\n        shuffle=False,\n        num_workers=self.num_workers,\n        drop_last=False,\n        pin_memory=True,\n        persistent_workers=self.num_workers &gt; 0,\n    )\n    return loader\n</code></pre>"},{"location":"reference/quadra/datamodules/classification.html#quadra.datamodules.classification.MultilabelClassificationDataModule.train_dataloader","title":"<code>train_dataloader()</code>","text":"<p>Returns the train dataloader.</p> <p>Raises:</p> <ul> <li> <code>ValueError</code>           \u2013          <p>If train dataset is not initialized.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataLoader</code>         \u2013          <p>Train dataloader.</p> </li> </ul> Source code in <code>quadra/datamodules/classification.py</code> <pre><code>def train_dataloader(self) -&gt; DataLoader:\n\"\"\"Returns the train dataloader.\n\n    Raises:\n        ValueError: If train dataset is not initialized.\n\n    Returns:\n        Train dataloader.\n    \"\"\"\n    if not self.train_dataset_available:\n        raise ValueError(\"Train dataset is not initialized\")\n    return DataLoader(\n        self.train_dataset,\n        batch_size=self.batch_size,\n        shuffle=True,\n        num_workers=self.num_workers,\n        drop_last=False,\n        pin_memory=True,\n        persistent_workers=self.num_workers &gt; 0,\n    )\n</code></pre>"},{"location":"reference/quadra/datamodules/classification.html#quadra.datamodules.classification.MultilabelClassificationDataModule.val_dataloader","title":"<code>val_dataloader()</code>","text":"<p>Returns the validation dataloader.</p> <p>Raises:</p> <ul> <li> <code>ValueError</code>           \u2013          <p>If validation dataset is not initialized.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataLoader</code>         \u2013          <p>val dataloader.</p> </li> </ul> Source code in <code>quadra/datamodules/classification.py</code> <pre><code>def val_dataloader(self) -&gt; DataLoader:\n\"\"\"Returns the validation dataloader.\n\n    Raises:\n        ValueError: If validation dataset is not initialized.\n\n    Returns:\n        val dataloader.\n    \"\"\"\n    if not self.val_dataset_available:\n        raise ValueError(\"Validation dataset is not initialized\")\n    return DataLoader(\n        self.val_dataset,\n        batch_size=self.batch_size,\n        shuffle=False,\n        num_workers=self.num_workers,\n        drop_last=False,\n        pin_memory=True,\n        persistent_workers=self.num_workers &gt; 0,\n    )\n</code></pre>"},{"location":"reference/quadra/datamodules/classification.html#quadra.datamodules.classification.SklearnClassificationDataModule","title":"<code>SklearnClassificationDataModule(data_path, exclude_filter=None, include_filter=None, val_size=0.2, class_to_idx=None, label_map=None, seed=42, batch_size=32, num_workers=6, train_transform=None, val_transform=None, test_transform=None, roi=None, n_splits=1, phase='train', cache=False, limit_training_data=None, train_split_file=None, test_split_file=None, name='sklearn_classification_datamodule', dataset=ImageClassificationListDataset, **kwargs)</code>","text":"<p>             Bases: <code>BaseDataModule</code></p> <p>A generic Data Module for classification with frozen torch backbone and sklearn classifier.</p> <p>It can also handle k-fold cross validation.</p> <p>Parameters:</p> <ul> <li> name             (<code>str</code>, default:                 <code>'sklearn_classification_datamodule'</code> )         \u2013          <p>The name for the data module. Defaults to \"sklearn_classification_datamodule\".</p> </li> <li> data_path             (<code>str</code>)         \u2013          <p>Path to images main folder</p> </li> <li> exclude_filter             (<code>list[str] | None</code>, default:                 <code>None</code> )         \u2013          <p>List of string filter to be used to exclude images. If None no filter will be applied.</p> </li> <li> include_filter             (<code>list[str] | None</code>, default:                 <code>None</code> )         \u2013          <p>List of string filter to be used to include images. Only images that satisfied at list one of             the filter will be included.</p> </li> <li> val_size             (<code>float</code>, default:                 <code>0.2</code> )         \u2013          <p>The validation split. Defaults to 0.2.</p> </li> <li> class_to_idx             (<code>dict[str, int] | None</code>, default:                 <code>None</code> )         \u2013          <p>Dictionary of conversion btw folder name and index. Only file whose label is in dictionary key list will be considered. If None all files will be considered and a custom conversion is created.</p> </li> <li> seed             (<code>int</code>, default:                 <code>42</code> )         \u2013          <p>Fixed seed for random operations</p> </li> <li> batch_size             (<code>int</code>, default:                 <code>32</code> )         \u2013          <p>Dimension of batches for dataloader</p> </li> <li> num_workers             (<code>int</code>, default:                 <code>6</code> )         \u2013          <p>Number of workers for dataloader</p> </li> <li> train_transform             (<code>Compose | None</code>, default:                 <code>None</code> )         \u2013          <p>Albumentation transformations for training set</p> </li> <li> val_transform             (<code>Compose | None</code>, default:                 <code>None</code> )         \u2013          <p>Albumentation transformations for validation set</p> </li> <li> test_transform             (<code>Compose | None</code>, default:                 <code>None</code> )         \u2013          <p>Albumentation transformations for test set</p> </li> <li> roi             (<code>tuple[int, int, int, int] | None</code>, default:                 <code>None</code> )         \u2013          <p>Optional cropping region</p> </li> <li> n_splits             (<code>int</code>, default:                 <code>1</code> )         \u2013          <p>Number of dataset subdivision (default 1 -&gt; train/test). Use a value &gt;= 2 for cross validation.</p> </li> <li> phase             (<code>str</code>, default:                 <code>'train'</code> )         \u2013          <p>Either train or test</p> </li> <li> cache             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>If true disable shuffling in all dataloader to enable feature caching</p> </li> <li> limit_training_data             (<code>int | None</code>, default:                 <code>None</code> )         \u2013          <p>if defined, each class will be donwsampled to this number. It must be &gt;= 2 to allow splitting</p> </li> <li> label_map             (<code>dict[str, Any] | None</code>, default:                 <code>None</code> )         \u2013          <p>Dictionary of conversion btw folder name and label.</p> </li> <li> train_split_file             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>Optional path to a csv file containing the train split samples.</p> </li> <li> test_split_file             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>Optional path to a csv file containing the test split samples.</p> </li> <li> **kwargs             (<code>Any</code>, default:                 <code>{}</code> )         \u2013          <p>Additional arguments for BaseDataModule</p> </li> </ul> Source code in <code>quadra/datamodules/classification.py</code> <pre><code>def __init__(\n    self,\n    data_path: str,\n    exclude_filter: list[str] | None = None,\n    include_filter: list[str] | None = None,\n    val_size: float = 0.2,\n    class_to_idx: dict[str, int] | None = None,\n    label_map: dict[str, Any] | None = None,\n    seed: int = 42,\n    batch_size: int = 32,\n    num_workers: int = 6,\n    train_transform: albumentations.Compose | None = None,\n    val_transform: albumentations.Compose | None = None,\n    test_transform: albumentations.Compose | None = None,\n    roi: tuple[int, int, int, int] | None = None,\n    n_splits: int = 1,\n    phase: str = \"train\",\n    cache: bool = False,\n    limit_training_data: int | None = None,\n    train_split_file: str | None = None,\n    test_split_file: str | None = None,\n    name: str = \"sklearn_classification_datamodule\",\n    dataset: type[ImageClassificationListDataset] = ImageClassificationListDataset,\n    **kwargs: Any,\n):\n    super().__init__(\n        data_path=data_path,\n        name=name,\n        seed=seed,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        train_transform=train_transform,\n        val_transform=val_transform,\n        test_transform=test_transform,\n        **kwargs,\n    )\n\n    self.class_to_idx = class_to_idx\n    self.roi = roi\n    self.cache = cache\n    self.limit_training_data = limit_training_data\n\n    self.dataset = dataset\n    self.phase = phase\n    self.n_splits = n_splits\n    self.train_split_file = train_split_file\n    self.test_split_file = test_split_file\n    self.exclude_filter = exclude_filter\n    self.include_filter = include_filter\n    self.val_size = val_size\n    self.label_map = label_map\n    self.full_dataset: ImageClassificationListDataset\n    self.train_dataset: list[ImageClassificationListDataset]\n    self.val_dataset: list[ImageClassificationListDataset]\n</code></pre>"},{"location":"reference/quadra/datamodules/classification.html#quadra.datamodules.classification.SklearnClassificationDataModule.full_dataloader","title":"<code>full_dataloader()</code>","text":"<p>Return a dataloader to perform training on the entire dataset.</p> <p>Returns:</p> <ul> <li> <code>DataLoader</code>         \u2013          <p>dataloader to perform training on the entire dataset after evaluation. This is useful</p> </li> <li> <code>DataLoader</code>         \u2013          <p>to perform a final training on the entire dataset after the evaluation phase.</p> </li> </ul> Source code in <code>quadra/datamodules/classification.py</code> <pre><code>def full_dataloader(self) -&gt; DataLoader:\n\"\"\"Return a dataloader to perform training on the entire dataset.\n\n    Returns:\n        dataloader to perform training on the entire dataset after evaluation. This is useful\n        to perform a final training on the entire dataset after the evaluation phase.\n\n    \"\"\"\n    if self.full_dataset is None:\n        raise ValueError(\"Full dataset is not initialized\")\n\n    return DataLoader(\n        self.full_dataset,\n        batch_size=self.batch_size,\n        shuffle=not self.cache,\n        num_workers=self.num_workers,\n        drop_last=False,\n        pin_memory=True,\n    )\n</code></pre>"},{"location":"reference/quadra/datamodules/classification.html#quadra.datamodules.classification.SklearnClassificationDataModule.predict_dataloader","title":"<code>predict_dataloader()</code>","text":"<p>Returns a dataloader used for predictions.</p> Source code in <code>quadra/datamodules/classification.py</code> <pre><code>def predict_dataloader(self) -&gt; DataLoader:\n\"\"\"Returns a dataloader used for predictions.\"\"\"\n    return self.test_dataloader()\n</code></pre>"},{"location":"reference/quadra/datamodules/classification.html#quadra.datamodules.classification.SklearnClassificationDataModule.setup","title":"<code>setup(stage)</code>","text":"<p>Setup data module based on stages of training.</p> Source code in <code>quadra/datamodules/classification.py</code> <pre><code>def setup(self, stage: str) -&gt; None:\n\"\"\"Setup data module based on stages of training.\"\"\"\n    if stage == \"fit\":\n        self.train_dataset = []\n        self.val_dataset = []\n\n        for cv_idx in range(self.n_splits):\n            cv_df = self.data[self.data[\"cv\"] == cv_idx]\n            train_samples = cv_df[cv_df[\"split\"] == \"train\"][\"samples\"].tolist()\n            train_targets = cv_df[cv_df[\"split\"] == \"train\"][\"targets\"].tolist()\n            val_samples = cv_df[cv_df[\"split\"] == \"val\"][\"samples\"].tolist()\n            val_targets = cv_df[cv_df[\"split\"] == \"val\"][\"targets\"].tolist()\n            self.train_dataset.append(\n                self.dataset(\n                    class_to_idx=self.class_to_idx,\n                    samples=train_samples,\n                    targets=train_targets,\n                    transform=self.train_transform,\n                    roi=self.roi,\n                )\n            )\n            self.val_dataset.append(\n                self.dataset(\n                    class_to_idx=self.class_to_idx,\n                    samples=val_samples,\n                    targets=val_targets,\n                    transform=self.val_transform,\n                    roi=self.roi,\n                )\n            )\n        all_samples = self.data[self.data[\"cv\"] == 0][\"samples\"].tolist()\n        all_targets = self.data[self.data[\"cv\"] == 0][\"targets\"].tolist()\n        self.full_dataset = self.dataset(\n            class_to_idx=self.class_to_idx,\n            samples=all_samples,\n            targets=all_targets,\n            transform=self.train_transform,\n            roi=self.roi,\n        )\n    if stage == \"test\":\n        test_samples = self.data[self.data[\"split\"] == \"test\"][\"samples\"].tolist()\n        test_targets = self.data[self.data[\"split\"] == \"test\"][\"targets\"]\n        self.test_dataset = self.dataset(\n            class_to_idx=self.class_to_idx,\n            samples=test_samples,\n            targets=test_targets.tolist(),\n            transform=self.test_transform,\n            roi=self.roi,\n            allow_missing_label=True,\n        )\n</code></pre>"},{"location":"reference/quadra/datamodules/classification.html#quadra.datamodules.classification.SklearnClassificationDataModule.test_dataloader","title":"<code>test_dataloader()</code>","text":"<p>Returns the test dataloader.</p> <p>Raises:</p> <ul> <li> <code>ValueError</code>           \u2013          <p>If test dataset is not initialized.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataLoader</code>         \u2013          <p>test dataloader.</p> </li> </ul> Source code in <code>quadra/datamodules/classification.py</code> <pre><code>def test_dataloader(self) -&gt; DataLoader:\n\"\"\"Returns the test dataloader.\n\n    Raises:\n        ValueError: If test dataset is not initialized.\n\n\n    Returns:\n        test dataloader.\n    \"\"\"\n    if not self.test_dataset_available:\n        raise ValueError(\"Test dataset is not initialized\")\n\n    loader = DataLoader(\n        self.test_dataset,\n        batch_size=self.batch_size,\n        shuffle=False,\n        num_workers=self.num_workers,\n        drop_last=False,\n        pin_memory=True,\n        persistent_workers=self.num_workers &gt; 0,\n    )\n    return loader\n</code></pre>"},{"location":"reference/quadra/datamodules/classification.html#quadra.datamodules.classification.SklearnClassificationDataModule.train_dataloader","title":"<code>train_dataloader()</code>","text":"<p>Returns a list of train dataloader.</p> <p>Raises:</p> <ul> <li> <code>ValueError</code>           \u2013          <p>If train dataset is not initialized.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[DataLoader]</code>         \u2013          <p>list of train dataloader.</p> </li> </ul> Source code in <code>quadra/datamodules/classification.py</code> <pre><code>def train_dataloader(self) -&gt; list[DataLoader]:\n\"\"\"Returns a list of train dataloader.\n\n    Raises:\n        ValueError: If train dataset is not initialized.\n\n    Returns:\n        list of train dataloader.\n    \"\"\"\n    if not self.train_dataset_available:\n        raise ValueError(\"Train dataset is not initialized\")\n\n    loader = []\n    for dataset in self.train_dataset:\n        loader.append(\n            DataLoader(\n                dataset,\n                batch_size=self.batch_size,\n                shuffle=not self.cache,\n                num_workers=self.num_workers,\n                drop_last=False,\n                pin_memory=True,\n            )\n        )\n    return loader\n</code></pre>"},{"location":"reference/quadra/datamodules/classification.html#quadra.datamodules.classification.SklearnClassificationDataModule.val_dataloader","title":"<code>val_dataloader()</code>","text":"<p>Returns a list of validation dataloader.</p> <p>Raises:</p> <ul> <li> <code>ValueError</code>           \u2013          <p>If validation dataset is not initialized.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[DataLoader]</code>         \u2013          <p>List of validation dataloader.</p> </li> </ul> Source code in <code>quadra/datamodules/classification.py</code> <pre><code>def val_dataloader(self) -&gt; list[DataLoader]:\n\"\"\"Returns a list of validation dataloader.\n\n    Raises:\n        ValueError: If validation dataset is not initialized.\n\n    Returns:\n        List of validation dataloader.\n    \"\"\"\n    if not self.val_dataset_available:\n        raise ValueError(\"Validation dataset is not initialized\")\n\n    loader = []\n    for dataset in self.val_dataset:\n        loader.append(\n            DataLoader(\n                dataset,\n                batch_size=self.batch_size,\n                shuffle=False,\n                num_workers=self.num_workers,\n                drop_last=False,\n                pin_memory=True,\n            )\n        )\n\n    return loader\n</code></pre>"},{"location":"reference/quadra/datamodules/patch.html","title":"patch","text":""},{"location":"reference/quadra/datamodules/patch.html#quadra.datamodules.patch.PatchSklearnClassificationDataModule","title":"<code>PatchSklearnClassificationDataModule(data_path, class_to_idx, name='patch_classification_datamodule', train_filename='dataset.txt', exclude_filter=None, include_filter=None, seed=42, batch_size=32, num_workers=6, train_transform=None, val_transform=None, test_transform=None, balance_classes=False, class_to_skip_training=None, **kwargs)</code>","text":"<p>             Bases: <code>BaseDataModule</code></p> <p>DataModule for patch classification.</p> <p>Parameters:</p> <ul> <li> data_path             (<code>str</code>)         \u2013          <p>Location of the dataset</p> </li> <li> name             (<code>str</code>, default:                 <code>'patch_classification_datamodule'</code> )         \u2013          <p>Name of the datamodule</p> </li> <li> train_filename             (<code>str</code>, default:                 <code>'dataset.txt'</code> )         \u2013          <p>Name of the file containing the list of training samples</p> </li> <li> exclude_filter             (<code>list[str] | None</code>, default:                 <code>None</code> )         \u2013          <p>Filter to exclude samples from the dataset</p> </li> <li> include_filter             (<code>list[str] | None</code>, default:                 <code>None</code> )         \u2013          <p>Filter to include samples from the dataset</p> </li> <li> class_to_idx             (<code>dict</code>)         \u2013          <p>Dictionary mapping class names to indices</p> </li> <li> seed             (<code>int</code>, default:                 <code>42</code> )         \u2013          <p>Random seed</p> </li> <li> batch_size             (<code>int</code>, default:                 <code>32</code> )         \u2013          <p>Batch size</p> </li> <li> num_workers             (<code>int</code>, default:                 <code>6</code> )         \u2013          <p>Number of workers</p> </li> <li> train_transform             (<code>Compose | None</code>, default:                 <code>None</code> )         \u2013          <p>Transform to apply to the training samples</p> </li> <li> val_transform             (<code>Compose | None</code>, default:                 <code>None</code> )         \u2013          <p>Transform to apply to the validation samples</p> </li> <li> test_transform             (<code>Compose | None</code>, default:                 <code>None</code> )         \u2013          <p>Transform to apply to the test samples</p> </li> <li> balance_classes             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>If True repeat low represented classes</p> </li> <li> class_to_skip_training             (<code>list | None</code>, default:                 <code>None</code> )         \u2013          <p>List of classes skipped during training.</p> </li> </ul> Source code in <code>quadra/datamodules/patch.py</code> <pre><code>def __init__(\n    self,\n    data_path: str,\n    class_to_idx: dict,\n    name: str = \"patch_classification_datamodule\",\n    train_filename: str = \"dataset.txt\",\n    exclude_filter: list[str] | None = None,\n    include_filter: list[str] | None = None,\n    seed: int = 42,\n    batch_size: int = 32,\n    num_workers: int = 6,\n    train_transform: albumentations.Compose | None = None,\n    val_transform: albumentations.Compose | None = None,\n    test_transform: albumentations.Compose | None = None,\n    balance_classes: bool = False,\n    class_to_skip_training: list | None = None,\n    **kwargs,\n):\n    super().__init__(\n        data_path=data_path,\n        name=name,\n        seed=seed,\n        num_workers=num_workers,\n        batch_size=batch_size,\n        train_transform=train_transform,\n        val_transform=val_transform,\n        test_transform=test_transform,\n        **kwargs,\n    )\n    self.class_to_idx = class_to_idx\n    self.balance_classes = balance_classes\n    self.train_filename = train_filename\n    self.include_filter = include_filter\n    self.exclude_filter = exclude_filter\n    self.class_to_skip_training = class_to_skip_training\n\n    self.train_folder = os.path.join(self.data_path, \"train\")\n    self.val_folder = os.path.join(self.data_path, \"val\")\n    self.test_folder = os.path.join(self.data_path, \"test\")\n    self.info: PatchDatasetInfo\n    self.train_dataset: PatchSklearnClassificationTrainDataset\n    self.val_dataset: ImageClassificationListDataset\n    self.test_dataset: ImageClassificationListDataset\n</code></pre>"},{"location":"reference/quadra/datamodules/patch.html#quadra.datamodules.patch.PatchSklearnClassificationDataModule.setup","title":"<code>setup(stage=None)</code>","text":"<p>Setup function.</p> Source code in <code>quadra/datamodules/patch.py</code> <pre><code>def setup(self, stage: str | None = None) -&gt; None:\n\"\"\"Setup function.\"\"\"\n    if stage == \"fit\":\n        self.train_dataset = PatchSklearnClassificationTrainDataset(\n            data_path=self.data_path,\n            class_to_idx=self.class_to_idx,\n            samples=self.data[self.data[\"split\"] == \"train\"][\"samples\"].tolist(),\n            targets=self.data[self.data[\"split\"] == \"train\"][\"targets\"].tolist(),\n            transform=self.train_transform,\n            balance_classes=self.balance_classes,\n        )\n\n        self.val_dataset = ImageClassificationListDataset(\n            class_to_idx=self.class_to_idx,\n            samples=self.data[self.data[\"split\"] == \"val\"][\"samples\"].tolist(),\n            targets=self.data[self.data[\"split\"] == \"val\"][\"targets\"].tolist(),\n            transform=self.val_transform,\n            allow_missing_label=False,\n        )\n\n    elif stage in [\"test\", \"predict\"]:\n        self.test_dataset = ImageClassificationListDataset(\n            class_to_idx=self.class_to_idx,\n            samples=self.data[self.data[\"split\"] == \"test\"][\"samples\"].tolist(),\n            targets=self.data[self.data[\"split\"] == \"test\"][\"targets\"].tolist(),\n            transform=self.test_transform,\n            allow_missing_label=True,\n        )\n</code></pre>"},{"location":"reference/quadra/datamodules/patch.html#quadra.datamodules.patch.PatchSklearnClassificationDataModule.test_dataloader","title":"<code>test_dataloader()</code>","text":"<p>Return the test dataloader.</p> Source code in <code>quadra/datamodules/patch.py</code> <pre><code>def test_dataloader(self) -&gt; DataLoader:\n\"\"\"Return the test dataloader.\"\"\"\n    if not self.test_dataset_available:\n        raise ValueError(\"No test dataset is available\")\n\n    return DataLoader(\n        self.test_dataset,\n        batch_size=self.batch_size,\n        shuffle=False,\n        num_workers=self.num_workers,\n        drop_last=False,\n        pin_memory=True,\n    )\n</code></pre>"},{"location":"reference/quadra/datamodules/patch.html#quadra.datamodules.patch.PatchSklearnClassificationDataModule.train_dataloader","title":"<code>train_dataloader()</code>","text":"<p>Return the train dataloader.</p> Source code in <code>quadra/datamodules/patch.py</code> <pre><code>def train_dataloader(self) -&gt; DataLoader:\n\"\"\"Return the train dataloader.\"\"\"\n    if not self.train_dataset_available:\n        raise ValueError(\"No training sample is available\")\n    return DataLoader(\n        self.train_dataset,\n        batch_size=self.batch_size,\n        shuffle=True,\n        num_workers=self.num_workers,\n        drop_last=False,\n        pin_memory=True,\n    )\n</code></pre>"},{"location":"reference/quadra/datamodules/patch.html#quadra.datamodules.patch.PatchSklearnClassificationDataModule.val_dataloader","title":"<code>val_dataloader()</code>","text":"<p>Return the validation dataloader.</p> Source code in <code>quadra/datamodules/patch.py</code> <pre><code>def val_dataloader(self) -&gt; DataLoader:\n\"\"\"Return the validation dataloader.\"\"\"\n    if not self.val_dataset_available:\n        raise ValueError(\"No validation dataset is available\")\n    return DataLoader(\n        self.val_dataset,\n        batch_size=self.batch_size,\n        shuffle=False,\n        num_workers=self.num_workers,\n        drop_last=False,\n        pin_memory=True,\n    )\n</code></pre>"},{"location":"reference/quadra/datamodules/segmentation.html","title":"segmentation","text":""},{"location":"reference/quadra/datamodules/segmentation.html#quadra.datamodules.segmentation.SegmentationDataModule","title":"<code>SegmentationDataModule(data_path, name='segmentation_datamodule', test_size=0.3, val_size=0.3, seed=42, dataset=SegmentationDataset, batch_size=32, num_workers=6, train_transform=None, test_transform=None, val_transform=None, train_split_file=None, test_split_file=None, val_split_file=None, num_data_class=None, exclude_good=False, **kwargs)</code>","text":"<p>             Bases: <code>BaseDataModule</code></p> <p>Base class for segmentation datasets.</p> <p>Parameters:</p> <ul> <li> data_path             (<code>str</code>)         \u2013          <p>Path to the data main folder.</p> </li> <li> name             (<code>str</code>, default:                 <code>'segmentation_datamodule'</code> )         \u2013          <p>The name for the data module. Defaults to \"segmentation_datamodule\".</p> </li> <li> val_size             (<code>float</code>, default:                 <code>0.3</code> )         \u2013          <p>The validation split. Defaults to 0.2.</p> </li> <li> test_size             (<code>float</code>, default:                 <code>0.3</code> )         \u2013          <p>The test split. Defaults to 0.2.</p> </li> <li> seed             (<code>int</code>, default:                 <code>42</code> )         \u2013          <p>Random generator seed. Defaults to 42.</p> </li> <li> dataset             (<code>type[SegmentationDataset]</code>, default:                 <code>SegmentationDataset</code> )         \u2013          <p>Dataset class.</p> </li> <li> batch_size             (<code>int</code>, default:                 <code>32</code> )         \u2013          <p>Batch size. Defaults to 32.</p> </li> <li> num_workers             (<code>int</code>, default:                 <code>6</code> )         \u2013          <p>Number of workers for dataloaders. Defaults to 16.</p> </li> <li> train_transform             (<code>Compose | None</code>, default:                 <code>None</code> )         \u2013          <p>Transformations for train dataset. Defaults to None.</p> </li> <li> val_transform             (<code>Compose | None</code>, default:                 <code>None</code> )         \u2013          <p>Transformations for validation dataset. Defaults to None.</p> </li> <li> test_transform             (<code>Compose | None</code>, default:                 <code>None</code> )         \u2013          <p>Transformations for test dataset. Defaults to None.</p> </li> <li> num_data_class             (<code>int | None</code>, default:                 <code>None</code> )         \u2013          <p>The number of samples per class. Defaults to None.</p> </li> <li> exclude_good             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>If True, exclude good samples from the dataset. Defaults to False.</p> </li> </ul> Source code in <code>quadra/datamodules/segmentation.py</code> <pre><code>def __init__(\n    self,\n    data_path: str,\n    name: str = \"segmentation_datamodule\",\n    test_size: float = 0.3,\n    val_size: float = 0.3,\n    seed: int = 42,\n    dataset: type[SegmentationDataset] = SegmentationDataset,\n    batch_size: int = 32,\n    num_workers: int = 6,\n    train_transform: albumentations.Compose | None = None,\n    test_transform: albumentations.Compose | None = None,\n    val_transform: albumentations.Compose | None = None,\n    train_split_file: str | None = None,\n    test_split_file: str | None = None,\n    val_split_file: str | None = None,\n    num_data_class: int | None = None,\n    exclude_good: bool = False,\n    **kwargs: Any,\n):\n    super().__init__(\n        data_path=data_path,\n        name=name,\n        seed=seed,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        train_transform=train_transform,\n        val_transform=val_transform,\n        test_transform=test_transform,\n        **kwargs,\n    )\n    self.test_size = test_size\n    self.val_size = val_size\n    self.num_data_class = num_data_class\n    self.exclude_good = exclude_good\n    self.train_split_file = train_split_file\n    self.test_split_file = test_split_file\n    self.val_split_file = val_split_file\n    self.dataset = dataset\n    self.train_dataset: SegmentationDataset\n    self.val_dataset: SegmentationDataset\n    self.test_dataset: SegmentationDataset\n</code></pre>"},{"location":"reference/quadra/datamodules/segmentation.html#quadra.datamodules.segmentation.SegmentationDataModule.predict_dataloader","title":"<code>predict_dataloader()</code>","text":"<p>Returns a dataloader used for predictions.</p> Source code in <code>quadra/datamodules/segmentation.py</code> <pre><code>def predict_dataloader(self) -&gt; DataLoader:\n\"\"\"Returns a dataloader used for predictions.\"\"\"\n    return self.test_dataloader()\n</code></pre>"},{"location":"reference/quadra/datamodules/segmentation.html#quadra.datamodules.segmentation.SegmentationDataModule.setup","title":"<code>setup(stage=None)</code>","text":"<p>Setup data module based on stages of training.</p> Source code in <code>quadra/datamodules/segmentation.py</code> <pre><code>def setup(self, stage=None):\n\"\"\"Setup data module based on stages of training.\"\"\"\n    if stage in [\"fit\", \"train\"]:\n        self.train_dataset = self.dataset(\n            image_paths=self.data[self.data[\"split\"] == \"train\"][\"samples\"].tolist(),\n            mask_paths=self.data[self.data[\"split\"] == \"train\"][\"masks\"].tolist(),\n            mask_preprocess=self._preprocess_mask,\n            labels=self.data[self.data[\"split\"] == \"train\"][\"targets\"].tolist(),\n            object_masks=None,\n            transform=self.train_transform,\n            batch_size=None,\n            defect_transform=None,\n            resize=None,\n        )\n        self.val_dataset = self.dataset(\n            image_paths=self.data[self.data[\"split\"] == \"val\"][\"samples\"].tolist(),\n            mask_paths=self.data[self.data[\"split\"] == \"val\"][\"masks\"].tolist(),\n            defect_transform=None,\n            labels=self.data[self.data[\"split\"] == \"val\"][\"targets\"].tolist(),\n            object_masks=None,\n            batch_size=None,\n            mask_preprocess=self._preprocess_mask,\n            transform=self.test_transform,\n            resize=None,\n        )\n    elif stage == \"test\":\n        self.test_dataset = self.dataset(\n            image_paths=self.data[self.data[\"split\"] == \"test\"][\"samples\"].tolist(),\n            mask_paths=self.data[self.data[\"split\"] == \"test\"][\"masks\"].tolist(),\n            labels=self.data[self.data[\"split\"] == \"test\"][\"targets\"].tolist(),\n            object_masks=None,\n            batch_size=None,\n            mask_preprocess=self._preprocess_mask,\n            transform=self.test_transform,\n            resize=None,\n        )\n    elif stage == \"predict\":\n        pass\n    else:\n        raise ValueError(f\"Unknown stage {stage}\")\n</code></pre>"},{"location":"reference/quadra/datamodules/segmentation.html#quadra.datamodules.segmentation.SegmentationDataModule.test_dataloader","title":"<code>test_dataloader()</code>","text":"<p>Returns the test dataloader.</p> <p>Raises:</p> <ul> <li> <code>ValueError</code>           \u2013          <p>If test dataset is not initialized.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataLoader</code>         \u2013          <p>test dataloader.</p> </li> </ul> Source code in <code>quadra/datamodules/segmentation.py</code> <pre><code>def test_dataloader(self) -&gt; DataLoader:\n\"\"\"Returns the test dataloader.\n\n    Raises:\n        ValueError: If test dataset is not initialized.\n\n\n    Returns:\n        test dataloader.\n    \"\"\"\n    if not self.test_dataset_available:\n        raise ValueError(\"Test dataset is not initialized\")\n\n    loader = DataLoader(\n        self.test_dataset,\n        batch_size=self.batch_size,\n        shuffle=False,\n        num_workers=self.num_workers,\n        drop_last=False,\n        pin_memory=True,\n        persistent_workers=self.num_workers &gt; 0,\n    )\n    return loader\n</code></pre>"},{"location":"reference/quadra/datamodules/segmentation.html#quadra.datamodules.segmentation.SegmentationDataModule.train_dataloader","title":"<code>train_dataloader()</code>","text":"<p>Returns the train dataloader.</p> <p>Raises:</p> <ul> <li> <code>ValueError</code>           \u2013          <p>If train dataset is not initialized.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataLoader</code>         \u2013          <p>Train dataloader.</p> </li> </ul> Source code in <code>quadra/datamodules/segmentation.py</code> <pre><code>def train_dataloader(self) -&gt; DataLoader:\n\"\"\"Returns the train dataloader.\n\n    Raises:\n        ValueError: If train dataset is not initialized.\n\n    Returns:\n        Train dataloader.\n    \"\"\"\n    if not self.train_dataset_available:\n        raise ValueError(\"Train dataset is not initialized\")\n\n    return DataLoader(\n        self.train_dataset,\n        batch_size=self.batch_size,\n        shuffle=True,\n        num_workers=self.num_workers,\n        drop_last=False,\n        pin_memory=True,\n        persistent_workers=self.num_workers &gt; 0,\n    )\n</code></pre>"},{"location":"reference/quadra/datamodules/segmentation.html#quadra.datamodules.segmentation.SegmentationDataModule.val_dataloader","title":"<code>val_dataloader()</code>","text":"<p>Returns the validation dataloader.</p> <p>Raises:</p> <ul> <li> <code>ValueError</code>           \u2013          <p>If validation dataset is not initialized.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataLoader</code>         \u2013          <p>val dataloader.</p> </li> </ul> Source code in <code>quadra/datamodules/segmentation.py</code> <pre><code>def val_dataloader(self) -&gt; DataLoader:\n\"\"\"Returns the validation dataloader.\n\n    Raises:\n        ValueError: If validation dataset is not initialized.\n\n    Returns:\n        val dataloader.\n    \"\"\"\n    if not self.val_dataset_available:\n        raise ValueError(\"Validation dataset is not initialized\")\n\n    return DataLoader(\n        self.val_dataset,\n        batch_size=self.batch_size,\n        shuffle=False,\n        num_workers=self.num_workers,\n        drop_last=False,\n        pin_memory=True,\n        persistent_workers=self.num_workers &gt; 0,\n    )\n</code></pre>"},{"location":"reference/quadra/datamodules/segmentation.html#quadra.datamodules.segmentation.SegmentationMulticlassDataModule","title":"<code>SegmentationMulticlassDataModule(data_path, idx_to_class, name='multiclass_segmentation_datamodule', dataset=SegmentationDatasetMulticlass, batch_size=32, test_size=0.3, val_size=0.3, seed=42, num_workers=6, train_transform=None, test_transform=None, val_transform=None, train_split_file=None, test_split_file=None, val_split_file=None, exclude_good=False, num_data_train=None, one_hot_encoding=False, **kwargs)</code>","text":"<p>             Bases: <code>BaseDataModule</code></p> <p>Base class for segmentation datasets with multiple classes.</p> <p>Parameters:</p> <ul> <li> data_path         \u2013          <p>Path to the data main folder.</p> </li> <li> idx_to_class             (<code>dict</code>)         \u2013          <p>dict with corrispondence btw mask index and classes: {1: class_1, 2: class_2, ..., N: class_N} except background class which is 0.</p> </li> <li> name         \u2013          <p>The name for the data module. Defaults to \"multiclass_segmentation_datamodule\".</p> </li> <li> dataset             (<code>type[SegmentationDatasetMulticlass]</code>, default:                 <code>SegmentationDatasetMulticlass</code> )         \u2013          <p>Dataset class.</p> </li> <li> batch_size         \u2013          <p>Batch size. Defaults to 32.</p> </li> <li> val_size         \u2013          <p>The validation split. Defaults to 0.3.</p> </li> <li> test_size         \u2013          <p>The test split. Defaults to 0.3.</p> </li> <li> seed         \u2013          <p>Random generator seed. Defaults to 42.</p> </li> <li> num_workers             (<code>int</code>, default:                 <code>6</code> )         \u2013          <p>Number of workers for dataloaders. Defaults to 6.</p> </li> <li> train_transform             (<code>Compose | None</code>, default:                 <code>None</code> )         \u2013          <p>Transformations for train dataset. Defaults to None.</p> </li> <li> val_transform         \u2013          <p>Transformations for validation dataset. Defaults to None.</p> </li> <li> test_transform         \u2013          <p>Transformations for test dataset. Defaults to None.</p> </li> <li> train_split_file             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>path to txt file with training samples list</p> </li> <li> val_split_file             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>path to txt file with validation samples list</p> </li> <li> test_split_file             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>path to txt file with test samples list</p> </li> <li> exclude_good         \u2013          <p>If True, exclude good samples from the dataset. Defaults to False.</p> </li> <li> num_data_train             (<code>int | None</code>, default:                 <code>None</code> )         \u2013          <p>number of samples to use in the train split (shuffle the samples and pick the first num_data_train)</p> </li> <li> one_hot_encoding             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>if True, the labels are one-hot encoded to N channels, where N is the number of classes. If False, masks are single channel that contains values as class indexes. Defaults to True.</p> </li> </ul> Source code in <code>quadra/datamodules/segmentation.py</code> <pre><code>def __init__(\n    self,\n    data_path: str,\n    idx_to_class: dict,\n    name: str = \"multiclass_segmentation_datamodule\",\n    dataset: type[SegmentationDatasetMulticlass] = SegmentationDatasetMulticlass,\n    batch_size: int = 32,\n    test_size: float = 0.3,\n    val_size: float = 0.3,\n    seed: int = 42,\n    num_workers: int = 6,\n    train_transform: albumentations.Compose | None = None,\n    test_transform: albumentations.Compose | None = None,\n    val_transform: albumentations.Compose | None = None,\n    train_split_file: str | None = None,\n    test_split_file: str | None = None,\n    val_split_file: str | None = None,\n    exclude_good: bool = False,\n    num_data_train: int | None = None,\n    one_hot_encoding: bool = False,\n    **kwargs: Any,\n):\n    super().__init__(\n        data_path=data_path,\n        name=name,\n        seed=seed,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        train_transform=train_transform,\n        val_transform=val_transform,\n        test_transform=test_transform,\n        **kwargs,\n    )\n    self.test_size = test_size\n    self.val_size = val_size\n    self.exclude_good = exclude_good\n    self.train_split_file = train_split_file\n    self.test_split_file = test_split_file\n    self.val_split_file = val_split_file\n    self.dataset = dataset\n    self.idx_to_class = idx_to_class\n    self.num_data_train = num_data_train\n    self.one_hot_encoding = one_hot_encoding\n    self.train_dataset: SegmentationDataset\n    self.val_dataset: SegmentationDataset\n    self.test_dataset: SegmentationDataset\n</code></pre>"},{"location":"reference/quadra/datamodules/segmentation.html#quadra.datamodules.segmentation.SegmentationMulticlassDataModule.predict_dataloader","title":"<code>predict_dataloader()</code>","text":"<p>Returns a dataloader used for predictions.</p> Source code in <code>quadra/datamodules/segmentation.py</code> <pre><code>def predict_dataloader(self) -&gt; DataLoader:\n\"\"\"Returns a dataloader used for predictions.\"\"\"\n    return self.test_dataloader()\n</code></pre>"},{"location":"reference/quadra/datamodules/segmentation.html#quadra.datamodules.segmentation.SegmentationMulticlassDataModule.setup","title":"<code>setup(stage=None)</code>","text":"<p>Setup data module based on stages of training.</p> Source code in <code>quadra/datamodules/segmentation.py</code> <pre><code>def setup(self, stage=None):\n\"\"\"Setup data module based on stages of training.\"\"\"\n    if stage in [\"fit\", \"train\"]:\n        train_data = self.data[self.data[\"split\"] == \"train\"]\n        val_data = self.data[self.data[\"split\"] == \"val\"]\n\n        self.train_dataset = self.dataset(\n            image_paths=train_data[\"samples\"].tolist(),\n            mask_paths=train_data[\"masks\"].tolist(),\n            idx_to_class=self.idx_to_class,\n            transform=self.train_transform,\n            one_hot=self.one_hot_encoding,\n        )\n        self.val_dataset = self.dataset(\n            image_paths=val_data[\"samples\"].tolist(),\n            mask_paths=val_data[\"masks\"].tolist(),\n            transform=self.val_transform,\n            idx_to_class=self.idx_to_class,\n            one_hot=self.one_hot_encoding,\n        )\n    elif stage == \"test\":\n        self.test_dataset = self.dataset(\n            image_paths=self.data[self.data[\"split\"] == \"test\"][\"samples\"].tolist(),\n            mask_paths=self.data[self.data[\"split\"] == \"test\"][\"masks\"].tolist(),\n            transform=self.test_transform,\n            idx_to_class=self.idx_to_class,\n            one_hot=self.one_hot_encoding,\n        )\n    elif stage == \"predict\":\n        pass\n    else:\n        raise ValueError(f\"Unknown stage {stage}\")\n</code></pre>"},{"location":"reference/quadra/datamodules/segmentation.html#quadra.datamodules.segmentation.SegmentationMulticlassDataModule.test_dataloader","title":"<code>test_dataloader()</code>","text":"<p>Returns the test dataloader.</p> <p>Raises:</p> <ul> <li> <code>ValueError</code>           \u2013          <p>If test dataset is not initialized.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataLoader</code>         \u2013          <p>test dataloader.</p> </li> </ul> Source code in <code>quadra/datamodules/segmentation.py</code> <pre><code>def test_dataloader(self) -&gt; DataLoader:\n\"\"\"Returns the test dataloader.\n\n    Raises:\n        ValueError: If test dataset is not initialized.\n\n\n    Returns:\n        test dataloader.\n    \"\"\"\n    if not self.test_dataset_available:\n        raise ValueError(\"Test dataset is not initialized\")\n\n    loader = DataLoader(\n        self.test_dataset,\n        batch_size=self.batch_size,\n        shuffle=False,\n        num_workers=self.num_workers,\n        drop_last=False,\n        pin_memory=True,\n        persistent_workers=self.num_workers &gt; 0,\n    )\n    return loader\n</code></pre>"},{"location":"reference/quadra/datamodules/segmentation.html#quadra.datamodules.segmentation.SegmentationMulticlassDataModule.train_dataloader","title":"<code>train_dataloader()</code>","text":"<p>Returns the train dataloader.</p> <p>Raises:</p> <ul> <li> <code>ValueError</code>           \u2013          <p>If train dataset is not initialized.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataLoader</code>         \u2013          <p>Train dataloader.</p> </li> </ul> Source code in <code>quadra/datamodules/segmentation.py</code> <pre><code>def train_dataloader(self) -&gt; DataLoader:\n\"\"\"Returns the train dataloader.\n\n    Raises:\n        ValueError: If train dataset is not initialized.\n\n    Returns:\n        Train dataloader.\n    \"\"\"\n    if not self.train_dataset_available:\n        raise ValueError(\"Train dataset is not initialized\")\n\n    return DataLoader(\n        self.train_dataset,\n        batch_size=self.batch_size,\n        shuffle=True,\n        num_workers=self.num_workers,\n        drop_last=False,\n        pin_memory=True,\n        persistent_workers=self.num_workers &gt; 0,\n    )\n</code></pre>"},{"location":"reference/quadra/datamodules/segmentation.html#quadra.datamodules.segmentation.SegmentationMulticlassDataModule.val_dataloader","title":"<code>val_dataloader()</code>","text":"<p>Returns the validation dataloader.</p> <p>Raises:</p> <ul> <li> <code>ValueError</code>           \u2013          <p>If validation dataset is not initialized.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataLoader</code>         \u2013          <p>val dataloader.</p> </li> </ul> Source code in <code>quadra/datamodules/segmentation.py</code> <pre><code>def val_dataloader(self) -&gt; DataLoader:\n\"\"\"Returns the validation dataloader.\n\n    Raises:\n        ValueError: If validation dataset is not initialized.\n\n    Returns:\n        val dataloader.\n    \"\"\"\n    if not self.val_dataset_available:\n        raise ValueError(\"Validation dataset is not initialized\")\n\n    return DataLoader(\n        self.val_dataset,\n        batch_size=self.batch_size,\n        shuffle=False,\n        num_workers=self.num_workers,\n        drop_last=False,\n        pin_memory=True,\n        persistent_workers=self.num_workers &gt; 0,\n    )\n</code></pre>"},{"location":"reference/quadra/datamodules/ssl.html","title":"ssl","text":""},{"location":"reference/quadra/datamodules/ssl.html#quadra.datamodules.ssl.SSLDataModule","title":"<code>SSLDataModule(data_path, augmentation_dataset, name='ssl_datamodule', split_validation=True, **kwargs)</code>","text":"<p>             Bases: <code>ClassificationDataModule</code></p> <p>Base class for all data modules for self supervised learning data modules.</p> <p>Parameters:</p> <ul> <li> data_path             (<code>str</code>)         \u2013          <p>Path to the data main folder.</p> </li> <li> augmentation_dataset             (<code>TwoAugmentationDataset | TwoSetAugmentationDataset</code>)         \u2013          <p>Augmentation dataset for training dataset.</p> </li> <li> name             (<code>str</code>, default:                 <code>'ssl_datamodule'</code> )         \u2013          <p>The name for the data module. Defaults to  \"ssl_datamodule\".</p> </li> <li> split_validation             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Whether to split the validation set if . Defaults to True.</p> </li> <li> **kwargs             (<code>Any</code>, default:                 <code>{}</code> )         \u2013          <p>The keyword arguments for the classification data module. Defaults to None.</p> </li> </ul> Source code in <code>quadra/datamodules/ssl.py</code> <pre><code>def __init__(\n    self,\n    data_path: str,\n    augmentation_dataset: TwoAugmentationDataset | TwoSetAugmentationDataset,\n    name: str = \"ssl_datamodule\",\n    split_validation: bool = True,\n    **kwargs: Any,\n):\n    super().__init__(\n        data_path=data_path,\n        name=name,\n        **kwargs,\n    )\n    self.augmentation_dataset = augmentation_dataset\n    self.classifier_train_dataset: torch.utils.data.Dataset | None = None\n    self.split_validation = split_validation\n</code></pre>"},{"location":"reference/quadra/datamodules/ssl.html#quadra.datamodules.ssl.SSLDataModule.classifier_train_dataloader","title":"<code>classifier_train_dataloader()</code>","text":"<p>Returns classifier train dataloader.</p> Source code in <code>quadra/datamodules/ssl.py</code> <pre><code>def classifier_train_dataloader(self) -&gt; DataLoader:\n\"\"\"Returns classifier train dataloader.\"\"\"\n    if self.classifier_train_dataset is None:\n        raise ValueError(\"Classifier train dataset is not initialized\")\n\n    loader = DataLoader(\n        self.classifier_train_dataset,\n        batch_size=self.batch_size,\n        shuffle=True,\n        num_workers=self.num_workers,\n        drop_last=False,\n        pin_memory=True,\n        persistent_workers=self.num_workers &gt; 0,\n    )\n    return loader\n</code></pre>"},{"location":"reference/quadra/datamodules/ssl.html#quadra.datamodules.ssl.SSLDataModule.setup","title":"<code>setup(stage=None)</code>","text":"<p>Setup data module based on stages of training.</p> Source code in <code>quadra/datamodules/ssl.py</code> <pre><code>def setup(self, stage: str | None = None) -&gt; None:\n\"\"\"Setup data module based on stages of training.\"\"\"\n    if stage == \"fit\":\n        self.train_dataset = self.dataset(\n            samples=self.train_data[\"samples\"].tolist(),\n            targets=self.train_data[\"targets\"].tolist(),\n            transform=self.train_transform,\n        )\n\n        if np.unique(self.train_data[\"targets\"]).shape[0] &gt; 1 and not self.split_validation:\n            self.classifier_train_dataset = self.dataset(\n                samples=self.train_data[\"samples\"].tolist(),\n                targets=self.train_data[\"targets\"].tolist(),\n                transform=self.val_transform,\n            )\n            self.val_dataset = self.dataset(\n                samples=self.val_data[\"samples\"].tolist(),\n                targets=self.val_data[\"targets\"].tolist(),\n                transform=self.val_transform,\n            )\n        else:\n            train_classifier_samples, val_samples, train_classifier_targets, val_targets = train_test_split(\n                self.val_data[\"samples\"],\n                self.val_data[\"targets\"],\n                test_size=0.3,\n                random_state=self.seed,\n                stratify=self.val_data[\"targets\"],\n            )\n\n            self.classifier_train_dataset = self.dataset(\n                samples=train_classifier_samples,\n                targets=train_classifier_targets,\n                transform=self.test_transform,\n            )\n\n            self.val_dataset = self.dataset(\n                samples=val_samples,\n                targets=val_targets,\n                transform=self.val_transform,\n            )\n\n            log.warning(\n                \"The training set contains only one class and cannot be used to train a classifier. To overcome \"\n                \"this issue 70% of the validation set is used to train the classifier. The remaining will be used \"\n                \"as standard validation. To disable this behaviour set the `split_validation` parameter to False.\"\n            )\n            self._check_train_dataset_config()\n    if stage == \"test\":\n        self.test_dataset = self.dataset(\n            samples=self.test_data[\"samples\"].tolist(),\n            targets=self.test_data[\"targets\"].tolist(),\n            transform=self.test_transform,\n        )\n</code></pre>"},{"location":"reference/quadra/datamodules/ssl.html#quadra.datamodules.ssl.SSLDataModule.train_dataloader","title":"<code>train_dataloader()</code>","text":"<p>Returns train dataloader.</p> Source code in <code>quadra/datamodules/ssl.py</code> <pre><code>def train_dataloader(self) -&gt; DataLoader:\n\"\"\"Returns train dataloader.\"\"\"\n    if not isinstance(self.train_dataset, torch.utils.data.Dataset):\n        raise ValueError(\"Train dataset is not a subclass of `torch.utils.data.Dataset`\")\n    self.augmentation_dataset.dataset = self.train_dataset\n    loader = DataLoader(\n        self.augmentation_dataset,\n        batch_size=self.batch_size,\n        shuffle=True,\n        num_workers=self.num_workers,\n        drop_last=False,\n        pin_memory=True,\n        persistent_workers=self.num_workers &gt; 0,\n    )\n    return loader\n</code></pre>"},{"location":"reference/quadra/datamodules/generic/index.html","title":"generic","text":""},{"location":"reference/quadra/datamodules/generic/index.html#python-files","title":"Python Files","text":"<ul> <li>mvtec.py</li> <li>oxford_pet.py</li> <li>mnist.py</li> <li>imagenette.py </li> </ul>"},{"location":"reference/quadra/datamodules/generic/imagenette.html","title":"imagenette","text":""},{"location":"reference/quadra/datamodules/generic/imagenette.html#quadra.datamodules.generic.imagenette.ImagenetteClassificationDataModule","title":"<code>ImagenetteClassificationDataModule(data_path, name='imagenette_classification_datamodule', imagenette_version='320', force_download=False, class_to_idx=None, **kwargs)</code>","text":"<p>             Bases: <code>ClassificationDataModule</code></p> <p>Initializes the classification data module for Imagenette dataset.</p> <p>Parameters:</p> <ul> <li> data_path             (<code>str</code>)         \u2013          <p>Path to the dataset.</p> </li> <li> name             (<code>str</code>, default:                 <code>'imagenette_classification_datamodule'</code> )         \u2013          <p>Name of the dataset.</p> </li> <li> imagenette_version             (<code>str</code>, default:                 <code>'320'</code> )         \u2013          <p>Version of the Imagenette dataset. Can be 320 or 160 or full.</p> </li> <li> force_download             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>If True, the dataset will be downloaded even if the data_path already exists. The data_path will be deleted and recreated.</p> </li> <li> class_to_idx             (<code>dict[str, int] | None</code>, default:                 <code>None</code> )         \u2013          <p>Dictionary mapping class names to class indices.</p> </li> <li> **kwargs             (<code>Any</code>, default:                 <code>{}</code> )         \u2013          <p>Keyword arguments for the ClassificationDataModule.</p> </li> </ul> Source code in <code>quadra/datamodules/generic/imagenette.py</code> <pre><code>def __init__(\n    self,\n    data_path: str,\n    name: str = \"imagenette_classification_datamodule\",\n    imagenette_version: str = \"320\",\n    force_download: bool = False,\n    class_to_idx: dict[str, int] | None = None,\n    **kwargs: Any,\n):\n    if imagenette_version not in [\"320\", \"160\", \"full\"]:\n        raise ValueError(f\"imagenette_version must be one of 320, 160 or full. Got {imagenette_version} instead.\")\n\n    if imagenette_version == \"full\":\n        imagenette_version = \"\"\n    else:\n        imagenette_version = f\"-{imagenette_version}\"\n\n    self.download_url = f\"https://s3.amazonaws.com/fast-ai-imageclas/imagenette2{imagenette_version}.tgz\"\n    self.force_download = force_download\n    self.imagenette_version = imagenette_version\n\n    if class_to_idx is None:\n        class_to_idx = DEFAULT_CLASS_TO_IDX\n\n    super().__init__(\n        data_path=data_path,\n        name=name,\n        test_split_file=None,\n        train_split_file=None,\n        val_size=None,\n        class_to_idx=class_to_idx,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/quadra/datamodules/generic/imagenette.html#quadra.datamodules.generic.imagenette.ImagenetteClassificationDataModule.download_data","title":"<code>download_data(download_url, force_download=False)</code>","text":"<p>Download the Imagenette dataset.</p> <p>Parameters:</p> <ul> <li> download_url             (<code>str</code>)         \u2013          <p>Dataset download url.</p> </li> <li> force_download             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>If True, the dataset will be downloaded even if the data_path already exists. The data_path will be removed.</p> </li> </ul> Source code in <code>quadra/datamodules/generic/imagenette.py</code> <pre><code>def download_data(self, download_url: str, force_download: bool = False) -&gt; None:\n\"\"\"Download the Imagenette dataset.\n\n    Args:\n        download_url: Dataset download url.\n        force_download: If True, the dataset will be downloaded even if the data_path already exists. The data_path\n            will be removed.\n    \"\"\"\n    if os.path.exists(self.data_path):\n        if force_download:\n            log.info(\"The path %s already exists. Removing it and downloading the dataset again.\", self.data_path)\n            shutil.rmtree(self.data_path)\n        else:\n            log.info(\"The path %s already exists. Skipping download.\", self.data_path)\n            return\n\n    log.info(\"Downloading and extracting Imagenette dataset to %s\", self.data_path)\n    download_and_extract_archive(download_url, self.data_path, remove_finished=True)\n</code></pre>"},{"location":"reference/quadra/datamodules/generic/imagenette.html#quadra.datamodules.generic.imagenette.ImagenetteSSLDataModule","title":"<code>ImagenetteSSLDataModule(*args, name='imagenette_ssl', **kwargs)</code>","text":"<p>             Bases: <code>ImagenetteClassificationDataModule</code>, <code>SSLDataModule</code></p> <p>Initializes the SSL data module for Imagenette dataset.</p> Source code in <code>quadra/datamodules/generic/imagenette.py</code> <pre><code>def __init__(\n    self,\n    *args: Any,\n    name=\"imagenette_ssl\",\n    **kwargs: Any,\n):\n    super().__init__(*args, name=name, **kwargs)  # type: ignore[misc]\n</code></pre>"},{"location":"reference/quadra/datamodules/generic/mnist.html","title":"mnist","text":""},{"location":"reference/quadra/datamodules/generic/mnist.html#quadra.datamodules.generic.mnist.MNISTAnomalyDataModule","title":"<code>MNISTAnomalyDataModule(data_path, good_number, limit_data=100, category=None, **kwargs)</code>","text":"<p>             Bases: <code>AnomalyDataModule</code></p> <p>Standard anomaly datamodule with automatic download of the MNIST dataset.</p> <p>Parameters:</p> <ul> <li> data_path             (<code>str</code>)         \u2013          <p>Path to the dataset</p> </li> <li> good_number             (<code>int</code>)         \u2013          <p>Which number to use as a good class, all other numbers are considered anomalies.</p> </li> <li> category             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>The category of the dataset. For mnist this is always None.</p> </li> <li> limit_data             (<code>int</code>, default:                 <code>100</code> )         \u2013          <p>Limit the number of images to use for training and testing. Defaults to 100.</p> </li> <li> **kwargs             (<code>Any</code>, default:                 <code>{}</code> )         \u2013          <p>Additional arguments to pass to the AnomalyDataModule.</p> </li> </ul> Source code in <code>quadra/datamodules/generic/mnist.py</code> <pre><code>def __init__(\n    self, data_path: str, good_number: int, limit_data: int = 100, category: str | None = None, **kwargs: Any\n):\n\"\"\"Initialize the MNIST anomaly datamodule.\n\n    Args:\n        data_path: Path to the dataset\n        good_number: Which number to use as a good class, all other numbers are considered anomalies.\n        category: The category of the dataset. For mnist this is always None.\n        limit_data: Limit the number of images to use for training and testing. Defaults to 100.\n        **kwargs: Additional arguments to pass to the AnomalyDataModule.\n    \"\"\"\n    super().__init__(data_path=data_path, category=None, **kwargs)\n    self.good_number = good_number\n    self.limit_data = limit_data\n</code></pre>"},{"location":"reference/quadra/datamodules/generic/mnist.html#quadra.datamodules.generic.mnist.MNISTAnomalyDataModule.download_data","title":"<code>download_data()</code>","text":"<p>Download the MNIST dataset and move images in the right folders.</p> Source code in <code>quadra/datamodules/generic/mnist.py</code> <pre><code>def download_data(self) -&gt; None:\n\"\"\"Download the MNIST dataset and move images in the right folders.\"\"\"\n    log.info(\"Generating MNIST anomaly dataset for good number %s\", self.good_number)\n\n    mnist_train_dataset = MNIST(root=self.data_path, train=True, download=True)\n    mnist_test_dataset = MNIST(root=self.data_path, train=False, download=True)\n\n    self.data_path = os.path.join(self.data_path, \"quadra_mnist_anomaly\")\n\n    if os.path.exists(self.data_path):\n        shutil.rmtree(self.data_path)\n\n    # Create the folder structure\n    train_good_folder = os.path.join(self.data_path, \"train\", \"good\")\n    test_good_folder = os.path.join(self.data_path, \"test\", \"good\")\n\n    os.makedirs(train_good_folder, exist_ok=True)\n    os.makedirs(test_good_folder, exist_ok=True)\n\n    # Copy the good train images to the correct folder\n    good_train_samples = mnist_train_dataset.data[mnist_train_dataset.targets == self.good_number]\n    for i, image in enumerate(good_train_samples.numpy()):\n        if i == self.limit_data:\n            break\n        cv2.imwrite(os.path.join(train_good_folder, f\"{i}.png\"), image)\n\n    for number in range(10):\n        if number == self.good_number:\n            good_train_samples = mnist_test_dataset.data[mnist_test_dataset.targets == number]\n            for i, image in enumerate(good_train_samples.numpy()):\n                if i == self.limit_data:\n                    break\n                cv2.imwrite(os.path.join(test_good_folder, f\"{number}_{i}.png\"), image)\n        else:\n            test_bad_folder = os.path.join(self.data_path, \"test\", str(number))\n            os.makedirs(test_bad_folder, exist_ok=True)\n            bad_train_samples = mnist_train_dataset.data[mnist_train_dataset.targets == number]\n            for i, image in enumerate(bad_train_samples.numpy()):\n                if i == self.limit_data:\n                    break\n\n                cv2.imwrite(os.path.join(test_bad_folder, f\"{number}_{i}.png\"), image)\n</code></pre>"},{"location":"reference/quadra/datamodules/generic/mvtec.html","title":"mvtec","text":""},{"location":"reference/quadra/datamodules/generic/mvtec.html#quadra.datamodules.generic.mvtec.MVTecDataModule","title":"<code>MVTecDataModule(data_path, category, **kwargs)</code>","text":"<p>             Bases: <code>AnomalyDataModule</code></p> <p>Standard anomaly datamodule with automatic download of the MVTec dataset.</p> Source code in <code>quadra/datamodules/generic/mvtec.py</code> <pre><code>def __init__(self, data_path: str, category: str, **kwargs):\n    if category not in DATASET_URL:\n        raise ValueError(f\"Unknown category {category}. Available categories are {list(DATASET_URL.keys())}\")\n\n    super().__init__(data_path=data_path, category=category, **kwargs)\n</code></pre>"},{"location":"reference/quadra/datamodules/generic/mvtec.html#quadra.datamodules.generic.mvtec.MVTecDataModule.download_data","title":"<code>download_data()</code>","text":"<p>Download the MVTec dataset.</p> Source code in <code>quadra/datamodules/generic/mvtec.py</code> <pre><code>def download_data(self) -&gt; None:\n\"\"\"Download the MVTec dataset.\"\"\"\n    if self.category is None:\n        raise ValueError(\"Category must be specified for MVTec dataset.\")\n\n    if os.path.exists(self.data_path):\n        log.info(\"The path %s already exists. Skipping download.\", os.path.join(self.data_path, self.category))\n        return\n\n    log.info(\"Downloading and extracting MVTec dataset for category %s to %s\", self.category, self.data_path)\n    # self.data_path is the path to the category folder that will be created by the download_and_extract_archive\n    data_path_no_category = str(Path(self.data_path).parent)\n    download_and_extract_archive(DATASET_URL[self.category], data_path_no_category, remove_finished=True)\n</code></pre>"},{"location":"reference/quadra/datamodules/generic/oxford_pet.html","title":"oxford_pet","text":""},{"location":"reference/quadra/datamodules/generic/oxford_pet.html#quadra.datamodules.generic.oxford_pet.OxfordPetSegmentationDataModule","title":"<code>OxfordPetSegmentationDataModule(data_path, idx_to_class, name='oxford_pet_segmentation_datamodule', dataset=SegmentationDatasetMulticlass, batch_size=32, test_size=0.3, val_size=0.3, seed=42, num_workers=6, train_transform=None, test_transform=None, val_transform=None, **kwargs)</code>","text":"<p>             Bases: <code>SegmentationMulticlassDataModule</code></p> <p>OxfordPetSegmentationDataModule.</p> <p>Parameters:</p> <ul> <li> data_path             (<code>str</code>)         \u2013          <p>path to the oxford pet dataset</p> </li> <li> idx_to_class             (<code>dict</code>)         \u2013          <p>dict with corrispondence btw mask index and classes: {1: class_1, 2: class_2, ..., N: class_N} except background class which is 0.</p> </li> <li> name             (<code>str</code>, default:                 <code>'oxford_pet_segmentation_datamodule'</code> )         \u2013          <p>Defaults to \"oxford_pet_segmentation_datamodule\".</p> </li> <li> dataset             (<code>type[SegmentationDatasetMulticlass]</code>, default:                 <code>SegmentationDatasetMulticlass</code> )         \u2013          <p>Defaults to SegmentationDataset.</p> </li> <li> batch_size             (<code>int</code>, default:                 <code>32</code> )         \u2013          <p>batch size for training. Defaults to 32.</p> </li> <li> test_size             (<code>float</code>, default:                 <code>0.3</code> )         \u2013          <p>Defaults to 0.3.</p> </li> <li> val_size             (<code>float</code>, default:                 <code>0.3</code> )         \u2013          <p>Defaults to 0.3.</p> </li> <li> seed             (<code>int</code>, default:                 <code>42</code> )         \u2013          <p>Defaults to 42.</p> </li> <li> num_workers             (<code>int</code>, default:                 <code>6</code> )         \u2013          <p>number of workers for data loading. Defaults to 6.</p> </li> <li> train_transform             (<code>Compose | None</code>, default:                 <code>None</code> )         \u2013          <p>Train transform. Defaults to None.</p> </li> <li> test_transform             (<code>Compose | None</code>, default:                 <code>None</code> )         \u2013          <p>Test transform. Defaults to None.</p> </li> <li> val_transform             (<code>Compose | None</code>, default:                 <code>None</code> )         \u2013          <p>Validation transform. Defaults to None.</p> </li> </ul> Source code in <code>quadra/datamodules/generic/oxford_pet.py</code> <pre><code>def __init__(\n    self,\n    data_path: str,\n    idx_to_class: dict,\n    name: str = \"oxford_pet_segmentation_datamodule\",\n    dataset: type[SegmentationDatasetMulticlass] = SegmentationDatasetMulticlass,\n    batch_size: int = 32,\n    test_size: float = 0.3,\n    val_size: float = 0.3,\n    seed: int = 42,\n    num_workers: int = 6,\n    train_transform: albumentations.Compose | None = None,\n    test_transform: albumentations.Compose | None = None,\n    val_transform: albumentations.Compose | None = None,\n    **kwargs: Any,\n):\n    super().__init__(\n        data_path=data_path,\n        idx_to_class=idx_to_class,\n        name=name,\n        dataset=dataset,\n        batch_size=batch_size,\n        test_size=test_size,\n        val_size=val_size,\n        seed=seed,\n        num_workers=num_workers,\n        train_transform=train_transform,\n        test_transform=test_transform,\n        val_transform=val_transform,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/quadra/datamodules/generic/oxford_pet.html#quadra.datamodules.generic.oxford_pet.OxfordPetSegmentationDataModule.download_data","title":"<code>download_data()</code>","text":"<p>Download the dataset if it is not already downloaded.</p> Source code in <code>quadra/datamodules/generic/oxford_pet.py</code> <pre><code>def download_data(self):\n\"\"\"Download the dataset if it is not already downloaded.\"\"\"\n    image_folder = os.path.join(self.data_path, \"images\")\n    annotation_folder = os.path.join(self.data_path, \"annotations\")\n    if not self._check_exists(image_folder, annotation_folder):\n        for url, md5 in self._RESOURCES:\n            download_and_extract_archive(url, download_root=self.data_path, md5=md5, remove_finished=True)\n        log.info(\"Fixing corrupted files...\")\n        images_filenames = sorted(os.listdir(image_folder))\n        for filename in images_filenames:\n            file_wo_ext = os.path.splitext(os.path.basename(filename))[0]\n            try:\n                mask = cv2.imread(os.path.join(annotation_folder, \"trimaps\", file_wo_ext + \".png\"))\n                mask = self._preprocess_mask(mask)\n                if np.sum(mask) == 0:\n                    os.remove(os.path.join(image_folder, filename))\n                    os.remove(os.path.join(annotation_folder, \"trimaps\", file_wo_ext + \".png\"))\n                    log.info(\"Removed %s\", filename)\n                else:\n                    img = cv2.imread(os.path.join(image_folder, filename))\n                    cv2.imwrite(os.path.join(image_folder, file_wo_ext + \".jpg\"), img)\n            except Exception:\n                ip = os.path.join(image_folder, filename)\n                mp = os.path.join(annotation_folder, \"trimaps\", file_wo_ext + \".png\")\n                if os.path.exists(ip):\n                    os.remove(ip)\n                if os.path.exists(mp):\n                    os.remove(mp)\n                log.info(\"Removed %s\", filename)\n</code></pre>"},{"location":"reference/quadra/datasets/index.html","title":"datasets","text":""},{"location":"reference/quadra/datasets/index.html#quadra.datasets.AnomalyDataset","title":"<code>AnomalyDataset(transform, samples, task='segmentation', valid_area_mask=None, crop_area=None)</code>","text":"<p>             Bases: <code>Dataset</code></p> <p>Anomaly Dataset.</p> <p>Parameters:</p> <ul> <li> transform             (<code>Compose</code>)         \u2013          <p>Albumentations compose.</p> </li> <li> task             (<code>str</code>, default:                 <code>'segmentation'</code> )         \u2013          <p><code>classification</code> or <code>segmentation</code></p> </li> <li> samples             (<code>DataFrame</code>)         \u2013          <p>Pandas dataframe containing samples following the same structure created by make_anomaly_dataset</p> </li> <li> valid_area_mask             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>Optional path to the mask to use to filter out the valid area of the image. If None, the whole image is considered valid.</p> </li> <li> crop_area             (<code>tuple[int, int, int, int] | None</code>, default:                 <code>None</code> )         \u2013          <p>Optional tuple of 4 integers (x1, y1, x2, y2) to crop the image to the specified area. If None, the whole image is considered valid.</p> </li> </ul> Source code in <code>quadra/datasets/anomaly.py</code> <pre><code>def __init__(\n    self,\n    transform: alb.Compose,\n    samples: DataFrame,\n    task: str = \"segmentation\",\n    valid_area_mask: str | None = None,\n    crop_area: tuple[int, int, int, int] | None = None,\n) -&gt; None:\n    self.task = task\n    self.transform = transform\n\n    self.samples = samples\n    self.samples = self.samples.reset_index(drop=True)\n    self.split = self.samples.split.unique()[0]\n\n    self.crop_area = crop_area\n    self.valid_area_mask: np.ndarray | None = None\n\n    if valid_area_mask is not None:\n        if not os.path.exists(valid_area_mask):\n            raise RuntimeError(f\"Valid area mask {valid_area_mask} does not exist.\")\n\n        self.valid_area_mask = cv2.imread(valid_area_mask, 0) &gt; 0  # type: ignore[operator]\n</code></pre>"},{"location":"reference/quadra/datasets/index.html#quadra.datasets.AnomalyDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Get dataset item for the index <code>index</code>.</p> <p>Parameters:</p> <ul> <li> index             (<code>int</code>)         \u2013          <p>Index to get the item.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, str | Tensor]</code>         \u2013          <p>Dict of image tensor during training.</p> </li> <li> <code>dict[str, str | Tensor]</code>         \u2013          <p>Otherwise, Dict containing image path, target path, image tensor, label and transformed bounding box.</p> </li> </ul> Source code in <code>quadra/datasets/anomaly.py</code> <pre><code>def __getitem__(self, index: int) -&gt; dict[str, str | Tensor]:\n\"\"\"Get dataset item for the index ``index``.\n\n    Args:\n        index: Index to get the item.\n\n    Returns:\n        Dict of image tensor during training.\n        Otherwise, Dict containing image path, target path, image tensor, label and transformed bounding box.\n    \"\"\"\n    item: dict[str, str | Tensor] = {}\n\n    image_path = self.samples.samples.iloc[index]\n    image = cv2.imread(image_path)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n    original_image_shape = image.shape\n    if self.valid_area_mask is not None:\n        image = image * self.valid_area_mask[:, :, np.newaxis]\n\n    if self.crop_area is not None:\n        image = image[self.crop_area[1] : self.crop_area[3], self.crop_area[0] : self.crop_area[2]]\n\n    label_index = self.samples.label_index[index]\n\n    if self.split == \"train\":\n        pre_processed = self.transform(image=image)\n        item = {\"image\": pre_processed[\"image\"], \"label\": label_index}\n    elif self.split in [\"val\", \"test\"]:\n        item[\"image_path\"] = image_path\n        item[\"label\"] = label_index\n\n        if self.task == \"segmentation\":\n            mask_path = self.samples.mask_path[index]\n\n            # If good images have no associated mask create an empty one\n            if label_index == 0:\n                mask = np.zeros(shape=original_image_shape[:2])\n            elif os.path.isfile(mask_path):\n                mask = cv2.imread(mask_path, flags=0) / 255.0  # type: ignore[operator]\n            else:\n                # We need ones in the mask to compute correctly at least image level f1 score\n                mask = np.ones(shape=original_image_shape[:2])\n\n            if self.valid_area_mask is not None:\n                mask = mask * self.valid_area_mask\n\n            if self.crop_area is not None:\n                mask = mask[self.crop_area[1] : self.crop_area[3], self.crop_area[0] : self.crop_area[2]]\n\n            pre_processed = self.transform(image=image, mask=mask)\n\n            item[\"mask_path\"] = mask_path\n            item[\"mask\"] = pre_processed[\"mask\"]\n        else:\n            pre_processed = self.transform(image=image)\n\n        item[\"image\"] = pre_processed[\"image\"]\n    return item\n</code></pre>"},{"location":"reference/quadra/datasets/index.html#quadra.datasets.AnomalyDataset.__len__","title":"<code>__len__()</code>","text":"<p>Get length of the dataset.</p> Source code in <code>quadra/datasets/anomaly.py</code> <pre><code>def __len__(self) -&gt; int:\n\"\"\"Get length of the dataset.\"\"\"\n    return len(self.samples)\n</code></pre>"},{"location":"reference/quadra/datasets/index.html#quadra.datasets.ClassificationDataset","title":"<code>ClassificationDataset(samples, targets, class_to_idx=None, resize=None, roi=None, transform=None, rgb=True, channel=3, random_padding=False, circular_crop=False)</code>","text":"<p>             Bases: <code>ImageClassificationListDataset</code></p> <p>Custom Classification Dataset.</p> <p>Parameters:</p> <ul> <li> samples             (<code>list[str]</code>)         \u2013          <p>List of paths to images</p> </li> <li> targets             (<code>list[str | int]</code>)         \u2013          <p>List of targets</p> </li> <li> class_to_idx             (<code>dict | None</code>, default:                 <code>None</code> )         \u2013          <p>Defaults to None.</p> </li> <li> resize             (<code>int | None</code>, default:                 <code>None</code> )         \u2013          <p>Resize image to this size. Defaults to None.</p> </li> <li> roi             (<code>tuple[int, int, int, int] | None</code>, default:                 <code>None</code> )         \u2013          <p>Region of interest. Defaults to None.</p> </li> <li> transform             (<code>Callable | None</code>, default:                 <code>None</code> )         \u2013          <p>transform function. Defaults to None.</p> </li> <li> rgb             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Use RGB space</p> </li> <li> channel             (<code>int</code>, default:                 <code>3</code> )         \u2013          <p>Number of channels. Defaults to 3.</p> </li> <li> random_padding             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Random padding. Defaults to False.</p> </li> <li> circular_crop             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Circular crop. Defaults to False.</p> </li> </ul> Source code in <code>quadra/datasets/classification.py</code> <pre><code>def __init__(\n    self,\n    samples: list[str],\n    targets: list[str | int],\n    class_to_idx: dict | None = None,\n    resize: int | None = None,\n    roi: tuple[int, int, int, int] | None = None,\n    transform: Callable | None = None,\n    rgb: bool = True,\n    channel: int = 3,\n    random_padding: bool = False,\n    circular_crop: bool = False,\n):\n    super().__init__(samples, targets, class_to_idx, resize, roi, transform, rgb, channel)\n    if transform is None:\n        self.transform = None\n\n    self.random_padding = random_padding\n    self.circular_crop = circular_crop\n</code></pre>"},{"location":"reference/quadra/datasets/index.html#quadra.datasets.ImageClassificationListDataset","title":"<code>ImageClassificationListDataset(samples, targets, class_to_idx=None, resize=None, roi=None, transform=None, rgb=True, channel=3, allow_missing_label=False)</code>","text":"<p>             Bases: <code>Dataset</code></p> <p>Standard classification dataset.</p> <p>Parameters:</p> <ul> <li> samples             (<code>list[str]</code>)         \u2013          <p>List of paths to images to be read</p> </li> <li> targets             (<code>list[str | int]</code>)         \u2013          <p>List of labels, one for every image in samples</p> </li> <li> class_to_idx             (<code>dict | None</code>, default:                 <code>None</code> )         \u2013          <p>mapping from classes to unique indexes. Defaults to None.</p> </li> <li> resize             (<code>int | None</code>, default:                 <code>None</code> )         \u2013          <p>Integer specifying the size of a first optional resize keeping the aspect ratio: the smaller side of the image will be resized to <code>resize</code>, while the longer will be resized keeping the aspect ratio. Defaults to None.</p> </li> <li> roi             (<code>tuple[int, int, int, int] | None</code>, default:                 <code>None</code> )         \u2013          <p>Optional ROI, with (x_upper_left, y_upper_left, x_bottom_right, y_bottom_right). Defaults to None.</p> </li> <li> transform             (<code>Callable | None</code>, default:                 <code>None</code> )         \u2013          <p>Optional Albumentations transform. Defaults to None.</p> </li> <li> rgb             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>if False, image will be converted in grayscale</p> </li> <li> channel             (<code>int</code>, default:                 <code>3</code> )         \u2013          <p>1 or 3. If rgb is True, then channel will be set at 3.</p> </li> <li> allow_missing_label             (<code>bool | None</code>, default:                 <code>False</code> )         \u2013          <p>If set to false warn the user if the dataset contains missing labels</p> </li> </ul> Source code in <code>quadra/datasets/classification.py</code> <pre><code>def __init__(\n    self,\n    samples: list[str],\n    targets: list[str | int],\n    class_to_idx: dict | None = None,\n    resize: int | None = None,\n    roi: tuple[int, int, int, int] | None = None,\n    transform: Callable | None = None,\n    rgb: bool = True,\n    channel: int = 3,\n    allow_missing_label: bool | None = False,\n):\n    super().__init__()\n    assert len(samples) == len(\n        targets\n    ), f\"Samples ({len(samples)}) and targets ({len(targets)}) must have the same length\"\n    # Setting the ROI\n    self.roi = roi\n\n    # Keep-Aspect-Ratio resize\n    self.resize = resize\n\n    if not allow_missing_label and None in targets:\n        warnings.warn(\n            (\n                \"Dataset contains empty targets but allow_missing_label is set to False, \"\n                \"be careful because None labels will not work inside Dataloaders\"\n            ),\n            UserWarning,\n            stacklevel=2,\n        )\n\n    targets = [-1 if target is None else target for target in targets]\n    # Data\n    self.x = np.array(samples)\n    self.y = np.array(targets)\n\n    if class_to_idx is None:\n        unique_targets = np.unique(targets)\n        class_to_idx = {c: i for i, c in enumerate(unique_targets)}\n\n    self.class_to_idx = class_to_idx\n    self.idx_to_class = {v: k for k, v in class_to_idx.items()}\n    self.samples = [\n        (path, self.class_to_idx[self.y[i]] if (self.y[i] != -1 and self.y[i] != \"-1\") else -1)\n        for i, path in enumerate(self.x)\n    ]\n\n    self.rgb = rgb\n    self.channel = 3 if rgb else channel\n\n    self.transform = transform\n</code></pre>"},{"location":"reference/quadra/datasets/index.html#quadra.datasets.MultilabelClassificationDataset","title":"<code>MultilabelClassificationDataset(samples, targets, class_to_idx=None, transform=None, rgb=True)</code>","text":"<p>             Bases: <code>Dataset</code></p> <p>Custom MultilabelClassification Dataset.</p> <p>Parameters:</p> <ul> <li> samples             (<code>list[str]</code>)         \u2013          <p>list of paths to images.</p> </li> <li> targets             (<code>ndarray</code>)         \u2013          <p>array of multiple targets per sample. The array must be a one-hot enoding. It must have a shape of (n_samples, n_targets).</p> </li> <li> class_to_idx             (<code>dict | None</code>, default:                 <code>None</code> )         \u2013          <p>Defaults to None.</p> </li> <li> transform             (<code>Callable | None</code>, default:                 <code>None</code> )         \u2013          <p>transform function. Defaults to None.</p> </li> <li> rgb             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Use RGB space</p> </li> </ul> Source code in <code>quadra/datasets/classification.py</code> <pre><code>def __init__(\n    self,\n    samples: list[str],\n    targets: np.ndarray,\n    class_to_idx: dict | None = None,\n    transform: Callable | None = None,\n    rgb: bool = True,\n):\n    super().__init__()\n    assert len(samples) == len(\n        targets\n    ), f\"Samples ({len(samples)}) and targets ({len(targets)}) must have the same length\"\n\n    # Data\n    self.x = samples\n    self.y = targets\n\n    # Class to idx and the other way around\n    if class_to_idx is None:\n        unique_targets = targets.shape[1]\n        class_to_idx = {c: i for i, c in enumerate(range(unique_targets))}\n    self.class_to_idx = class_to_idx\n    self.idx_to_class = {v: k for k, v in class_to_idx.items()}\n    self.samples = list(zip(self.x, self.y))\n    self.rgb = rgb\n    self.transform = transform\n</code></pre>"},{"location":"reference/quadra/datasets/index.html#quadra.datasets.PatchSklearnClassificationTrainDataset","title":"<code>PatchSklearnClassificationTrainDataset(data_path, samples, targets, class_to_idx=None, resize=None, transform=None, rgb=True, channel=3, balance_classes=False)</code>","text":"<p>             Bases: <code>Dataset</code></p> <p>Dataset used for patch sampling, it expects samples to be paths to h5 files containing all the required information for patch sampling from images.</p> <p>Parameters:</p> <ul> <li> data_path             (<code>str</code>)         \u2013          <p>base path to the dataset</p> </li> <li> samples             (<code>list[str]</code>)         \u2013          <p>Paths to h5 files</p> </li> <li> targets             (<code>list[str | int]</code>)         \u2013          <p>Labels associated with each sample</p> </li> <li> class_to_idx             (<code>dict | None</code>, default:                 <code>None</code> )         \u2013          <p>Mapping between class and corresponding index</p> </li> <li> resize             (<code>int | None</code>, default:                 <code>None</code> )         \u2013          <p>Whether to perform an aspect ratio resize of the patch before the transformations</p> </li> <li> transform             (<code>Callable | None</code>, default:                 <code>None</code> )         \u2013          <p>Optional function applied to the image</p> </li> <li> rgb             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>if False, image will be converted in grayscale</p> </li> <li> channel             (<code>int</code>, default:                 <code>3</code> )         \u2013          <p>1 or 3. If rgb is True, then channel will be set at 3.</p> </li> <li> balance_classes             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>if True, the dataset will be balanced by duplicating samples of the minority class</p> </li> </ul> Source code in <code>quadra/datasets/patch.py</code> <pre><code>def __init__(\n    self,\n    data_path: str,\n    samples: list[str],\n    targets: list[str | int],\n    class_to_idx: dict | None = None,\n    resize: int | None = None,\n    transform: Callable | None = None,\n    rgb: bool = True,\n    channel: int = 3,\n    balance_classes: bool = False,\n):\n    super().__init__()\n\n    # Keep-Aspect-Ratio resize\n    self.resize = resize\n    self.data_path = data_path\n\n    if balance_classes:\n        samples_array = np.array(samples)\n        targets_array = np.array(targets)\n        samples_to_use: list[str] = []\n        targets_to_use: list[str | int] = []\n\n        cls, counts = np.unique(targets_array, return_counts=True)\n        max_count = np.max(counts)\n        for cl, count in zip(cls, counts):\n            idx_to_pick = list(np.where(targets_array == cl)[0])\n\n            if count &lt; max_count:\n                idx_to_pick += random.choices(idx_to_pick, k=max_count - count)\n\n            samples_to_use.extend(samples_array[idx_to_pick])\n            targets_to_use.extend(targets_array[idx_to_pick])\n    else:\n        samples_to_use = samples\n        targets_to_use = targets\n\n    # Data\n    self.x = np.array(samples_to_use)\n    self.y = np.array(targets_to_use)\n\n    if class_to_idx is None:\n        unique_targets = np.unique(targets_to_use)\n        class_to_idx = {c: i for i, c in enumerate(unique_targets)}\n\n    self.class_to_idx = class_to_idx\n    self.idx_to_class = {v: k for k, v in class_to_idx.items()}\n\n    self.samples = [\n        (path, self.class_to_idx[self.y[i]] if self.y[i] is not None else None) for i, path in enumerate(self.x)\n    ]\n\n    self.rgb = rgb\n    self.channel = 3 if rgb else channel\n\n    self.transform = transform\n</code></pre>"},{"location":"reference/quadra/datasets/index.html#quadra.datasets.SegmentationDataset","title":"<code>SegmentationDataset(image_paths, mask_paths, batch_size=None, object_masks=None, resize=224, mask_preprocess=None, labels=None, transform=None, mask_smoothing=False, defect_transform=None)</code>","text":"<p>             Bases: <code>Dataset</code></p> <p>Custom SegmentationDataset class for loading images and masks.</p> <p>Parameters:</p> <ul> <li> image_paths             (<code>list[str]</code>)         \u2013          <p>List of paths to images.</p> </li> <li> mask_paths             (<code>list[str]</code>)         \u2013          <p>List of paths to masks.</p> </li> <li> batch_size             (<code>int | None</code>, default:                 <code>None</code> )         \u2013          <p>Batch size.</p> </li> <li> object_masks             (<code>list[ndarray | Any] | None</code>, default:                 <code>None</code> )         \u2013          <p>List of paths to object masks.</p> </li> <li> resize             (<code>int</code>, default:                 <code>224</code> )         \u2013          <p>Resize image to this size.</p> </li> <li> mask_preprocess             (<code>Callable | None</code>, default:                 <code>None</code> )         \u2013          <p>Preprocess mask.</p> </li> <li> labels             (<code>list[str] | None</code>, default:                 <code>None</code> )         \u2013          <p>List of labels.</p> </li> <li> transform             (<code>Compose | None</code>, default:                 <code>None</code> )         \u2013          <p>Transformations to apply to images and masks.</p> </li> <li> mask_smoothing             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Smooth mask.</p> </li> <li> defect_transform             (<code>Compose | None</code>, default:                 <code>None</code> )         \u2013          <p>Transformations to apply to images and masks for defects.</p> </li> </ul> Source code in <code>quadra/datasets/segmentation.py</code> <pre><code>def __init__(\n    self,\n    image_paths: list[str],\n    mask_paths: list[str],\n    batch_size: int | None = None,\n    object_masks: list[np.ndarray | Any] | None = None,\n    resize: int = 224,\n    mask_preprocess: Callable | None = None,\n    labels: list[str] | None = None,\n    transform: albumentations.Compose | None = None,\n    mask_smoothing: bool = False,\n    defect_transform: albumentations.Compose | None = None,\n):\n    self.transform = transform\n    self.defect_transform = defect_transform\n    self.image_paths = image_paths\n    self.mask_paths = mask_paths\n    self.labels = labels\n    self.mask_preprocess = mask_preprocess\n    self.resize = resize\n    self.object_masks = object_masks\n    self.data_len = len(self.image_paths)\n    self.batch_size = None if batch_size is None else max(batch_size, self.data_len)\n    self.smooth_mask = mask_smoothing\n</code></pre>"},{"location":"reference/quadra/datasets/index.html#quadra.datasets.SegmentationDatasetMulticlass","title":"<code>SegmentationDatasetMulticlass(image_paths, mask_paths, idx_to_class, batch_size=None, transform=None, one_hot=False)</code>","text":"<p>             Bases: <code>Dataset</code></p> <p>Custom SegmentationDataset class for loading images and multilabel masks.</p> <p>Parameters:</p> <ul> <li> image_paths             (<code>list[str]</code>)         \u2013          <p>List of paths to images.</p> </li> <li> mask_paths             (<code>list[str]</code>)         \u2013          <p>List of paths to masks.</p> </li> <li> idx_to_class             (<code>dict</code>)         \u2013          <p>dict with corrispondence btw mask index and classes: {1: class_1, 2: class_2, ..., N: class_N}</p> </li> <li> batch_size             (<code>int | None</code>, default:                 <code>None</code> )         \u2013          <p>Batch size.</p> </li> <li> transform             (<code>Compose | None</code>, default:                 <code>None</code> )         \u2013          <p>Transformations to apply to images and masks.</p> </li> <li> one_hot             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>if True return a binary mask (n_classxHxW), otherwise the labelled mask HxW. SMP loss requires the second format.</p> </li> </ul> Source code in <code>quadra/datasets/segmentation.py</code> <pre><code>def __init__(\n    self,\n    image_paths: list[str],\n    mask_paths: list[str],\n    idx_to_class: dict,\n    batch_size: int | None = None,\n    transform: albumentations.Compose | None = None,\n    one_hot: bool = False,\n):\n    self.transform = transform\n    self.image_paths = image_paths\n    self.mask_paths = mask_paths\n    self.idx_to_class = idx_to_class\n    self.data_len = len(self.image_paths)\n    self.batch_size = None if batch_size is None else max(batch_size, self.data_len)\n    self.one_hot = one_hot\n</code></pre>"},{"location":"reference/quadra/datasets/index.html#quadra.datasets.SegmentationDatasetMulticlass.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Get image and mask.</p> Source code in <code>quadra/datasets/segmentation.py</code> <pre><code>def __getitem__(self, index):\n\"\"\"Get image and mask.\"\"\"\n    # This is required to avoid infinite loop when running the dataset outside of a dataloader\n    if self.batch_size is not None and self.batch_size == index:\n        raise StopIteration\n    if self.batch_size is None and self.data_len == index:\n        raise StopIteration\n\n    index = index % self.data_len\n    image_path = self.image_paths[index]\n\n    image = cv2.imread(str(image_path))\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n    if (\n        self.mask_paths[index] is np.nan\n        or self.mask_paths[index] is None\n        or not os.path.isfile(self.mask_paths[index])\n    ):\n        mask = np.zeros((image.shape[0], image.shape[1]), dtype=np.uint8)\n    else:\n        mask_path = self.mask_paths[index]\n        mask = cv2.imread(str(mask_path), 0)\n\n    # we go back to binary masks avoid transformation errors\n    mask = self._preprocess_mask(mask)\n\n    if self.transform is not None:\n        masks = list(mask)\n        aug = self.transform(image=image, masks=masks)\n        image = aug[\"image\"]\n        mask = np.stack(aug[\"masks\"])  # C x H x W\n\n    # we compute single channel mask again\n    # zero is the background\n    if not self.one_hot:  # one hot is done by smp dice loss\n        mask_out = np.zeros(mask.shape[1:])\n        for i in range(1, mask.shape[0]):\n            mask_out[mask[i] == 1] = i\n        # mask_out shape -&gt; HxW\n    else:\n        mask_out = mask\n        # mask_out shape -&gt; CxHxW where C is number of classes (included the background)\n\n    return image, mask_out.astype(int), 0\n</code></pre>"},{"location":"reference/quadra/datasets/index.html#quadra.datasets.SegmentationDatasetMulticlass.__len__","title":"<code>__len__()</code>","text":"<p>Returns the dataset lenght.</p> Source code in <code>quadra/datasets/segmentation.py</code> <pre><code>def __len__(self):\n\"\"\"Returns the dataset lenght.\"\"\"\n    if self.batch_size is None:\n        return self.data_len\n\n    return max(self.data_len, self.batch_size)\n</code></pre>"},{"location":"reference/quadra/datasets/index.html#quadra.datasets.TwoAugmentationDataset","title":"<code>TwoAugmentationDataset(dataset, transform, strategy=AugmentationStrategy.SAME_IMAGE)</code>","text":"<p>             Bases: <code>Dataset</code></p> <p>Two Image Augmentation Dataset for using in self-supervised learning.</p> <p>Parameters:</p> <ul> <li> dataset             (<code>Dataset</code>)         \u2013          <p>A torch Dataset object</p> </li> <li> transform             (<code>Compose | tuple[Compose, Compose]</code>)         \u2013          <p>albumentation transformations for each image. If you use single transformation, it will be applied to both images. If you use tuple, it will be applied to first image and second image separately.</p> </li> <li> strategy             (<code>AugmentationStrategy</code>, default:                 <code>SAME_IMAGE</code> )         \u2013          <p>Defaults to AugmentationStrategy.SAME_IMAGE.</p> </li> </ul> Source code in <code>quadra/datasets/ssl.py</code> <pre><code>def __init__(\n    self,\n    dataset: Dataset,\n    transform: A.Compose | tuple[A.Compose, A.Compose],\n    strategy: AugmentationStrategy = AugmentationStrategy.SAME_IMAGE,\n):\n    self.dataset = dataset\n    self.transform = transform\n    self.stategy = strategy\n    if isinstance(transform, Iterable) and not isinstance(transform, str) and len(set(transform)) != 2:\n        raise ValueError(\"transform must be an Iterable of length 2\")\n</code></pre>"},{"location":"reference/quadra/datasets/index.html#quadra.datasets.TwoSetAugmentationDataset","title":"<code>TwoSetAugmentationDataset(dataset, global_transforms, local_transform, num_local_transforms)</code>","text":"<p>             Bases: <code>Dataset</code></p> <p>Two Set Augmentation Dataset for using in self-supervised learning (DINO).</p> <p>Parameters:</p> <ul> <li> dataset             (<code>Dataset</code>)         \u2013          <p>Base dataset</p> </li> <li> global_transforms             (<code>tuple[Compose, Compose]</code>)         \u2013          <p>Global transformations for each image.</p> </li> <li> local_transform             (<code>Compose</code>)         \u2013          <p>Local transformations for each image.</p> </li> <li> num_local_transforms             (<code>int</code>)         \u2013          <p>Number of local transformations to apply. In total you will have two + num_local_transforms transformations for each image. First element of the array will always return the original image.</p> </li> </ul> Example <p>images[0] = global_transform0 images[1] = global_transform1 images[2:] = local_transform(s)(original_image)</p> Source code in <code>quadra/datasets/ssl.py</code> <pre><code>def __init__(\n    self,\n    dataset: Dataset,\n    global_transforms: tuple[A.Compose, A.Compose],\n    local_transform: A.Compose,\n    num_local_transforms: int,\n):\n    self.dataset = dataset\n    self.global_transforms = global_transforms\n    self.local_transform = local_transform\n    self.num_local_transforms = num_local_transforms\n\n    if num_local_transforms &lt; 1:\n        raise ValueError(\"num_local_transforms must be greater than 0\")\n</code></pre>"},{"location":"reference/quadra/datasets/anomaly.html","title":"anomaly","text":""},{"location":"reference/quadra/datasets/anomaly.html#quadra.datasets.anomaly.AnomalyDataset","title":"<code>AnomalyDataset(transform, samples, task='segmentation', valid_area_mask=None, crop_area=None)</code>","text":"<p>             Bases: <code>Dataset</code></p> <p>Anomaly Dataset.</p> <p>Parameters:</p> <ul> <li> transform             (<code>Compose</code>)         \u2013          <p>Albumentations compose.</p> </li> <li> task             (<code>str</code>, default:                 <code>'segmentation'</code> )         \u2013          <p><code>classification</code> or <code>segmentation</code></p> </li> <li> samples             (<code>DataFrame</code>)         \u2013          <p>Pandas dataframe containing samples following the same structure created by make_anomaly_dataset</p> </li> <li> valid_area_mask             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>Optional path to the mask to use to filter out the valid area of the image. If None, the whole image is considered valid.</p> </li> <li> crop_area             (<code>tuple[int, int, int, int] | None</code>, default:                 <code>None</code> )         \u2013          <p>Optional tuple of 4 integers (x1, y1, x2, y2) to crop the image to the specified area. If None, the whole image is considered valid.</p> </li> </ul> Source code in <code>quadra/datasets/anomaly.py</code> <pre><code>def __init__(\n    self,\n    transform: alb.Compose,\n    samples: DataFrame,\n    task: str = \"segmentation\",\n    valid_area_mask: str | None = None,\n    crop_area: tuple[int, int, int, int] | None = None,\n) -&gt; None:\n    self.task = task\n    self.transform = transform\n\n    self.samples = samples\n    self.samples = self.samples.reset_index(drop=True)\n    self.split = self.samples.split.unique()[0]\n\n    self.crop_area = crop_area\n    self.valid_area_mask: np.ndarray | None = None\n\n    if valid_area_mask is not None:\n        if not os.path.exists(valid_area_mask):\n            raise RuntimeError(f\"Valid area mask {valid_area_mask} does not exist.\")\n\n        self.valid_area_mask = cv2.imread(valid_area_mask, 0) &gt; 0  # type: ignore[operator]\n</code></pre>"},{"location":"reference/quadra/datasets/anomaly.html#quadra.datasets.anomaly.AnomalyDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Get dataset item for the index <code>index</code>.</p> <p>Parameters:</p> <ul> <li> index             (<code>int</code>)         \u2013          <p>Index to get the item.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, str | Tensor]</code>         \u2013          <p>Dict of image tensor during training.</p> </li> <li> <code>dict[str, str | Tensor]</code>         \u2013          <p>Otherwise, Dict containing image path, target path, image tensor, label and transformed bounding box.</p> </li> </ul> Source code in <code>quadra/datasets/anomaly.py</code> <pre><code>def __getitem__(self, index: int) -&gt; dict[str, str | Tensor]:\n\"\"\"Get dataset item for the index ``index``.\n\n    Args:\n        index: Index to get the item.\n\n    Returns:\n        Dict of image tensor during training.\n        Otherwise, Dict containing image path, target path, image tensor, label and transformed bounding box.\n    \"\"\"\n    item: dict[str, str | Tensor] = {}\n\n    image_path = self.samples.samples.iloc[index]\n    image = cv2.imread(image_path)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n    original_image_shape = image.shape\n    if self.valid_area_mask is not None:\n        image = image * self.valid_area_mask[:, :, np.newaxis]\n\n    if self.crop_area is not None:\n        image = image[self.crop_area[1] : self.crop_area[3], self.crop_area[0] : self.crop_area[2]]\n\n    label_index = self.samples.label_index[index]\n\n    if self.split == \"train\":\n        pre_processed = self.transform(image=image)\n        item = {\"image\": pre_processed[\"image\"], \"label\": label_index}\n    elif self.split in [\"val\", \"test\"]:\n        item[\"image_path\"] = image_path\n        item[\"label\"] = label_index\n\n        if self.task == \"segmentation\":\n            mask_path = self.samples.mask_path[index]\n\n            # If good images have no associated mask create an empty one\n            if label_index == 0:\n                mask = np.zeros(shape=original_image_shape[:2])\n            elif os.path.isfile(mask_path):\n                mask = cv2.imread(mask_path, flags=0) / 255.0  # type: ignore[operator]\n            else:\n                # We need ones in the mask to compute correctly at least image level f1 score\n                mask = np.ones(shape=original_image_shape[:2])\n\n            if self.valid_area_mask is not None:\n                mask = mask * self.valid_area_mask\n\n            if self.crop_area is not None:\n                mask = mask[self.crop_area[1] : self.crop_area[3], self.crop_area[0] : self.crop_area[2]]\n\n            pre_processed = self.transform(image=image, mask=mask)\n\n            item[\"mask_path\"] = mask_path\n            item[\"mask\"] = pre_processed[\"mask\"]\n        else:\n            pre_processed = self.transform(image=image)\n\n        item[\"image\"] = pre_processed[\"image\"]\n    return item\n</code></pre>"},{"location":"reference/quadra/datasets/anomaly.html#quadra.datasets.anomaly.AnomalyDataset.__len__","title":"<code>__len__()</code>","text":"<p>Get length of the dataset.</p> Source code in <code>quadra/datasets/anomaly.py</code> <pre><code>def __len__(self) -&gt; int:\n\"\"\"Get length of the dataset.\"\"\"\n    return len(self.samples)\n</code></pre>"},{"location":"reference/quadra/datasets/anomaly.html#quadra.datasets.anomaly.create_validation_set_from_test_set","title":"<code>create_validation_set_from_test_set(samples, seed=0)</code>","text":"<p>Craete Validation Set from Test Set.</p> <p>This function creates a validation set from test set by splitting both normal and abnormal samples to two.</p> <p>Parameters:</p> <ul> <li> samples             (<code>DataFrame</code>)         \u2013          <p>Dataframe containing dataset info such as filenames, splits etc.</p> </li> <li> seed             (<code>int</code>, default:                 <code>0</code> )         \u2013          <p>Random seed to ensure reproducibility. Defaults to 0.</p> </li> </ul> Source code in <code>quadra/datasets/anomaly.py</code> <pre><code>def create_validation_set_from_test_set(samples: DataFrame, seed: int = 0) -&gt; DataFrame:\n\"\"\"Craete Validation Set from Test Set.\n\n    This function creates a validation set from test set by splitting both\n    normal and abnormal samples to two.\n\n    Args:\n        samples: Dataframe containing dataset info such as filenames, splits etc.\n        seed: Random seed to ensure reproducibility. Defaults to 0.\n    \"\"\"\n    if seed &gt; 0:\n        random.seed(seed)\n\n    # Split normal images.\n    normal_test_image_indices = samples.index[(samples.split == \"test\") &amp; (samples.targets == \"good\")].to_list()\n    num_normal_valid_images = len(normal_test_image_indices) // 2\n\n    indices_to_sample = random.sample(population=normal_test_image_indices, k=num_normal_valid_images)\n    samples.loc[indices_to_sample, \"split\"] = \"val\"\n\n    # Split abnormal images.\n    abnormal_test_image_indices = samples.index[(samples.split == \"test\") &amp; (samples.targets != \"good\")].to_list()\n    num_abnormal_valid_images = len(abnormal_test_image_indices) // 2\n\n    indices_to_sample = random.sample(population=abnormal_test_image_indices, k=num_abnormal_valid_images)\n    samples.loc[indices_to_sample, \"split\"] = \"val\"\n\n    return samples\n</code></pre>"},{"location":"reference/quadra/datasets/anomaly.html#quadra.datasets.anomaly.make_anomaly_dataset","title":"<code>make_anomaly_dataset(path, split=None, split_ratio=0.1, seed=0, mask_suffix=None, create_test_set_if_empty=True)</code>","text":"<p>Create dataframe by parsing a folder following the MVTec data file structure.</p> The files are expected to follow the structure <p>path/to/dataset/split/label/image_filename.xyz path/to/dataset/ground_truth/label/mask_filename.png</p> <p>Masks MUST be png images, no other format is allowed Split can be either train/val/test</p> <p>This function creates a dataframe to store the parsed information based on the following format: |---|---------------|-------|---------|--------------|-----------------------------------------------|-------------| |   | path          | split | targets | samples      | mask_path                                     | label_index | |---|---------------|-------|---------|--------------|-----------------------------------------------|-------------| | 0 | datasets/name |  test |  defect | filename.xyz | ground_truth/defect/filename{mask_suffix}.png | 1           | |---|---------------|-------|---------|--------------|-----------------------------------------------|-------------|</p> <p>Parameters:</p> <ul> <li> path             (<code>Path</code>)         \u2013          <p>Path to dataset</p> </li> <li> split             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>Dataset split (i.e., either train or test). Defaults to None.</p> </li> <li> split_ratio             (<code>float</code>, default:                 <code>0.1</code> )         \u2013          <p>Ratio to split normal training images and add to the test set in case test set doesn't contain any normal images. Defaults to 0.1.</p> </li> <li> seed             (<code>int</code>, default:                 <code>0</code> )         \u2013          <p>Random seed to ensure reproducibility when splitting. Defaults to 0.</p> </li> <li> mask_suffix             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>String to append to the base filename to get the mask name, by default for MVTec dataset masks are saved as imagename_mask.png in this case the parameter shoul be filled with \"_mask\"</p> </li> <li> create_test_set_if_empty             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>If True, create a test set if the test set is empty.</p> </li> </ul> Example <p>The following example shows how to get training samples from MVTec bottle category:</p> <p>root = Path('./MVTec') category = 'bottle' path = root / category path PosixPath('MVTec/bottle')</p> <p>samples = make_anomaly_dataset(path, split='train', split_ratio=0.1, seed=0) samples.head()    path         split label image_path                           mask_path                   label_index 0  MVTec/bottle train good MVTec/bottle/train/good/105.png MVTec/bottle/ground_truth/good/105_mask.png 0 1  MVTec/bottle train good MVTec/bottle/train/good/017.png MVTec/bottle/ground_truth/good/017_mask.png 0 2  MVTec/bottle train good MVTec/bottle/train/good/137.png MVTec/bottle/ground_truth/good/137_mask.png 0 3  MVTec/bottle train good MVTec/bottle/train/good/152.png MVTec/bottle/ground_truth/good/152_mask.png 0 4  MVTec/bottle train good MVTec/bottle/train/good/109.png MVTec/bottle/ground_truth/good/109_mask.png 0</p> <p>Returns:</p> <ul> <li> <code>DataFrame</code>         \u2013          <p>An output dataframe containing samples for the requested split (ie., train or test)</p> </li> </ul> Source code in <code>quadra/datasets/anomaly.py</code> <pre><code>def make_anomaly_dataset(\n    path: Path,\n    split: str | None = None,\n    split_ratio: float = 0.1,\n    seed: int = 0,\n    mask_suffix: str | None = None,\n    create_test_set_if_empty: bool = True,\n) -&gt; DataFrame:\n\"\"\"Create dataframe by parsing a folder following the MVTec data file structure.\n\n    The files are expected to follow the structure:\n        path/to/dataset/split/label/image_filename.xyz\n        path/to/dataset/ground_truth/label/mask_filename.png\n\n    Masks MUST be png images, no other format is allowed\n    Split can be either train/val/test\n\n    This function creates a dataframe to store the parsed information based on the following format:\n    |---|---------------|-------|---------|--------------|-----------------------------------------------|-------------|\n    |   | path          | split | targets | samples      | mask_path                                     | label_index |\n    |---|---------------|-------|---------|--------------|-----------------------------------------------|-------------|\n    | 0 | datasets/name |  test |  defect | filename.xyz | ground_truth/defect/filename{mask_suffix}.png | 1           |\n    |---|---------------|-------|---------|--------------|-----------------------------------------------|-------------|\n\n    Args:\n        path: Path to dataset\n        split: Dataset split (i.e., either train or test). Defaults to None.\n        split_ratio: Ratio to split normal training images and add to the\n            test set in case test set doesn't contain any normal images.\n            Defaults to 0.1.\n        seed: Random seed to ensure reproducibility when splitting. Defaults to 0.\n        mask_suffix: String to append to the base filename to get the mask name, by default for MVTec dataset masks\n            are saved as imagename_mask.png in this case the parameter shoul be filled with \"_mask\"\n        create_test_set_if_empty: If True, create a test set if the test set is empty.\n\n\n    Example:\n        The following example shows how to get training samples from MVTec bottle category:\n\n        &gt;&gt;&gt; root = Path('./MVTec')\n        &gt;&gt;&gt; category = 'bottle'\n        &gt;&gt;&gt; path = root / category\n        &gt;&gt;&gt; path\n        PosixPath('MVTec/bottle')\n\n        &gt;&gt;&gt; samples = make_anomaly_dataset(path, split='train', split_ratio=0.1, seed=0)\n        &gt;&gt;&gt; samples.head()\n           path         split label image_path                           mask_path                   label_index\n        0  MVTec/bottle train good MVTec/bottle/train/good/105.png MVTec/bottle/ground_truth/good/105_mask.png 0\n        1  MVTec/bottle train good MVTec/bottle/train/good/017.png MVTec/bottle/ground_truth/good/017_mask.png 0\n        2  MVTec/bottle train good MVTec/bottle/train/good/137.png MVTec/bottle/ground_truth/good/137_mask.png 0\n        3  MVTec/bottle train good MVTec/bottle/train/good/152.png MVTec/bottle/ground_truth/good/152_mask.png 0\n        4  MVTec/bottle train good MVTec/bottle/train/good/109.png MVTec/bottle/ground_truth/good/109_mask.png 0\n\n    Returns:\n        An output dataframe containing samples for the requested split (ie., train or test)\n    \"\"\"\n    samples_list = [\n        (str(path),) + filename.parts[-3:]\n        for filename in path.glob(\"**/*\")\n        if filename.is_file()\n        and os.path.splitext(filename)[-1].lower() in IMAGE_EXTENSIONS\n        and \".ipynb_checkpoints\" not in str(filename)\n    ]\n\n    if len(samples_list) == 0:\n        raise RuntimeError(f\"Found 0 images in {path}\")\n\n    samples_list.sort()\n\n    data = pd.DataFrame(samples_list, columns=[\"path\", \"split\", \"targets\", \"samples\"])\n    data = data[data.split != \"ground_truth\"]\n\n    # Create mask_path column, masks MUST have png extension\n    data[\"mask_path\"] = (\n        data.path\n        + \"/ground_truth/\"\n        + data.targets\n        + \"/\"\n        + data.samples.apply(lambda x: os.path.splitext(os.path.basename(x))[0])\n        + (f\"{mask_suffix}.png\" if mask_suffix is not None else \".png\")\n    )\n\n    # Modify image_path column by converting to absolute path\n    data[\"samples\"] = data.path + \"/\" + data.split + \"/\" + data.targets + \"/\" + data.samples\n\n    # Split the normal images in training set if test set doesn't\n    # contain any normal images. This is needed because AUC score\n    # cannot be computed based on 1-class\n    if sum((data.split == \"test\") &amp; (data.targets == \"good\")) == 0 and create_test_set_if_empty:\n        data = split_normal_images_in_train_set(data, split_ratio, seed)\n\n    # Good images don't have mask\n    data.loc[(data.split == \"test\") &amp; (data.targets == \"good\"), \"mask_path\"] = \"\"\n\n    # Create label index for normal (0), anomalous (1) and unknown (-1) images.\n    data.loc[data.targets == \"good\", \"label_index\"] = 0\n    data.loc[~data.targets.isin([\"good\", \"unknown\"]), \"label_index\"] = 1\n    data.loc[data.targets == \"unknown\", \"label_index\"] = -1\n    data.label_index = data.label_index.astype(int)\n\n    # Get the data frame for the split.\n    if split is not None and split in [\"train\", \"val\", \"test\"]:\n        data = data[data.split == split]\n        data = data.reset_index(drop=True)\n\n    return data\n</code></pre>"},{"location":"reference/quadra/datasets/anomaly.html#quadra.datasets.anomaly.split_normal_images_in_train_set","title":"<code>split_normal_images_in_train_set(samples, split_ratio=0.1, seed=0)</code>","text":"<p>Split normal images in train set.</p> <pre><code>This function splits the normal images in training set and assigns the\nvalues to the test set. This is particularly useful especially when the\ntest set does not contain any normal images.\n\nThis is important because when the test set doesn't have any normal images,\nAUC computation fails due to having single class.\n</code></pre> <p>Parameters:</p> <ul> <li> samples             (<code>DataFrame</code>)         \u2013          <p>Dataframe containing dataset info such as filenames, splits etc.</p> </li> <li> split_ratio             (<code>float</code>, default:                 <code>0.1</code> )         \u2013          <p>Train-Test normal image split ratio. Defaults to 0.1.</p> </li> <li> seed             (<code>int</code>, default:                 <code>0</code> )         \u2013          <p>Random seed to ensure reproducibility. Defaults to 0.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>         \u2013          <p>Output dataframe where the part of the training set is assigned to test set.</p> </li> </ul> Source code in <code>quadra/datasets/anomaly.py</code> <pre><code>def split_normal_images_in_train_set(samples: DataFrame, split_ratio: float = 0.1, seed: int = 0) -&gt; DataFrame:\n\"\"\"Split normal images in train set.\n\n        This function splits the normal images in training set and assigns the\n        values to the test set. This is particularly useful especially when the\n        test set does not contain any normal images.\n\n        This is important because when the test set doesn't have any normal images,\n        AUC computation fails due to having single class.\n\n    Args:\n        samples: Dataframe containing dataset info such as filenames, splits etc.\n        split_ratio: Train-Test normal image split ratio. Defaults to 0.1.\n        seed: Random seed to ensure reproducibility. Defaults to 0.\n\n    Returns:\n        Output dataframe where the part of the training set is assigned to test set.\n    \"\"\"\n    if seed &gt; 0:\n        random.seed(seed)\n\n    normal_train_image_indices = samples.index[(samples.split == \"train\") &amp; (samples.targets == \"good\")].to_list()\n    num_normal_train_images = len(normal_train_image_indices)\n    num_normal_valid_images = int(num_normal_train_images * split_ratio)\n\n    indices_to_split_from_train_set = random.sample(population=normal_train_image_indices, k=num_normal_valid_images)\n    samples.loc[indices_to_split_from_train_set, \"split\"] = \"test\"\n\n    return samples\n</code></pre>"},{"location":"reference/quadra/datasets/classification.html","title":"classification","text":""},{"location":"reference/quadra/datasets/classification.html#quadra.datasets.classification.ClassificationDataset","title":"<code>ClassificationDataset(samples, targets, class_to_idx=None, resize=None, roi=None, transform=None, rgb=True, channel=3, random_padding=False, circular_crop=False)</code>","text":"<p>             Bases: <code>ImageClassificationListDataset</code></p> <p>Custom Classification Dataset.</p> <p>Parameters:</p> <ul> <li> samples             (<code>list[str]</code>)         \u2013          <p>List of paths to images</p> </li> <li> targets             (<code>list[str | int]</code>)         \u2013          <p>List of targets</p> </li> <li> class_to_idx             (<code>dict | None</code>, default:                 <code>None</code> )         \u2013          <p>Defaults to None.</p> </li> <li> resize             (<code>int | None</code>, default:                 <code>None</code> )         \u2013          <p>Resize image to this size. Defaults to None.</p> </li> <li> roi             (<code>tuple[int, int, int, int] | None</code>, default:                 <code>None</code> )         \u2013          <p>Region of interest. Defaults to None.</p> </li> <li> transform             (<code>Callable | None</code>, default:                 <code>None</code> )         \u2013          <p>transform function. Defaults to None.</p> </li> <li> rgb             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Use RGB space</p> </li> <li> channel             (<code>int</code>, default:                 <code>3</code> )         \u2013          <p>Number of channels. Defaults to 3.</p> </li> <li> random_padding             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Random padding. Defaults to False.</p> </li> <li> circular_crop             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Circular crop. Defaults to False.</p> </li> </ul> Source code in <code>quadra/datasets/classification.py</code> <pre><code>def __init__(\n    self,\n    samples: list[str],\n    targets: list[str | int],\n    class_to_idx: dict | None = None,\n    resize: int | None = None,\n    roi: tuple[int, int, int, int] | None = None,\n    transform: Callable | None = None,\n    rgb: bool = True,\n    channel: int = 3,\n    random_padding: bool = False,\n    circular_crop: bool = False,\n):\n    super().__init__(samples, targets, class_to_idx, resize, roi, transform, rgb, channel)\n    if transform is None:\n        self.transform = None\n\n    self.random_padding = random_padding\n    self.circular_crop = circular_crop\n</code></pre>"},{"location":"reference/quadra/datasets/classification.html#quadra.datasets.classification.ImageClassificationListDataset","title":"<code>ImageClassificationListDataset(samples, targets, class_to_idx=None, resize=None, roi=None, transform=None, rgb=True, channel=3, allow_missing_label=False)</code>","text":"<p>             Bases: <code>Dataset</code></p> <p>Standard classification dataset.</p> <p>Parameters:</p> <ul> <li> samples             (<code>list[str]</code>)         \u2013          <p>List of paths to images to be read</p> </li> <li> targets             (<code>list[str | int]</code>)         \u2013          <p>List of labels, one for every image in samples</p> </li> <li> class_to_idx             (<code>dict | None</code>, default:                 <code>None</code> )         \u2013          <p>mapping from classes to unique indexes. Defaults to None.</p> </li> <li> resize             (<code>int | None</code>, default:                 <code>None</code> )         \u2013          <p>Integer specifying the size of a first optional resize keeping the aspect ratio: the smaller side of the image will be resized to <code>resize</code>, while the longer will be resized keeping the aspect ratio. Defaults to None.</p> </li> <li> roi             (<code>tuple[int, int, int, int] | None</code>, default:                 <code>None</code> )         \u2013          <p>Optional ROI, with (x_upper_left, y_upper_left, x_bottom_right, y_bottom_right). Defaults to None.</p> </li> <li> transform             (<code>Callable | None</code>, default:                 <code>None</code> )         \u2013          <p>Optional Albumentations transform. Defaults to None.</p> </li> <li> rgb             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>if False, image will be converted in grayscale</p> </li> <li> channel             (<code>int</code>, default:                 <code>3</code> )         \u2013          <p>1 or 3. If rgb is True, then channel will be set at 3.</p> </li> <li> allow_missing_label             (<code>bool | None</code>, default:                 <code>False</code> )         \u2013          <p>If set to false warn the user if the dataset contains missing labels</p> </li> </ul> Source code in <code>quadra/datasets/classification.py</code> <pre><code>def __init__(\n    self,\n    samples: list[str],\n    targets: list[str | int],\n    class_to_idx: dict | None = None,\n    resize: int | None = None,\n    roi: tuple[int, int, int, int] | None = None,\n    transform: Callable | None = None,\n    rgb: bool = True,\n    channel: int = 3,\n    allow_missing_label: bool | None = False,\n):\n    super().__init__()\n    assert len(samples) == len(\n        targets\n    ), f\"Samples ({len(samples)}) and targets ({len(targets)}) must have the same length\"\n    # Setting the ROI\n    self.roi = roi\n\n    # Keep-Aspect-Ratio resize\n    self.resize = resize\n\n    if not allow_missing_label and None in targets:\n        warnings.warn(\n            (\n                \"Dataset contains empty targets but allow_missing_label is set to False, \"\n                \"be careful because None labels will not work inside Dataloaders\"\n            ),\n            UserWarning,\n            stacklevel=2,\n        )\n\n    targets = [-1 if target is None else target for target in targets]\n    # Data\n    self.x = np.array(samples)\n    self.y = np.array(targets)\n\n    if class_to_idx is None:\n        unique_targets = np.unique(targets)\n        class_to_idx = {c: i for i, c in enumerate(unique_targets)}\n\n    self.class_to_idx = class_to_idx\n    self.idx_to_class = {v: k for k, v in class_to_idx.items()}\n    self.samples = [\n        (path, self.class_to_idx[self.y[i]] if (self.y[i] != -1 and self.y[i] != \"-1\") else -1)\n        for i, path in enumerate(self.x)\n    ]\n\n    self.rgb = rgb\n    self.channel = 3 if rgb else channel\n\n    self.transform = transform\n</code></pre>"},{"location":"reference/quadra/datasets/classification.html#quadra.datasets.classification.MultilabelClassificationDataset","title":"<code>MultilabelClassificationDataset(samples, targets, class_to_idx=None, transform=None, rgb=True)</code>","text":"<p>             Bases: <code>Dataset</code></p> <p>Custom MultilabelClassification Dataset.</p> <p>Parameters:</p> <ul> <li> samples             (<code>list[str]</code>)         \u2013          <p>list of paths to images.</p> </li> <li> targets             (<code>ndarray</code>)         \u2013          <p>array of multiple targets per sample. The array must be a one-hot enoding. It must have a shape of (n_samples, n_targets).</p> </li> <li> class_to_idx             (<code>dict | None</code>, default:                 <code>None</code> )         \u2013          <p>Defaults to None.</p> </li> <li> transform             (<code>Callable | None</code>, default:                 <code>None</code> )         \u2013          <p>transform function. Defaults to None.</p> </li> <li> rgb             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Use RGB space</p> </li> </ul> Source code in <code>quadra/datasets/classification.py</code> <pre><code>def __init__(\n    self,\n    samples: list[str],\n    targets: np.ndarray,\n    class_to_idx: dict | None = None,\n    transform: Callable | None = None,\n    rgb: bool = True,\n):\n    super().__init__()\n    assert len(samples) == len(\n        targets\n    ), f\"Samples ({len(samples)}) and targets ({len(targets)}) must have the same length\"\n\n    # Data\n    self.x = samples\n    self.y = targets\n\n    # Class to idx and the other way around\n    if class_to_idx is None:\n        unique_targets = targets.shape[1]\n        class_to_idx = {c: i for i, c in enumerate(range(unique_targets))}\n    self.class_to_idx = class_to_idx\n    self.idx_to_class = {v: k for k, v in class_to_idx.items()}\n    self.samples = list(zip(self.x, self.y))\n    self.rgb = rgb\n    self.transform = transform\n</code></pre>"},{"location":"reference/quadra/datasets/patch.html","title":"patch","text":""},{"location":"reference/quadra/datasets/patch.html#quadra.datasets.patch.PatchSklearnClassificationTrainDataset","title":"<code>PatchSklearnClassificationTrainDataset(data_path, samples, targets, class_to_idx=None, resize=None, transform=None, rgb=True, channel=3, balance_classes=False)</code>","text":"<p>             Bases: <code>Dataset</code></p> <p>Dataset used for patch sampling, it expects samples to be paths to h5 files containing all the required information for patch sampling from images.</p> <p>Parameters:</p> <ul> <li> data_path             (<code>str</code>)         \u2013          <p>base path to the dataset</p> </li> <li> samples             (<code>list[str]</code>)         \u2013          <p>Paths to h5 files</p> </li> <li> targets             (<code>list[str | int]</code>)         \u2013          <p>Labels associated with each sample</p> </li> <li> class_to_idx             (<code>dict | None</code>, default:                 <code>None</code> )         \u2013          <p>Mapping between class and corresponding index</p> </li> <li> resize             (<code>int | None</code>, default:                 <code>None</code> )         \u2013          <p>Whether to perform an aspect ratio resize of the patch before the transformations</p> </li> <li> transform             (<code>Callable | None</code>, default:                 <code>None</code> )         \u2013          <p>Optional function applied to the image</p> </li> <li> rgb             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>if False, image will be converted in grayscale</p> </li> <li> channel             (<code>int</code>, default:                 <code>3</code> )         \u2013          <p>1 or 3. If rgb is True, then channel will be set at 3.</p> </li> <li> balance_classes             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>if True, the dataset will be balanced by duplicating samples of the minority class</p> </li> </ul> Source code in <code>quadra/datasets/patch.py</code> <pre><code>def __init__(\n    self,\n    data_path: str,\n    samples: list[str],\n    targets: list[str | int],\n    class_to_idx: dict | None = None,\n    resize: int | None = None,\n    transform: Callable | None = None,\n    rgb: bool = True,\n    channel: int = 3,\n    balance_classes: bool = False,\n):\n    super().__init__()\n\n    # Keep-Aspect-Ratio resize\n    self.resize = resize\n    self.data_path = data_path\n\n    if balance_classes:\n        samples_array = np.array(samples)\n        targets_array = np.array(targets)\n        samples_to_use: list[str] = []\n        targets_to_use: list[str | int] = []\n\n        cls, counts = np.unique(targets_array, return_counts=True)\n        max_count = np.max(counts)\n        for cl, count in zip(cls, counts):\n            idx_to_pick = list(np.where(targets_array == cl)[0])\n\n            if count &lt; max_count:\n                idx_to_pick += random.choices(idx_to_pick, k=max_count - count)\n\n            samples_to_use.extend(samples_array[idx_to_pick])\n            targets_to_use.extend(targets_array[idx_to_pick])\n    else:\n        samples_to_use = samples\n        targets_to_use = targets\n\n    # Data\n    self.x = np.array(samples_to_use)\n    self.y = np.array(targets_to_use)\n\n    if class_to_idx is None:\n        unique_targets = np.unique(targets_to_use)\n        class_to_idx = {c: i for i, c in enumerate(unique_targets)}\n\n    self.class_to_idx = class_to_idx\n    self.idx_to_class = {v: k for k, v in class_to_idx.items()}\n\n    self.samples = [\n        (path, self.class_to_idx[self.y[i]] if self.y[i] is not None else None) for i, path in enumerate(self.x)\n    ]\n\n    self.rgb = rgb\n    self.channel = 3 if rgb else channel\n\n    self.transform = transform\n</code></pre>"},{"location":"reference/quadra/datasets/segmentation.html","title":"segmentation","text":""},{"location":"reference/quadra/datasets/segmentation.html#quadra.datasets.segmentation.SegmentationDataset","title":"<code>SegmentationDataset(image_paths, mask_paths, batch_size=None, object_masks=None, resize=224, mask_preprocess=None, labels=None, transform=None, mask_smoothing=False, defect_transform=None)</code>","text":"<p>             Bases: <code>Dataset</code></p> <p>Custom SegmentationDataset class for loading images and masks.</p> <p>Parameters:</p> <ul> <li> image_paths             (<code>list[str]</code>)         \u2013          <p>List of paths to images.</p> </li> <li> mask_paths             (<code>list[str]</code>)         \u2013          <p>List of paths to masks.</p> </li> <li> batch_size             (<code>int | None</code>, default:                 <code>None</code> )         \u2013          <p>Batch size.</p> </li> <li> object_masks             (<code>list[ndarray | Any] | None</code>, default:                 <code>None</code> )         \u2013          <p>List of paths to object masks.</p> </li> <li> resize             (<code>int</code>, default:                 <code>224</code> )         \u2013          <p>Resize image to this size.</p> </li> <li> mask_preprocess             (<code>Callable | None</code>, default:                 <code>None</code> )         \u2013          <p>Preprocess mask.</p> </li> <li> labels             (<code>list[str] | None</code>, default:                 <code>None</code> )         \u2013          <p>List of labels.</p> </li> <li> transform             (<code>Compose | None</code>, default:                 <code>None</code> )         \u2013          <p>Transformations to apply to images and masks.</p> </li> <li> mask_smoothing             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Smooth mask.</p> </li> <li> defect_transform             (<code>Compose | None</code>, default:                 <code>None</code> )         \u2013          <p>Transformations to apply to images and masks for defects.</p> </li> </ul> Source code in <code>quadra/datasets/segmentation.py</code> <pre><code>def __init__(\n    self,\n    image_paths: list[str],\n    mask_paths: list[str],\n    batch_size: int | None = None,\n    object_masks: list[np.ndarray | Any] | None = None,\n    resize: int = 224,\n    mask_preprocess: Callable | None = None,\n    labels: list[str] | None = None,\n    transform: albumentations.Compose | None = None,\n    mask_smoothing: bool = False,\n    defect_transform: albumentations.Compose | None = None,\n):\n    self.transform = transform\n    self.defect_transform = defect_transform\n    self.image_paths = image_paths\n    self.mask_paths = mask_paths\n    self.labels = labels\n    self.mask_preprocess = mask_preprocess\n    self.resize = resize\n    self.object_masks = object_masks\n    self.data_len = len(self.image_paths)\n    self.batch_size = None if batch_size is None else max(batch_size, self.data_len)\n    self.smooth_mask = mask_smoothing\n</code></pre>"},{"location":"reference/quadra/datasets/segmentation.html#quadra.datasets.segmentation.SegmentationDatasetMulticlass","title":"<code>SegmentationDatasetMulticlass(image_paths, mask_paths, idx_to_class, batch_size=None, transform=None, one_hot=False)</code>","text":"<p>             Bases: <code>Dataset</code></p> <p>Custom SegmentationDataset class for loading images and multilabel masks.</p> <p>Parameters:</p> <ul> <li> image_paths             (<code>list[str]</code>)         \u2013          <p>List of paths to images.</p> </li> <li> mask_paths             (<code>list[str]</code>)         \u2013          <p>List of paths to masks.</p> </li> <li> idx_to_class             (<code>dict</code>)         \u2013          <p>dict with corrispondence btw mask index and classes: {1: class_1, 2: class_2, ..., N: class_N}</p> </li> <li> batch_size             (<code>int | None</code>, default:                 <code>None</code> )         \u2013          <p>Batch size.</p> </li> <li> transform             (<code>Compose | None</code>, default:                 <code>None</code> )         \u2013          <p>Transformations to apply to images and masks.</p> </li> <li> one_hot             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>if True return a binary mask (n_classxHxW), otherwise the labelled mask HxW. SMP loss requires the second format.</p> </li> </ul> Source code in <code>quadra/datasets/segmentation.py</code> <pre><code>def __init__(\n    self,\n    image_paths: list[str],\n    mask_paths: list[str],\n    idx_to_class: dict,\n    batch_size: int | None = None,\n    transform: albumentations.Compose | None = None,\n    one_hot: bool = False,\n):\n    self.transform = transform\n    self.image_paths = image_paths\n    self.mask_paths = mask_paths\n    self.idx_to_class = idx_to_class\n    self.data_len = len(self.image_paths)\n    self.batch_size = None if batch_size is None else max(batch_size, self.data_len)\n    self.one_hot = one_hot\n</code></pre>"},{"location":"reference/quadra/datasets/segmentation.html#quadra.datasets.segmentation.SegmentationDatasetMulticlass.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Get image and mask.</p> Source code in <code>quadra/datasets/segmentation.py</code> <pre><code>def __getitem__(self, index):\n\"\"\"Get image and mask.\"\"\"\n    # This is required to avoid infinite loop when running the dataset outside of a dataloader\n    if self.batch_size is not None and self.batch_size == index:\n        raise StopIteration\n    if self.batch_size is None and self.data_len == index:\n        raise StopIteration\n\n    index = index % self.data_len\n    image_path = self.image_paths[index]\n\n    image = cv2.imread(str(image_path))\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n    if (\n        self.mask_paths[index] is np.nan\n        or self.mask_paths[index] is None\n        or not os.path.isfile(self.mask_paths[index])\n    ):\n        mask = np.zeros((image.shape[0], image.shape[1]), dtype=np.uint8)\n    else:\n        mask_path = self.mask_paths[index]\n        mask = cv2.imread(str(mask_path), 0)\n\n    # we go back to binary masks avoid transformation errors\n    mask = self._preprocess_mask(mask)\n\n    if self.transform is not None:\n        masks = list(mask)\n        aug = self.transform(image=image, masks=masks)\n        image = aug[\"image\"]\n        mask = np.stack(aug[\"masks\"])  # C x H x W\n\n    # we compute single channel mask again\n    # zero is the background\n    if not self.one_hot:  # one hot is done by smp dice loss\n        mask_out = np.zeros(mask.shape[1:])\n        for i in range(1, mask.shape[0]):\n            mask_out[mask[i] == 1] = i\n        # mask_out shape -&gt; HxW\n    else:\n        mask_out = mask\n        # mask_out shape -&gt; CxHxW where C is number of classes (included the background)\n\n    return image, mask_out.astype(int), 0\n</code></pre>"},{"location":"reference/quadra/datasets/segmentation.html#quadra.datasets.segmentation.SegmentationDatasetMulticlass.__len__","title":"<code>__len__()</code>","text":"<p>Returns the dataset lenght.</p> Source code in <code>quadra/datasets/segmentation.py</code> <pre><code>def __len__(self):\n\"\"\"Returns the dataset lenght.\"\"\"\n    if self.batch_size is None:\n        return self.data_len\n\n    return max(self.data_len, self.batch_size)\n</code></pre>"},{"location":"reference/quadra/datasets/ssl.html","title":"ssl","text":""},{"location":"reference/quadra/datasets/ssl.html#quadra.datasets.ssl.AugmentationStrategy","title":"<code>AugmentationStrategy</code>","text":"<p>             Bases: <code>Enum</code></p> <p>Augmentation Strategy for TwoAugmentationDataset.</p>"},{"location":"reference/quadra/datasets/ssl.html#quadra.datasets.ssl.TwoAugmentationDataset","title":"<code>TwoAugmentationDataset(dataset, transform, strategy=AugmentationStrategy.SAME_IMAGE)</code>","text":"<p>             Bases: <code>Dataset</code></p> <p>Two Image Augmentation Dataset for using in self-supervised learning.</p> <p>Parameters:</p> <ul> <li> dataset             (<code>Dataset</code>)         \u2013          <p>A torch Dataset object</p> </li> <li> transform             (<code>Compose | tuple[Compose, Compose]</code>)         \u2013          <p>albumentation transformations for each image. If you use single transformation, it will be applied to both images. If you use tuple, it will be applied to first image and second image separately.</p> </li> <li> strategy             (<code>AugmentationStrategy</code>, default:                 <code>SAME_IMAGE</code> )         \u2013          <p>Defaults to AugmentationStrategy.SAME_IMAGE.</p> </li> </ul> Source code in <code>quadra/datasets/ssl.py</code> <pre><code>def __init__(\n    self,\n    dataset: Dataset,\n    transform: A.Compose | tuple[A.Compose, A.Compose],\n    strategy: AugmentationStrategy = AugmentationStrategy.SAME_IMAGE,\n):\n    self.dataset = dataset\n    self.transform = transform\n    self.stategy = strategy\n    if isinstance(transform, Iterable) and not isinstance(transform, str) and len(set(transform)) != 2:\n        raise ValueError(\"transform must be an Iterable of length 2\")\n</code></pre>"},{"location":"reference/quadra/datasets/ssl.html#quadra.datasets.ssl.TwoSetAugmentationDataset","title":"<code>TwoSetAugmentationDataset(dataset, global_transforms, local_transform, num_local_transforms)</code>","text":"<p>             Bases: <code>Dataset</code></p> <p>Two Set Augmentation Dataset for using in self-supervised learning (DINO).</p> <p>Parameters:</p> <ul> <li> dataset             (<code>Dataset</code>)         \u2013          <p>Base dataset</p> </li> <li> global_transforms             (<code>tuple[Compose, Compose]</code>)         \u2013          <p>Global transformations for each image.</p> </li> <li> local_transform             (<code>Compose</code>)         \u2013          <p>Local transformations for each image.</p> </li> <li> num_local_transforms             (<code>int</code>)         \u2013          <p>Number of local transformations to apply. In total you will have two + num_local_transforms transformations for each image. First element of the array will always return the original image.</p> </li> </ul> Example <p>images[0] = global_transform0 images[1] = global_transform1 images[2:] = local_transform(s)(original_image)</p> Source code in <code>quadra/datasets/ssl.py</code> <pre><code>def __init__(\n    self,\n    dataset: Dataset,\n    global_transforms: tuple[A.Compose, A.Compose],\n    local_transform: A.Compose,\n    num_local_transforms: int,\n):\n    self.dataset = dataset\n    self.global_transforms = global_transforms\n    self.local_transform = local_transform\n    self.num_local_transforms = num_local_transforms\n\n    if num_local_transforms &lt; 1:\n        raise ValueError(\"num_local_transforms must be greater than 0\")\n</code></pre>"},{"location":"reference/quadra/losses/index.html","title":"losses","text":""},{"location":"reference/quadra/losses/index.html#submodules","title":"Submodules","text":"<ul> <li>classification</li> <li>ssl </li> </ul>"},{"location":"reference/quadra/losses/classification/index.html","title":"classification","text":""},{"location":"reference/quadra/losses/classification/index.html#quadra.losses.classification.AsymmetricLoss","title":"<code>AsymmetricLoss(gamma_neg=4, gamma_pos=0, m=0.05, eps=1e-08, disable_torch_grad_focal_loss=False, apply_sigmoid=True)</code>","text":"<p>             Bases: <code>Module</code></p> <p>Notice - optimized version, minimizes memory allocation and gpu uploading, favors inplace operations.</p> <p>Parameters:</p> <ul> <li> gamma_neg             (<code>float</code>, default:                 <code>4</code> )         \u2013          <p>gamma for negative samples</p> </li> <li> gamma_pos             (<code>float</code>, default:                 <code>0</code> )         \u2013          <p>gamma for positive samples</p> </li> <li> m             (<code>float</code>, default:                 <code>0.05</code> )         \u2013          <p>bias value added to negative samples</p> </li> <li> eps             (<code>float</code>, default:                 <code>1e-08</code> )         \u2013          <p>epsilon to avoid division by zero</p> </li> <li> disable_torch_grad_focal_loss             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>if True, disables torch grad for focal loss</p> </li> <li> apply_sigmoid             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>if True, applies sigmoid to input before computing loss</p> </li> </ul> Source code in <code>quadra/losses/classification/asl.py</code> <pre><code>def __init__(\n    self,\n    gamma_neg: float = 4,\n    gamma_pos: float = 0,\n    m: float = 0.05,\n    eps: float = 1e-8,\n    disable_torch_grad_focal_loss: bool = False,\n    apply_sigmoid: bool = True,\n):\n    super().__init__()\n\n    self.gamma_neg = gamma_neg\n    self.gamma_pos = gamma_pos\n    self.m = m\n    self.eps = eps\n    self.disable_torch_grad_focal_loss = disable_torch_grad_focal_loss\n    self.apply_sigmoid = apply_sigmoid\n\n    # prevent memory allocation and gpu uploading every iteration, and encourages inplace operations\n    self.targets: torch.Tensor\n    self.anti_targets: torch.Tensor\n    self.xs_pos: torch.Tensor\n    self.xs_neg: torch.Tensor\n    self.asymmetric_w: torch.Tensor\n    self.loss: torch.Tensor\n</code></pre>"},{"location":"reference/quadra/losses/classification/index.html#quadra.losses.classification.AsymmetricLoss.forward","title":"<code>forward(x, y)</code>","text":"<p>Compute the asymmetric loss.</p> <p>Parameters:</p> <ul> <li> x             (<code>Tensor</code>)         \u2013          <p>input logits (after sigmoid)</p> </li> <li> y             (<code>Tensor</code>)         \u2013          <p>targets (multi-label binarized vector)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>         \u2013          <p>asymettric loss</p> </li> </ul> Source code in <code>quadra/losses/classification/asl.py</code> <pre><code>def forward(self, x: torch.Tensor, y: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"Compute the asymmetric loss.\n\n    Args:\n        x: input logits (after sigmoid)\n        y: targets (multi-label binarized vector)\n\n    Returns:\n        asymettric loss\n    \"\"\"\n    self.targets = y\n    self.anti_targets = 1 - y\n\n    # Calculating Probabilities\n    self.xs_pos = x\n    if self.apply_sigmoid:\n        self.xs_pos = torch.sigmoid(self.xs_pos)\n    self.xs_neg = 1.0 - self.xs_pos\n\n    # Asymmetric clipping\n    if self.m is not None and self.m &gt; 0:\n        self.xs_neg.add_(self.m).clamp_(max=1)\n\n    # Basic CE calculation\n    self.loss = self.targets * torch.log(self.xs_pos.clamp(min=self.eps))\n    self.loss.add_(self.anti_targets * torch.log(self.xs_neg.clamp(min=self.eps)))\n\n    # Asymmetric Focusing\n    if self.gamma_neg &gt; 0 or self.gamma_pos &gt; 0:\n        if self.disable_torch_grad_focal_loss:\n            torch.set_grad_enabled(False)\n        self.xs_pos = self.xs_pos * self.targets\n        self.xs_neg = self.xs_neg * self.anti_targets\n        self.asymmetric_w = torch.pow(\n            1 - self.xs_pos - self.xs_neg, self.gamma_pos * self.targets + self.gamma_neg * self.anti_targets\n        )\n        if self.disable_torch_grad_focal_loss:\n            torch.set_grad_enabled(True)\n        self.loss *= self.asymmetric_w\n\n    return -self.loss.sum()\n</code></pre>"},{"location":"reference/quadra/losses/classification/index.html#quadra.losses.classification.FocalLoss","title":"<code>FocalLoss(alpha, gamma=2.0, reduction='none', eps=None)</code>","text":"<p>             Bases: <code>Module</code></p> <p>Criterion that computes Focal loss.</p> <p>According to :cite:<code>lin2018focal</code>, the Focal loss is computed as follows:</p> <p>.. math::</p> <pre><code>\\text{FL}(p_t) = -\\alpha_t (1 - p_t)^{\\gamma} \\, \\text{log}(p_t)\n</code></pre> Where <ul> <li>:math:<code>p_t</code> is the model's estimated probability for each class.</li> </ul> <p>Parameters:</p> <ul> <li> alpha             (<code>float</code>)         \u2013          <p>Weighting factor :math:<code>\\alpha \\in [0, 1]</code>.</p> </li> <li> gamma             (<code>float</code>, default:                 <code>2.0</code> )         \u2013          <p>Focusing parameter :math:<code>\\gamma &gt;= 0</code>.</p> </li> <li> reduction             (<code>str</code>, default:                 <code>'none'</code> )         \u2013          <p>Specifies the reduction to apply to the output: <code>'none'</code> | <code>'mean'</code> | <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the sum of the output will be divided by the number of elements in the output, <code>'sum'</code>: the output will be summed.</p> </li> <li> eps             (<code>float | None</code>, default:                 <code>None</code> )         \u2013          <p>used.</p> </li> </ul> Shape <ul> <li>Input: :math:<code>(N, C, *)</code> where C = number of classes.</li> <li>Target: :math:<code>(N, *)</code> where each value is   :math:<code>0 \u2264 targets[i] \u2264 C\u22121</code>.</li> </ul> Example <p>N = 5  # num_classes kwargs = {\"alpha\": 0.5, \"gamma\": 2.0, \"reduction\": 'mean'} criterion = FocalLoss(**kwargs) input = torch.randn(1, N, 3, 5, requires_grad=True) target = torch.empty(1, 3, 5, dtype=torch.long).random_(N) output = criterion(input, target) output.backward()</p> Source code in <code>quadra/losses/classification/focal.py</code> <pre><code>def __init__(self, alpha: float, gamma: float = 2.0, reduction: str = \"none\", eps: float | None = None) -&gt; None:\n    super().__init__()\n    self.alpha: float = alpha\n    self.gamma: float = gamma\n    self.reduction: str = reduction\n    self.eps: float | None = eps\n</code></pre>"},{"location":"reference/quadra/losses/classification/index.html#quadra.losses.classification.FocalLoss.forward","title":"<code>forward(input_tensor, target)</code>","text":"<p>Forward call computation.</p> Source code in <code>quadra/losses/classification/focal.py</code> <pre><code>def forward(self, input_tensor: torch.Tensor, target: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"Forward call computation.\"\"\"\n    return focal_loss(input_tensor, target, self.alpha, self.gamma, self.reduction, self.eps)\n</code></pre>"},{"location":"reference/quadra/losses/classification/asl.html","title":"asl","text":""},{"location":"reference/quadra/losses/classification/asl.html#quadra.losses.classification.asl.AsymmetricLoss","title":"<code>AsymmetricLoss(gamma_neg=4, gamma_pos=0, m=0.05, eps=1e-08, disable_torch_grad_focal_loss=False, apply_sigmoid=True)</code>","text":"<p>             Bases: <code>Module</code></p> <p>Notice - optimized version, minimizes memory allocation and gpu uploading, favors inplace operations.</p> <p>Parameters:</p> <ul> <li> gamma_neg             (<code>float</code>, default:                 <code>4</code> )         \u2013          <p>gamma for negative samples</p> </li> <li> gamma_pos             (<code>float</code>, default:                 <code>0</code> )         \u2013          <p>gamma for positive samples</p> </li> <li> m             (<code>float</code>, default:                 <code>0.05</code> )         \u2013          <p>bias value added to negative samples</p> </li> <li> eps             (<code>float</code>, default:                 <code>1e-08</code> )         \u2013          <p>epsilon to avoid division by zero</p> </li> <li> disable_torch_grad_focal_loss             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>if True, disables torch grad for focal loss</p> </li> <li> apply_sigmoid             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>if True, applies sigmoid to input before computing loss</p> </li> </ul> Source code in <code>quadra/losses/classification/asl.py</code> <pre><code>def __init__(\n    self,\n    gamma_neg: float = 4,\n    gamma_pos: float = 0,\n    m: float = 0.05,\n    eps: float = 1e-8,\n    disable_torch_grad_focal_loss: bool = False,\n    apply_sigmoid: bool = True,\n):\n    super().__init__()\n\n    self.gamma_neg = gamma_neg\n    self.gamma_pos = gamma_pos\n    self.m = m\n    self.eps = eps\n    self.disable_torch_grad_focal_loss = disable_torch_grad_focal_loss\n    self.apply_sigmoid = apply_sigmoid\n\n    # prevent memory allocation and gpu uploading every iteration, and encourages inplace operations\n    self.targets: torch.Tensor\n    self.anti_targets: torch.Tensor\n    self.xs_pos: torch.Tensor\n    self.xs_neg: torch.Tensor\n    self.asymmetric_w: torch.Tensor\n    self.loss: torch.Tensor\n</code></pre>"},{"location":"reference/quadra/losses/classification/asl.html#quadra.losses.classification.asl.AsymmetricLoss.forward","title":"<code>forward(x, y)</code>","text":"<p>Compute the asymmetric loss.</p> <p>Parameters:</p> <ul> <li> x             (<code>Tensor</code>)         \u2013          <p>input logits (after sigmoid)</p> </li> <li> y             (<code>Tensor</code>)         \u2013          <p>targets (multi-label binarized vector)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>         \u2013          <p>asymettric loss</p> </li> </ul> Source code in <code>quadra/losses/classification/asl.py</code> <pre><code>def forward(self, x: torch.Tensor, y: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"Compute the asymmetric loss.\n\n    Args:\n        x: input logits (after sigmoid)\n        y: targets (multi-label binarized vector)\n\n    Returns:\n        asymettric loss\n    \"\"\"\n    self.targets = y\n    self.anti_targets = 1 - y\n\n    # Calculating Probabilities\n    self.xs_pos = x\n    if self.apply_sigmoid:\n        self.xs_pos = torch.sigmoid(self.xs_pos)\n    self.xs_neg = 1.0 - self.xs_pos\n\n    # Asymmetric clipping\n    if self.m is not None and self.m &gt; 0:\n        self.xs_neg.add_(self.m).clamp_(max=1)\n\n    # Basic CE calculation\n    self.loss = self.targets * torch.log(self.xs_pos.clamp(min=self.eps))\n    self.loss.add_(self.anti_targets * torch.log(self.xs_neg.clamp(min=self.eps)))\n\n    # Asymmetric Focusing\n    if self.gamma_neg &gt; 0 or self.gamma_pos &gt; 0:\n        if self.disable_torch_grad_focal_loss:\n            torch.set_grad_enabled(False)\n        self.xs_pos = self.xs_pos * self.targets\n        self.xs_neg = self.xs_neg * self.anti_targets\n        self.asymmetric_w = torch.pow(\n            1 - self.xs_pos - self.xs_neg, self.gamma_pos * self.targets + self.gamma_neg * self.anti_targets\n        )\n        if self.disable_torch_grad_focal_loss:\n            torch.set_grad_enabled(True)\n        self.loss *= self.asymmetric_w\n\n    return -self.loss.sum()\n</code></pre>"},{"location":"reference/quadra/losses/classification/focal.html","title":"focal","text":""},{"location":"reference/quadra/losses/classification/focal.html#quadra.losses.classification.focal.BinaryFocalLossWithLogits","title":"<code>BinaryFocalLossWithLogits(alpha, gamma=2.0, reduction='none')</code>","text":"<p>             Bases: <code>Module</code></p> <p>Criterion that computes Focal loss.</p> <p>According to :cite:<code>lin2018focal</code>, the Focal loss is computed as follows:</p> <p>.. math::</p> <pre><code>\\text{FL}(p_t) = -\\alpha_t (1 - p_t)^{\\gamma} \\, \\text{log}(p_t)\n</code></pre> where <ul> <li>:math:<code>p_t</code> is the model's estimated probability for each class.</li> </ul> <p>Parameters:</p> <ul> <li> alpha             (<code>float</code>)         \u2013          <p>Weighting factor for the rare class :math:<code>\\alpha \\in [0, 1]</code>.</p> </li> <li> gamma             (<code>float</code>, default:                 <code>2.0</code> )         \u2013          <p>Focusing parameter :math:<code>\\gamma &gt;= 0</code>.</p> </li> <li> reduction             (<code>str</code>, default:                 <code>'none'</code> )         \u2013          <p>Specifies the reduction to apply to the output: <code>'none'</code> | <code>'mean'</code> | <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the sum of the output will be divided by the number of elements in the output, <code>'sum'</code>: the output will be summed.</p> </li> </ul> Shape <ul> <li>Input: :math:<code>(N, *)</code>.</li> <li>Target: :math:<code>(N, *)</code>.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; kwargs = {\"alpha\": 0.25, \"gamma\": 2.0, \"reduction\": 'mean'}\n&gt;&gt;&gt; loss = BinaryFocalLossWithLogits(**kwargs)\n&gt;&gt;&gt; input = torch.randn(1, 3, 5, requires_grad=True)\n&gt;&gt;&gt; target = torch.empty(1, 3, 5, dtype=torch.long).random_(2)\n&gt;&gt;&gt; output = loss(input, target)\n&gt;&gt;&gt; output.backward()\n</code></pre> Source code in <code>quadra/losses/classification/focal.py</code> <pre><code>def __init__(self, alpha: float, gamma: float = 2.0, reduction: str = \"none\") -&gt; None:\n    super().__init__()\n    self.alpha: float = alpha\n    self.gamma: float = gamma\n    self.reduction: str = reduction\n</code></pre>"},{"location":"reference/quadra/losses/classification/focal.html#quadra.losses.classification.focal.BinaryFocalLossWithLogits.forward","title":"<code>forward(input_tensor, target)</code>","text":"<p>Forward call computation.</p> Source code in <code>quadra/losses/classification/focal.py</code> <pre><code>def forward(self, input_tensor: torch.Tensor, target: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"Forward call computation.\"\"\"\n    return binary_focal_loss_with_logits(input_tensor, target, self.alpha, self.gamma, self.reduction)\n</code></pre>"},{"location":"reference/quadra/losses/classification/focal.html#quadra.losses.classification.focal.FocalLoss","title":"<code>FocalLoss(alpha, gamma=2.0, reduction='none', eps=None)</code>","text":"<p>             Bases: <code>Module</code></p> <p>Criterion that computes Focal loss.</p> <p>According to :cite:<code>lin2018focal</code>, the Focal loss is computed as follows:</p> <p>.. math::</p> <pre><code>\\text{FL}(p_t) = -\\alpha_t (1 - p_t)^{\\gamma} \\, \\text{log}(p_t)\n</code></pre> Where <ul> <li>:math:<code>p_t</code> is the model's estimated probability for each class.</li> </ul> <p>Parameters:</p> <ul> <li> alpha             (<code>float</code>)         \u2013          <p>Weighting factor :math:<code>\\alpha \\in [0, 1]</code>.</p> </li> <li> gamma             (<code>float</code>, default:                 <code>2.0</code> )         \u2013          <p>Focusing parameter :math:<code>\\gamma &gt;= 0</code>.</p> </li> <li> reduction             (<code>str</code>, default:                 <code>'none'</code> )         \u2013          <p>Specifies the reduction to apply to the output: <code>'none'</code> | <code>'mean'</code> | <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the sum of the output will be divided by the number of elements in the output, <code>'sum'</code>: the output will be summed.</p> </li> <li> eps             (<code>float | None</code>, default:                 <code>None</code> )         \u2013          <p>used.</p> </li> </ul> Shape <ul> <li>Input: :math:<code>(N, C, *)</code> where C = number of classes.</li> <li>Target: :math:<code>(N, *)</code> where each value is   :math:<code>0 \u2264 targets[i] \u2264 C\u22121</code>.</li> </ul> Example <p>N = 5  # num_classes kwargs = {\"alpha\": 0.5, \"gamma\": 2.0, \"reduction\": 'mean'} criterion = FocalLoss(**kwargs) input = torch.randn(1, N, 3, 5, requires_grad=True) target = torch.empty(1, 3, 5, dtype=torch.long).random_(N) output = criterion(input, target) output.backward()</p> Source code in <code>quadra/losses/classification/focal.py</code> <pre><code>def __init__(self, alpha: float, gamma: float = 2.0, reduction: str = \"none\", eps: float | None = None) -&gt; None:\n    super().__init__()\n    self.alpha: float = alpha\n    self.gamma: float = gamma\n    self.reduction: str = reduction\n    self.eps: float | None = eps\n</code></pre>"},{"location":"reference/quadra/losses/classification/focal.html#quadra.losses.classification.focal.FocalLoss.forward","title":"<code>forward(input_tensor, target)</code>","text":"<p>Forward call computation.</p> Source code in <code>quadra/losses/classification/focal.py</code> <pre><code>def forward(self, input_tensor: torch.Tensor, target: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"Forward call computation.\"\"\"\n    return focal_loss(input_tensor, target, self.alpha, self.gamma, self.reduction, self.eps)\n</code></pre>"},{"location":"reference/quadra/losses/classification/focal.html#quadra.losses.classification.focal.binary_focal_loss_with_logits","title":"<code>binary_focal_loss_with_logits(input_tensor, target, alpha=0.25, gamma=2.0, reduction='none', eps=None)</code>","text":"<p>Function that computes Binary Focal loss.</p> <p>.. math::</p> <pre><code>\\text{FL}(p_t) = -\\alpha_t (1 - p_t)^{\\gamma} \\, \\text{log}(p_t)\n</code></pre> where <ul> <li>:math:<code>p_t</code> is the model's estimated probability for each class.</li> </ul> <p>Parameters:</p> <ul> <li> input_tensor             (<code>Tensor</code>)         \u2013          <p>input data tensor of arbitrary shape.</p> </li> <li> target             (<code>Tensor</code>)         \u2013          <p>the target tensor with shape matching input.</p> </li> <li> alpha             (<code>float</code>, default:                 <code>0.25</code> )         \u2013          <p>Weighting factor for the rare class :math:<code>\\alpha \\in [0, 1]</code>.</p> </li> <li> gamma             (<code>float</code>, default:                 <code>2.0</code> )         \u2013          <p>Focusing parameter :math:<code>\\gamma &gt;= 0</code>.</p> </li> <li> reduction             (<code>str</code>, default:                 <code>'none'</code> )         \u2013          <p>Specifies the reduction to apply to the output: <code>'none'</code> | <code>'mean'</code> | <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the sum of the output will be divided by the number of elements in the output, <code>'sum'</code>: the output will be summed.</p> </li> <li> eps             (<code>float | None</code>, default:                 <code>None</code> )         \u2013          </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>         \u2013          <p>the computed loss.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; kwargs = {\"alpha\": 0.25, \"gamma\": 2.0, \"reduction\": 'mean'}\n&gt;&gt;&gt; logits = torch.tensor([[[6.325]],[[5.26]],[[87.49]]])\n&gt;&gt;&gt; labels = torch.tensor([[[1.]],[[1.]],[[0.]]])\n&gt;&gt;&gt; binary_focal_loss_with_logits(logits, labels, **kwargs)\ntensor(21.8725)\n</code></pre> Source code in <code>quadra/losses/classification/focal.py</code> <pre><code>def binary_focal_loss_with_logits(\n    input_tensor: torch.Tensor,\n    target: torch.Tensor,\n    alpha: float = 0.25,\n    gamma: float = 2.0,\n    reduction: str = \"none\",\n    eps: float | None = None,\n) -&gt; torch.Tensor:\nr\"\"\"Function that computes Binary Focal loss.\n\n    .. math::\n\n        \\text{FL}(p_t) = -\\alpha_t (1 - p_t)^{\\gamma} \\, \\text{log}(p_t)\n\n    where:\n       - :math:`p_t` is the model's estimated probability for each class.\n\n    Args:\n        input_tensor: input data tensor of arbitrary shape.\n        target: the target tensor with shape matching input.\n        alpha: Weighting factor for the rare class :math:`\\alpha \\in [0, 1]`.\n        gamma: Focusing parameter :math:`\\gamma &gt;= 0`.\n        reduction: Specifies the reduction to apply to the\n            output: ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction\n            will be applied, ``'mean'``: the sum of the output will be divided by\n            the number of elements in the output, ``'sum'``: the output will be\n            summed.\n        eps: Deprecated: scalar for numerically stability when dividing. This is no longer used.\n\n    Returns:\n        the computed loss.\n\n    Examples:\n        &gt;&gt;&gt; kwargs = {\"alpha\": 0.25, \"gamma\": 2.0, \"reduction\": 'mean'}\n        &gt;&gt;&gt; logits = torch.tensor([[[6.325]],[[5.26]],[[87.49]]])\n        &gt;&gt;&gt; labels = torch.tensor([[[1.]],[[1.]],[[0.]]])\n        &gt;&gt;&gt; binary_focal_loss_with_logits(logits, labels, **kwargs)\n        tensor(21.8725)\n    \"\"\"\n    if eps is not None and not torch.jit.is_scripting():\n        warnings.warn(\n            \"`binary_focal_loss_with_logits` has been reworked for improved numerical stability \"\n            \"and the `eps` argument is no longer necessary\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n\n    if not isinstance(input_tensor, torch.Tensor):\n        raise TypeError(f\"Input type is not a torch.Tensor. Got {type(input_tensor)}\")\n\n    if not len(input_tensor.shape) &gt;= 2:\n        raise ValueError(f\"Invalid input shape, we expect BxCx*. Got: {input_tensor.shape}\")\n\n    if input_tensor.size(0) != target.size(0):\n        raise ValueError(\n            f\"Expected input batch_size ({input_tensor.size(0)}) to match target batch_size ({target.size(0)}).\"\n        )\n\n    probs_pos = torch.sigmoid(input_tensor)\n    probs_neg = torch.sigmoid(-input_tensor)\n    loss_tmp = -alpha * torch.pow(probs_neg, gamma) * target * F.logsigmoid(input_tensor) - (1 - alpha) * torch.pow(\n        probs_pos, gamma\n    ) * (1.0 - target) * F.logsigmoid(-input_tensor)\n\n    if reduction == \"none\":\n        loss = loss_tmp\n    elif reduction == \"mean\":\n        loss = torch.mean(loss_tmp)\n    elif reduction == \"sum\":\n        loss = torch.sum(loss_tmp)\n    else:\n        raise NotImplementedError(f\"Invalid reduction mode: {reduction}\")\n    return loss\n</code></pre>"},{"location":"reference/quadra/losses/classification/focal.html#quadra.losses.classification.focal.focal_loss","title":"<code>focal_loss(input_tensor, target, alpha, gamma=2.0, reduction='none', eps=None)</code>","text":"<p>Criterion that computes Focal loss.</p> <p>According to :cite:<code>lin2018focal</code>, the Focal loss is computed as follows:</p> <p>.. math::</p> <pre><code>\\text{FL}(p_t) = -\\alpha_t (1 - p_t)^{\\gamma} \\, \\text{log}(p_t)\n</code></pre> Where <ul> <li>:math:<code>p_t</code> is the model's estimated probability for each class.</li> </ul> <p>Parameters:</p> <ul> <li> input_tensor             (<code>Tensor</code>)         \u2013          <p>Logits tensor with shape :math:<code>(N, C, *)</code> where C = number of classes.</p> </li> <li> target             (<code>Tensor</code>)         \u2013          <p>Labels tensor with shape :math:<code>(N, *)</code> where each value is :math:<code>0 \u2264 targets[i] \u2264 C\u22121</code>.</p> </li> <li> alpha             (<code>float</code>)         \u2013          <p>Weighting factor :math:<code>\\alpha \\in [0, 1]</code>.</p> </li> <li> gamma             (<code>float</code>, default:                 <code>2.0</code> )         \u2013          <p>Focusing parameter :math:<code>\\gamma &gt;= 0</code>.</p> </li> <li> reduction             (<code>str</code>, default:                 <code>'none'</code> )         \u2013          <p>Specifies the reduction to apply to the output: <code>'none'</code> | <code>'mean'</code> | <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the sum of the output will be divided by the number of elements in the output, <code>'sum'</code>: the output will be summed.</p> </li> <li> eps             (<code>float | None</code>, default:                 <code>None</code> )         \u2013          </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>         \u2013          <p>The computed loss.</p> </li> </ul> Example <p>N = 5  # num_classes input = torch.randn(1, N, 3, 5, requires_grad=True) target = torch.empty(1, 3, 5, dtype=torch.long).random_(N) output = focal_loss(input, target, alpha=0.5, gamma=2.0, reduction='mean') output.backward()</p> Source code in <code>quadra/losses/classification/focal.py</code> <pre><code>def focal_loss(\n    input_tensor: torch.Tensor,\n    target: torch.Tensor,\n    alpha: float,\n    gamma: float = 2.0,\n    reduction: str = \"none\",\n    eps: float | None = None,\n) -&gt; torch.Tensor:\nr\"\"\"Criterion that computes Focal loss.\n\n    According to :cite:`lin2018focal`, the Focal loss is computed as follows:\n\n    .. math::\n\n        \\text{FL}(p_t) = -\\alpha_t (1 - p_t)^{\\gamma} \\, \\text{log}(p_t)\n\n    Where:\n       - :math:`p_t` is the model's estimated probability for each class.\n\n    Args:\n        input_tensor: Logits tensor with shape :math:`(N, C, *)` where C = number of classes.\n        target: Labels tensor with shape :math:`(N, *)` where each value is :math:`0 \u2264 targets[i] \u2264 C\u22121`.\n        alpha: Weighting factor :math:`\\alpha \\in [0, 1]`.\n        gamma: Focusing parameter :math:`\\gamma &gt;= 0`.\n        reduction: Specifies the reduction to apply to the\n            output: ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction\n            will be applied, ``'mean'``: the sum of the output will be divided by\n            the number of elements in the output, ``'sum'``: the output will be\n            summed.\n        eps: Deprecated: scalar to enforce numerical stabiliy. This is no longer used.\n\n    Returns:\n        The computed loss.\n\n    Example:\n        &gt;&gt;&gt; N = 5  # num_classes\n        &gt;&gt;&gt; input = torch.randn(1, N, 3, 5, requires_grad=True)\n        &gt;&gt;&gt; target = torch.empty(1, 3, 5, dtype=torch.long).random_(N)\n        &gt;&gt;&gt; output = focal_loss(input, target, alpha=0.5, gamma=2.0, reduction='mean')\n        &gt;&gt;&gt; output.backward()\n    \"\"\"\n    if eps is not None and not torch.jit.is_scripting():\n        warnings.warn(\n            \"`focal_loss` has been reworked for improved numerical stability \"\n            \"and the `eps` argument is no longer necessary\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n\n    if not isinstance(input_tensor, torch.Tensor):\n        raise TypeError(f\"Input type is not a torch.Tensor. Got {type(input_tensor)}\")\n\n    if not len(input_tensor.shape) &gt;= 2:\n        raise ValueError(f\"Invalid input shape, we expect BxCx*. Got: {input_tensor.shape}\")\n\n    if input_tensor.size(0) != target.size(0):\n        raise ValueError(\n            f\"Expected input batch_size ({input_tensor.size(0)}) to match target batch_size ({target.size(0)}).\"\n        )\n\n    n = input_tensor.size(0)\n    out_size = (n,) + input_tensor.size()[2:]\n    if target.size()[1:] != input_tensor.size()[2:]:\n        raise ValueError(f\"Expected target size {out_size}, got {target.size()}\")\n\n    if not input_tensor.device == target.device:\n        raise ValueError(f\"input and target must be in the same device. Got: {input_tensor.device} and {target.device}\")\n\n    # compute softmax over the classes axis\n    input_soft: torch.Tensor = F.softmax(input_tensor, dim=1)\n    log_input_soft: torch.Tensor = F.log_softmax(input_tensor, dim=1)\n\n    # create the labels one hot tensor\n    target_one_hot: torch.Tensor = one_hot(\n        target, num_classes=input_tensor.shape[1], device=input_tensor.device, dtype=input_tensor.dtype\n    )\n\n    # compute the actual focal loss\n    weight = torch.pow(-input_soft + 1.0, gamma)\n\n    focal = -alpha * weight * log_input_soft\n    loss_tmp = torch.einsum(\"bc...,bc...-&gt;b...\", (target_one_hot, focal))\n\n    if reduction == \"none\":\n        loss = loss_tmp\n    elif reduction == \"mean\":\n        loss = torch.mean(loss_tmp)\n    elif reduction == \"sum\":\n        loss = torch.sum(loss_tmp)\n    else:\n        raise NotImplementedError(f\"Invalid reduction mode: {reduction}\")\n    return loss\n</code></pre>"},{"location":"reference/quadra/losses/classification/focal.html#quadra.losses.classification.focal.one_hot","title":"<code>one_hot(labels, num_classes, device=None, dtype=None, eps=1e-06)</code>","text":"<p>Convert an integer label x-D tensor to a one-hot (x+1)-D tensor.</p> <p>Parameters:</p> <ul> <li> labels             (<code>Tensor</code>)         \u2013          <p>tensor with labels of shape :math:<code>(N, *)</code>, where N is batch size. Each value is an integer representing correct classification.</p> </li> <li> num_classes             (<code>int</code>)         \u2013          <p>number of classes in labels.</p> </li> <li> device             (<code>device | None</code>, default:                 <code>None</code> )         \u2013          <p>the desired device of returned tensor.</p> </li> <li> dtype             (<code>dtype | None</code>, default:                 <code>None</code> )         \u2013          <p>the desired data type of returned tensor.</p> </li> <li> eps             (<code>float</code>, default:                 <code>1e-06</code> )         \u2013          <p>a value added to the returned tensor.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>         \u2013          <p>the labels in one hot tensor of shape :math:<code>(N, C, *)</code>,</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; labels = torch.LongTensor([[[0, 1], [2, 0]]])\n&gt;&gt;&gt; one_hot(labels, num_classes=3)\ntensor([[[[1.0000e+00, 1.0000e-06],\n          [1.0000e-06, 1.0000e+00]],\n\n         [[1.0000e-06, 1.0000e+00],\n          [1.0000e-06, 1.0000e-06]],\n\n         [[1.0000e-06, 1.0000e-06],\n          [1.0000e+00, 1.0000e-06]]]])\n</code></pre> Source code in <code>quadra/losses/classification/focal.py</code> <pre><code>def one_hot(\n    labels: torch.Tensor,\n    num_classes: int,\n    device: torch.device | None = None,\n    dtype: torch.dtype | None = None,\n    eps: float = 1e-6,\n) -&gt; torch.Tensor:\nr\"\"\"Convert an integer label x-D tensor to a one-hot (x+1)-D tensor.\n\n    Args:\n        labels: tensor with labels of shape :math:`(N, *)`, where N is batch size.\n            Each value is an integer representing correct classification.\n        num_classes: number of classes in labels.\n        device: the desired device of returned tensor.\n        dtype: the desired data type of returned tensor.\n        eps: a value added to the returned tensor.\n\n    Returns:\n        the labels in one hot tensor of shape :math:`(N, C, *)`,\n\n    Examples:\n        &gt;&gt;&gt; labels = torch.LongTensor([[[0, 1], [2, 0]]])\n        &gt;&gt;&gt; one_hot(labels, num_classes=3)\n        tensor([[[[1.0000e+00, 1.0000e-06],\n                  [1.0000e-06, 1.0000e+00]],\n        &lt;BLANKLINE&gt;\n                 [[1.0000e-06, 1.0000e+00],\n                  [1.0000e-06, 1.0000e-06]],\n        &lt;BLANKLINE&gt;\n                 [[1.0000e-06, 1.0000e-06],\n                  [1.0000e+00, 1.0000e-06]]]])\n\n    \"\"\"\n    if not isinstance(labels, torch.Tensor):\n        raise TypeError(f\"Input labels type is not a torch.Tensor. Got {type(labels)}\")\n\n    if not labels.dtype == torch.int64:\n        raise ValueError(f\"labels must be of the same dtype torch.int64. Got: {labels.dtype}\")\n\n    if num_classes &lt; 1:\n        raise ValueError(f\"The number of classes must be bigger than one. Got: {num_classes}\")\n\n    shape = labels.shape\n    one_hot_output = torch.zeros((shape[0], num_classes) + shape[1:], device=device, dtype=dtype)\n\n    return one_hot_output.scatter_(1, labels.unsqueeze(1), 1.0) + eps\n</code></pre>"},{"location":"reference/quadra/losses/classification/prototypical.html","title":"prototypical","text":""},{"location":"reference/quadra/losses/classification/prototypical.html#quadra.losses.classification.prototypical.euclidean_dist","title":"<code>euclidean_dist(query, prototypes, sen=True, eps_pos=1.0, eps_neg=-1e-07, eps=1e-07)</code>","text":"<p>Compute euclidean distance between two tensors.     SEN dissimilarity from https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123680120.pdf Args:     query: feature of the network     prototypes: prototypes of the center     sen: Sen dissimilarity flag     eps_pos: similarity arg     eps_neg: similarity arg     eps: similarity arg.</p> <p>Returns:</p> <ul> <li> <code>Tensor</code>         \u2013          <p>Euclidian loss</p> </li> </ul> Source code in <code>quadra/losses/classification/prototypical.py</code> <pre><code>def euclidean_dist(\n    query: torch.Tensor,\n    prototypes: torch.Tensor,\n    sen: bool = True,\n    eps_pos: float = 1.0,\n    eps_neg: float = -1e-7,\n    eps: float = 1e-7,\n) -&gt; torch.Tensor:\n\"\"\"Compute euclidean distance between two tensors.\n        SEN dissimilarity from https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123680120.pdf\n    Args:\n        query: feature of the network\n        prototypes: prototypes of the center\n        sen: Sen dissimilarity flag\n        eps_pos: similarity arg\n        eps_neg: similarity arg\n        eps: similarity arg.\n\n    Returns:\n        Euclidian loss\n\n    \"\"\"\n    # query: (n_classes * n_query) x d\n    # prototypes: n_classes x d\n    n = query.size(0)\n    m = prototypes.size(0)\n    d = query.size(1)\n    if d != prototypes.size(1):\n        raise ValueError(\"query and prototypes size[1] should be equal\")\n\n    if sen:\n        # SEN dissimilarity from https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123680120.pdf\n        norm_query = torch.linalg.norm(query, ord=2, dim=1)  # (n_classes * n_query) X 1\n        norm_prototypes = torch.linalg.norm(prototypes, ord=2, dim=1)  # n_classes X 1\n\n        # We have to compute (||z|| - ||c||)^2 between all query points w.r.t.\n        # all support points\n\n        # Replicate each single query norm value m times\n        norm_query = norm_query.view(-1, 1).unsqueeze(1).expand(n, m, 1)\n        # Replicate all prototypes norm values n times\n        norm_prototypes = norm_prototypes.view(-1, 1).unsqueeze(0).expand(n, m, 1)\n        norm_diff = torch.pow(norm_query - norm_prototypes, 2).squeeze(2)\n        epsilon = torch.full((n, m), eps_neg).type_as(query)\n        if eps_pos != eps_neg:\n            # n_query = n // m\n            # for i in range(m):\n            #     epsilon[i * n_query : (i + 1) * n_query, i] = 1.0\n\n            # Since query points with class i need to have a positive epsilon\n            # whenever they refer to support point with class i and since\n            # query and support points are ordered, we need to set:\n            # the 1st column of the 1st n_query rows to eps_pos\n            # the 2nd column of the 2nd n_query rows to eps_pos\n            # and so on\n            idxs = torch.eye(m, dtype=torch.bool).unsqueeze(1).expand(m, n // m, m).reshape(-1, m)\n            epsilon[idxs] = eps_pos\n        norm_diff = norm_diff * epsilon\n\n    # Replicate each single query point value m times\n    query = query.unsqueeze(1).expand(n, m, d)\n    # Replicate all prototype points values n times\n    prototypes = prototypes.unsqueeze(0).expand(n, m, d)\n\n    norm = torch.pow(query - prototypes, 2).sum(2)\n    if sen:\n        return torch.sqrt(norm + norm_diff + eps)\n\n    return norm\n</code></pre>"},{"location":"reference/quadra/losses/classification/prototypical.html#quadra.losses.classification.prototypical.prototypical_loss","title":"<code>prototypical_loss(coords, target, n_support, prototypes=None, sen=True, eps_pos=1.0, eps_neg=-1e-07)</code>","text":"<p>Prototypical loss implementation.</p> <p>Inspired by https://github.com/jakesnell/prototypical-networks/blob/master/protonets/models/few_shot.py     Compute the barycentres by averaging the features of n_support     samples for each class in target, computes then the distances from each     samples' features to each one of the barycentres, computes the     log_probability for each n_query samples for each one of the current     classes, of appartaining to a class c, loss and accuracy are then computed and returned.</p> <p>Parameters:</p> <ul> <li> coords             (<code>Tensor</code>)         \u2013          <p>The model output for a batch of samples</p> </li> <li> target             (<code>Tensor</code>)         \u2013          <p>Ground truth for the above batch of samples</p> </li> <li> n_support             (<code>int</code>)         \u2013          <p>Number of samples to keep in account when computing barycentres, for each one of the current classes</p> </li> <li> prototypes             (<code>Tensor | None</code>, default:                 <code>None</code> )         \u2013          <p>if not None, is used for classification</p> </li> <li> sen             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Sen dissimilarity flag</p> </li> <li> eps_pos             (<code>float</code>, default:                 <code>1.0</code> )         \u2013          <p>Sen positive similarity arg</p> </li> <li> eps_neg             (<code>float</code>, default:                 <code>-1e-07</code> )         \u2013          <p>Sen negative similarity arg</p> </li> </ul> Source code in <code>quadra/losses/classification/prototypical.py</code> <pre><code>def prototypical_loss(\n    coords: torch.Tensor,\n    target: torch.Tensor,\n    n_support: int,\n    prototypes: torch.Tensor | None = None,\n    sen: bool = True,\n    eps_pos: float = 1.0,\n    eps_neg: float = -1e-7,\n):\n\"\"\"Prototypical loss implementation.\n\n    Inspired by https://github.com/jakesnell/prototypical-networks/blob/master/protonets/models/few_shot.py\n        Compute the barycentres by averaging the features of n_support\n        samples for each class in target, computes then the distances from each\n        samples' features to each one of the barycentres, computes the\n        log_probability for each n_query samples for each one of the current\n        classes, of appartaining to a class c, loss and accuracy are then computed and returned.\n\n    Args:\n        coords: The model output for a batch of samples\n        target: Ground truth for the above batch of samples\n        n_support: Number of samples to keep in account when computing\n            barycentres, for each one of the current classes\n        prototypes: if not None, is used for classification\n        sen: Sen dissimilarity flag\n        eps_pos: Sen positive similarity arg\n        eps_neg: Sen negative similarity arg\n    \"\"\"\n    classes = torch.unique(target, sorted=True)\n    n_classes = len(classes)\n    n_query = len(torch.where(target == classes[0])[0]) - n_support\n\n    # Check equality between classes and target with broadcasting:\n    # class_idxs[i, j] = True iff classes[i] == target[j]\n    class_idxs = classes.unsqueeze(1) == target\n    if prototypes is None:\n        # Get the prototypes as the mean of the support points,\n        # ordered by class\n        prototypes = torch.stack([coords[idx_list][:n_support] for idx_list in class_idxs]).mean(1)  # n_classes X d\n    # Get query samples as the points NOT in the support set,\n    # where, after .view(-1, d), one has that\n    # the 1st n_query points refer to class 1\n    # the 2nd n_query points refer to class 2\n    # and so on\n    query_samples = torch.stack([coords[idx_list][n_support:] for idx_list in class_idxs]).view(\n        -1, prototypes.shape[-1]\n    )  # (n_classes * n_query) X d\n    # Get distances, where dists[i, j] is the distance between\n    # query point i to support point j\n    dists = euclidean_dist(\n        query_samples, prototypes, sen=sen, eps_pos=eps_pos, eps_neg=eps_neg\n    )  # (n_classes * n_query) X n_classes\n    log_p_y = F.log_softmax(-dists, dim=1)\n    log_p_y = log_p_y.view(n_classes, n_query, -1)  # n_classes X n_query X n_classes\n\n    target_inds = torch.arange(0, n_classes).view(n_classes, 1, 1)\n    # One solution is to use type_as(coords[0])\n    target_inds = target_inds.type_as(coords)\n    target_inds = target_inds.expand(n_classes, n_query, 1).long()\n\n    # Since we need to backpropagate the log softmax of query points\n    # of class i that refers to support of the same class for every i,\n    # and since query and support are ordered we select:\n    # from the 1st n_query X n_classes the 1st column\n    # from the 2nd n_query X n_classes the 2st column\n    # and so on\n    loss_val = -log_p_y.gather(2, target_inds).squeeze().view(-1).mean()\n    _, y_hat = log_p_y.max(2)\n    acc_val = y_hat.eq(target_inds.squeeze()).float().mean()\n\n    return loss_val, acc_val, prototypes\n</code></pre>"},{"location":"reference/quadra/losses/ssl/index.html","title":"ssl","text":""},{"location":"reference/quadra/losses/ssl/index.html#quadra.losses.ssl.BYOLRegressionLoss","title":"<code>BYOLRegressionLoss</code>","text":"<p>             Bases: <code>Module</code></p> <p>BYOL regression loss module.</p>"},{"location":"reference/quadra/losses/ssl/index.html#quadra.losses.ssl.BYOLRegressionLoss.forward","title":"<code>forward(x, y)</code>","text":"<p>Compute the BYOL regression loss.</p> <p>Parameters:</p> <ul> <li> x             (<code>Tensor</code>)         \u2013          <p>First Tensor</p> </li> <li> y             (<code>Tensor</code>)         \u2013          <p>Second Tensor</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>         \u2013          <p>BYOL regression loss</p> </li> </ul> Source code in <code>quadra/losses/ssl/byol.py</code> <pre><code>def forward(\n    self,\n    x: torch.Tensor,\n    y: torch.Tensor,\n) -&gt; torch.Tensor:\n\"\"\"Compute the BYOL regression loss.\n\n    Args:\n        x: First Tensor\n        y: Second Tensor\n\n    Returns:\n        BYOL regression loss\n    \"\"\"\n    return byol_regression_loss(x, y)\n</code></pre>"},{"location":"reference/quadra/losses/ssl/index.html#quadra.losses.ssl.BarlowTwinsLoss","title":"<code>BarlowTwinsLoss(lambd)</code>","text":"<p>             Bases: <code>Module</code></p> <p>BarlowTwin loss.</p> <p>Parameters:</p> <ul> <li> lambd             (<code>float</code>)         \u2013          <p>lambda of the loss.</p> </li> </ul> Source code in <code>quadra/losses/ssl/barlowtwins.py</code> <pre><code>def __init__(self, lambd: float):\n    super().__init__()\n    self.lambd = lambd\n</code></pre>"},{"location":"reference/quadra/losses/ssl/index.html#quadra.losses.ssl.BarlowTwinsLoss.forward","title":"<code>forward(z1, z2)</code>","text":"<p>Compute the BarlowTwins loss.</p> Source code in <code>quadra/losses/ssl/barlowtwins.py</code> <pre><code>def forward(self, z1: torch.Tensor, z2: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"Compute the BarlowTwins loss.\"\"\"\n    return barlowtwins_loss(z1, z2, self.lambd)\n</code></pre>"},{"location":"reference/quadra/losses/ssl/index.html#quadra.losses.ssl.DinoDistillationLoss","title":"<code>DinoDistillationLoss(output_dim, max_epochs, warmup_teacher_temp=0.04, teacher_temp=0.07, warmup_teacher_temp_epochs=30, student_temp=0.1, center_momentum=0.9)</code>","text":"<p>             Bases: <code>Module</code></p> <p>Dino distillation loss module.</p> <p>Parameters:</p> <ul> <li> output_dim             (<code>int</code>)         \u2013          <p>output dim.</p> </li> <li> max_epochs             (<code>int</code>)         \u2013          <p>max epochs.</p> </li> <li> warmup_teacher_temp             (<code>float</code>, default:                 <code>0.04</code> )         \u2013          <p>warmup temperature.</p> </li> <li> teacher_temp             (<code>float</code>, default:                 <code>0.07</code> )         \u2013          <p>teacher temperature.</p> </li> <li> warmup_teacher_temp_epochs             (<code>int</code>, default:                 <code>30</code> )         \u2013          <p>warmup teacher epocs.</p> </li> <li> student_temp             (<code>float</code>, default:                 <code>0.1</code> )         \u2013          <p>student temperature.</p> </li> <li> center_momentum             (<code>float</code>, default:                 <code>0.9</code> )         \u2013          <p>center momentum.</p> </li> </ul> Source code in <code>quadra/losses/ssl/dino.py</code> <pre><code>def __init__(\n    self,\n    output_dim: int,\n    max_epochs: int,\n    warmup_teacher_temp: float = 0.04,\n    teacher_temp: float = 0.07,\n    warmup_teacher_temp_epochs: int = 30,\n    student_temp: float = 0.1,\n    center_momentum: float = 0.9,\n):\n    super().__init__()\n    self.student_temp = student_temp\n    self.center_momentum = center_momentum\n    self.center: torch.Tensor\n    # we apply a warm up for the teacher temperature because\n    # a too high temperature makes the training instable at the beginning\n\n    if warmup_teacher_temp_epochs &gt;= max_epochs:\n        raise ValueError(\n            f\"Number of warmup epochs ({warmup_teacher_temp_epochs}) must be smaller than max_epochs ({max_epochs})\"\n        )\n\n    if warmup_teacher_temp_epochs &lt; 30:\n        log.warning(\"Warmup teacher epochs is very small (&lt; 30). This may cause instabilities in the training\")\n\n    self.teacher_temp_schedule = np.concatenate(\n        (\n            np.linspace(warmup_teacher_temp, teacher_temp, warmup_teacher_temp_epochs),\n            np.ones(max_epochs - warmup_teacher_temp_epochs) * teacher_temp,\n        )\n    )\n    self.register_buffer(\"center\", torch.zeros(1, output_dim))\n</code></pre>"},{"location":"reference/quadra/losses/ssl/index.html#quadra.losses.ssl.DinoDistillationLoss.forward","title":"<code>forward(current_epoch, student_output, teacher_output)</code>","text":"<p>Runs forward.</p> Source code in <code>quadra/losses/ssl/dino.py</code> <pre><code>def forward(\n    self,\n    current_epoch: int,\n    student_output: torch.Tensor,\n    teacher_output: torch.Tensor,\n) -&gt; torch.Tensor:\n\"\"\"Runs forward.\"\"\"\n    teacher_temp = self.teacher_temp_schedule[current_epoch]\n    loss = dino_distillation_loss(\n        student_output,\n        teacher_output,\n        center_vector=self.center,\n        teacher_temp=teacher_temp,\n        student_temp=self.student_temp,\n    )\n\n    self.update_center(teacher_output)\n    return loss\n</code></pre>"},{"location":"reference/quadra/losses/ssl/index.html#quadra.losses.ssl.DinoDistillationLoss.update_center","title":"<code>update_center(teacher_output)</code>","text":"<p>Update center of the distribution of the teacher Args:     teacher_output: teacher output.</p> <p>Returns:</p> <ul> <li> <code>None</code>         \u2013          <p>None</p> </li> </ul> Source code in <code>quadra/losses/ssl/dino.py</code> <pre><code>@torch.no_grad()\ndef update_center(self, teacher_output: torch.Tensor) -&gt; None:\n\"\"\"Update center of the distribution of the teacher\n    Args:\n        teacher_output: teacher output.\n\n    Returns:\n        None\n    \"\"\"\n    # TODO: check if this is correct\n    # torch.cat expects a list of tensors but teacher_output is a tensor\n    batch_center = torch.cat(teacher_output).mean(dim=0, keepdim=True)  # type: ignore[call-overload]\n    self.center = self.center * self.center_momentum + batch_center * (1 - self.center_momentum)\n</code></pre>"},{"location":"reference/quadra/losses/ssl/index.html#quadra.losses.ssl.IDMMLoss","title":"<code>IDMMLoss(smoothing=0.1)</code>","text":"<p>             Bases: <code>Module</code></p> <p>IDMM loss described in https://arxiv.org/abs/2201.10728.</p> Source code in <code>quadra/losses/ssl/idmm.py</code> <pre><code>def __init__(self, smoothing: float = 0.1):\n    super().__init__()\n    self.smoothing = smoothing\n</code></pre>"},{"location":"reference/quadra/losses/ssl/index.html#quadra.losses.ssl.IDMMLoss.forward","title":"<code>forward(p1, y1)</code>","text":"<p>IDMM loss described in https://arxiv.org/abs/2201.10728.</p> <p>Parameters:</p> <ul> <li> p1             (<code>Tensor</code>)         \u2013          <p>Prediction labels for <code>z1</code></p> </li> <li> y1             (<code>Tensor</code>)         \u2013          <p>Instance labels for <code>z1</code></p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>         \u2013          <p>IDMM loss</p> </li> </ul> Source code in <code>quadra/losses/ssl/idmm.py</code> <pre><code>def forward(\n    self,\n    p1: torch.Tensor,\n    y1: torch.Tensor,\n) -&gt; torch.Tensor:\n\"\"\"IDMM loss described in https://arxiv.org/abs/2201.10728.\n\n    Args:\n        p1: Prediction labels for `z1`\n        y1: Instance labels for `z1`\n\n    Returns:\n        IDMM loss\n    \"\"\"\n    return idmm_loss(\n        p1,\n        y1,\n        self.smoothing,\n    )\n</code></pre>"},{"location":"reference/quadra/losses/ssl/index.html#quadra.losses.ssl.SimCLRLoss","title":"<code>SimCLRLoss(temperature=1.0)</code>","text":"<p>             Bases: <code>Module</code></p> <p>SIMCLRloss module.</p> <p>Parameters:</p> <ul> <li> temperature             (<code>float</code>, default:                 <code>1.0</code> )         \u2013          <p>temperature of SIM loss.</p> </li> </ul> Source code in <code>quadra/losses/ssl/simclr.py</code> <pre><code>def __init__(self, temperature: float = 1.0):\n    super().__init__()\n    self.temperature = temperature\n</code></pre>"},{"location":"reference/quadra/losses/ssl/index.html#quadra.losses.ssl.SimCLRLoss.forward","title":"<code>forward(x1, x2)</code>","text":"<p>Forward pass of the loss.</p> Source code in <code>quadra/losses/ssl/simclr.py</code> <pre><code>def forward(self, x1: torch.Tensor, x2: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"Forward pass of the loss.\"\"\"\n    return simclr_loss(x1, x2, temperature=self.temperature)\n</code></pre>"},{"location":"reference/quadra/losses/ssl/index.html#quadra.losses.ssl.SimSIAMLoss","title":"<code>SimSIAMLoss</code>","text":"<p>             Bases: <code>Module</code></p> <p>SimSIAM loss module.</p>"},{"location":"reference/quadra/losses/ssl/index.html#quadra.losses.ssl.SimSIAMLoss.forward","title":"<code>forward(p1, p2, z1, z2)</code>","text":"<p>Compute the SimSIAM loss.</p> Source code in <code>quadra/losses/ssl/simsiam.py</code> <pre><code>def forward(self, p1: torch.Tensor, p2: torch.Tensor, z1: torch.Tensor, z2: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"Compute the SimSIAM loss.\"\"\"\n    return simsiam_loss(p1, p2, z1, z2)\n</code></pre>"},{"location":"reference/quadra/losses/ssl/index.html#quadra.losses.ssl.VICRegLoss","title":"<code>VICRegLoss(lambd, mu, nu=1, gamma=1)</code>","text":"<p>             Bases: <code>Module</code></p> <p>VIC regression loss module.</p> <p>Parameters:</p> <ul> <li> lambd             (<code>float</code>)         \u2013          <p>lambda multiplier for redundancy term.</p> </li> <li> mu             (<code>float</code>)         \u2013          <p>mu multiplier for similarity term.</p> </li> <li> nu             (<code>float</code>, default:                 <code>1</code> )         \u2013          <p>nu multiplier for variance term. Default: 1.</p> </li> <li> gamma             (<code>float</code>, default:                 <code>1</code> )         \u2013          <p>gamma multiplier for covariance term. Default: 1.</p> </li> </ul> Source code in <code>quadra/losses/ssl/vicreg.py</code> <pre><code>def __init__(\n    self,\n    lambd: float,\n    mu: float,\n    nu: float = 1,\n    gamma: float = 1,\n):\n    super().__init__()\n    self.lambd = lambd\n    self.mu = mu\n    self.nu = nu\n    self.gamma = gamma\n</code></pre>"},{"location":"reference/quadra/losses/ssl/index.html#quadra.losses.ssl.VICRegLoss.forward","title":"<code>forward(z1, z2)</code>","text":"<p>Computes VICReg loss.</p> Source code in <code>quadra/losses/ssl/vicreg.py</code> <pre><code>def forward(self, z1: torch.Tensor, z2: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"Computes VICReg loss.\"\"\"\n    return vicreg_loss(z1, z2, self.lambd, self.mu, self.nu, self.gamma)\n</code></pre>"},{"location":"reference/quadra/losses/ssl/barlowtwins.html","title":"barlowtwins","text":""},{"location":"reference/quadra/losses/ssl/barlowtwins.html#quadra.losses.ssl.barlowtwins.BarlowTwinsLoss","title":"<code>BarlowTwinsLoss(lambd)</code>","text":"<p>             Bases: <code>Module</code></p> <p>BarlowTwin loss.</p> <p>Parameters:</p> <ul> <li> lambd             (<code>float</code>)         \u2013          <p>lambda of the loss.</p> </li> </ul> Source code in <code>quadra/losses/ssl/barlowtwins.py</code> <pre><code>def __init__(self, lambd: float):\n    super().__init__()\n    self.lambd = lambd\n</code></pre>"},{"location":"reference/quadra/losses/ssl/barlowtwins.html#quadra.losses.ssl.barlowtwins.BarlowTwinsLoss.forward","title":"<code>forward(z1, z2)</code>","text":"<p>Compute the BarlowTwins loss.</p> Source code in <code>quadra/losses/ssl/barlowtwins.py</code> <pre><code>def forward(self, z1: torch.Tensor, z2: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"Compute the BarlowTwins loss.\"\"\"\n    return barlowtwins_loss(z1, z2, self.lambd)\n</code></pre>"},{"location":"reference/quadra/losses/ssl/barlowtwins.html#quadra.losses.ssl.barlowtwins.barlowtwins_loss","title":"<code>barlowtwins_loss(z1, z2, lambd)</code>","text":"<p>BarlowTwins loss described in https://arxiv.org/abs/2103.03230.</p> <p>Parameters:</p> <ul> <li> z1             (<code>Tensor</code>)         \u2013          <p>First <code>augmented</code> normalized features (i.e. f(T(x))). The normalization can be obtained with z1_norm = (z1 - z1.mean(0)) / z1.std(0)</p> </li> <li> z2             (<code>Tensor</code>)         \u2013          <p>Second <code>augmented</code> normalized features (i.e. f(T(x))). The normalization can be obtained with z2_norm = (z2 - z2.mean(0)) / z2.std(0)</p> </li> <li> lambd             (<code>float</code>)         \u2013          <p>lambda multiplier for redundancy term.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>         \u2013          <p>BarlowTwins loss</p> </li> </ul> Source code in <code>quadra/losses/ssl/barlowtwins.py</code> <pre><code>def barlowtwins_loss(\n    z1: torch.Tensor,\n    z2: torch.Tensor,\n    lambd: float,\n) -&gt; torch.Tensor:\n\"\"\"BarlowTwins loss described in https://arxiv.org/abs/2103.03230.\n\n    Args:\n        z1: First `augmented` normalized features (i.e. f(T(x))).\n            The normalization can be obtained with\n            z1_norm = (z1 - z1.mean(0)) / z1.std(0)\n        z2: Second `augmented` normalized features (i.e. f(T(x))).\n            The normalization can be obtained with\n            z2_norm = (z2 - z2.mean(0)) / z2.std(0)\n        lambd: lambda multiplier for redundancy term.\n\n    Returns:\n        BarlowTwins loss\n    \"\"\"\n    z1 = (z1 - z1.mean(0)) / z1.std(0)\n    z1 = (z2 - z2.mean(0)) / z2.std(0)\n    cov = z1.T @ z2\n    cov.div_(z1.size(0))\n    n = cov.size(0)\n    invariance_term = torch.diagonal(cov).add_(-1).pow_(2).sum()\n    off_diag = cov.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()\n    redundancy_term = off_diag.pow_(2).sum()\n    return invariance_term + lambd * redundancy_term\n</code></pre>"},{"location":"reference/quadra/losses/ssl/byol.html","title":"byol","text":""},{"location":"reference/quadra/losses/ssl/byol.html#quadra.losses.ssl.byol.BYOLRegressionLoss","title":"<code>BYOLRegressionLoss</code>","text":"<p>             Bases: <code>Module</code></p> <p>BYOL regression loss module.</p>"},{"location":"reference/quadra/losses/ssl/byol.html#quadra.losses.ssl.byol.BYOLRegressionLoss.forward","title":"<code>forward(x, y)</code>","text":"<p>Compute the BYOL regression loss.</p> <p>Parameters:</p> <ul> <li> x             (<code>Tensor</code>)         \u2013          <p>First Tensor</p> </li> <li> y             (<code>Tensor</code>)         \u2013          <p>Second Tensor</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>         \u2013          <p>BYOL regression loss</p> </li> </ul> Source code in <code>quadra/losses/ssl/byol.py</code> <pre><code>def forward(\n    self,\n    x: torch.Tensor,\n    y: torch.Tensor,\n) -&gt; torch.Tensor:\n\"\"\"Compute the BYOL regression loss.\n\n    Args:\n        x: First Tensor\n        y: Second Tensor\n\n    Returns:\n        BYOL regression loss\n    \"\"\"\n    return byol_regression_loss(x, y)\n</code></pre>"},{"location":"reference/quadra/losses/ssl/byol.html#quadra.losses.ssl.byol.byol_regression_loss","title":"<code>byol_regression_loss(x, y)</code>","text":"<p>Byol regression loss Args:     x: tensor     y: tensor.</p> <p>Returns:</p> <ul> <li> <code>Tensor</code>         \u2013          <p>tensor</p> </li> </ul> Source code in <code>quadra/losses/ssl/byol.py</code> <pre><code>def byol_regression_loss(x: torch.Tensor, y: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"Byol regression loss\n    Args:\n        x: tensor\n        y: tensor.\n\n    Returns:\n        tensor\n    \"\"\"\n    x = F.normalize(x, dim=-1)\n    y = F.normalize(y, dim=-1)\n    return 2 - 2 * (x * y).sum(dim=1).mean()\n</code></pre>"},{"location":"reference/quadra/losses/ssl/dino.html","title":"dino","text":""},{"location":"reference/quadra/losses/ssl/dino.html#quadra.losses.ssl.dino.DinoDistillationLoss","title":"<code>DinoDistillationLoss(output_dim, max_epochs, warmup_teacher_temp=0.04, teacher_temp=0.07, warmup_teacher_temp_epochs=30, student_temp=0.1, center_momentum=0.9)</code>","text":"<p>             Bases: <code>Module</code></p> <p>Dino distillation loss module.</p> <p>Parameters:</p> <ul> <li> output_dim             (<code>int</code>)         \u2013          <p>output dim.</p> </li> <li> max_epochs             (<code>int</code>)         \u2013          <p>max epochs.</p> </li> <li> warmup_teacher_temp             (<code>float</code>, default:                 <code>0.04</code> )         \u2013          <p>warmup temperature.</p> </li> <li> teacher_temp             (<code>float</code>, default:                 <code>0.07</code> )         \u2013          <p>teacher temperature.</p> </li> <li> warmup_teacher_temp_epochs             (<code>int</code>, default:                 <code>30</code> )         \u2013          <p>warmup teacher epocs.</p> </li> <li> student_temp             (<code>float</code>, default:                 <code>0.1</code> )         \u2013          <p>student temperature.</p> </li> <li> center_momentum             (<code>float</code>, default:                 <code>0.9</code> )         \u2013          <p>center momentum.</p> </li> </ul> Source code in <code>quadra/losses/ssl/dino.py</code> <pre><code>def __init__(\n    self,\n    output_dim: int,\n    max_epochs: int,\n    warmup_teacher_temp: float = 0.04,\n    teacher_temp: float = 0.07,\n    warmup_teacher_temp_epochs: int = 30,\n    student_temp: float = 0.1,\n    center_momentum: float = 0.9,\n):\n    super().__init__()\n    self.student_temp = student_temp\n    self.center_momentum = center_momentum\n    self.center: torch.Tensor\n    # we apply a warm up for the teacher temperature because\n    # a too high temperature makes the training instable at the beginning\n\n    if warmup_teacher_temp_epochs &gt;= max_epochs:\n        raise ValueError(\n            f\"Number of warmup epochs ({warmup_teacher_temp_epochs}) must be smaller than max_epochs ({max_epochs})\"\n        )\n\n    if warmup_teacher_temp_epochs &lt; 30:\n        log.warning(\"Warmup teacher epochs is very small (&lt; 30). This may cause instabilities in the training\")\n\n    self.teacher_temp_schedule = np.concatenate(\n        (\n            np.linspace(warmup_teacher_temp, teacher_temp, warmup_teacher_temp_epochs),\n            np.ones(max_epochs - warmup_teacher_temp_epochs) * teacher_temp,\n        )\n    )\n    self.register_buffer(\"center\", torch.zeros(1, output_dim))\n</code></pre>"},{"location":"reference/quadra/losses/ssl/dino.html#quadra.losses.ssl.dino.DinoDistillationLoss.forward","title":"<code>forward(current_epoch, student_output, teacher_output)</code>","text":"<p>Runs forward.</p> Source code in <code>quadra/losses/ssl/dino.py</code> <pre><code>def forward(\n    self,\n    current_epoch: int,\n    student_output: torch.Tensor,\n    teacher_output: torch.Tensor,\n) -&gt; torch.Tensor:\n\"\"\"Runs forward.\"\"\"\n    teacher_temp = self.teacher_temp_schedule[current_epoch]\n    loss = dino_distillation_loss(\n        student_output,\n        teacher_output,\n        center_vector=self.center,\n        teacher_temp=teacher_temp,\n        student_temp=self.student_temp,\n    )\n\n    self.update_center(teacher_output)\n    return loss\n</code></pre>"},{"location":"reference/quadra/losses/ssl/dino.html#quadra.losses.ssl.dino.DinoDistillationLoss.update_center","title":"<code>update_center(teacher_output)</code>","text":"<p>Update center of the distribution of the teacher Args:     teacher_output: teacher output.</p> <p>Returns:</p> <ul> <li> <code>None</code>         \u2013          <p>None</p> </li> </ul> Source code in <code>quadra/losses/ssl/dino.py</code> <pre><code>@torch.no_grad()\ndef update_center(self, teacher_output: torch.Tensor) -&gt; None:\n\"\"\"Update center of the distribution of the teacher\n    Args:\n        teacher_output: teacher output.\n\n    Returns:\n        None\n    \"\"\"\n    # TODO: check if this is correct\n    # torch.cat expects a list of tensors but teacher_output is a tensor\n    batch_center = torch.cat(teacher_output).mean(dim=0, keepdim=True)  # type: ignore[call-overload]\n    self.center = self.center * self.center_momentum + batch_center * (1 - self.center_momentum)\n</code></pre>"},{"location":"reference/quadra/losses/ssl/dino.html#quadra.losses.ssl.dino.dino_distillation_loss","title":"<code>dino_distillation_loss(student_output, teacher_output, center_vector, teacher_temp=0.04, student_temp=0.1)</code>","text":"<p>Compute the DINO distillation loss.</p> <p>Parameters:</p> <ul> <li> student_output             (<code>Tensor</code>)         \u2013          <p>tensor of the student output</p> </li> <li> teacher_output             (<code>Tensor</code>)         \u2013          <p>tensor of the teacher output</p> </li> <li> center_vector             (<code>Tensor</code>)         \u2013          <p>center vector of distribution</p> </li> <li> teacher_temp             (<code>float</code>, default:                 <code>0.04</code> )         \u2013          <p>temperature teacher</p> </li> <li> student_temp             (<code>float</code>, default:                 <code>0.1</code> )         \u2013          <p>temperature student.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>         \u2013          <p>The computed loss</p> </li> </ul> Source code in <code>quadra/losses/ssl/dino.py</code> <pre><code>def dino_distillation_loss(\n    student_output: torch.Tensor,\n    teacher_output: torch.Tensor,\n    center_vector: torch.Tensor,\n    teacher_temp: float = 0.04,\n    student_temp: float = 0.1,\n) -&gt; torch.Tensor:\n\"\"\"Compute the DINO distillation loss.\n\n    Args:\n        student_output: tensor of the student output\n        teacher_output: tensor of the teacher output\n        center_vector: center vector of distribution\n        teacher_temp: temperature teacher\n        student_temp: temperature student.\n\n    Returns:\n        The computed loss\n    \"\"\"\n    student_temp = [s / student_temp for s in student_output]\n    teacher_temp = [(t - center_vector) / teacher_temp for t in teacher_output]\n\n    student_sm = [F.log_softmax(s, dim=-1) for s in student_temp]\n    teacher_sm = [F.softmax(t, dim=-1).detach() for t in teacher_temp]\n\n    total_loss = torch.tensor(0.0, device=student_output[0].device)\n    n_loss_terms = torch.tensor(0.0, device=student_output[0].device)\n\n    for t_ix, t in enumerate(teacher_sm):\n        for s_ix, s in enumerate(student_sm):\n            if t_ix == s_ix:\n                continue\n\n            loss = torch.sum(-t * s, dim=-1)  # (n_samples,)\n            total_loss += loss.mean()  # scalar\n            n_loss_terms += 1\n\n    total_loss /= n_loss_terms\n    return total_loss\n</code></pre>"},{"location":"reference/quadra/losses/ssl/hyperspherical.html","title":"hyperspherical","text":""},{"location":"reference/quadra/losses/ssl/hyperspherical.html#quadra.losses.ssl.hyperspherical.align_loss","title":"<code>align_loss(x, y, alpha=2)</code>","text":"<p>Mean(l2^alpha).</p> <p>Parameters:</p> <ul> <li> x             (<code>Tensor</code>)         \u2013          <p>feature n1</p> </li> <li> y             (<code>Tensor</code>)         \u2013          <p>feature n2</p> </li> <li> alpha             (<code>int</code>, default:                 <code>2</code> )         \u2013          <p>pow of the norm loss.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>         \u2013          <p>Align loss</p> </li> </ul> Source code in <code>quadra/losses/ssl/hyperspherical.py</code> <pre><code>def align_loss(x: torch.Tensor, y: torch.Tensor, alpha: int = 2) -&gt; torch.Tensor:\n\"\"\"Mean(l2^alpha).\n\n    Args:\n        x: feature n1\n        y: feature n2\n        alpha: pow of the norm loss.\n\n    Returns:\n        Align loss\n    \"\"\"\n    norm = torch.norm(x - y, p=2, dim=1)\n    return torch.mean(torch.pow(norm, alpha))\n</code></pre>"},{"location":"reference/quadra/losses/ssl/hyperspherical.html#quadra.losses.ssl.hyperspherical.cosine_align_loss","title":"<code>cosine_align_loss(x, y)</code>","text":"<p>Computes mean of cosine distance based on similarity mean(1 - cosine_similarity).</p> <p>Parameters:</p> <ul> <li> x             (<code>Tensor</code>)         \u2013          <p>feature n1</p> </li> <li> y             (<code>Tensor</code>)         \u2013          <p>feature n2.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>         \u2013          <p>cosine align loss</p> </li> </ul> Source code in <code>quadra/losses/ssl/hyperspherical.py</code> <pre><code>def cosine_align_loss(x: torch.Tensor, y: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"Computes mean of cosine distance based on similarity mean(1 - cosine_similarity).\n\n    Args:\n        x: feature n1\n        y: feature n2.\n\n    Returns:\n        cosine align loss\n    \"\"\"\n    cos = 1 - cosine_similarity(x, y, dim=1)\n    return torch.mean(cos)\n</code></pre>"},{"location":"reference/quadra/losses/ssl/hyperspherical.html#quadra.losses.ssl.hyperspherical.uniform_loss","title":"<code>uniform_loss(x, t=2.0)</code>","text":"<p>log(mean(exp(-t*dist_p2))).</p> <p>Parameters:</p> <ul> <li> x             (<code>Tensor</code>)         \u2013          <p>feature tensor</p> </li> <li> t             (<code>float</code>, default:                 <code>2.0</code> )         \u2013          <p>temperature of the dist_p2.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>         \u2013          <p>Uniform loss</p> </li> </ul> Source code in <code>quadra/losses/ssl/hyperspherical.py</code> <pre><code>def uniform_loss(x: torch.Tensor, t: float = 2.0) -&gt; torch.Tensor:\n\"\"\"log(mean(exp(-t*dist_p2))).\n\n    Args:\n        x: feature tensor\n        t: temperature of the dist_p2.\n\n    Returns:\n        Uniform loss\n    \"\"\"\n    return torch.log(torch.mean(torch.exp(torch.pow(torch.pdist(x, p=2), 2) * -t)))\n</code></pre>"},{"location":"reference/quadra/losses/ssl/idmm.html","title":"idmm","text":""},{"location":"reference/quadra/losses/ssl/idmm.html#quadra.losses.ssl.idmm.IDMMLoss","title":"<code>IDMMLoss(smoothing=0.1)</code>","text":"<p>             Bases: <code>Module</code></p> <p>IDMM loss described in https://arxiv.org/abs/2201.10728.</p> Source code in <code>quadra/losses/ssl/idmm.py</code> <pre><code>def __init__(self, smoothing: float = 0.1):\n    super().__init__()\n    self.smoothing = smoothing\n</code></pre>"},{"location":"reference/quadra/losses/ssl/idmm.html#quadra.losses.ssl.idmm.IDMMLoss.forward","title":"<code>forward(p1, y1)</code>","text":"<p>IDMM loss described in https://arxiv.org/abs/2201.10728.</p> <p>Parameters:</p> <ul> <li> p1             (<code>Tensor</code>)         \u2013          <p>Prediction labels for <code>z1</code></p> </li> <li> y1             (<code>Tensor</code>)         \u2013          <p>Instance labels for <code>z1</code></p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>         \u2013          <p>IDMM loss</p> </li> </ul> Source code in <code>quadra/losses/ssl/idmm.py</code> <pre><code>def forward(\n    self,\n    p1: torch.Tensor,\n    y1: torch.Tensor,\n) -&gt; torch.Tensor:\n\"\"\"IDMM loss described in https://arxiv.org/abs/2201.10728.\n\n    Args:\n        p1: Prediction labels for `z1`\n        y1: Instance labels for `z1`\n\n    Returns:\n        IDMM loss\n    \"\"\"\n    return idmm_loss(\n        p1,\n        y1,\n        self.smoothing,\n    )\n</code></pre>"},{"location":"reference/quadra/losses/ssl/idmm.html#quadra.losses.ssl.idmm.idmm_loss","title":"<code>idmm_loss(p1, y1, smoothing=0.1)</code>","text":"<p>IDMM loss described in https://arxiv.org/abs/2201.10728.</p> <p>Parameters:</p> <ul> <li> p1             (<code>Tensor</code>)         \u2013          <p>Prediction labels for <code>z1</code></p> </li> <li> y1             (<code>Tensor</code>)         \u2013          <p>Instance labels for <code>z1</code></p> </li> <li> smoothing             (<code>float</code>, default:                 <code>0.1</code> )         \u2013          <p>smoothing factor used for label smoothing. Defaults to 0.1.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>         \u2013          <p>IDMM loss</p> </li> </ul> Source code in <code>quadra/losses/ssl/idmm.py</code> <pre><code>def idmm_loss(\n    p1: torch.Tensor,\n    y1: torch.Tensor,\n    smoothing: float = 0.1,\n) -&gt; torch.Tensor:\n\"\"\"IDMM loss described in https://arxiv.org/abs/2201.10728.\n\n    Args:\n        p1: Prediction labels for `z1`\n        y1: Instance labels for `z1`\n        smoothing: smoothing factor used for label smoothing.\n            Defaults to 0.1.\n\n    Returns:\n        IDMM loss\n    \"\"\"\n    loss = F.cross_entropy(p1, y1, label_smoothing=smoothing)\n    return loss\n</code></pre>"},{"location":"reference/quadra/losses/ssl/simclr.html","title":"simclr","text":""},{"location":"reference/quadra/losses/ssl/simclr.html#quadra.losses.ssl.simclr.SimCLRLoss","title":"<code>SimCLRLoss(temperature=1.0)</code>","text":"<p>             Bases: <code>Module</code></p> <p>SIMCLRloss module.</p> <p>Parameters:</p> <ul> <li> temperature             (<code>float</code>, default:                 <code>1.0</code> )         \u2013          <p>temperature of SIM loss.</p> </li> </ul> Source code in <code>quadra/losses/ssl/simclr.py</code> <pre><code>def __init__(self, temperature: float = 1.0):\n    super().__init__()\n    self.temperature = temperature\n</code></pre>"},{"location":"reference/quadra/losses/ssl/simclr.html#quadra.losses.ssl.simclr.SimCLRLoss.forward","title":"<code>forward(x1, x2)</code>","text":"<p>Forward pass of the loss.</p> Source code in <code>quadra/losses/ssl/simclr.py</code> <pre><code>def forward(self, x1: torch.Tensor, x2: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"Forward pass of the loss.\"\"\"\n    return simclr_loss(x1, x2, temperature=self.temperature)\n</code></pre>"},{"location":"reference/quadra/losses/ssl/simclr.html#quadra.losses.ssl.simclr.simclr_loss","title":"<code>simclr_loss(features1, features2, temperature=1.0)</code>","text":"<p>SimCLR loss described in https://arxiv.org/pdf/2002.05709.pdf.</p> <p>Parameters:</p> <ul> <li> temperature             (<code>float</code>, default:                 <code>1.0</code> )         \u2013          <p>optional temperature</p> </li> <li> features1             (<code>Tensor</code>)         \u2013          <p>First augmented features (i.e. T(features))</p> </li> <li> features2             (<code>Tensor</code>)         \u2013          <p>Second augmented features (i.e. T'(features))</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>         \u2013          <p>SimCLR loss</p> </li> </ul> Source code in <code>quadra/losses/ssl/simclr.py</code> <pre><code>def simclr_loss(\n    features1: torch.Tensor,\n    features2: torch.Tensor,\n    temperature: float = 1.0,\n) -&gt; torch.Tensor:\n\"\"\"SimCLR loss described in https://arxiv.org/pdf/2002.05709.pdf.\n\n    Args:\n        temperature: optional temperature\n        features1: First augmented features (i.e. T(features))\n        features2: Second augmented features (i.e. T'(features))\n\n    Returns:\n        SimCLR loss\n    \"\"\"\n    features1 = F.normalize(features1, dim=-1)\n    features2 = F.normalize(features2, dim=-1)\n    if torch.distributed.is_available() and torch.distributed.is_initialized():\n        features1_dist = AllGatherSyncFunction.apply(features1)\n        features2_dist = AllGatherSyncFunction.apply(features2)\n    else:\n        features1_dist = features1\n        features2_dist = features2\n    features = torch.cat([features1, features2], dim=0)  # [2B, d]\n    features_dist = torch.cat([features1_dist, features2_dist], dim=0)  # [2B * DIST_SIZE, d]\n\n    # Similarity matrix\n    sim = torch.exp(torch.div(torch.mm(features, features_dist.t()), temperature))  # [2B, 2B * DIST_SIZE]\n\n    # Negatives\n    neg = sim.sum(dim=-1)\n\n    # From each row, subtract e^(1/temp) to remove similarity measure for zi * zi, since\n    # (zi^T * zi) / ||zi||^2 = 1\n    row_sub = torch.full_like(neg, math.e ** (1 / temperature), device=neg.device)\n    neg = torch.clamp(neg - row_sub, min=1e-6)  # clamp for numerical stability\n\n    # Positive similarity, pos becomes [2 * batch_size]\n    pos = torch.exp(torch.div(torch.sum(features1 * features2, dim=-1), temperature))\n    pos = torch.cat([pos, pos], dim=0)\n\n    loss = -torch.log(pos / (neg + 1e-6)).mean()\n    return loss\n</code></pre>"},{"location":"reference/quadra/losses/ssl/simsiam.html","title":"simsiam","text":""},{"location":"reference/quadra/losses/ssl/simsiam.html#quadra.losses.ssl.simsiam.SimSIAMLoss","title":"<code>SimSIAMLoss</code>","text":"<p>             Bases: <code>Module</code></p> <p>SimSIAM loss module.</p>"},{"location":"reference/quadra/losses/ssl/simsiam.html#quadra.losses.ssl.simsiam.SimSIAMLoss.forward","title":"<code>forward(p1, p2, z1, z2)</code>","text":"<p>Compute the SimSIAM loss.</p> Source code in <code>quadra/losses/ssl/simsiam.py</code> <pre><code>def forward(self, p1: torch.Tensor, p2: torch.Tensor, z1: torch.Tensor, z2: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"Compute the SimSIAM loss.\"\"\"\n    return simsiam_loss(p1, p2, z1, z2)\n</code></pre>"},{"location":"reference/quadra/losses/ssl/simsiam.html#quadra.losses.ssl.simsiam.simsiam_loss","title":"<code>simsiam_loss(p1, p2, z1, z2)</code>","text":"<p>SimSIAM loss described in https://arxiv.org/abs/2011.10566.</p> <p>Parameters:</p> <ul> <li> p1             (<code>Tensor</code>)         \u2013          <p>First <code>predicted</code> features (i.e. h(f(T(x1))))</p> </li> <li> p2             (<code>Tensor</code>)         \u2013          <p>Second <code>predicted</code> features (i.e. h(f(T'(x2))))</p> </li> <li> z1             (<code>Tensor</code>)         \u2013          <p>First 'projected features (i.e. f(T(x1)))</p> </li> <li> z2             (<code>Tensor</code>)         \u2013          <p>Second 'projected features (i.e. f(T(x2)))</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>         \u2013          <p>SimSIAM loss</p> </li> </ul> Source code in <code>quadra/losses/ssl/simsiam.py</code> <pre><code>def simsiam_loss(\n    p1: torch.Tensor,\n    p2: torch.Tensor,\n    z1: torch.Tensor,\n    z2: torch.Tensor,\n) -&gt; torch.Tensor:\n\"\"\"SimSIAM loss described in https://arxiv.org/abs/2011.10566.\n\n    Args:\n        p1: First `predicted` features (i.e. h(f(T(x1))))\n        p2: Second `predicted` features (i.e. h(f(T'(x2))))\n        z1: First 'projected features (i.e. f(T(x1)))\n        z2: Second 'projected features (i.e. f(T(x2)))\n\n    Returns:\n        SimSIAM loss\n    \"\"\"\n    return -(F.cosine_similarity(p1, z2).mean() + F.cosine_similarity(p2, z1).mean()) * 0.5\n</code></pre>"},{"location":"reference/quadra/losses/ssl/vicreg.html","title":"vicreg","text":""},{"location":"reference/quadra/losses/ssl/vicreg.html#quadra.losses.ssl.vicreg.VICRegLoss","title":"<code>VICRegLoss(lambd, mu, nu=1, gamma=1)</code>","text":"<p>             Bases: <code>Module</code></p> <p>VIC regression loss module.</p> <p>Parameters:</p> <ul> <li> lambd             (<code>float</code>)         \u2013          <p>lambda multiplier for redundancy term.</p> </li> <li> mu             (<code>float</code>)         \u2013          <p>mu multiplier for similarity term.</p> </li> <li> nu             (<code>float</code>, default:                 <code>1</code> )         \u2013          <p>nu multiplier for variance term. Default: 1.</p> </li> <li> gamma             (<code>float</code>, default:                 <code>1</code> )         \u2013          <p>gamma multiplier for covariance term. Default: 1.</p> </li> </ul> Source code in <code>quadra/losses/ssl/vicreg.py</code> <pre><code>def __init__(\n    self,\n    lambd: float,\n    mu: float,\n    nu: float = 1,\n    gamma: float = 1,\n):\n    super().__init__()\n    self.lambd = lambd\n    self.mu = mu\n    self.nu = nu\n    self.gamma = gamma\n</code></pre>"},{"location":"reference/quadra/losses/ssl/vicreg.html#quadra.losses.ssl.vicreg.VICRegLoss.forward","title":"<code>forward(z1, z2)</code>","text":"<p>Computes VICReg loss.</p> Source code in <code>quadra/losses/ssl/vicreg.py</code> <pre><code>def forward(self, z1: torch.Tensor, z2: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"Computes VICReg loss.\"\"\"\n    return vicreg_loss(z1, z2, self.lambd, self.mu, self.nu, self.gamma)\n</code></pre>"},{"location":"reference/quadra/losses/ssl/vicreg.html#quadra.losses.ssl.vicreg.vicreg_loss","title":"<code>vicreg_loss(z1, z2, lambd, mu, nu=1, gamma=1)</code>","text":"<p>VICReg loss described in https://arxiv.org/abs/2105.04906.</p> <p>Parameters:</p> <ul> <li> z1             (<code>Tensor</code>)         \u2013          <p>First <code>augmented</code> normalized features (i.e. f(T(x))). The normalization can be obtained with z1_norm = (z1 - z1.mean(0)) / z1.std(0)</p> </li> <li> z2             (<code>Tensor</code>)         \u2013          <p>Second <code>augmented</code> normalized features (i.e. f(T(x))). The normalization can be obtained with z2_norm = (z2 - z2.mean(0)) / z2.std(0)</p> </li> <li> lambd             (<code>float</code>)         \u2013          <p>lambda multiplier for redundancy term.</p> </li> <li> mu             (<code>float</code>)         \u2013          <p>mu multiplier for similarity term.</p> </li> <li> nu             (<code>float</code>, default:                 <code>1</code> )         \u2013          <p>nu multiplier for variance term. Default: 1</p> </li> <li> gamma             (<code>float</code>, default:                 <code>1</code> )         \u2013          <p>gamma multiplier for covariance term. Default: 1</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>         \u2013          <p>VICReg loss</p> </li> </ul> Source code in <code>quadra/losses/ssl/vicreg.py</code> <pre><code>def vicreg_loss(\n    z1: torch.Tensor,\n    z2: torch.Tensor,\n    lambd: float,\n    mu: float,\n    nu: float = 1,\n    gamma: float = 1,\n) -&gt; torch.Tensor:\n\"\"\"VICReg loss described in https://arxiv.org/abs/2105.04906.\n\n    Args:\n        z1: First `augmented` normalized features (i.e. f(T(x))). The normalization can be obtained with\n            z1_norm = (z1 - z1.mean(0)) / z1.std(0)\n        z2: Second `augmented` normalized features (i.e. f(T(x))). The normalization can be obtained with\n            z2_norm = (z2 - z2.mean(0)) / z2.std(0)\n        lambd: lambda multiplier for redundancy term.\n        mu: mu multiplier for similarity term.\n        nu: nu multiplier for variance term. Default: 1\n        gamma: gamma multiplier for covariance term. Default: 1\n\n    Returns:\n        VICReg loss\n    \"\"\"\n    # Variance loss\n    std_z1 = torch.sqrt(z1.var(dim=0) + 0.0001)\n    std_z2 = torch.sqrt(z2.var(dim=0) + 0.0001)\n    v_z1 = torch.nn.functional.relu(gamma - std_z1).mean()\n    v_z2 = torch.nn.functional.relu(gamma - std_z2).mean()\n    var_loss = v_z1 + v_z2\n\n    # Similarity loss\n    sim_loss = torch.nn.functional.mse_loss(z1, z2)\n\n    # Covariance loss\n    n = z1.size(0)\n    d = z1.size(1)\n    z1 = z1 - z1.mean(dim=0)\n    z2 = z2 - z2.mean(dim=0)\n    cov_z1 = (z1.T @ z1) / (n - 1)\n    cov_z2 = (z2.T @ z2) / (n - 1)\n    off_diagonal_cov_z1 = cov_z1.flatten()[:-1].view(d - 1, d + 1)[:, 1:].flatten()\n    off_diagonal_cov_z2 = cov_z2.flatten()[:-1].view(d - 1, d + 1)[:, 1:].flatten()\n    cov_loss = off_diagonal_cov_z1.pow_(2).sum() / d + off_diagonal_cov_z2.pow_(2).sum() / d\n\n    return lambd * sim_loss + mu * var_loss + nu * cov_loss\n</code></pre>"},{"location":"reference/quadra/metrics/index.html","title":"metrics","text":""},{"location":"reference/quadra/metrics/index.html#quadra.metrics.segmentation_props","title":"<code>segmentation_props(pred, mask)</code>","text":"<p>Return some information regarding a segmentation task.</p> <p>Parameters:</p> <ul> <li> pred             (<code>ndarray[bool]</code>)         \u2013          <p>Prediction of a segmentation model as a binary image.</p> </li> <li> mask             (<code>ndarray[bool]</code>)         \u2013          <p>Ground truth mask as binary image</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float</code>         \u2013          <p>1-Dice(pred, mask) Given a matrix (a_ij) = (1-Dice)(prediction_i, ground_truth_j), where prediction_i is the i-th prediction connected component and ground_truth_j is the j-th ground truth connected component, I compute the LSA (Linear Sum Assignment) to find the optimal 1-to-1 assignment between predictions and ground truths that minimize the (1-Dice) score. Then, for every unique pair of (predictioni, ground_truthj) we compute Average  (1-Dice)(predictioni, ground_truthj)</p> </li> <li> <code>float</code>         \u2013          <p>Average (1-Dice)(predictioni, ground_truthj) between True Positives (that is predictions associated to a ground truth), which is gratis from a[i,j], where the average is computed w.r.t. the total number of True Positives found.</p> </li> <li> <code>float</code>         \u2013          <p>Average IoU(predictioni, ground_truthj) between True Positives (that is predictions associated to a ground truth), where the average is computed w.r.t. the total number of True Positives found. The IoU is computed between the minimum enclosing bounding box of a prediction and a ground truth.</p> </li> <li> <code>float</code>         \u2013          <p>Average area of False Positives</p> </li> <li> <code>list[float]</code>         \u2013          <p>Histogram of false positives</p> </li> <li> <code>float</code>         \u2013          <p>Average area of False Negatives</p> </li> <li> <code>int</code>         \u2013          <p>Number of True Positives (predictions associated to a ground truth)</p> </li> <li> <code>int</code>         \u2013          <p>Number of False Positives (predictions without a ground truth associated)     and their avg. area (avg is taken w.r.t. the total number of False Positives found)</p> </li> <li> <code>int</code>         \u2013          <p>Number of False Negatives (ground truth without a predictions associated) and their avg. area (avg is taken w.r.t. the total number of False Negatives found)</p> </li> <li> <code>int</code>         \u2013          <p>Number of labels in the mask.</p> </li> </ul> Source code in <code>quadra/metrics/segmentation.py</code> <pre><code>def segmentation_props(\n    pred: np.ndarray, mask: np.ndarray\n) -&gt; tuple[float, float, float, float, list[float], float, int, int, int, int]:\n\"\"\"Return some information regarding a segmentation task.\n\n    Args:\n        pred (np.ndarray[bool]): Prediction of a segmentation model as\n            a binary image.\n        mask (np.ndarray[bool]): Ground truth mask as binary image\n\n    Returns:\n        1-Dice(pred, mask) Given a matrix (a_ij) = (1-Dice)(prediction_i, ground_truth_j),\n            where prediction_i is the i-th prediction connected component and\n            ground_truth_j is the j-th ground truth connected component,\n            I compute the LSA (Linear Sum Assignment) to find the optimal 1-to-1 assignment\n            between predictions and ground truths that minimize the (1-Dice) score.\n            Then, for every unique pair of (predictioni, ground_truthj) we compute Average\n             (1-Dice)(predictioni, ground_truthj)\n        Average (1-Dice)(predictioni, ground_truthj) between True Positives\n            (that is predictions associated to a ground truth), which is gratis from a[i,j],\n            where the average is computed w.r.t. the total number of True Positives found.\n        Average IoU(predictioni, ground_truthj) between True Positives\n            (that is predictions associated to a ground truth),\n            where the average is computed w.r.t. the total number of True Positives found.\n            The IoU is computed between the minimum enclosing bounding box\n            of a prediction and a ground truth.\n        Average area of False Positives\n        Histogram of false positives\n        Average area of False Negatives\n        Number of True Positives (predictions associated to a ground truth)\n        Number of False Positives (predictions without a ground truth associated)\n                and their avg. area (avg is taken w.r.t. the total number of False Positives found)\n        Number of False Negatives (ground truth without a predictions associated)\n            and their avg. area (avg is taken w.r.t. the total number of False Negatives found)\n        Number of labels in the mask.\n    \"\"\"\n    labels_pred, n_labels_pred = label(pred, connectivity=2, return_num=True, background=0)\n    labels_mask, n_labels_mask = label(mask, connectivity=2, return_num=True, background=0)\n\n    labels_pred = cast(np.ndarray, labels_pred)\n    labels_mask = cast(np.ndarray, labels_mask)\n    n_labels_pred = cast(int, n_labels_pred)\n    n_labels_mask = cast(int, n_labels_mask)\n\n    props_pred = regionprops(labels_pred)\n    props_mask = regionprops(labels_mask)\n    pred_bbox = np.array([props_pred[i].bbox for i in range(len(props_pred))])\n    mask_bbox = np.array([props_mask[i].bbox for i in range(len(props_mask))])\n\n    global_dice = float(\n        dice(\n            torch.Tensor(pred).unsqueeze(0).unsqueeze(0),\n            torch.Tensor(mask).unsqueeze(0).unsqueeze(0),\n        ).item()\n    )\n    lsa_iou = 0.0\n    lsa_dice = 0.0\n    tp_num = 0\n    fp_num = 0\n    fn_num = 0\n    fp_area = 0.0\n    fn_area = 0.0\n    fp_hist: list[float] = []\n    if n_labels_pred &gt; 0 and n_labels_mask &gt; 0:\n        dice_mat = _get_dice_matrix(labels_pred, n_labels_pred, labels_mask, n_labels_mask)\n        # Thresholding over Dice scores\n        dice_mat = np.where(dice_mat &lt;= 0.9, dice_mat, 1.0)\n        iou_mat = _get_iou(pred_bbox, mask_bbox, approx_iou=False)\n        dice_mat_shape = dice_mat.shape\n        max_dim = np.max(dice_mat_shape)\n        # Add dummy Dices so LSA is unique and i can compute FP and FN\n        dice_mat = _pad_to_shape(dice_mat, (max_dim, max_dim), 1)\n        lsa = linear_sum_assignment(dice_mat, maximize=False)\n        for row, col in zip(lsa[0], lsa[1]):\n            # More preds than GTs --&gt; False Positive\n            if row &lt; n_labels_pred and col &gt;= n_labels_mask:\n                min_row = pred_bbox[row][0]\n                min_col = pred_bbox[row][1]\n                h = pred_bbox[row][2] - min_row\n                w = pred_bbox[row][3] - min_col\n                fp_num += 1\n                area = pred[min_row : min_row + h, min_col : min_col + w].sum()\n                fp_area += area\n                fp_hist.append(area)\n                continue\n\n            # More GTs than preds --&gt; False Negative\n            if col &lt; n_labels_mask and row &gt;= n_labels_pred:\n                min_row = mask_bbox[col][0]\n                min_col = mask_bbox[col][1]\n                h = mask_bbox[col][2] - min_row\n                w = mask_bbox[col][3] - min_col\n                fn_num += 1\n                fn_area += mask[min_row : min_row + h, min_col : min_col + w].sum()\n                continue\n\n            # Real True Positive: a prediction has been assigned to a gt\n            # with at least a 1-Dice score of 0.9\n            if dice_mat[row, col] &lt;= 0.9:\n                tp_num += 1\n                lsa_iou += iou_mat[row, col]\n                lsa_dice += dice_mat[row, col]\n            else:\n                # Here we have both a FP and a FN\n                min_row = pred_bbox[row][0]\n                min_col = pred_bbox[row][1]\n                h = pred_bbox[row][2] - min_row\n                w = pred_bbox[row][3] - min_col\n                fp_num += 1\n                area = pred[min_row : min_row + h, min_col : min_col + w].sum()\n                fp_area += area\n                fp_hist.append(area)\n\n                min_row = mask_bbox[col][0]\n                min_col = mask_bbox[col][1]\n                h = mask_bbox[col][2] - min_row\n                w = mask_bbox[col][3] - min_col\n                fn_num += 1\n                fn_area += mask[min_row : min_row + h, min_col : min_col + w].sum()\n    elif len(pred_bbox) &gt; 0 and len(mask_bbox) == 0:  # No GTs --&gt; FP\n        for p_bbox in pred_bbox:\n            min_row = p_bbox[0]\n            min_col = p_bbox[1]\n            h = p_bbox[2] - min_row\n            w = p_bbox[3] - min_col\n            fp_num += 1\n            # print(\"FP area:\", pred[min_row : min_row + h, min_col : min_col + w].sum())\n            area = pred[min_row : min_row + h, min_col : min_col + w].sum()\n            fp_area += area\n            fp_hist.append(area)\n    elif len(pred_bbox) == 0 and len(mask_bbox) &gt; 0:  # No preds --&gt; FN\n        for m_bbox in mask_bbox:\n            min_row = m_bbox[0]\n            min_col = m_bbox[1]\n            h = m_bbox[2] - min_row\n            w = m_bbox[3] - min_col\n            fn_num += 1\n            # print(\"FN area:\", mask[min_row : min_row + h, min_col : min_col + w].sum())\n            fn_area += mask[min_row : min_row + h, min_col : min_col + w].sum()\n    return (\n        global_dice,\n        lsa_dice,\n        lsa_iou,\n        fp_area,\n        fp_hist,\n        fn_area,\n        tp_num,\n        fp_num,\n        fn_num,\n        n_labels_mask,\n    )\n</code></pre>"},{"location":"reference/quadra/metrics/segmentation.html","title":"segmentation","text":""},{"location":"reference/quadra/metrics/segmentation.html#quadra.metrics.segmentation.segmentation_props","title":"<code>segmentation_props(pred, mask)</code>","text":"<p>Return some information regarding a segmentation task.</p> <p>Parameters:</p> <ul> <li> pred             (<code>ndarray[bool]</code>)         \u2013          <p>Prediction of a segmentation model as a binary image.</p> </li> <li> mask             (<code>ndarray[bool]</code>)         \u2013          <p>Ground truth mask as binary image</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float</code>         \u2013          <p>1-Dice(pred, mask) Given a matrix (a_ij) = (1-Dice)(prediction_i, ground_truth_j), where prediction_i is the i-th prediction connected component and ground_truth_j is the j-th ground truth connected component, I compute the LSA (Linear Sum Assignment) to find the optimal 1-to-1 assignment between predictions and ground truths that minimize the (1-Dice) score. Then, for every unique pair of (predictioni, ground_truthj) we compute Average  (1-Dice)(predictioni, ground_truthj)</p> </li> <li> <code>float</code>         \u2013          <p>Average (1-Dice)(predictioni, ground_truthj) between True Positives (that is predictions associated to a ground truth), which is gratis from a[i,j], where the average is computed w.r.t. the total number of True Positives found.</p> </li> <li> <code>float</code>         \u2013          <p>Average IoU(predictioni, ground_truthj) between True Positives (that is predictions associated to a ground truth), where the average is computed w.r.t. the total number of True Positives found. The IoU is computed between the minimum enclosing bounding box of a prediction and a ground truth.</p> </li> <li> <code>float</code>         \u2013          <p>Average area of False Positives</p> </li> <li> <code>list[float]</code>         \u2013          <p>Histogram of false positives</p> </li> <li> <code>float</code>         \u2013          <p>Average area of False Negatives</p> </li> <li> <code>int</code>         \u2013          <p>Number of True Positives (predictions associated to a ground truth)</p> </li> <li> <code>int</code>         \u2013          <p>Number of False Positives (predictions without a ground truth associated)     and their avg. area (avg is taken w.r.t. the total number of False Positives found)</p> </li> <li> <code>int</code>         \u2013          <p>Number of False Negatives (ground truth without a predictions associated) and their avg. area (avg is taken w.r.t. the total number of False Negatives found)</p> </li> <li> <code>int</code>         \u2013          <p>Number of labels in the mask.</p> </li> </ul> Source code in <code>quadra/metrics/segmentation.py</code> <pre><code>def segmentation_props(\n    pred: np.ndarray, mask: np.ndarray\n) -&gt; tuple[float, float, float, float, list[float], float, int, int, int, int]:\n\"\"\"Return some information regarding a segmentation task.\n\n    Args:\n        pred (np.ndarray[bool]): Prediction of a segmentation model as\n            a binary image.\n        mask (np.ndarray[bool]): Ground truth mask as binary image\n\n    Returns:\n        1-Dice(pred, mask) Given a matrix (a_ij) = (1-Dice)(prediction_i, ground_truth_j),\n            where prediction_i is the i-th prediction connected component and\n            ground_truth_j is the j-th ground truth connected component,\n            I compute the LSA (Linear Sum Assignment) to find the optimal 1-to-1 assignment\n            between predictions and ground truths that minimize the (1-Dice) score.\n            Then, for every unique pair of (predictioni, ground_truthj) we compute Average\n             (1-Dice)(predictioni, ground_truthj)\n        Average (1-Dice)(predictioni, ground_truthj) between True Positives\n            (that is predictions associated to a ground truth), which is gratis from a[i,j],\n            where the average is computed w.r.t. the total number of True Positives found.\n        Average IoU(predictioni, ground_truthj) between True Positives\n            (that is predictions associated to a ground truth),\n            where the average is computed w.r.t. the total number of True Positives found.\n            The IoU is computed between the minimum enclosing bounding box\n            of a prediction and a ground truth.\n        Average area of False Positives\n        Histogram of false positives\n        Average area of False Negatives\n        Number of True Positives (predictions associated to a ground truth)\n        Number of False Positives (predictions without a ground truth associated)\n                and their avg. area (avg is taken w.r.t. the total number of False Positives found)\n        Number of False Negatives (ground truth without a predictions associated)\n            and their avg. area (avg is taken w.r.t. the total number of False Negatives found)\n        Number of labels in the mask.\n    \"\"\"\n    labels_pred, n_labels_pred = label(pred, connectivity=2, return_num=True, background=0)\n    labels_mask, n_labels_mask = label(mask, connectivity=2, return_num=True, background=0)\n\n    labels_pred = cast(np.ndarray, labels_pred)\n    labels_mask = cast(np.ndarray, labels_mask)\n    n_labels_pred = cast(int, n_labels_pred)\n    n_labels_mask = cast(int, n_labels_mask)\n\n    props_pred = regionprops(labels_pred)\n    props_mask = regionprops(labels_mask)\n    pred_bbox = np.array([props_pred[i].bbox for i in range(len(props_pred))])\n    mask_bbox = np.array([props_mask[i].bbox for i in range(len(props_mask))])\n\n    global_dice = float(\n        dice(\n            torch.Tensor(pred).unsqueeze(0).unsqueeze(0),\n            torch.Tensor(mask).unsqueeze(0).unsqueeze(0),\n        ).item()\n    )\n    lsa_iou = 0.0\n    lsa_dice = 0.0\n    tp_num = 0\n    fp_num = 0\n    fn_num = 0\n    fp_area = 0.0\n    fn_area = 0.0\n    fp_hist: list[float] = []\n    if n_labels_pred &gt; 0 and n_labels_mask &gt; 0:\n        dice_mat = _get_dice_matrix(labels_pred, n_labels_pred, labels_mask, n_labels_mask)\n        # Thresholding over Dice scores\n        dice_mat = np.where(dice_mat &lt;= 0.9, dice_mat, 1.0)\n        iou_mat = _get_iou(pred_bbox, mask_bbox, approx_iou=False)\n        dice_mat_shape = dice_mat.shape\n        max_dim = np.max(dice_mat_shape)\n        # Add dummy Dices so LSA is unique and i can compute FP and FN\n        dice_mat = _pad_to_shape(dice_mat, (max_dim, max_dim), 1)\n        lsa = linear_sum_assignment(dice_mat, maximize=False)\n        for row, col in zip(lsa[0], lsa[1]):\n            # More preds than GTs --&gt; False Positive\n            if row &lt; n_labels_pred and col &gt;= n_labels_mask:\n                min_row = pred_bbox[row][0]\n                min_col = pred_bbox[row][1]\n                h = pred_bbox[row][2] - min_row\n                w = pred_bbox[row][3] - min_col\n                fp_num += 1\n                area = pred[min_row : min_row + h, min_col : min_col + w].sum()\n                fp_area += area\n                fp_hist.append(area)\n                continue\n\n            # More GTs than preds --&gt; False Negative\n            if col &lt; n_labels_mask and row &gt;= n_labels_pred:\n                min_row = mask_bbox[col][0]\n                min_col = mask_bbox[col][1]\n                h = mask_bbox[col][2] - min_row\n                w = mask_bbox[col][3] - min_col\n                fn_num += 1\n                fn_area += mask[min_row : min_row + h, min_col : min_col + w].sum()\n                continue\n\n            # Real True Positive: a prediction has been assigned to a gt\n            # with at least a 1-Dice score of 0.9\n            if dice_mat[row, col] &lt;= 0.9:\n                tp_num += 1\n                lsa_iou += iou_mat[row, col]\n                lsa_dice += dice_mat[row, col]\n            else:\n                # Here we have both a FP and a FN\n                min_row = pred_bbox[row][0]\n                min_col = pred_bbox[row][1]\n                h = pred_bbox[row][2] - min_row\n                w = pred_bbox[row][3] - min_col\n                fp_num += 1\n                area = pred[min_row : min_row + h, min_col : min_col + w].sum()\n                fp_area += area\n                fp_hist.append(area)\n\n                min_row = mask_bbox[col][0]\n                min_col = mask_bbox[col][1]\n                h = mask_bbox[col][2] - min_row\n                w = mask_bbox[col][3] - min_col\n                fn_num += 1\n                fn_area += mask[min_row : min_row + h, min_col : min_col + w].sum()\n    elif len(pred_bbox) &gt; 0 and len(mask_bbox) == 0:  # No GTs --&gt; FP\n        for p_bbox in pred_bbox:\n            min_row = p_bbox[0]\n            min_col = p_bbox[1]\n            h = p_bbox[2] - min_row\n            w = p_bbox[3] - min_col\n            fp_num += 1\n            # print(\"FP area:\", pred[min_row : min_row + h, min_col : min_col + w].sum())\n            area = pred[min_row : min_row + h, min_col : min_col + w].sum()\n            fp_area += area\n            fp_hist.append(area)\n    elif len(pred_bbox) == 0 and len(mask_bbox) &gt; 0:  # No preds --&gt; FN\n        for m_bbox in mask_bbox:\n            min_row = m_bbox[0]\n            min_col = m_bbox[1]\n            h = m_bbox[2] - min_row\n            w = m_bbox[3] - min_col\n            fn_num += 1\n            # print(\"FN area:\", mask[min_row : min_row + h, min_col : min_col + w].sum())\n            fn_area += mask[min_row : min_row + h, min_col : min_col + w].sum()\n    return (\n        global_dice,\n        lsa_dice,\n        lsa_iou,\n        fp_area,\n        fp_hist,\n        fn_area,\n        tp_num,\n        fp_num,\n        fn_num,\n        n_labels_mask,\n    )\n</code></pre>"},{"location":"reference/quadra/models/index.html","title":"models","text":""},{"location":"reference/quadra/models/index.html#submodules","title":"Submodules","text":"<ul> <li>classification</li> </ul>"},{"location":"reference/quadra/models/index.html#python-files","title":"Python Files","text":"<ul> <li>base.py</li> <li>evaluation.py </li> </ul>"},{"location":"reference/quadra/models/base.html","title":"base","text":""},{"location":"reference/quadra/models/base.html#quadra.models.base.ModelSignatureWrapper","title":"<code>ModelSignatureWrapper(model)</code>","text":"<p>             Bases: <code>Module</code></p> <p>Model wrapper used to retrieve input shape. It can be used as a decorator of nn.Module, the first call to the forward method will retrieve the input shape and store it in the input_shapes attribute. It will also save the model summary in a file called model_summary.txt in the current working directory.</p> Source code in <code>quadra/models/base.py</code> <pre><code>def __init__(self, model: nn.Module):\n    super().__init__()\n    self.instance = model\n    self.input_shapes: Any = None\n    self.disable = False\n\n    if isinstance(self.instance, ModelSignatureWrapper):\n        # Handle nested ModelSignatureWrapper\n        self.input_shapes = self.instance.input_shapes\n        self.instance = self.instance.instance\n</code></pre>"},{"location":"reference/quadra/models/base.html#quadra.models.base.ModelSignatureWrapper.cpu","title":"<code>cpu(*args, **kwargs)</code>","text":"<p>Handle calls to to method returning the underlying model.</p> Source code in <code>quadra/models/base.py</code> <pre><code>def cpu(self, *args, **kwargs):\n\"\"\"Handle calls to to method returning the underlying model.\"\"\"\n    self.instance = self.instance.cpu(*args, **kwargs)\n\n    return self\n</code></pre>"},{"location":"reference/quadra/models/base.html#quadra.models.base.ModelSignatureWrapper.forward","title":"<code>forward(*args, **kwargs)</code>","text":"<p>Retrieve the input shape and forward the model, if the input shape is already retrieved it will just forward the model.</p> Source code in <code>quadra/models/base.py</code> <pre><code>def forward(self, *args: Any, **kwargs: Any) -&gt; torch.Tensor:\n\"\"\"Retrieve the input shape and forward the model, if the input shape is already retrieved it will just forward\n    the model.\n    \"\"\"\n    if self.input_shapes is None and not self.disable:\n        try:\n            self.input_shapes = self._get_input_shapes(*args, **kwargs)\n        except Exception:\n            log.warning(\n                \"Failed to retrieve input shapes after forward! To export the model you'll need to \"\n                \"provide the input shapes manually setting the config.export.input_shapes parameter! \"\n                \"Alternatively you could try to use a forward with supported input types (and their compositions) \"\n                \"(list, tuple, dict, tensors).\"\n            )\n            self.disable = True\n\n    return self.instance.forward(*args, **kwargs)\n</code></pre>"},{"location":"reference/quadra/models/base.html#quadra.models.base.ModelSignatureWrapper.half","title":"<code>half(*args, **kwargs)</code>","text":"<p>Handle calls to to method returning the underlying model.</p> Source code in <code>quadra/models/base.py</code> <pre><code>def half(self, *args, **kwargs):\n\"\"\"Handle calls to to method returning the underlying model.\"\"\"\n    self.instance = self.instance.half(*args, **kwargs)\n\n    return self\n</code></pre>"},{"location":"reference/quadra/models/base.html#quadra.models.base.ModelSignatureWrapper.to","title":"<code>to(*args, **kwargs)</code>","text":"<p>Handle calls to to method returning the underlying model.</p> Source code in <code>quadra/models/base.py</code> <pre><code>def to(self, *args, **kwargs):\n\"\"\"Handle calls to to method returning the underlying model.\"\"\"\n    self.instance = self.instance.to(*args, **kwargs)\n\n    return self\n</code></pre>"},{"location":"reference/quadra/models/evaluation.html","title":"evaluation","text":""},{"location":"reference/quadra/models/evaluation.html#quadra.models.evaluation.BaseEvaluationModel","title":"<code>BaseEvaluationModel(config)</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Base interface for all evaluation models.</p> Source code in <code>quadra/models/evaluation.py</code> <pre><code>def __init__(self, config: DictConfig) -&gt; None:\n    self.model: Any\n    self.model_path: str | None\n    self.device: str\n    self.config = config\n    self.is_loaded = False\n    self.model_dtype: np.dtype | torch.dtype\n</code></pre>"},{"location":"reference/quadra/models/evaluation.html#quadra.models.evaluation.BaseEvaluationModel.device","title":"<code>device: str</code>  <code>property</code> <code>writable</code>","text":"<p>Return the device of the model.</p>"},{"location":"reference/quadra/models/evaluation.html#quadra.models.evaluation.BaseEvaluationModel.training","title":"<code>training: bool</code>  <code>property</code>","text":"<p>Return whether model is in training mode.</p>"},{"location":"reference/quadra/models/evaluation.html#quadra.models.evaluation.BaseEvaluationModel.cpu","title":"<code>cpu()</code>  <code>abstractmethod</code>","text":"<p>Move model to cpu.</p> Source code in <code>quadra/models/evaluation.py</code> <pre><code>@abstractmethod\ndef cpu(self):\n\"\"\"Move model to cpu.\"\"\"\n</code></pre>"},{"location":"reference/quadra/models/evaluation.html#quadra.models.evaluation.BaseEvaluationModel.eval","title":"<code>eval()</code>  <code>abstractmethod</code>","text":"<p>Set model to evaluation mode.</p> Source code in <code>quadra/models/evaluation.py</code> <pre><code>@abstractmethod\ndef eval(self):\n\"\"\"Set model to evaluation mode.\"\"\"\n</code></pre>"},{"location":"reference/quadra/models/evaluation.html#quadra.models.evaluation.BaseEvaluationModel.half","title":"<code>half()</code>  <code>abstractmethod</code>","text":"<p>Convert model to half precision.</p> Source code in <code>quadra/models/evaluation.py</code> <pre><code>@abstractmethod\ndef half(self):\n\"\"\"Convert model to half precision.\"\"\"\n</code></pre>"},{"location":"reference/quadra/models/evaluation.html#quadra.models.evaluation.BaseEvaluationModel.load_from_disk","title":"<code>load_from_disk(model_path, device='cpu')</code>  <code>abstractmethod</code>","text":"<p>Load model from disk.</p> Source code in <code>quadra/models/evaluation.py</code> <pre><code>@abstractmethod\ndef load_from_disk(self, model_path: str, device: str = \"cpu\"):\n\"\"\"Load model from disk.\"\"\"\n</code></pre>"},{"location":"reference/quadra/models/evaluation.html#quadra.models.evaluation.BaseEvaluationModel.to","title":"<code>to(device)</code>  <code>abstractmethod</code>","text":"<p>Move model to device.</p> Source code in <code>quadra/models/evaluation.py</code> <pre><code>@abstractmethod\ndef to(self, device: str):\n\"\"\"Move model to device.\"\"\"\n</code></pre>"},{"location":"reference/quadra/models/evaluation.html#quadra.models.evaluation.ONNXEvaluationModel","title":"<code>ONNXEvaluationModel(config)</code>","text":"<p>             Bases: <code>BaseEvaluationModel</code></p> <p>Wrapper for ONNX models. It's designed to provide a similar interface to standard torch models.</p> Source code in <code>quadra/models/evaluation.py</code> <pre><code>def __init__(self, config: DictConfig) -&gt; None:\n    if not ONNX_AVAILABLE:\n        raise ImportError(\n            \"onnxruntime is not installed. Please install ONNX capabilities for quadra with: poetry install -E onnx\"\n        )\n    super().__init__(config=config)\n    self.session_options = self.generate_session_options()\n</code></pre>"},{"location":"reference/quadra/models/evaluation.html#quadra.models.evaluation.ONNXEvaluationModel.__call__","title":"<code>__call__(*inputs)</code>","text":"<p>Run inference on the model and return the output as torch tensors.</p> Source code in <code>quadra/models/evaluation.py</code> <pre><code>def __call__(self, *inputs: np.ndarray | torch.Tensor) -&gt; Any:\n\"\"\"Run inference on the model and return the output as torch tensors.\"\"\"\n    # TODO: Maybe we can support also kwargs\n    use_pytorch = False\n\n    onnx_inputs: dict[str, np.ndarray | torch.Tensor] = {}\n\n    for onnx_input, current_input in zip(self.model.get_inputs(), inputs):\n        if isinstance(current_input, torch.Tensor):\n            onnx_inputs[onnx_input.name] = current_input\n            use_pytorch = True\n        elif isinstance(current_input, np.ndarray):\n            onnx_inputs[onnx_input.name] = current_input\n        else:\n            raise ValueError(f\"Invalid input type: {type(inputs)}\")\n\n        if use_pytorch and isinstance(current_input, np.ndarray):\n            raise ValueError(\"Cannot mix torch and numpy inputs\")\n\n    if use_pytorch:\n        onnx_output = self._forward_from_pytorch(cast(dict[str, torch.Tensor], onnx_inputs))\n    else:\n        onnx_output = self._forward_from_numpy(cast(dict[str, np.ndarray], onnx_inputs))\n\n    onnx_output = [torch.from_numpy(x).to(self.device) if isinstance(x, np.ndarray) else x for x in onnx_output]\n\n    if len(onnx_output) == 1:\n        onnx_output = onnx_output[0]\n\n    return onnx_output\n</code></pre>"},{"location":"reference/quadra/models/evaluation.html#quadra.models.evaluation.ONNXEvaluationModel.cast_onnx_dtype","title":"<code>cast_onnx_dtype(onnx_dtype)</code>","text":"<p>Cast ONNX dtype to numpy or pytorch dtype.</p> Source code in <code>quadra/models/evaluation.py</code> <pre><code>def cast_onnx_dtype(self, onnx_dtype: str) -&gt; torch.dtype | np.dtype:\n\"\"\"Cast ONNX dtype to numpy or pytorch dtype.\"\"\"\n    return onnx_to_torch_dtype_dict[onnx_dtype]\n</code></pre>"},{"location":"reference/quadra/models/evaluation.html#quadra.models.evaluation.ONNXEvaluationModel.cpu","title":"<code>cpu()</code>","text":"<p>Move model to cpu.</p> Source code in <code>quadra/models/evaluation.py</code> <pre><code>def cpu(self):\n\"\"\"Move model to cpu.\"\"\"\n    self.to(\"cpu\")\n</code></pre>"},{"location":"reference/quadra/models/evaluation.html#quadra.models.evaluation.ONNXEvaluationModel.eval","title":"<code>eval()</code>","text":"<p>Fake interface to match torch models.</p> Source code in <code>quadra/models/evaluation.py</code> <pre><code>def eval(self):\n\"\"\"Fake interface to match torch models.\"\"\"\n    return self\n</code></pre>"},{"location":"reference/quadra/models/evaluation.html#quadra.models.evaluation.ONNXEvaluationModel.generate_session_options","title":"<code>generate_session_options()</code>","text":"<p>Generate session options from the current config.</p> Source code in <code>quadra/models/evaluation.py</code> <pre><code>def generate_session_options(self) -&gt; ort.SessionOptions:\n\"\"\"Generate session options from the current config.\"\"\"\n    session_options = ort.SessionOptions()\n\n    if hasattr(self.config, \"session_options\") and self.config.session_options is not None:\n        session_options_dict = cast(\n            dict[str, Any], OmegaConf.to_container(self.config.session_options, resolve=True)\n        )\n        for key, value in session_options_dict.items():\n            final_value = value\n            if isinstance(value, dict) and \"_target_\" in value:\n                final_value = instantiate(final_value)\n\n            setattr(session_options, key, final_value)\n\n    return session_options\n</code></pre>"},{"location":"reference/quadra/models/evaluation.html#quadra.models.evaluation.ONNXEvaluationModel.half","title":"<code>half()</code>","text":"<p>Convert model to half precision.</p> Source code in <code>quadra/models/evaluation.py</code> <pre><code>def half(self):\n\"\"\"Convert model to half precision.\"\"\"\n    raise NotImplementedError(\"At the moment ONNX models do not support half method.\")\n</code></pre>"},{"location":"reference/quadra/models/evaluation.html#quadra.models.evaluation.ONNXEvaluationModel.load_from_disk","title":"<code>load_from_disk(model_path, device='cpu')</code>","text":"<p>Load model from disk.</p> Source code in <code>quadra/models/evaluation.py</code> <pre><code>def load_from_disk(self, model_path: str, device: str = \"cpu\"):\n\"\"\"Load model from disk.\"\"\"\n    self.model_path = model_path\n    self.device = device\n\n    ort_providers = self._get_providers(device)\n    self.model = ort.InferenceSession(self.model_path, providers=ort_providers, sess_options=self.session_options)\n    self.model_dtype = self.cast_onnx_dtype(self.model.get_inputs()[0].type)\n    self.is_loaded = True\n</code></pre>"},{"location":"reference/quadra/models/evaluation.html#quadra.models.evaluation.ONNXEvaluationModel.to","title":"<code>to(device)</code>","text":"<p>Move model to device.</p> Source code in <code>quadra/models/evaluation.py</code> <pre><code>def to(self, device: str):\n\"\"\"Move model to device.\"\"\"\n    self.device = device\n    ort_providers = self._get_providers(device)\n    self.model.set_providers(ort_providers)\n</code></pre>"},{"location":"reference/quadra/models/evaluation.html#quadra.models.evaluation.TorchEvaluationModel","title":"<code>TorchEvaluationModel(config, model_architecture)</code>","text":"<p>             Bases: <code>TorchscriptEvaluationModel</code></p> <p>Wrapper for torch models.</p> <p>Parameters:</p> <ul> <li> model_architecture             (<code>Module</code>)         \u2013          <p>Optional torch model architecture</p> </li> </ul> Source code in <code>quadra/models/evaluation.py</code> <pre><code>def __init__(self, config: DictConfig, model_architecture: nn.Module) -&gt; None:\n    super().__init__(config=config)\n    self.model = model_architecture\n    self.model.eval()\n    device = next(self.model.parameters()).device\n    self.device = str(device)\n</code></pre>"},{"location":"reference/quadra/models/evaluation.html#quadra.models.evaluation.TorchEvaluationModel.load_from_disk","title":"<code>load_from_disk(model_path, device='cpu')</code>","text":"<p>Load model from disk.</p> Source code in <code>quadra/models/evaluation.py</code> <pre><code>def load_from_disk(self, model_path: str, device: str = \"cpu\"):\n\"\"\"Load model from disk.\"\"\"\n    self.model_path = model_path\n    self.device = device\n    self.model.load_state_dict(torch.load(self.model_path))\n    self.model.eval()\n    self.model.to(self.device)\n\n    parameter_types = {param.dtype for param in self.model.parameters()}\n    if len(parameter_types) == 2:\n        # TODO: There could be models with mixed precision?\n        raise ValueError(f\"Expected only one type of parameters, found {parameter_types}\")\n\n    self.model_dtype = list(parameter_types)[0]\n    self.is_loaded = True\n</code></pre>"},{"location":"reference/quadra/models/evaluation.html#quadra.models.evaluation.TorchscriptEvaluationModel","title":"<code>TorchscriptEvaluationModel</code>","text":"<p>             Bases: <code>BaseEvaluationModel</code></p> <p>Wrapper for torchscript models.</p>"},{"location":"reference/quadra/models/evaluation.html#quadra.models.evaluation.TorchscriptEvaluationModel.training","title":"<code>training: bool</code>  <code>property</code>","text":"<p>Return whether model is in training mode.</p>"},{"location":"reference/quadra/models/evaluation.html#quadra.models.evaluation.TorchscriptEvaluationModel.cpu","title":"<code>cpu()</code>","text":"<p>Move model to cpu.</p> Source code in <code>quadra/models/evaluation.py</code> <pre><code>def cpu(self):\n\"\"\"Move model to cpu.\"\"\"\n    self.model.cpu()\n</code></pre>"},{"location":"reference/quadra/models/evaluation.html#quadra.models.evaluation.TorchscriptEvaluationModel.eval","title":"<code>eval()</code>","text":"<p>Set model to evaluation mode.</p> Source code in <code>quadra/models/evaluation.py</code> <pre><code>def eval(self):\n\"\"\"Set model to evaluation mode.\"\"\"\n    self.model.eval()\n</code></pre>"},{"location":"reference/quadra/models/evaluation.html#quadra.models.evaluation.TorchscriptEvaluationModel.half","title":"<code>half()</code>","text":"<p>Convert model to half precision.</p> Source code in <code>quadra/models/evaluation.py</code> <pre><code>def half(self):\n\"\"\"Convert model to half precision.\"\"\"\n    self.model.half()\n</code></pre>"},{"location":"reference/quadra/models/evaluation.html#quadra.models.evaluation.TorchscriptEvaluationModel.load_from_disk","title":"<code>load_from_disk(model_path, device='cpu')</code>","text":"<p>Load model from disk.</p> Source code in <code>quadra/models/evaluation.py</code> <pre><code>def load_from_disk(self, model_path: str, device: str = \"cpu\"):\n\"\"\"Load model from disk.\"\"\"\n    self.model_path = model_path\n    self.device = device\n\n    model = cast(RecursiveScriptModule, torch.jit.load(self.model_path))\n    model.eval()\n    model.to(self.device)\n\n    parameter_types = {param.dtype for param in model.parameters()}\n    if len(parameter_types) == 2:\n        # TODO: There could be models with mixed precision?\n        raise ValueError(f\"Expected only one type of parameters, found {parameter_types}\")\n\n    self.model_dtype = list(parameter_types)[0]\n    self.model = model\n    self.is_loaded = True\n</code></pre>"},{"location":"reference/quadra/models/evaluation.html#quadra.models.evaluation.TorchscriptEvaluationModel.to","title":"<code>to(device)</code>","text":"<p>Move model to device.</p> Source code in <code>quadra/models/evaluation.py</code> <pre><code>def to(self, device: str):\n\"\"\"Move model to device.\"\"\"\n    self.model.to(device)\n    self.device = device\n</code></pre>"},{"location":"reference/quadra/models/classification/index.html","title":"classification","text":""},{"location":"reference/quadra/models/classification/index.html#quadra.models.classification.BaseNetworkBuilder","title":"<code>BaseNetworkBuilder(features_extractor, pre_classifier=None, classifier=None, freeze=True, hyperspherical=False, flatten_features=True)</code>","text":"<p>             Bases: <code>Module</code></p> <p>Baseline Feature Extractor, with the possibility to map features to an hypersphere.     If hypershperical is True the classifier is ignored.</p> <p>Parameters:</p> <ul> <li> features_extractor             (<code>Module</code>)         \u2013          <p>Feature extractor as a toch.nn.Module.</p> </li> <li> pre_classifier             (<code>Module | None</code>, default:                 <code>None</code> )         \u2013          <p>Pre classifier as a torch.nn.Module. Defaults to nn.Identity() if None.</p> </li> <li> classifier             (<code>Module | None</code>, default:                 <code>None</code> )         \u2013          <p>Classifier as a torch.nn.Module. Defaults to nn.Identity() if None.</p> </li> <li> freeze             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Whether to freeze the feature extractor. Defaults to True.</p> </li> <li> hyperspherical             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to map features to an hypersphere. Defaults to False.</p> </li> <li> flatten_features             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Whether to flatten the features before the pre_classifier. May be required if your model is outputting a feature map rather than a vector. Defaults to True.</p> </li> </ul> Source code in <code>quadra/models/classification/base.py</code> <pre><code>def __init__(\n    self,\n    features_extractor: nn.Module,\n    pre_classifier: nn.Module | None = None,\n    classifier: nn.Module | None = None,\n    freeze: bool = True,\n    hyperspherical: bool = False,\n    flatten_features: bool = True,\n):\n    super().__init__()\n    if pre_classifier is None:\n        pre_classifier = nn.Identity()\n\n    if classifier is None:\n        classifier = nn.Identity()\n\n    self.features_extractor = features_extractor\n    self.freeze = freeze\n    self.hyperspherical = hyperspherical\n    self.pre_classifier = pre_classifier\n    self.classifier = classifier\n    self.flatten: bool = False\n    self._hyperspherical: bool = False\n    self.l2: L2Norm | None = None\n    self.flatten_features = flatten_features\n\n    self.freeze = freeze\n    self.hyperspherical = hyperspherical\n\n    if self.freeze:\n        for p in self.features_extractor.parameters():\n            p.requires_grad = False\n</code></pre>"},{"location":"reference/quadra/models/classification/index.html#quadra.models.classification.BaseNetworkBuilder.freeze","title":"<code>freeze: bool</code>  <code>property</code> <code>writable</code>","text":"<p>Whether to freeze the feature extractor.</p>"},{"location":"reference/quadra/models/classification/index.html#quadra.models.classification.BaseNetworkBuilder.hyperspherical","title":"<code>hyperspherical: bool</code>  <code>property</code> <code>writable</code>","text":"<p>Whether to map the extracted features into an hypersphere.</p>"},{"location":"reference/quadra/models/classification/index.html#quadra.models.classification.TimmNetworkBuilder","title":"<code>TimmNetworkBuilder(model_name, pretrained=True, pre_classifier=None, classifier=None, freeze=True, hyperspherical=False, flatten_features=True, checkpoint_path=None, **timm_kwargs)</code>","text":"<p>             Bases: <code>BaseNetworkBuilder</code></p> <p>Torchvision feature extractor, with the possibility to map features to an hypersphere.</p> <p>Parameters:</p> <ul> <li> model_name             (<code>str</code>)         \u2013          <p>Timm model name</p> </li> <li> pretrained             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Whether to load the pretrained weights for the model.</p> </li> <li> pre_classifier             (<code>Module | None</code>, default:                 <code>None</code> )         \u2013          <p>Pre classifier as a torch.nn.Module. Defaults to nn.Identity() if None.</p> </li> <li> classifier             (<code>Module | None</code>, default:                 <code>None</code> )         \u2013          <p>Classifier as a torch.nn.Module. Defaults to nn.Identity() if None.</p> </li> <li> freeze             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Whether to freeze the feature extractor. Defaults to True.</p> </li> <li> hyperspherical             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to map features to an hypersphere. Defaults to False.</p> </li> <li> flatten_features             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Whether to flatten the features before the pre_classifier. Defaults to True.</p> </li> <li> checkpoint_path             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>Path to a checkpoint to load after the model is initialized. Defaults to None.</p> </li> <li> **timm_kwargs             (<code>Any</code>, default:                 <code>{}</code> )         \u2013          <p>Additional arguments to pass to timm.create_model</p> </li> </ul> Source code in <code>quadra/models/classification/backbones.py</code> <pre><code>def __init__(\n    self,\n    model_name: str,\n    pretrained: bool = True,\n    pre_classifier: nn.Module | None = None,\n    classifier: nn.Module | None = None,\n    freeze: bool = True,\n    hyperspherical: bool = False,\n    flatten_features: bool = True,\n    checkpoint_path: str | None = None,\n    **timm_kwargs: Any,\n):\n    self.pretrained = pretrained\n    features_extractor = timm.create_model(\n        model_name, pretrained=self.pretrained, num_classes=0, checkpoint_path=checkpoint_path, **timm_kwargs\n    )\n\n    super().__init__(\n        features_extractor=features_extractor,\n        pre_classifier=pre_classifier,\n        classifier=classifier,\n        freeze=freeze,\n        hyperspherical=hyperspherical,\n        flatten_features=flatten_features,\n    )\n</code></pre>"},{"location":"reference/quadra/models/classification/index.html#quadra.models.classification.TorchHubNetworkBuilder","title":"<code>TorchHubNetworkBuilder(repo_or_dir, model_name, pretrained=True, pre_classifier=None, classifier=None, freeze=True, hyperspherical=False, flatten_features=True, checkpoint_path=None, **torch_hub_kwargs)</code>","text":"<p>             Bases: <code>BaseNetworkBuilder</code></p> <p>TorchHub feature extractor, with the possibility to map features to an hypersphere.</p> <p>Parameters:</p> <ul> <li> repo_or_dir             (<code>str</code>)         \u2013          <p>The name of the repository or the path to the directory containing the model.</p> </li> <li> model_name             (<code>str</code>)         \u2013          <p>The name of the model within the repository.</p> </li> <li> pretrained             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Whether to load the pretrained weights for the model.</p> </li> <li> pre_classifier             (<code>Module | None</code>, default:                 <code>None</code> )         \u2013          <p>Pre classifier as a torch.nn.Module. Defaults to nn.Identity() if None.</p> </li> <li> classifier             (<code>Module | None</code>, default:                 <code>None</code> )         \u2013          <p>Classifier as a torch.nn.Module. Defaults to nn.Identity() if None.</p> </li> <li> freeze             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Whether to freeze the feature extractor. Defaults to True.</p> </li> <li> hyperspherical             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to map features to an hypersphere. Defaults to False.</p> </li> <li> flatten_features             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Whether to flatten the features before the pre_classifier. Defaults to True.</p> </li> <li> checkpoint_path             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>Path to a checkpoint to load after the model is initialized. Defaults to None.</p> </li> <li> **torch_hub_kwargs             (<code>Any</code>, default:                 <code>{}</code> )         \u2013          <p>Additional arguments to pass to torch.hub.load</p> </li> </ul> Source code in <code>quadra/models/classification/backbones.py</code> <pre><code>def __init__(\n    self,\n    repo_or_dir: str,\n    model_name: str,\n    pretrained: bool = True,\n    pre_classifier: nn.Module | None = None,\n    classifier: nn.Module | None = None,\n    freeze: bool = True,\n    hyperspherical: bool = False,\n    flatten_features: bool = True,\n    checkpoint_path: str | None = None,\n    **torch_hub_kwargs: Any,\n):\n    self.pretrained = pretrained\n    features_extractor = torch.hub.load(\n        repo_or_dir=repo_or_dir, model=model_name, pretrained=self.pretrained, **torch_hub_kwargs\n    )\n    if checkpoint_path:\n        log.info(\"Loading checkpoint from %s\", checkpoint_path)\n        load_checkpoint(model=features_extractor, checkpoint_path=checkpoint_path)\n\n    super().__init__(\n        features_extractor=features_extractor,\n        pre_classifier=pre_classifier,\n        classifier=classifier,\n        freeze=freeze,\n        hyperspherical=hyperspherical,\n        flatten_features=flatten_features,\n    )\n</code></pre>"},{"location":"reference/quadra/models/classification/index.html#quadra.models.classification.TorchVisionNetworkBuilder","title":"<code>TorchVisionNetworkBuilder(model_name, pretrained=True, pre_classifier=None, classifier=None, freeze=True, hyperspherical=False, flatten_features=True, checkpoint_path=None, **torchvision_kwargs)</code>","text":"<p>             Bases: <code>BaseNetworkBuilder</code></p> <p>Torchvision feature extractor, with the possibility to map features to an hypersphere.</p> <p>Parameters:</p> <ul> <li> model_name             (<code>str</code>)         \u2013          <p>Torchvision model function that will be evaluated, for example: torchvision.models.resnet18.</p> </li> <li> pretrained             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Whether to load the pretrained weights for the model.</p> </li> <li> pre_classifier             (<code>Module | None</code>, default:                 <code>None</code> )         \u2013          <p>Pre classifier as a torch.nn.Module. Defaults to nn.Identity() if None.</p> </li> <li> classifier             (<code>Module | None</code>, default:                 <code>None</code> )         \u2013          <p>Classifier as a torch.nn.Module. Defaults to nn.Identity() if None.</p> </li> <li> freeze             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Whether to freeze the feature extractor. Defaults to True.</p> </li> <li> hyperspherical             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to map features to an hypersphere. Defaults to False.</p> </li> <li> flatten_features             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Whether to flatten the features before the pre_classifier. Defaults to True.</p> </li> <li> checkpoint_path             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>Path to a checkpoint to load after the model is initialized. Defaults to None.</p> </li> <li> **torchvision_kwargs             (<code>Any</code>, default:                 <code>{}</code> )         \u2013          <p>Additional arguments to pass to the model function.</p> </li> </ul> Source code in <code>quadra/models/classification/backbones.py</code> <pre><code>def __init__(\n    self,\n    model_name: str,\n    pretrained: bool = True,\n    pre_classifier: nn.Module | None = None,\n    classifier: nn.Module | None = None,\n    freeze: bool = True,\n    hyperspherical: bool = False,\n    flatten_features: bool = True,\n    checkpoint_path: str | None = None,\n    **torchvision_kwargs: Any,\n):\n    self.pretrained = pretrained\n    model_function = models.__dict__[model_name]\n    features_extractor = model_function(pretrained=self.pretrained, progress=True, **torchvision_kwargs)\n    if checkpoint_path:\n        log.info(\"Loading checkpoint from %s\", checkpoint_path)\n        load_checkpoint(model=features_extractor, checkpoint_path=checkpoint_path)\n\n    # Remove classifier\n    features_extractor.classifier = nn.Identity()\n    super().__init__(\n        features_extractor=features_extractor,\n        pre_classifier=pre_classifier,\n        classifier=classifier,\n        freeze=freeze,\n        hyperspherical=hyperspherical,\n        flatten_features=flatten_features,\n    )\n</code></pre>"},{"location":"reference/quadra/models/classification/backbones.html","title":"backbones","text":""},{"location":"reference/quadra/models/classification/backbones.html#quadra.models.classification.backbones.TimmNetworkBuilder","title":"<code>TimmNetworkBuilder(model_name, pretrained=True, pre_classifier=None, classifier=None, freeze=True, hyperspherical=False, flatten_features=True, checkpoint_path=None, **timm_kwargs)</code>","text":"<p>             Bases: <code>BaseNetworkBuilder</code></p> <p>Torchvision feature extractor, with the possibility to map features to an hypersphere.</p> <p>Parameters:</p> <ul> <li> model_name             (<code>str</code>)         \u2013          <p>Timm model name</p> </li> <li> pretrained             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Whether to load the pretrained weights for the model.</p> </li> <li> pre_classifier             (<code>Module | None</code>, default:                 <code>None</code> )         \u2013          <p>Pre classifier as a torch.nn.Module. Defaults to nn.Identity() if None.</p> </li> <li> classifier             (<code>Module | None</code>, default:                 <code>None</code> )         \u2013          <p>Classifier as a torch.nn.Module. Defaults to nn.Identity() if None.</p> </li> <li> freeze             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Whether to freeze the feature extractor. Defaults to True.</p> </li> <li> hyperspherical             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to map features to an hypersphere. Defaults to False.</p> </li> <li> flatten_features             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Whether to flatten the features before the pre_classifier. Defaults to True.</p> </li> <li> checkpoint_path             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>Path to a checkpoint to load after the model is initialized. Defaults to None.</p> </li> <li> **timm_kwargs             (<code>Any</code>, default:                 <code>{}</code> )         \u2013          <p>Additional arguments to pass to timm.create_model</p> </li> </ul> Source code in <code>quadra/models/classification/backbones.py</code> <pre><code>def __init__(\n    self,\n    model_name: str,\n    pretrained: bool = True,\n    pre_classifier: nn.Module | None = None,\n    classifier: nn.Module | None = None,\n    freeze: bool = True,\n    hyperspherical: bool = False,\n    flatten_features: bool = True,\n    checkpoint_path: str | None = None,\n    **timm_kwargs: Any,\n):\n    self.pretrained = pretrained\n    features_extractor = timm.create_model(\n        model_name, pretrained=self.pretrained, num_classes=0, checkpoint_path=checkpoint_path, **timm_kwargs\n    )\n\n    super().__init__(\n        features_extractor=features_extractor,\n        pre_classifier=pre_classifier,\n        classifier=classifier,\n        freeze=freeze,\n        hyperspherical=hyperspherical,\n        flatten_features=flatten_features,\n    )\n</code></pre>"},{"location":"reference/quadra/models/classification/backbones.html#quadra.models.classification.backbones.TorchHubNetworkBuilder","title":"<code>TorchHubNetworkBuilder(repo_or_dir, model_name, pretrained=True, pre_classifier=None, classifier=None, freeze=True, hyperspherical=False, flatten_features=True, checkpoint_path=None, **torch_hub_kwargs)</code>","text":"<p>             Bases: <code>BaseNetworkBuilder</code></p> <p>TorchHub feature extractor, with the possibility to map features to an hypersphere.</p> <p>Parameters:</p> <ul> <li> repo_or_dir             (<code>str</code>)         \u2013          <p>The name of the repository or the path to the directory containing the model.</p> </li> <li> model_name             (<code>str</code>)         \u2013          <p>The name of the model within the repository.</p> </li> <li> pretrained             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Whether to load the pretrained weights for the model.</p> </li> <li> pre_classifier             (<code>Module | None</code>, default:                 <code>None</code> )         \u2013          <p>Pre classifier as a torch.nn.Module. Defaults to nn.Identity() if None.</p> </li> <li> classifier             (<code>Module | None</code>, default:                 <code>None</code> )         \u2013          <p>Classifier as a torch.nn.Module. Defaults to nn.Identity() if None.</p> </li> <li> freeze             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Whether to freeze the feature extractor. Defaults to True.</p> </li> <li> hyperspherical             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to map features to an hypersphere. Defaults to False.</p> </li> <li> flatten_features             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Whether to flatten the features before the pre_classifier. Defaults to True.</p> </li> <li> checkpoint_path             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>Path to a checkpoint to load after the model is initialized. Defaults to None.</p> </li> <li> **torch_hub_kwargs             (<code>Any</code>, default:                 <code>{}</code> )         \u2013          <p>Additional arguments to pass to torch.hub.load</p> </li> </ul> Source code in <code>quadra/models/classification/backbones.py</code> <pre><code>def __init__(\n    self,\n    repo_or_dir: str,\n    model_name: str,\n    pretrained: bool = True,\n    pre_classifier: nn.Module | None = None,\n    classifier: nn.Module | None = None,\n    freeze: bool = True,\n    hyperspherical: bool = False,\n    flatten_features: bool = True,\n    checkpoint_path: str | None = None,\n    **torch_hub_kwargs: Any,\n):\n    self.pretrained = pretrained\n    features_extractor = torch.hub.load(\n        repo_or_dir=repo_or_dir, model=model_name, pretrained=self.pretrained, **torch_hub_kwargs\n    )\n    if checkpoint_path:\n        log.info(\"Loading checkpoint from %s\", checkpoint_path)\n        load_checkpoint(model=features_extractor, checkpoint_path=checkpoint_path)\n\n    super().__init__(\n        features_extractor=features_extractor,\n        pre_classifier=pre_classifier,\n        classifier=classifier,\n        freeze=freeze,\n        hyperspherical=hyperspherical,\n        flatten_features=flatten_features,\n    )\n</code></pre>"},{"location":"reference/quadra/models/classification/backbones.html#quadra.models.classification.backbones.TorchVisionNetworkBuilder","title":"<code>TorchVisionNetworkBuilder(model_name, pretrained=True, pre_classifier=None, classifier=None, freeze=True, hyperspherical=False, flatten_features=True, checkpoint_path=None, **torchvision_kwargs)</code>","text":"<p>             Bases: <code>BaseNetworkBuilder</code></p> <p>Torchvision feature extractor, with the possibility to map features to an hypersphere.</p> <p>Parameters:</p> <ul> <li> model_name             (<code>str</code>)         \u2013          <p>Torchvision model function that will be evaluated, for example: torchvision.models.resnet18.</p> </li> <li> pretrained             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Whether to load the pretrained weights for the model.</p> </li> <li> pre_classifier             (<code>Module | None</code>, default:                 <code>None</code> )         \u2013          <p>Pre classifier as a torch.nn.Module. Defaults to nn.Identity() if None.</p> </li> <li> classifier             (<code>Module | None</code>, default:                 <code>None</code> )         \u2013          <p>Classifier as a torch.nn.Module. Defaults to nn.Identity() if None.</p> </li> <li> freeze             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Whether to freeze the feature extractor. Defaults to True.</p> </li> <li> hyperspherical             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to map features to an hypersphere. Defaults to False.</p> </li> <li> flatten_features             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Whether to flatten the features before the pre_classifier. Defaults to True.</p> </li> <li> checkpoint_path             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>Path to a checkpoint to load after the model is initialized. Defaults to None.</p> </li> <li> **torchvision_kwargs             (<code>Any</code>, default:                 <code>{}</code> )         \u2013          <p>Additional arguments to pass to the model function.</p> </li> </ul> Source code in <code>quadra/models/classification/backbones.py</code> <pre><code>def __init__(\n    self,\n    model_name: str,\n    pretrained: bool = True,\n    pre_classifier: nn.Module | None = None,\n    classifier: nn.Module | None = None,\n    freeze: bool = True,\n    hyperspherical: bool = False,\n    flatten_features: bool = True,\n    checkpoint_path: str | None = None,\n    **torchvision_kwargs: Any,\n):\n    self.pretrained = pretrained\n    model_function = models.__dict__[model_name]\n    features_extractor = model_function(pretrained=self.pretrained, progress=True, **torchvision_kwargs)\n    if checkpoint_path:\n        log.info(\"Loading checkpoint from %s\", checkpoint_path)\n        load_checkpoint(model=features_extractor, checkpoint_path=checkpoint_path)\n\n    # Remove classifier\n    features_extractor.classifier = nn.Identity()\n    super().__init__(\n        features_extractor=features_extractor,\n        pre_classifier=pre_classifier,\n        classifier=classifier,\n        freeze=freeze,\n        hyperspherical=hyperspherical,\n        flatten_features=flatten_features,\n    )\n</code></pre>"},{"location":"reference/quadra/models/classification/base.html","title":"base","text":""},{"location":"reference/quadra/models/classification/base.html#quadra.models.classification.base.BaseNetworkBuilder","title":"<code>BaseNetworkBuilder(features_extractor, pre_classifier=None, classifier=None, freeze=True, hyperspherical=False, flatten_features=True)</code>","text":"<p>             Bases: <code>Module</code></p> <p>Baseline Feature Extractor, with the possibility to map features to an hypersphere.     If hypershperical is True the classifier is ignored.</p> <p>Parameters:</p> <ul> <li> features_extractor             (<code>Module</code>)         \u2013          <p>Feature extractor as a toch.nn.Module.</p> </li> <li> pre_classifier             (<code>Module | None</code>, default:                 <code>None</code> )         \u2013          <p>Pre classifier as a torch.nn.Module. Defaults to nn.Identity() if None.</p> </li> <li> classifier             (<code>Module | None</code>, default:                 <code>None</code> )         \u2013          <p>Classifier as a torch.nn.Module. Defaults to nn.Identity() if None.</p> </li> <li> freeze             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Whether to freeze the feature extractor. Defaults to True.</p> </li> <li> hyperspherical             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to map features to an hypersphere. Defaults to False.</p> </li> <li> flatten_features             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Whether to flatten the features before the pre_classifier. May be required if your model is outputting a feature map rather than a vector. Defaults to True.</p> </li> </ul> Source code in <code>quadra/models/classification/base.py</code> <pre><code>def __init__(\n    self,\n    features_extractor: nn.Module,\n    pre_classifier: nn.Module | None = None,\n    classifier: nn.Module | None = None,\n    freeze: bool = True,\n    hyperspherical: bool = False,\n    flatten_features: bool = True,\n):\n    super().__init__()\n    if pre_classifier is None:\n        pre_classifier = nn.Identity()\n\n    if classifier is None:\n        classifier = nn.Identity()\n\n    self.features_extractor = features_extractor\n    self.freeze = freeze\n    self.hyperspherical = hyperspherical\n    self.pre_classifier = pre_classifier\n    self.classifier = classifier\n    self.flatten: bool = False\n    self._hyperspherical: bool = False\n    self.l2: L2Norm | None = None\n    self.flatten_features = flatten_features\n\n    self.freeze = freeze\n    self.hyperspherical = hyperspherical\n\n    if self.freeze:\n        for p in self.features_extractor.parameters():\n            p.requires_grad = False\n</code></pre>"},{"location":"reference/quadra/models/classification/base.html#quadra.models.classification.base.BaseNetworkBuilder.freeze","title":"<code>freeze: bool</code>  <code>property</code> <code>writable</code>","text":"<p>Whether to freeze the feature extractor.</p>"},{"location":"reference/quadra/models/classification/base.html#quadra.models.classification.base.BaseNetworkBuilder.hyperspherical","title":"<code>hyperspherical: bool</code>  <code>property</code> <code>writable</code>","text":"<p>Whether to map the extracted features into an hypersphere.</p>"},{"location":"reference/quadra/modules/index.html","title":"modules","text":""},{"location":"reference/quadra/modules/index.html#submodules","title":"Submodules","text":"<ul> <li>classification</li> <li>ssl</li> </ul>"},{"location":"reference/quadra/modules/index.html#python-files","title":"Python Files","text":"<ul> <li>backbone.py</li> <li>base.py </li> </ul>"},{"location":"reference/quadra/modules/backbone.html","title":"backbone","text":""},{"location":"reference/quadra/modules/backbone.html#quadra.modules.backbone.create_smp_backbone","title":"<code>create_smp_backbone(arch, encoder_name, freeze_encoder=False, in_channels=3, num_classes=0, **kwargs)</code>","text":"<p>Create Segmentation.models.pytorch model backbone Args:     arch: architecture name     encoder_name: architecture name     freeze_encoder: freeze encoder or not     in_channels: number of input channels     num_classes: number of classes     **kwargs: extra arguments for model (for example classification head).</p> Source code in <code>quadra/modules/backbone.py</code> <pre><code>def create_smp_backbone(\n    arch: str,\n    encoder_name: str,\n    freeze_encoder: bool = False,\n    in_channels: int = 3,\n    num_classes: int = 0,\n    **kwargs: Any,\n):\n\"\"\"Create Segmentation.models.pytorch model backbone\n    Args:\n        arch: architecture name\n        encoder_name: architecture name\n        freeze_encoder: freeze encoder or not\n        in_channels: number of input channels\n        num_classes: number of classes\n        **kwargs: extra arguments for model (for example classification head).\n    \"\"\"\n    model = smp.create_model(\n        arch=arch, encoder_name=encoder_name, in_channels=in_channels, classes=num_classes, **kwargs\n    )\n    if freeze_encoder:\n        for child in model.encoder.children():\n            for param in child.parameters():\n                param.requires_grad = False\n    return model\n</code></pre>"},{"location":"reference/quadra/modules/base.html","title":"base","text":""},{"location":"reference/quadra/modules/base.html#quadra.modules.base.BaseLightningModule","title":"<code>BaseLightningModule(model, optimizer=None, lr_scheduler=None, lr_scheduler_interval='epoch')</code>","text":"<p>             Bases: <code>LightningModule</code></p> <p>Base lightning module.</p> <p>Parameters:</p> <ul> <li> model             (<code>Module</code>)         \u2013          <p>Network Module used for extract features</p> </li> <li> optimizer             (<code>Optimizer | None</code>, default:                 <code>None</code> )         \u2013          <p>optimizer of the training. If None a default Adam is used.</p> </li> <li> lr_scheduler             (<code>object | None</code>, default:                 <code>None</code> )         \u2013          <p>lr scheduler. If None a default ReduceLROnPlateau is used.</p> </li> </ul> Source code in <code>quadra/modules/base.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    optimizer: Optimizer | None = None,\n    lr_scheduler: object | None = None,\n    lr_scheduler_interval: str | None = \"epoch\",\n):\n    super().__init__()\n    self.model = ModelSignatureWrapper(model)\n    self.optimizer = optimizer\n    self.schedulers = lr_scheduler\n    self.lr_scheduler_interval = lr_scheduler_interval\n</code></pre>"},{"location":"reference/quadra/modules/base.html#quadra.modules.base.BaseLightningModule.configure_optimizers","title":"<code>configure_optimizers()</code>","text":"<p>Get default optimizer if not passed a value.</p> <p>Returns:</p> <ul> <li> <code>tuple[list[Any], list[dict[str, Any]]]</code>         \u2013          <p>optimizer and lr scheduler as Tuple containing a list of optimizers and a list of lr schedulers</p> </li> </ul> Source code in <code>quadra/modules/base.py</code> <pre><code>def configure_optimizers(self) -&gt; tuple[list[Any], list[dict[str, Any]]]:\n\"\"\"Get default optimizer if not passed a value.\n\n    Returns:\n        optimizer and lr scheduler as Tuple containing a list of optimizers and a list of lr schedulers\n    \"\"\"\n    # get default optimizer\n    if getattr(self, \"optimizer\", None) is None or not self.optimizer:\n        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=3e-4)\n\n    # get default scheduler\n    if getattr(self, \"schedulers\", None) is None or not self.schedulers:\n        self.schedulers = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.5, patience=30)\n\n    lr_scheduler_conf = {\n        \"scheduler\": self.schedulers,\n        \"interval\": self.lr_scheduler_interval,\n        \"monitor\": \"val_loss\",\n        \"strict\": False,\n    }\n    return [self.optimizer], [lr_scheduler_conf]\n</code></pre>"},{"location":"reference/quadra/modules/base.html#quadra.modules.base.BaseLightningModule.forward","title":"<code>forward(x)</code>","text":"<p>Forward method Args:     x: input tensor.</p> <p>Returns:</p> <ul> <li> <code>Tensor</code>         \u2013          <p>model inference</p> </li> </ul> Source code in <code>quadra/modules/base.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"Forward method\n    Args:\n        x: input tensor.\n\n    Returns:\n        model inference\n    \"\"\"\n    return self.model(x)\n</code></pre>"},{"location":"reference/quadra/modules/base.html#quadra.modules.base.BaseLightningModule.optimizer_zero_grad","title":"<code>optimizer_zero_grad(epoch, batch_idx, optimizer, optimizer_idx=0)</code>","text":"<p>Redefine optimizer zero grad.</p> Source code in <code>quadra/modules/base.py</code> <pre><code>def optimizer_zero_grad(self, epoch, batch_idx, optimizer, optimizer_idx: int = 0):\n\"\"\"Redefine optimizer zero grad.\"\"\"\n    optimizer.zero_grad(set_to_none=True)\n</code></pre>"},{"location":"reference/quadra/modules/base.html#quadra.modules.base.SSLModule","title":"<code>SSLModule(model, criterion, classifier=None, optimizer=None, lr_scheduler=None, lr_scheduler_interval='epoch')</code>","text":"<p>             Bases: <code>BaseLightningModule</code></p> <p>Base module for self supervised learning.</p> <p>Parameters:</p> <ul> <li> model             (<code>Module</code>)         \u2013          <p>Network Module used for extract features</p> </li> <li> criterion             (<code>Module</code>)         \u2013          <p>SSL loss to be applied</p> </li> <li> classifier             (<code>ClassifierMixin | None</code>, default:                 <code>None</code> )         \u2013          <p>Standard sklearn classifiers</p> </li> <li> optimizer             (<code>Optimizer | None</code>, default:                 <code>None</code> )         \u2013          <p>optimizer of the training. If None a default Adam is used.</p> </li> <li> lr_scheduler             (<code>object | None</code>, default:                 <code>None</code> )         \u2013          <p>lr scheduler. If None a default ReduceLROnPlateau is used.</p> </li> </ul> Source code in <code>quadra/modules/base.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    criterion: nn.Module,\n    classifier: sklearn.base.ClassifierMixin | None = None,\n    optimizer: Optimizer | None = None,\n    lr_scheduler: object | None = None,\n    lr_scheduler_interval: str | None = \"epoch\",\n):\n    super().__init__(model, optimizer, lr_scheduler, lr_scheduler_interval)\n    self.criterion = criterion\n    self.classifier_train_loader: torch.utils.data.DataLoader | None\n    if classifier is None:\n        self.classifier = LogisticRegression(max_iter=10000, n_jobs=8, random_state=42)\n    else:\n        self.classifier = classifier\n\n    self.val_acc = torchmetrics.Accuracy()\n</code></pre>"},{"location":"reference/quadra/modules/base.html#quadra.modules.base.SSLModule.calculate_accuracy","title":"<code>calculate_accuracy(batch)</code>","text":"<p>Calculate accuracy on a batch of data.</p> Source code in <code>quadra/modules/base.py</code> <pre><code>def calculate_accuracy(self, batch):\n\"\"\"Calculate accuracy on a batch of data.\"\"\"\n    images, labels = batch\n    with torch.no_grad():\n        embedding = self.model(images).cpu().numpy()\n\n    predictions = self.classifier.predict(embedding)\n    labels = labels.detach()\n    acc = self.val_acc(torch.tensor(predictions, device=self.device), labels)\n\n    return acc\n</code></pre>"},{"location":"reference/quadra/modules/base.html#quadra.modules.base.SSLModule.fit_estimator","title":"<code>fit_estimator()</code>","text":"<p>Fit a classifier on the embeddings extracted from the current trained model.</p> Source code in <code>quadra/modules/base.py</code> <pre><code>def fit_estimator(self):\n\"\"\"Fit a classifier on the embeddings extracted from the current trained model.\"\"\"\n    targets = []\n    train_embeddings = []\n    self.model.eval()\n    with torch.no_grad():\n        for im, target in self.classifier_train_loader:\n            emb = self.model(im.to(self.device))\n            targets.append(target)\n            train_embeddings.append(emb)\n    targets = torch.cat(targets, dim=0).cpu().numpy()\n    train_embeddings = torch.cat(train_embeddings, dim=0).cpu().numpy()\n    self.classifier.fit(train_embeddings, targets)\n</code></pre>"},{"location":"reference/quadra/modules/base.html#quadra.modules.base.SegmentationModel","title":"<code>SegmentationModel(model, loss_fun, optimizer=None, lr_scheduler=None)</code>","text":"<p>             Bases: <code>BaseLightningModule</code></p> <p>Generic segmentation model.</p> <p>Parameters:</p> <ul> <li> model             (<code>Module</code>)         \u2013          <p>segmentation model to be used.</p> </li> <li> loss_fun             (<code>Callable</code>)         \u2013          <p>loss function to be used.</p> </li> <li> optimizer             (<code>Optimizer | None</code>, default:                 <code>None</code> )         \u2013          <p>Optimizer to be used. Defaults to None.</p> </li> <li> lr_scheduler             (<code>object | None</code>, default:                 <code>None</code> )         \u2013          <p>lr scheduler to be used. Defaults to None.</p> </li> </ul> Source code in <code>quadra/modules/base.py</code> <pre><code>def __init__(\n    self,\n    model: torch.nn.Module,\n    loss_fun: Callable,\n    optimizer: Optimizer | None = None,\n    lr_scheduler: object | None = None,\n):\n    super().__init__(model, optimizer, lr_scheduler)\n    self.loss_fun = loss_fun\n</code></pre>"},{"location":"reference/quadra/modules/base.html#quadra.modules.base.SegmentationModel.compute_loss","title":"<code>compute_loss(pred_masks, target_masks)</code>","text":"<p>Compute loss Args:     pred_masks: predicted masks     target_masks: target masks.</p> <p>Returns:</p> <ul> <li> <code>Tensor</code>         \u2013          <p>The computed loss</p> </li> </ul> Source code in <code>quadra/modules/base.py</code> <pre><code>def compute_loss(self, pred_masks: torch.Tensor, target_masks: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"Compute loss\n    Args:\n        pred_masks: predicted masks\n        target_masks: target masks.\n\n    Returns:\n        The computed loss\n\n    \"\"\"\n    loss = self.loss_fun(pred_masks, target_masks)\n    return loss\n</code></pre>"},{"location":"reference/quadra/modules/base.html#quadra.modules.base.SegmentationModel.forward","title":"<code>forward(x)</code>","text":"<p>Forward method Args:     x: input tensor.</p> <p>Returns:</p> <ul> <li> <code>Tensor</code>         \u2013          <p>model inference</p> </li> </ul> Source code in <code>quadra/modules/base.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"Forward method\n    Args:\n        x: input tensor.\n\n    Returns:\n        model inference\n    \"\"\"\n    x = self.model(x)\n    return x\n</code></pre>"},{"location":"reference/quadra/modules/base.html#quadra.modules.base.SegmentationModel.predict_step","title":"<code>predict_step(batch, batch_idx, dataloader_idx=None)</code>","text":"<p>Predict step.</p> Source code in <code>quadra/modules/base.py</code> <pre><code>def predict_step(\n    self,\n    batch: tuple[torch.Tensor, torch.Tensor, torch.Tensor],\n    batch_idx: int,\n    dataloader_idx: int | None = None,\n) -&gt; Any:\n\"\"\"Predict step.\"\"\"\n    # pylint: disable=unused-argument\n    images, masks, labels = batch\n    pred_masks = self(images)\n    return images.cpu(), masks.cpu(), pred_masks.cpu(), labels.cpu()\n</code></pre>"},{"location":"reference/quadra/modules/base.html#quadra.modules.base.SegmentationModel.step","title":"<code>step(batch)</code>","text":"<p>Compute loss Args:     batch: batch.</p> <p>Returns:</p> <ul> <li> <code>tuple[Tensor, Tensor]</code>         \u2013          <p>Prediction and target masks</p> </li> </ul> Source code in <code>quadra/modules/base.py</code> <pre><code>def step(self, batch: tuple[torch.Tensor, torch.Tensor, torch.Tensor]) -&gt; tuple[torch.Tensor, torch.Tensor]:\n\"\"\"Compute loss\n    Args:\n        batch: batch.\n\n    Returns:\n        Prediction and target masks\n    \"\"\"\n    images, target_masks, _ = batch\n    pred_masks = self(images)\n    if len(pred_masks.shape) == 3:\n        pred_masks = pred_masks.unsqueeze(1)\n    if len(target_masks.shape) == 3:\n        target_masks = target_masks.unsqueeze(1)\n    assert pred_masks.shape == target_masks.shape\n\n    return pred_masks, target_masks\n</code></pre>"},{"location":"reference/quadra/modules/base.html#quadra.modules.base.SegmentationModel.test_step","title":"<code>test_step(batch, batch_idx)</code>","text":"<p>Test step.</p> Source code in <code>quadra/modules/base.py</code> <pre><code>def test_step(self, batch: tuple[torch.Tensor, torch.Tensor, torch.Tensor], batch_idx: int):\n\"\"\"Test step.\"\"\"\n    # pylint: disable=unused-argument\n    pred_masks, target_masks = self.step(batch)\n    loss = self.compute_loss(pred_masks, target_masks)\n    self.log_dict(\n        {\"test_loss\": loss},\n        on_step=True,\n        on_epoch=True,\n        prog_bar=True,\n    )\n    return loss\n</code></pre>"},{"location":"reference/quadra/modules/base.html#quadra.modules.base.SegmentationModel.training_step","title":"<code>training_step(batch, batch_idx)</code>","text":"<p>Training step.</p> Source code in <code>quadra/modules/base.py</code> <pre><code>def training_step(self, batch: tuple[torch.Tensor, torch.Tensor, torch.Tensor], batch_idx: int):\n\"\"\"Training step.\"\"\"\n    # pylint: disable=unused-argument\n    pred_masks, target_masks = self.step(batch)\n    loss = self.compute_loss(pred_masks, target_masks)\n    self.log_dict(\n        {\"loss\": loss},\n        on_step=True,\n        on_epoch=True,\n        prog_bar=True,\n    )\n    return loss\n</code></pre>"},{"location":"reference/quadra/modules/base.html#quadra.modules.base.SegmentationModel.validation_step","title":"<code>validation_step(batch, batch_idx)</code>","text":"<p>Validation step.</p> Source code in <code>quadra/modules/base.py</code> <pre><code>def validation_step(self, batch: tuple[torch.Tensor, torch.Tensor, torch.Tensor], batch_idx):\n\"\"\"Validation step.\"\"\"\n    # pylint: disable=unused-argument\n    pred_masks, target_masks = self.step(batch)\n    loss = self.compute_loss(pred_masks, target_masks)\n    self.log_dict(\n        {\"val_loss\": loss},\n        on_step=True,\n        on_epoch=True,\n        prog_bar=True,\n    )\n    return loss\n</code></pre>"},{"location":"reference/quadra/modules/base.html#quadra.modules.base.SegmentationModelMulticlass","title":"<code>SegmentationModelMulticlass(model, loss_fun, optimizer=None, lr_scheduler=None)</code>","text":"<p>             Bases: <code>SegmentationModel</code></p> <p>Generic multiclass segmentation model.</p> <p>Parameters:</p> <ul> <li> model             (<code>Module</code>)         \u2013          <p>segmentation model to be used.</p> </li> <li> loss_fun             (<code>Callable</code>)         \u2013          <p>loss function to be used.</p> </li> <li> optimizer             (<code>Optimizer | None</code>, default:                 <code>None</code> )         \u2013          <p>Optimizer to be used. Defaults to None.</p> </li> <li> lr_scheduler             (<code>object | None</code>, default:                 <code>None</code> )         \u2013          <p>lr scheduler to be used. Defaults to None.</p> </li> </ul> Source code in <code>quadra/modules/base.py</code> <pre><code>def __init__(\n    self,\n    model: torch.nn.Module,\n    loss_fun: Callable,\n    optimizer: Optimizer | None = None,\n    lr_scheduler: object | None = None,\n):\n    super().__init__(model=model, optimizer=optimizer, lr_scheduler=lr_scheduler, loss_fun=loss_fun)\n</code></pre>"},{"location":"reference/quadra/modules/base.html#quadra.modules.base.SegmentationModelMulticlass.step","title":"<code>step(batch)</code>","text":"<p>Compute step Args:     batch: batch.</p> <p>Returns:</p> <ul> <li> <code>tuple[Tensor, Tensor]</code>         \u2013          <p>prediction, target</p> </li> </ul> Source code in <code>quadra/modules/base.py</code> <pre><code>def step(self, batch: tuple[torch.Tensor, torch.Tensor, torch.Tensor]) -&gt; tuple[torch.Tensor, torch.Tensor]:\n\"\"\"Compute step\n    Args:\n        batch: batch.\n\n    Returns:\n        prediction, target\n\n    \"\"\"\n    images, target_masks, _ = batch\n    pred_masks = self(images)\n\n    return pred_masks, target_masks\n</code></pre>"},{"location":"reference/quadra/modules/classification/index.html","title":"classification","text":""},{"location":"reference/quadra/modules/classification/index.html#quadra.modules.classification.ClassificationModule","title":"<code>ClassificationModule(model, criterion, optimizer=None, lr_scheduler=None, lr_scheduler_interval='epoch', gradcam=False)</code>","text":"<p>             Bases: <code>BaseLightningModule</code></p> <p>Lightning module for classification tasks.</p> <p>Parameters:</p> <ul> <li> model             (<code>Module</code>)         \u2013          <p>Feature extractor as PyTorch <code>torch.nn.Module</code></p> </li> <li> criterion             (<code>Module</code>)         \u2013          <p>the loss to be applied as a PyTorch <code>torch.nn.Module</code>.</p> </li> <li> optimizer             (<code>None | Optimizer</code>, default:                 <code>None</code> )         \u2013          <p>optimizer of the training. Defaults to None.</p> </li> <li> lr_scheduler             (<code>None | object</code>, default:                 <code>None</code> )         \u2013          <p>Pytorch learning rate scheduler. If None a default ReduceLROnPlateau is used. Defaults to None.</p> </li> <li> lr_scheduler_interval             (<code>str | None</code>, default:                 <code>'epoch'</code> )         \u2013          <p>the learning rate scheduler interval. Defaults to \"epoch\".</p> </li> <li> gradcam             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to compute gradcam during prediction step</p> </li> </ul> Source code in <code>quadra/modules/classification/base.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    criterion: nn.Module,\n    optimizer: None | optim.Optimizer = None,\n    lr_scheduler: None | object = None,\n    lr_scheduler_interval: str | None = \"epoch\",\n    gradcam: bool = False,\n):\n    super().__init__(model, optimizer, lr_scheduler, lr_scheduler_interval)\n\n    self.criterion = criterion\n    self.gradcam = gradcam\n    self.train_acc = torchmetrics.Accuracy()\n    self.val_acc = torchmetrics.Accuracy()\n    self.test_acc = torchmetrics.Accuracy()\n    self.cam: GradCAM | None = None\n    self.grad_rollout: VitAttentionGradRollout | None = None\n\n    if not isinstance(self.model.features_extractor, timm.models.resnet.ResNet) and not is_vision_transformer(\n        cast(BaseNetworkBuilder, self.model).features_extractor\n    ):\n        log.warning(\n            \"Backbone not compatible with gradcam. Only timm ResNets, timm ViTs and TorchHub dinoViTs supported\",\n        )\n        self.gradcam = False\n\n    self.original_requires_grads: list[bool] = []\n</code></pre>"},{"location":"reference/quadra/modules/classification/index.html#quadra.modules.classification.ClassificationModule.on_predict_end","title":"<code>on_predict_end()</code>","text":"<p>If we computed gradcam, requires_grad values are reset to original value.</p> Source code in <code>quadra/modules/classification/base.py</code> <pre><code>def on_predict_end(self) -&gt; None:\n\"\"\"If we computed gradcam, requires_grad values are reset to original value.\"\"\"\n    if self.gradcam:\n        # Get back to initial state\n        for i, p in enumerate(self.model.parameters()):\n            p.requires_grad = self.original_requires_grads[i]\n\n        # We are using GradCAM package only for resnets at the moment\n        if isinstance(self.model.features_extractor, timm.models.resnet.ResNet) and self.cam is not None:\n            # Needed to solve jitting bug\n            self.cam.activations_and_grads.release()\n        elif (\n            is_vision_transformer(cast(BaseNetworkBuilder, self.model).features_extractor)\n            and self.grad_rollout is not None\n        ):\n            for handle in self.grad_rollout.f_hook_handles:\n                handle.remove()\n            for handle in self.grad_rollout.b_hook_handles:\n                handle.remove()\n\n    return super().on_predict_end()\n</code></pre>"},{"location":"reference/quadra/modules/classification/index.html#quadra.modules.classification.ClassificationModule.on_predict_start","title":"<code>on_predict_start()</code>","text":"<p>If gradcam, prepares gradcam and saves params requires_grad state.</p> Source code in <code>quadra/modules/classification/base.py</code> <pre><code>def on_predict_start(self) -&gt; None:\n\"\"\"If gradcam, prepares gradcam and saves params requires_grad state.\"\"\"\n    if self.gradcam:\n        # Saving params requires_grad state\n        for p in self.model.parameters():\n            self.original_requires_grads.append(p.requires_grad)\n        self.prepare_gradcam()\n\n    return super().on_predict_start()\n</code></pre>"},{"location":"reference/quadra/modules/classification/index.html#quadra.modules.classification.ClassificationModule.predict_step","title":"<code>predict_step(batch, batch_idx, dataloader_idx=0)</code>","text":"<p>Prediction step.</p> <p>Parameters:</p> <ul> <li> batch             (<code>Any</code>)         \u2013          <p>Tuple composed by (image, target)</p> </li> <li> batch_idx             (<code>int</code>)         \u2013          <p>Batch index</p> </li> <li> dataloader_idx             (<code>int</code>, default:                 <code>0</code> )         \u2013          <p>Dataloader index</p> </li> </ul> Source code in <code>quadra/modules/classification/base.py</code> <pre><code>def predict_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -&gt; Any:\n\"\"\"Prediction step.\n\n    Args:\n        batch: Tuple composed by (image, target)\n        batch_idx: Batch index\n        dataloader_idx: Dataloader index\n    Returns:\n        Tuple containing:\n            predicted_classes: indexes of predicted classes\n            grayscale_cam: gray scale gradcams\n    \"\"\"\n    im, _ = batch\n    outputs = self(im)\n    probs = torch.softmax(outputs, dim=1)\n    predicted_classes = torch.max(probs, dim=1).indices.tolist()\n    if self.gradcam:\n        # inference_mode set to false because gradcam needs gradients\n        with torch.inference_mode(False):\n            im = im.clone()\n\n            if isinstance(self.model.features_extractor, timm.models.resnet.ResNet) and self.cam:\n                grayscale_cam = self.cam(input_tensor=im, targets=None)\n            elif (\n                is_vision_transformer(cast(BaseNetworkBuilder, self.model).features_extractor) and self.grad_rollout\n            ):\n                grayscale_cam_low_res = self.grad_rollout(input_tensor=im, targets_list=predicted_classes)\n                orig_shape = grayscale_cam_low_res.shape\n                new_shape = (orig_shape[0], im.shape[2], im.shape[3])\n                zoom_factors = tuple(np.array(new_shape) / np.array(orig_shape))\n                grayscale_cam = ndimage.zoom(grayscale_cam_low_res, zoom_factors, order=1)\n    else:\n        grayscale_cam = None\n    return predicted_classes, grayscale_cam, torch.max(probs, dim=1)[0].tolist()\n</code></pre>"},{"location":"reference/quadra/modules/classification/index.html#quadra.modules.classification.ClassificationModule.prepare_gradcam","title":"<code>prepare_gradcam()</code>","text":"<p>Instantiate gradcam handlers.</p> Source code in <code>quadra/modules/classification/base.py</code> <pre><code>def prepare_gradcam(self) -&gt; None:\n\"\"\"Instantiate gradcam handlers.\"\"\"\n    if isinstance(self.model.features_extractor, timm.models.resnet.ResNet):\n        target_layers = [cast(BaseNetworkBuilder, self.model).features_extractor.layer4[-1]]\n\n        self.cam = GradCAM(\n            model=self.model,\n            target_layers=target_layers,\n        )\n        # Activating gradients\n        for p in self.model.features_extractor.layer4[-1].parameters():\n            p.requires_grad = True\n    elif is_vision_transformer(cast(BaseNetworkBuilder, self.model).features_extractor):\n        self.grad_rollout = VitAttentionGradRollout(self.model)\n    else:\n        log.warning(\"Gradcam not implemented for this backbone, it won't be computed\")\n        self.original_requires_grads.clear()\n        self.gradcam = False\n</code></pre>"},{"location":"reference/quadra/modules/classification/index.html#quadra.modules.classification.MultilabelClassificationModule","title":"<code>MultilabelClassificationModule(model, criterion, optimizer=None, lr_scheduler=None, lr_scheduler_interval='epoch', gradcam=False)</code>","text":"<p>             Bases: <code>BaseLightningModule</code></p> <p>SklearnClassification model: train a generic SklearnClassification model for a multilabel problem.</p> <p>Parameters:</p> <ul> <li> model             (<code>Sequential</code>)         \u2013          <p>Feature extractor as PyTorch <code>torch.nn.Module</code></p> </li> <li> criterion             (<code>Module</code>)         \u2013          <p>the loss to be applied as a PyTorch <code>torch.nn.Module</code>.</p> </li> <li> optimizer             (<code>None | Optimizer</code>, default:                 <code>None</code> )         \u2013          <p>optimizer of the training. Defaults to None.</p> </li> <li> lr_scheduler             (<code>None | object</code>, default:                 <code>None</code> )         \u2013          <p>Pytorch learning rate scheduler. If None a default ReduceLROnPlateau is used. Defaults to None.</p> </li> <li> lr_scheduler_interval             (<code>str | None</code>, default:                 <code>'epoch'</code> )         \u2013          <p>the learning rate scheduler interval. Defaults to \"epoch\".</p> </li> </ul> Source code in <code>quadra/modules/classification/base.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Sequential,\n    criterion: nn.Module,\n    optimizer: None | optim.Optimizer = None,\n    lr_scheduler: None | object = None,\n    lr_scheduler_interval: str | None = \"epoch\",\n    gradcam: bool = False,\n):\n    super().__init__(model, optimizer, lr_scheduler, lr_scheduler_interval)\n    self.criterion = criterion\n    self.gradcam = gradcam\n\n    # TODO: can we use gradcam with more backbones?\n    if self.gradcam:\n        if not isinstance(model[0].features_extractor, timm.models.resnet.ResNet):\n            log.warning(\n                \"Backbone must be compatible with gradcam, at the moment only ResNets supported, disabling gradcam\"\n            )\n            self.gradcam = False\n        else:\n            target_layers = [model[0].features_extractor.layer4[-1]]\n            self.cam = GradCAM(model=model, target_layers=target_layers)\n</code></pre>"},{"location":"reference/quadra/modules/classification/base.html","title":"base","text":""},{"location":"reference/quadra/modules/classification/base.html#quadra.modules.classification.base.ClassificationModule","title":"<code>ClassificationModule(model, criterion, optimizer=None, lr_scheduler=None, lr_scheduler_interval='epoch', gradcam=False)</code>","text":"<p>             Bases: <code>BaseLightningModule</code></p> <p>Lightning module for classification tasks.</p> <p>Parameters:</p> <ul> <li> model             (<code>Module</code>)         \u2013          <p>Feature extractor as PyTorch <code>torch.nn.Module</code></p> </li> <li> criterion             (<code>Module</code>)         \u2013          <p>the loss to be applied as a PyTorch <code>torch.nn.Module</code>.</p> </li> <li> optimizer             (<code>None | Optimizer</code>, default:                 <code>None</code> )         \u2013          <p>optimizer of the training. Defaults to None.</p> </li> <li> lr_scheduler             (<code>None | object</code>, default:                 <code>None</code> )         \u2013          <p>Pytorch learning rate scheduler. If None a default ReduceLROnPlateau is used. Defaults to None.</p> </li> <li> lr_scheduler_interval             (<code>str | None</code>, default:                 <code>'epoch'</code> )         \u2013          <p>the learning rate scheduler interval. Defaults to \"epoch\".</p> </li> <li> gradcam             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to compute gradcam during prediction step</p> </li> </ul> Source code in <code>quadra/modules/classification/base.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    criterion: nn.Module,\n    optimizer: None | optim.Optimizer = None,\n    lr_scheduler: None | object = None,\n    lr_scheduler_interval: str | None = \"epoch\",\n    gradcam: bool = False,\n):\n    super().__init__(model, optimizer, lr_scheduler, lr_scheduler_interval)\n\n    self.criterion = criterion\n    self.gradcam = gradcam\n    self.train_acc = torchmetrics.Accuracy()\n    self.val_acc = torchmetrics.Accuracy()\n    self.test_acc = torchmetrics.Accuracy()\n    self.cam: GradCAM | None = None\n    self.grad_rollout: VitAttentionGradRollout | None = None\n\n    if not isinstance(self.model.features_extractor, timm.models.resnet.ResNet) and not is_vision_transformer(\n        cast(BaseNetworkBuilder, self.model).features_extractor\n    ):\n        log.warning(\n            \"Backbone not compatible with gradcam. Only timm ResNets, timm ViTs and TorchHub dinoViTs supported\",\n        )\n        self.gradcam = False\n\n    self.original_requires_grads: list[bool] = []\n</code></pre>"},{"location":"reference/quadra/modules/classification/base.html#quadra.modules.classification.base.ClassificationModule.on_predict_end","title":"<code>on_predict_end()</code>","text":"<p>If we computed gradcam, requires_grad values are reset to original value.</p> Source code in <code>quadra/modules/classification/base.py</code> <pre><code>def on_predict_end(self) -&gt; None:\n\"\"\"If we computed gradcam, requires_grad values are reset to original value.\"\"\"\n    if self.gradcam:\n        # Get back to initial state\n        for i, p in enumerate(self.model.parameters()):\n            p.requires_grad = self.original_requires_grads[i]\n\n        # We are using GradCAM package only for resnets at the moment\n        if isinstance(self.model.features_extractor, timm.models.resnet.ResNet) and self.cam is not None:\n            # Needed to solve jitting bug\n            self.cam.activations_and_grads.release()\n        elif (\n            is_vision_transformer(cast(BaseNetworkBuilder, self.model).features_extractor)\n            and self.grad_rollout is not None\n        ):\n            for handle in self.grad_rollout.f_hook_handles:\n                handle.remove()\n            for handle in self.grad_rollout.b_hook_handles:\n                handle.remove()\n\n    return super().on_predict_end()\n</code></pre>"},{"location":"reference/quadra/modules/classification/base.html#quadra.modules.classification.base.ClassificationModule.on_predict_start","title":"<code>on_predict_start()</code>","text":"<p>If gradcam, prepares gradcam and saves params requires_grad state.</p> Source code in <code>quadra/modules/classification/base.py</code> <pre><code>def on_predict_start(self) -&gt; None:\n\"\"\"If gradcam, prepares gradcam and saves params requires_grad state.\"\"\"\n    if self.gradcam:\n        # Saving params requires_grad state\n        for p in self.model.parameters():\n            self.original_requires_grads.append(p.requires_grad)\n        self.prepare_gradcam()\n\n    return super().on_predict_start()\n</code></pre>"},{"location":"reference/quadra/modules/classification/base.html#quadra.modules.classification.base.ClassificationModule.predict_step","title":"<code>predict_step(batch, batch_idx, dataloader_idx=0)</code>","text":"<p>Prediction step.</p> <p>Parameters:</p> <ul> <li> batch             (<code>Any</code>)         \u2013          <p>Tuple composed by (image, target)</p> </li> <li> batch_idx             (<code>int</code>)         \u2013          <p>Batch index</p> </li> <li> dataloader_idx             (<code>int</code>, default:                 <code>0</code> )         \u2013          <p>Dataloader index</p> </li> </ul> Source code in <code>quadra/modules/classification/base.py</code> <pre><code>def predict_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -&gt; Any:\n\"\"\"Prediction step.\n\n    Args:\n        batch: Tuple composed by (image, target)\n        batch_idx: Batch index\n        dataloader_idx: Dataloader index\n    Returns:\n        Tuple containing:\n            predicted_classes: indexes of predicted classes\n            grayscale_cam: gray scale gradcams\n    \"\"\"\n    im, _ = batch\n    outputs = self(im)\n    probs = torch.softmax(outputs, dim=1)\n    predicted_classes = torch.max(probs, dim=1).indices.tolist()\n    if self.gradcam:\n        # inference_mode set to false because gradcam needs gradients\n        with torch.inference_mode(False):\n            im = im.clone()\n\n            if isinstance(self.model.features_extractor, timm.models.resnet.ResNet) and self.cam:\n                grayscale_cam = self.cam(input_tensor=im, targets=None)\n            elif (\n                is_vision_transformer(cast(BaseNetworkBuilder, self.model).features_extractor) and self.grad_rollout\n            ):\n                grayscale_cam_low_res = self.grad_rollout(input_tensor=im, targets_list=predicted_classes)\n                orig_shape = grayscale_cam_low_res.shape\n                new_shape = (orig_shape[0], im.shape[2], im.shape[3])\n                zoom_factors = tuple(np.array(new_shape) / np.array(orig_shape))\n                grayscale_cam = ndimage.zoom(grayscale_cam_low_res, zoom_factors, order=1)\n    else:\n        grayscale_cam = None\n    return predicted_classes, grayscale_cam, torch.max(probs, dim=1)[0].tolist()\n</code></pre>"},{"location":"reference/quadra/modules/classification/base.html#quadra.modules.classification.base.ClassificationModule.prepare_gradcam","title":"<code>prepare_gradcam()</code>","text":"<p>Instantiate gradcam handlers.</p> Source code in <code>quadra/modules/classification/base.py</code> <pre><code>def prepare_gradcam(self) -&gt; None:\n\"\"\"Instantiate gradcam handlers.\"\"\"\n    if isinstance(self.model.features_extractor, timm.models.resnet.ResNet):\n        target_layers = [cast(BaseNetworkBuilder, self.model).features_extractor.layer4[-1]]\n\n        self.cam = GradCAM(\n            model=self.model,\n            target_layers=target_layers,\n        )\n        # Activating gradients\n        for p in self.model.features_extractor.layer4[-1].parameters():\n            p.requires_grad = True\n    elif is_vision_transformer(cast(BaseNetworkBuilder, self.model).features_extractor):\n        self.grad_rollout = VitAttentionGradRollout(self.model)\n    else:\n        log.warning(\"Gradcam not implemented for this backbone, it won't be computed\")\n        self.original_requires_grads.clear()\n        self.gradcam = False\n</code></pre>"},{"location":"reference/quadra/modules/classification/base.html#quadra.modules.classification.base.MultilabelClassificationModule","title":"<code>MultilabelClassificationModule(model, criterion, optimizer=None, lr_scheduler=None, lr_scheduler_interval='epoch', gradcam=False)</code>","text":"<p>             Bases: <code>BaseLightningModule</code></p> <p>SklearnClassification model: train a generic SklearnClassification model for a multilabel problem.</p> <p>Parameters:</p> <ul> <li> model             (<code>Sequential</code>)         \u2013          <p>Feature extractor as PyTorch <code>torch.nn.Module</code></p> </li> <li> criterion             (<code>Module</code>)         \u2013          <p>the loss to be applied as a PyTorch <code>torch.nn.Module</code>.</p> </li> <li> optimizer             (<code>None | Optimizer</code>, default:                 <code>None</code> )         \u2013          <p>optimizer of the training. Defaults to None.</p> </li> <li> lr_scheduler             (<code>None | object</code>, default:                 <code>None</code> )         \u2013          <p>Pytorch learning rate scheduler. If None a default ReduceLROnPlateau is used. Defaults to None.</p> </li> <li> lr_scheduler_interval             (<code>str | None</code>, default:                 <code>'epoch'</code> )         \u2013          <p>the learning rate scheduler interval. Defaults to \"epoch\".</p> </li> </ul> Source code in <code>quadra/modules/classification/base.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Sequential,\n    criterion: nn.Module,\n    optimizer: None | optim.Optimizer = None,\n    lr_scheduler: None | object = None,\n    lr_scheduler_interval: str | None = \"epoch\",\n    gradcam: bool = False,\n):\n    super().__init__(model, optimizer, lr_scheduler, lr_scheduler_interval)\n    self.criterion = criterion\n    self.gradcam = gradcam\n\n    # TODO: can we use gradcam with more backbones?\n    if self.gradcam:\n        if not isinstance(model[0].features_extractor, timm.models.resnet.ResNet):\n            log.warning(\n                \"Backbone must be compatible with gradcam, at the moment only ResNets supported, disabling gradcam\"\n            )\n            self.gradcam = False\n        else:\n            target_layers = [model[0].features_extractor.layer4[-1]]\n            self.cam = GradCAM(model=model, target_layers=target_layers)\n</code></pre>"},{"location":"reference/quadra/modules/ssl/index.html","title":"ssl","text":""},{"location":"reference/quadra/modules/ssl/index.html#quadra.modules.ssl.BYOL","title":"<code>BYOL(student, teacher, student_projection_mlp, student_prediction_mlp, teacher_projection_mlp, criterion, classifier=None, optimizer=None, lr_scheduler=None, lr_scheduler_interval='epoch', teacher_momentum=0.9995, teacher_momentum_cosine_decay=True)</code>","text":"<p>             Bases: <code>SSLModule</code></p> <p>BYOL module, inspired by https://arxiv.org/abs/2006.07733.</p> <p>Parameters:</p> <ul> <li> student         \u2013          <p>student model.</p> </li> <li> teacher         \u2013          <p>teacher model.</p> </li> <li> student_projection_mlp         \u2013          <p>student projection MLP.</p> </li> <li> student_prediction_mlp         \u2013          <p>student prediction MLP.</p> </li> <li> teacher_projection_mlp         \u2013          <p>teacher projection MLP.</p> </li> <li> criterion         \u2013          <p>loss function.</p> </li> <li> classifier             (<code>ClassifierMixin | None</code>, default:                 <code>None</code> )         \u2013          <p>Standard sklearn classifier.</p> </li> <li> optimizer             (<code>Optimizer | None</code>, default:                 <code>None</code> )         \u2013          <p>optimizer of the training. If None a default Adam is used.</p> </li> <li> lr_scheduler             (<code>object | None</code>, default:                 <code>None</code> )         \u2013          <p>lr scheduler. If None a default ReduceLROnPlateau is used.</p> </li> <li> lr_scheduler_interval             (<code>str | None</code>, default:                 <code>'epoch'</code> )         \u2013          <p>interval at which the lr scheduler is updated.</p> </li> <li> teacher_momentum             (<code>float</code>, default:                 <code>0.9995</code> )         \u2013          <p>momentum of the teacher parameters.</p> </li> <li> teacher_momentum_cosine_decay             (<code>bool | None</code>, default:                 <code>True</code> )         \u2013          <p>whether to use cosine decay for the teacher momentum. Default: True</p> </li> </ul> Source code in <code>quadra/modules/ssl/byol.py</code> <pre><code>def __init__(\n    self,\n    student: nn.Module,\n    teacher: nn.Module,\n    student_projection_mlp: nn.Module,\n    student_prediction_mlp: nn.Module,\n    teacher_projection_mlp: nn.Module,\n    criterion: nn.Module,\n    classifier: sklearn.base.ClassifierMixin | None = None,\n    optimizer: Optimizer | None = None,\n    lr_scheduler: object | None = None,\n    lr_scheduler_interval: str | None = \"epoch\",\n    teacher_momentum: float = 0.9995,\n    teacher_momentum_cosine_decay: bool | None = True,\n):\n    super().__init__(\n        model=student,\n        criterion=criterion,\n        classifier=classifier,\n        optimizer=optimizer,\n        lr_scheduler=lr_scheduler,\n        lr_scheduler_interval=lr_scheduler_interval,\n    )\n    # Student model\n    self.max_steps: int\n    self.student_projection_mlp = student_projection_mlp\n    self.student_prediction_mlp = student_prediction_mlp\n\n    # Teacher model\n    self.teacher = teacher\n    self.teacher_projection_mlp = teacher_projection_mlp\n    self.teacher_initialized = False\n    self.teacher_momentum = teacher_momentum\n    self.teacher_momentum_cosine_decay = teacher_momentum_cosine_decay\n\n    self.initialize_teacher()\n</code></pre>"},{"location":"reference/quadra/modules/ssl/index.html#quadra.modules.ssl.BYOL.calculate_accuracy","title":"<code>calculate_accuracy(batch)</code>","text":"<p>Calculate accuracy on the given batch.</p> Source code in <code>quadra/modules/ssl/byol.py</code> <pre><code>def calculate_accuracy(self, batch):\n\"\"\"Calculate accuracy on the given batch.\"\"\"\n    images, labels = batch\n    embedding = self.model(images).detach().cpu().numpy()\n    predictions = self.classifier.predict(embedding)\n    labels = labels.detach()\n    acc = self.val_acc(torch.tensor(predictions, device=self.device), labels)\n\n    return acc\n</code></pre>"},{"location":"reference/quadra/modules/ssl/index.html#quadra.modules.ssl.BYOL.initialize_teacher","title":"<code>initialize_teacher()</code>","text":"<p>Initialize teacher from the state dict of the student one, checking also that student model requires greadient correctly.</p> Source code in <code>quadra/modules/ssl/byol.py</code> <pre><code>def initialize_teacher(self):\n\"\"\"Initialize teacher from the state dict of the student one,\n    checking also that student model requires greadient correctly.\n    \"\"\"\n    self.teacher_projection_mlp.load_state_dict(self.student_projection_mlp.state_dict())\n    for p in self.teacher_projection_mlp.parameters():\n        p.requires_grad = False\n\n    self.teacher.load_state_dict(self.model.state_dict())\n    for p in self.teacher.parameters():\n        p.requires_grad = False\n\n    for p in self.student_projection_mlp.parameters():\n        assert p.requires_grad is True\n    for p in self.student_prediction_mlp.parameters():\n        assert p.requires_grad is True\n\n    self.teacher_initialized = True\n</code></pre>"},{"location":"reference/quadra/modules/ssl/index.html#quadra.modules.ssl.BYOL.optimizer_step","title":"<code>optimizer_step(epoch, batch_idx, optimizer, optimizer_closure=None)</code>","text":"<p>Override optimizer step to update the teacher parameters.</p> Source code in <code>quadra/modules/ssl/byol.py</code> <pre><code>def optimizer_step(\n    self,\n    epoch: int,\n    batch_idx: int,\n    optimizer: Optimizer | LightningOptimizer,\n    optimizer_closure: Callable[[], Any] | None = None,\n) -&gt; None:\n\"\"\"Override optimizer step to update the teacher parameters.\"\"\"\n    super().optimizer_step(\n        epoch,\n        batch_idx,\n        optimizer,\n        optimizer_closure=optimizer_closure,\n    )\n    self.update_teacher()\n</code></pre>"},{"location":"reference/quadra/modules/ssl/index.html#quadra.modules.ssl.BYOL.test_step","title":"<code>test_step(batch, *args)</code>","text":"<p>Calculate accuracy on the test set for the given batch.</p> Source code in <code>quadra/modules/ssl/byol.py</code> <pre><code>def test_step(self, batch, *args: list[Any]) -&gt; None:\n\"\"\"Calculate accuracy on the test set for the given batch.\"\"\"\n    acc = self.calculate_accuracy(batch)\n    self.log(name=\"test_acc\", value=acc, on_step=False, on_epoch=True, prog_bar=True)\n    return acc\n</code></pre>"},{"location":"reference/quadra/modules/ssl/index.html#quadra.modules.ssl.BYOL.update_teacher","title":"<code>update_teacher()</code>","text":"<p>Update teacher given <code>self.teacher_momentum</code> by an exponential moving average of the student parameters, that is: theta_t * tau + theta_s * (1 - tau), where <code>theta_{s,t}</code> are the parameters of the student and the teacher model, while <code>tau</code> is the teacher momentum. If <code>self.teacher_momentum_cosine_decay</code> is True, then the teacher momentum will follow a cosine scheduling from <code>self.teacher_momentum</code> to 1: tau = 1 - (1 - tau) * (cos(pi * t / T) + 1) / 2, where <code>t</code> is the current step and <code>T</code> is the max number of steps.</p> Source code in <code>quadra/modules/ssl/byol.py</code> <pre><code>def update_teacher(self):\n\"\"\"Update teacher given `self.teacher_momentum` by an exponential moving average\n    of the student parameters, that is: theta_t * tau + theta_s * (1 - tau), where\n    `theta_{s,t}` are the parameters of the student and the teacher model, while `tau` is the\n    teacher momentum. If `self.teacher_momentum_cosine_decay` is True, then the teacher\n    momentum will follow a cosine scheduling from `self.teacher_momentum` to 1:\n    tau = 1 - (1 - tau) * (cos(pi * t / T) + 1) / 2, where `t` is the current step and\n    `T` is the max number of steps.\n    \"\"\"\n    with torch.no_grad():\n        if self.teacher_momentum_cosine_decay:\n            teacher_momentum = (\n                1\n                - (1 - self.teacher_momentum)\n                * (math.cos(math.pi * self.trainer.global_step / self.max_steps) + 1)\n                / 2\n            )\n        else:\n            teacher_momentum = self.teacher_momentum\n        self.log(\"teacher_momentum\", teacher_momentum, prog_bar=True)\n        for student_ps, teacher_ps in zip(\n            list(self.model.parameters()) + list(self.student_projection_mlp.parameters()),\n            list(self.teacher.parameters()) + list(self.teacher_projection_mlp.parameters()),\n        ):\n            teacher_ps.data = teacher_ps.data * teacher_momentum + (1 - teacher_momentum) * student_ps.data\n</code></pre>"},{"location":"reference/quadra/modules/ssl/index.html#quadra.modules.ssl.BarlowTwins","title":"<code>BarlowTwins(model, projection_mlp, criterion, classifier=None, optimizer=None, lr_scheduler=None, lr_scheduler_interval='epoch')</code>","text":"<p>             Bases: <code>SSLModule</code></p> <p>BarlowTwins model.</p> <p>Parameters:</p> <ul> <li> model             (<code>Module</code>)         \u2013          <p>Network Module used for extract features</p> </li> <li> projection_mlp             (<code>Module</code>)         \u2013          <p>Module to project extracted features</p> </li> <li> criterion             (<code>Module</code>)         \u2013          <p>SSL loss to be applied</p> </li> <li> classifier             (<code>ClassifierMixin | None</code>, default:                 <code>None</code> )         \u2013          <p>Standard sklearn classifier. Defaults to None.</p> </li> <li> optimizer             (<code>Optimizer | None</code>, default:                 <code>None</code> )         \u2013          <p>optimizer of the training. If None a default Adam is used. Defaults to None.</p> </li> <li> lr_scheduler             (<code>object | None</code>, default:                 <code>None</code> )         \u2013          <p>lr scheduler. If None a default ReduceLROnPlateau is used. Defaults to None.</p> </li> <li> lr_scheduler_interval             (<code>str | None</code>, default:                 <code>'epoch'</code> )         \u2013          <p>interval at which the lr scheduler is updated. Defaults to \"epoch\".</p> </li> </ul> Source code in <code>quadra/modules/ssl/barlowtwins.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    projection_mlp: nn.Module,\n    criterion: nn.Module,\n    classifier: sklearn.base.ClassifierMixin | None = None,\n    optimizer: optim.Optimizer | None = None,\n    lr_scheduler: object | None = None,\n    lr_scheduler_interval: str | None = \"epoch\",\n):\n    super().__init__(model, criterion, classifier, optimizer, lr_scheduler, lr_scheduler_interval)\n    # self.save_hyperparameters()\n    self.projection_mlp = projection_mlp\n    self.criterion = criterion\n</code></pre>"},{"location":"reference/quadra/modules/ssl/index.html#quadra.modules.ssl.Dino","title":"<code>Dino(student, teacher, student_projection_mlp, teacher_projection_mlp, criterion, freeze_last_layer=1, classifier=None, optimizer=None, lr_scheduler=None, lr_scheduler_interval='epoch', teacher_momentum=0.9995, teacher_momentum_cosine_decay=True)</code>","text":"<p>             Bases: <code>BYOL</code></p> <p>DINO pytorch-lightning module.</p> <p>Parameters:</p> <ul> <li> student         \u2013          <p>student model</p> </li> <li> teacher         \u2013          <p>teacher model</p> </li> <li> student_projection_mlp         \u2013          <p>student projection MLP</p> </li> <li> teacher_projection_mlp         \u2013          <p>teacher projection MLP</p> </li> <li> criterion         \u2013          <p>loss function</p> </li> <li> freeze_last_layer         \u2013          <p>number of layers to freeze in the student model. Default: 1</p> </li> <li> classifier             (<code>ClassifierMixin | None</code>, default:                 <code>None</code> )         \u2013          <p>Standard sklearn classifier</p> </li> <li> optimizer             (<code>Optimizer | None</code>, default:                 <code>None</code> )         \u2013          <p>optimizer of the training. If None a default Adam is used.</p> </li> <li> lr_scheduler             (<code>object | None</code>, default:                 <code>None</code> )         \u2013          <p>lr scheduler. If None a default ReduceLROnPlateau is used.</p> </li> <li> lr_scheduler_interval             (<code>str | None</code>, default:                 <code>'epoch'</code> )         \u2013          <p>interval at which the lr scheduler is updated.</p> </li> <li> teacher_momentum             (<code>float</code>, default:                 <code>0.9995</code> )         \u2013          <p>momentum of the teacher parameters</p> </li> <li> teacher_momentum_cosine_decay             (<code>bool | None</code>, default:                 <code>True</code> )         \u2013          <p>whether to use cosine decay for the teacher momentum</p> </li> </ul> Source code in <code>quadra/modules/ssl/dino.py</code> <pre><code>def __init__(\n    self,\n    student: nn.Module,\n    teacher: nn.Module,\n    student_projection_mlp: nn.Module,\n    teacher_projection_mlp: nn.Module,\n    criterion: nn.Module,\n    freeze_last_layer: int = 1,\n    classifier: sklearn.base.ClassifierMixin | None = None,\n    optimizer: Optimizer | None = None,\n    lr_scheduler: object | None = None,\n    lr_scheduler_interval: str | None = \"epoch\",\n    teacher_momentum: float = 0.9995,\n    teacher_momentum_cosine_decay: bool | None = True,\n):\n    super().__init__(\n        student=student,\n        teacher=teacher,\n        student_projection_mlp=student_projection_mlp,\n        student_prediction_mlp=nn.Identity(),\n        teacher_projection_mlp=teacher_projection_mlp,\n        criterion=criterion,\n        teacher_momentum=teacher_momentum,\n        teacher_momentum_cosine_decay=teacher_momentum_cosine_decay,\n        classifier=classifier,\n        optimizer=optimizer,\n        lr_scheduler=lr_scheduler,\n        lr_scheduler_interval=lr_scheduler_interval,\n    )\n    self.freeze_last_layer = freeze_last_layer\n</code></pre>"},{"location":"reference/quadra/modules/ssl/index.html#quadra.modules.ssl.Dino.cancel_gradients_last_layer","title":"<code>cancel_gradients_last_layer(epoch, freeze_last_layer)</code>","text":"<p>Zero out the gradient of the last layer, as specified in the paper.</p> <p>Parameters:</p> <ul> <li> epoch             (<code>int</code>)         \u2013          <p>current epoch</p> </li> <li> freeze_last_layer             (<code>int</code>)         \u2013          <p>maximum freeze epoch: if <code>epoch</code> &gt;= <code>freeze_last_layer</code> then the gradient of the last layer will not be freezed</p> </li> </ul> Source code in <code>quadra/modules/ssl/dino.py</code> <pre><code>def cancel_gradients_last_layer(self, epoch: int, freeze_last_layer: int):\n\"\"\"Zero out the gradient of the last layer, as specified in the paper.\n\n    Args:\n        epoch: current epoch\n        freeze_last_layer: maximum freeze epoch: if `epoch` &gt;= `freeze_last_layer`\n            then the gradient of the last layer will not be freezed\n    \"\"\"\n    if epoch &gt;= freeze_last_layer:\n        return\n    for n, p in self.student_projection_mlp.named_parameters():\n        if \"last_layer\" in n:\n            p.grad = None\n</code></pre>"},{"location":"reference/quadra/modules/ssl/index.html#quadra.modules.ssl.Dino.configure_gradient_clipping","title":"<code>configure_gradient_clipping(optimizer, gradient_clip_val=None, gradient_clip_algorithm=None)</code>","text":"<p>Configure gradient clipping for the optimizer.</p> Source code in <code>quadra/modules/ssl/dino.py</code> <pre><code>def configure_gradient_clipping(\n    self,\n    optimizer: Optimizer,\n    gradient_clip_val: int | float | None = None,\n    gradient_clip_algorithm: str | None = None,\n):\n\"\"\"Configure gradient clipping for the optimizer.\"\"\"\n    if gradient_clip_algorithm is not None and gradient_clip_val is not None:\n        clip_gradients(self.model, gradient_clip_val)\n        clip_gradients(self.student_projection_mlp, gradient_clip_val)\n    self.cancel_gradients_last_layer(self.current_epoch, self.freeze_last_layer)\n</code></pre>"},{"location":"reference/quadra/modules/ssl/index.html#quadra.modules.ssl.Dino.initialize_teacher","title":"<code>initialize_teacher()</code>","text":"<p>Initialize teacher from the state dict of the student one, checking also that student model requires greadient correctly.</p> Source code in <code>quadra/modules/ssl/dino.py</code> <pre><code>def initialize_teacher(self):\n\"\"\"Initialize teacher from the state dict of the student one,\n    checking also that student model requires greadient correctly.\n    \"\"\"\n    self.teacher_projection_mlp.load_state_dict(self.student_projection_mlp.state_dict())\n    for p in self.teacher_projection_mlp.parameters():\n        p.requires_grad = False\n\n    self.teacher.load_state_dict(self.model.state_dict())\n    for p in self.teacher.parameters():\n        p.requires_grad = False\n\n    all_frozen = True\n    for p in self.model.parameters():\n        all_frozen = all_frozen and (not p.requires_grad)\n\n    if all_frozen:\n        log.warning(\n            \"All parameters of the student model are frozen, the model will not be trained, automatically\"\n            \" unfreezing all the layers\"\n        )\n\n        for p in self.model.parameters():\n            p.requires_grad = True\n\n    for name, p in self.student_projection_mlp.named_parameters():\n        if name != \"last_layer.weight_g\":\n            assert p.requires_grad is True\n\n    self.teacher_initialized = True\n</code></pre>"},{"location":"reference/quadra/modules/ssl/index.html#quadra.modules.ssl.Dino.optimizer_step","title":"<code>optimizer_step(epoch, batch_idx, optimizer, optimizer_closure=None)</code>","text":"<p>Override optimizer step to update the teacher parameters.</p> Source code in <code>quadra/modules/ssl/dino.py</code> <pre><code>def optimizer_step(\n    self,\n    epoch: int,\n    batch_idx: int,\n    optimizer: Optimizer | LightningOptimizer,\n    optimizer_closure: Callable[[], Any] | None = None,\n) -&gt; None:\n\"\"\"Override optimizer step to update the teacher parameters.\"\"\"\n    super().optimizer_step(\n        epoch,\n        batch_idx,\n        optimizer,\n        optimizer_closure=optimizer_closure,\n    )\n    self.update_teacher()\n</code></pre>"},{"location":"reference/quadra/modules/ssl/index.html#quadra.modules.ssl.Dino.student_multicrop_forward","title":"<code>student_multicrop_forward(x)</code>","text":"<p>Student forward on the multicrop imges.</p> <p>Parameters:</p> <ul> <li> x             (<code>list[Tensor]</code>)         \u2013          <p>List of torch.Tensor containing multicropped augmented images</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>         \u2013          <p>torch.Tensor: a tensor of shape NxBxD, where N is the number crops corresponding to the length of the input list <code>x</code>, B is the batch size and D is the output dimension</p> </li> </ul> Source code in <code>quadra/modules/ssl/dino.py</code> <pre><code>def student_multicrop_forward(self, x: list[torch.Tensor]) -&gt; torch.Tensor:\n\"\"\"Student forward on the multicrop imges.\n\n    Args:\n        x: List of torch.Tensor containing multicropped augmented images\n\n    Returns:\n        torch.Tensor: a tensor of shape NxBxD, where N is the number crops\n            corresponding to the length of the input list `x`, B is the batch size\n            and D is the output dimension\n    \"\"\"\n    n_crops = len(x)\n    concatenated = torch.cat(x, dim=0)  # (n_samples * n_crops, C, H, W)\n    embedding = self.model(concatenated)  # (n_samples * n_crops, in_dim)\n    logits = self.student_projection_mlp(embedding)  # (n_samples * n_crops, out_dim)\n    chunks = logits.chunk(n_crops)  # n_crops * (n_samples, out_dim)\n    return chunks\n</code></pre>"},{"location":"reference/quadra/modules/ssl/index.html#quadra.modules.ssl.Dino.teacher_multicrop_forward","title":"<code>teacher_multicrop_forward(x)</code>","text":"<p>Teacher forward on the multicrop imges.</p> <p>Parameters:</p> <ul> <li> x             (<code>list[Tensor]</code>)         \u2013          <p>List of torch.Tensor containing multicropped augmented images</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>         \u2013          <p>torch.Tensor: a tensor of shape NxBxD, where N is the number crops corresponding to the length of the input list <code>x</code>, B is the batch size and D is the output dimension</p> </li> </ul> Source code in <code>quadra/modules/ssl/dino.py</code> <pre><code>def teacher_multicrop_forward(self, x: list[torch.Tensor]) -&gt; torch.Tensor:\n\"\"\"Teacher forward on the multicrop imges.\n\n    Args:\n        x: List of torch.Tensor containing multicropped augmented images\n\n    Returns:\n        torch.Tensor: a tensor of shape NxBxD, where N is the number crops\n            corresponding to the length of the input list `x`, B is the batch size\n            and D is the output dimension\n    \"\"\"\n    n_crops = len(x)\n    concatenated = torch.cat(x, dim=0)  # (n_samples * n_crops, C, H, W)\n    embedding = self.teacher(concatenated)  # (n_samples * n_crops, in_dim)\n    logits = self.teacher_projection_mlp(embedding)  # (n_samples * n_crops, out_dim)\n    chunks = logits.chunk(n_crops)  # n_crops * (n_samples, out_dim)\n    return chunks\n</code></pre>"},{"location":"reference/quadra/modules/ssl/index.html#quadra.modules.ssl.IDMM","title":"<code>IDMM(model, prediction_mlp, criterion, multiview_loss=True, mixup_fn=None, classifier=None, optimizer=None, lr_scheduler=None, lr_scheduler_interval='epoch')</code>","text":"<p>             Bases: <code>SSLModule</code></p> <p>IDMM model.</p> <p>Parameters:</p> <ul> <li> model             (<code>Module</code>)         \u2013          <p>backbone model</p> </li> <li> prediction_mlp             (<code>Module</code>)         \u2013          <p>student prediction MLP</p> </li> <li> criterion             (<code>Module</code>)         \u2013          <p>loss function</p> </li> <li> multiview_loss             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>whether to use the multiview loss as definied in https://arxiv.org/abs/2201.10728. Defaults to True.</p> </li> <li> mixup_fn             (<code>Mixup | None</code>, default:                 <code>None</code> )         \u2013          <p>the mixup/cutmix function to be applied to a batch of images. Defaults to None.</p> </li> <li> classifier             (<code>ClassifierMixin | None</code>, default:                 <code>None</code> )         \u2013          <p>Standard sklearn classifier</p> </li> <li> optimizer             (<code>Optimizer | None</code>, default:                 <code>None</code> )         \u2013          <p>optimizer of the training. If None a default Adam is used.</p> </li> <li> lr_scheduler             (<code>object | None</code>, default:                 <code>None</code> )         \u2013          <p>lr scheduler. If None a default ReduceLROnPlateau is used.</p> </li> <li> lr_scheduler_interval             (<code>str | None</code>, default:                 <code>'epoch'</code> )         \u2013          <p>interval at which the lr scheduler is updated.</p> </li> </ul> Source code in <code>quadra/modules/ssl/idmm.py</code> <pre><code>def __init__(\n    self,\n    model: torch.nn.Module,\n    prediction_mlp: torch.nn.Module,\n    criterion: torch.nn.Module,\n    multiview_loss: bool = True,\n    mixup_fn: timm.data.Mixup | None = None,\n    classifier: sklearn.base.ClassifierMixin | None = None,\n    optimizer: torch.optim.Optimizer | None = None,\n    lr_scheduler: object | None = None,\n    lr_scheduler_interval: str | None = \"epoch\",\n):\n    super().__init__(\n        model,\n        criterion,\n        classifier,\n        optimizer,\n        lr_scheduler,\n        lr_scheduler_interval,\n    )\n    # self.save_hyperparameters()\n    self.prediction_mlp = prediction_mlp\n    self.mixup_fn = mixup_fn\n    self.multiview_loss = multiview_loss\n</code></pre>"},{"location":"reference/quadra/modules/ssl/index.html#quadra.modules.ssl.SimCLR","title":"<code>SimCLR(model, projection_mlp, criterion, classifier=None, optimizer=None, lr_scheduler=None, lr_scheduler_interval='epoch')</code>","text":"<p>             Bases: <code>SSLModule</code></p> <p>SIMCLR class.</p> <p>Parameters:</p> <ul> <li> model             (<code>Module</code>)         \u2013          <p>Feature extractor as pytorch <code>torch.nn.Module</code></p> </li> <li> projection_mlp             (<code>Module</code>)         \u2013          <p>projection head as pytorch <code>torch.nn.Module</code></p> </li> <li> criterion             (<code>Module</code>)         \u2013          <p>SSL loss to be applied</p> </li> <li> classifier             (<code>ClassifierMixin | None</code>, default:                 <code>None</code> )         \u2013          <p>Standard sklearn classifier. Defaults to None.</p> </li> <li> optimizer             (<code>Optimizer | None</code>, default:                 <code>None</code> )         \u2013          <p>optimizer of the training. If None a default Adam is used. Defaults to None.</p> </li> <li> lr_scheduler             (<code>object | None</code>, default:                 <code>None</code> )         \u2013          <p>lr scheduler. If None a default ReduceLROnPlateau is used. Defaults to None.</p> </li> <li> lr_scheduler_interval             (<code>str | None</code>, default:                 <code>'epoch'</code> )         \u2013          <p>interval at which the lr scheduler is updated. Defaults to \"epoch\".</p> </li> </ul> Source code in <code>quadra/modules/ssl/simclr.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    projection_mlp: nn.Module,\n    criterion: torch.nn.Module,\n    classifier: sklearn.base.ClassifierMixin | None = None,\n    optimizer: torch.optim.Optimizer | None = None,\n    lr_scheduler: object | None = None,\n    lr_scheduler_interval: str | None = \"epoch\",\n):\n    super().__init__(\n        model,\n        criterion,\n        classifier,\n        optimizer,\n        lr_scheduler,\n        lr_scheduler_interval,\n    )\n    self.projection_mlp = projection_mlp\n</code></pre>"},{"location":"reference/quadra/modules/ssl/index.html#quadra.modules.ssl.SimCLR.training_step","title":"<code>training_step(batch, batch_idx)</code>","text":"<p>Parameters:</p> <ul> <li> batch             (<code>tuple[tuple[Tensor, Tensor], Tensor]</code>)         \u2013          <p>The batch of data</p> </li> <li> batch_idx             (<code>int</code>)         \u2013          <p>The index of the batch.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>         \u2013          <p>The computed loss</p> </li> </ul> Source code in <code>quadra/modules/ssl/simclr.py</code> <pre><code>def training_step(\n    self, batch: tuple[tuple[torch.Tensor, torch.Tensor], torch.Tensor], batch_idx: int\n) -&gt; torch.Tensor:\n\"\"\"Args:\n        batch: The batch of data\n        batch_idx: The index of the batch.\n\n    Returns:\n        The computed loss\n    \"\"\"\n    # pylint: disable=unused-argument\n    (im_x, im_y), _ = batch\n    emb_x = self(im_x)\n    emb_y = self(im_y)\n    loss = self.criterion(emb_x, emb_y)\n\n    self.log(\n        \"loss\",\n        loss,\n        on_epoch=True,\n        on_step=True,\n        logger=True,\n        prog_bar=True,\n    )\n    return loss\n</code></pre>"},{"location":"reference/quadra/modules/ssl/index.html#quadra.modules.ssl.SimSIAM","title":"<code>SimSIAM(model, projection_mlp, prediction_mlp, criterion, classifier=None, optimizer=None, lr_scheduler=None, lr_scheduler_interval='epoch')</code>","text":"<p>             Bases: <code>SSLModule</code></p> <p>SimSIAM model.</p> <p>Parameters:</p> <ul> <li> model             (<code>Module</code>)         \u2013          <p>Feature extractor as pytorch <code>torch.nn.Module</code></p> </li> <li> projection_mlp             (<code>Module</code>)         \u2013          <p>optional projection head as pytorch <code>torch.nn.Module</code></p> </li> <li> prediction_mlp             (<code>Module</code>)         \u2013          <p>optional predicition head as pytorch <code>torch.nn.Module</code></p> </li> <li> criterion             (<code>Module</code>)         \u2013          <p>loss to be applied.</p> </li> <li> classifier             (<code>ClassifierMixin | None</code>, default:                 <code>None</code> )         \u2013          <p>Standard sklearn classifier.</p> </li> <li> optimizer             (<code>Optimizer | None</code>, default:                 <code>None</code> )         \u2013          <p>optimizer of the training. If None a default Adam is used.</p> </li> <li> lr_scheduler             (<code>object | None</code>, default:                 <code>None</code> )         \u2013          <p>lr scheduler. If None a default ReduceLROnPlateau is used.</p> </li> <li> lr_scheduler_interval             (<code>str | None</code>, default:                 <code>'epoch'</code> )         \u2013          <p>interval at which the lr scheduler is updated.</p> </li> </ul> Source code in <code>quadra/modules/ssl/simsiam.py</code> <pre><code>def __init__(\n    self,\n    model: torch.nn.Module,\n    projection_mlp: torch.nn.Module,\n    prediction_mlp: torch.nn.Module,\n    criterion: torch.nn.Module,\n    classifier: sklearn.base.ClassifierMixin | None = None,\n    optimizer: torch.optim.Optimizer | None = None,\n    lr_scheduler: object | None = None,\n    lr_scheduler_interval: str | None = \"epoch\",\n):\n    super().__init__(\n        model,\n        criterion,\n        classifier,\n        optimizer,\n        lr_scheduler,\n        lr_scheduler_interval,\n    )\n    # self.save_hyperparameters()\n    self.projection_mlp = projection_mlp\n    self.prediction_mlp = prediction_mlp\n</code></pre>"},{"location":"reference/quadra/modules/ssl/index.html#quadra.modules.ssl.VICReg","title":"<code>VICReg(model, projection_mlp, criterion, classifier=None, optimizer=None, lr_scheduler=None, lr_scheduler_interval='epoch')</code>","text":"<p>             Bases: <code>SSLModule</code></p> <p>VICReg model.</p> <p>Parameters:</p> <ul> <li> model             (<code>Module</code>)         \u2013          <p>Network Module used for extract features</p> </li> <li> projection_mlp             (<code>Module</code>)         \u2013          <p>Module to project extracted features</p> </li> <li> criterion             (<code>Module</code>)         \u2013          <p>SSL loss to be applied.</p> </li> <li> classifier             (<code>ClassifierMixin | None</code>, default:                 <code>None</code> )         \u2013          <p>Standard sklearn classifier. Defaults to None.</p> </li> <li> optimizer             (<code>Optimizer | None</code>, default:                 <code>None</code> )         \u2013          <p>optimizer of the training. If None a default Adam is used. Defaults to None.</p> </li> <li> lr_scheduler             (<code>object | None</code>, default:                 <code>None</code> )         \u2013          <p>lr scheduler. If None a default ReduceLROnPlateau is used. Defaults to None.</p> </li> <li> lr_scheduler_interval             (<code>str | None</code>, default:                 <code>'epoch'</code> )         \u2013          <p>interval at which the lr scheduler is updated. Defaults to \"epoch\".</p> </li> </ul> Source code in <code>quadra/modules/ssl/vicreg.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    projection_mlp: nn.Module,\n    criterion: nn.Module,\n    classifier: sklearn.base.ClassifierMixin | None = None,\n    optimizer: optim.Optimizer | None = None,\n    lr_scheduler: object | None = None,\n    lr_scheduler_interval: str | None = \"epoch\",\n):\n    super().__init__(\n        model,\n        criterion,\n        classifier,\n        optimizer,\n        lr_scheduler,\n        lr_scheduler_interval,\n    )\n    # self.save_hyperparameters()\n    self.projection_mlp = projection_mlp\n    self.criterion = criterion\n</code></pre>"},{"location":"reference/quadra/modules/ssl/barlowtwins.html","title":"barlowtwins","text":""},{"location":"reference/quadra/modules/ssl/barlowtwins.html#quadra.modules.ssl.barlowtwins.BarlowTwins","title":"<code>BarlowTwins(model, projection_mlp, criterion, classifier=None, optimizer=None, lr_scheduler=None, lr_scheduler_interval='epoch')</code>","text":"<p>             Bases: <code>SSLModule</code></p> <p>BarlowTwins model.</p> <p>Parameters:</p> <ul> <li> model             (<code>Module</code>)         \u2013          <p>Network Module used for extract features</p> </li> <li> projection_mlp             (<code>Module</code>)         \u2013          <p>Module to project extracted features</p> </li> <li> criterion             (<code>Module</code>)         \u2013          <p>SSL loss to be applied</p> </li> <li> classifier             (<code>ClassifierMixin | None</code>, default:                 <code>None</code> )         \u2013          <p>Standard sklearn classifier. Defaults to None.</p> </li> <li> optimizer             (<code>Optimizer | None</code>, default:                 <code>None</code> )         \u2013          <p>optimizer of the training. If None a default Adam is used. Defaults to None.</p> </li> <li> lr_scheduler             (<code>object | None</code>, default:                 <code>None</code> )         \u2013          <p>lr scheduler. If None a default ReduceLROnPlateau is used. Defaults to None.</p> </li> <li> lr_scheduler_interval             (<code>str | None</code>, default:                 <code>'epoch'</code> )         \u2013          <p>interval at which the lr scheduler is updated. Defaults to \"epoch\".</p> </li> </ul> Source code in <code>quadra/modules/ssl/barlowtwins.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    projection_mlp: nn.Module,\n    criterion: nn.Module,\n    classifier: sklearn.base.ClassifierMixin | None = None,\n    optimizer: optim.Optimizer | None = None,\n    lr_scheduler: object | None = None,\n    lr_scheduler_interval: str | None = \"epoch\",\n):\n    super().__init__(model, criterion, classifier, optimizer, lr_scheduler, lr_scheduler_interval)\n    # self.save_hyperparameters()\n    self.projection_mlp = projection_mlp\n    self.criterion = criterion\n</code></pre>"},{"location":"reference/quadra/modules/ssl/byol.html","title":"byol","text":""},{"location":"reference/quadra/modules/ssl/byol.html#quadra.modules.ssl.byol.BYOL","title":"<code>BYOL(student, teacher, student_projection_mlp, student_prediction_mlp, teacher_projection_mlp, criterion, classifier=None, optimizer=None, lr_scheduler=None, lr_scheduler_interval='epoch', teacher_momentum=0.9995, teacher_momentum_cosine_decay=True)</code>","text":"<p>             Bases: <code>SSLModule</code></p> <p>BYOL module, inspired by https://arxiv.org/abs/2006.07733.</p> <p>Parameters:</p> <ul> <li> student         \u2013          <p>student model.</p> </li> <li> teacher         \u2013          <p>teacher model.</p> </li> <li> student_projection_mlp         \u2013          <p>student projection MLP.</p> </li> <li> student_prediction_mlp         \u2013          <p>student prediction MLP.</p> </li> <li> teacher_projection_mlp         \u2013          <p>teacher projection MLP.</p> </li> <li> criterion         \u2013          <p>loss function.</p> </li> <li> classifier             (<code>ClassifierMixin | None</code>, default:                 <code>None</code> )         \u2013          <p>Standard sklearn classifier.</p> </li> <li> optimizer             (<code>Optimizer | None</code>, default:                 <code>None</code> )         \u2013          <p>optimizer of the training. If None a default Adam is used.</p> </li> <li> lr_scheduler             (<code>object | None</code>, default:                 <code>None</code> )         \u2013          <p>lr scheduler. If None a default ReduceLROnPlateau is used.</p> </li> <li> lr_scheduler_interval             (<code>str | None</code>, default:                 <code>'epoch'</code> )         \u2013          <p>interval at which the lr scheduler is updated.</p> </li> <li> teacher_momentum             (<code>float</code>, default:                 <code>0.9995</code> )         \u2013          <p>momentum of the teacher parameters.</p> </li> <li> teacher_momentum_cosine_decay             (<code>bool | None</code>, default:                 <code>True</code> )         \u2013          <p>whether to use cosine decay for the teacher momentum. Default: True</p> </li> </ul> Source code in <code>quadra/modules/ssl/byol.py</code> <pre><code>def __init__(\n    self,\n    student: nn.Module,\n    teacher: nn.Module,\n    student_projection_mlp: nn.Module,\n    student_prediction_mlp: nn.Module,\n    teacher_projection_mlp: nn.Module,\n    criterion: nn.Module,\n    classifier: sklearn.base.ClassifierMixin | None = None,\n    optimizer: Optimizer | None = None,\n    lr_scheduler: object | None = None,\n    lr_scheduler_interval: str | None = \"epoch\",\n    teacher_momentum: float = 0.9995,\n    teacher_momentum_cosine_decay: bool | None = True,\n):\n    super().__init__(\n        model=student,\n        criterion=criterion,\n        classifier=classifier,\n        optimizer=optimizer,\n        lr_scheduler=lr_scheduler,\n        lr_scheduler_interval=lr_scheduler_interval,\n    )\n    # Student model\n    self.max_steps: int\n    self.student_projection_mlp = student_projection_mlp\n    self.student_prediction_mlp = student_prediction_mlp\n\n    # Teacher model\n    self.teacher = teacher\n    self.teacher_projection_mlp = teacher_projection_mlp\n    self.teacher_initialized = False\n    self.teacher_momentum = teacher_momentum\n    self.teacher_momentum_cosine_decay = teacher_momentum_cosine_decay\n\n    self.initialize_teacher()\n</code></pre>"},{"location":"reference/quadra/modules/ssl/byol.html#quadra.modules.ssl.byol.BYOL.calculate_accuracy","title":"<code>calculate_accuracy(batch)</code>","text":"<p>Calculate accuracy on the given batch.</p> Source code in <code>quadra/modules/ssl/byol.py</code> <pre><code>def calculate_accuracy(self, batch):\n\"\"\"Calculate accuracy on the given batch.\"\"\"\n    images, labels = batch\n    embedding = self.model(images).detach().cpu().numpy()\n    predictions = self.classifier.predict(embedding)\n    labels = labels.detach()\n    acc = self.val_acc(torch.tensor(predictions, device=self.device), labels)\n\n    return acc\n</code></pre>"},{"location":"reference/quadra/modules/ssl/byol.html#quadra.modules.ssl.byol.BYOL.initialize_teacher","title":"<code>initialize_teacher()</code>","text":"<p>Initialize teacher from the state dict of the student one, checking also that student model requires greadient correctly.</p> Source code in <code>quadra/modules/ssl/byol.py</code> <pre><code>def initialize_teacher(self):\n\"\"\"Initialize teacher from the state dict of the student one,\n    checking also that student model requires greadient correctly.\n    \"\"\"\n    self.teacher_projection_mlp.load_state_dict(self.student_projection_mlp.state_dict())\n    for p in self.teacher_projection_mlp.parameters():\n        p.requires_grad = False\n\n    self.teacher.load_state_dict(self.model.state_dict())\n    for p in self.teacher.parameters():\n        p.requires_grad = False\n\n    for p in self.student_projection_mlp.parameters():\n        assert p.requires_grad is True\n    for p in self.student_prediction_mlp.parameters():\n        assert p.requires_grad is True\n\n    self.teacher_initialized = True\n</code></pre>"},{"location":"reference/quadra/modules/ssl/byol.html#quadra.modules.ssl.byol.BYOL.optimizer_step","title":"<code>optimizer_step(epoch, batch_idx, optimizer, optimizer_closure=None)</code>","text":"<p>Override optimizer step to update the teacher parameters.</p> Source code in <code>quadra/modules/ssl/byol.py</code> <pre><code>def optimizer_step(\n    self,\n    epoch: int,\n    batch_idx: int,\n    optimizer: Optimizer | LightningOptimizer,\n    optimizer_closure: Callable[[], Any] | None = None,\n) -&gt; None:\n\"\"\"Override optimizer step to update the teacher parameters.\"\"\"\n    super().optimizer_step(\n        epoch,\n        batch_idx,\n        optimizer,\n        optimizer_closure=optimizer_closure,\n    )\n    self.update_teacher()\n</code></pre>"},{"location":"reference/quadra/modules/ssl/byol.html#quadra.modules.ssl.byol.BYOL.test_step","title":"<code>test_step(batch, *args)</code>","text":"<p>Calculate accuracy on the test set for the given batch.</p> Source code in <code>quadra/modules/ssl/byol.py</code> <pre><code>def test_step(self, batch, *args: list[Any]) -&gt; None:\n\"\"\"Calculate accuracy on the test set for the given batch.\"\"\"\n    acc = self.calculate_accuracy(batch)\n    self.log(name=\"test_acc\", value=acc, on_step=False, on_epoch=True, prog_bar=True)\n    return acc\n</code></pre>"},{"location":"reference/quadra/modules/ssl/byol.html#quadra.modules.ssl.byol.BYOL.update_teacher","title":"<code>update_teacher()</code>","text":"<p>Update teacher given <code>self.teacher_momentum</code> by an exponential moving average of the student parameters, that is: theta_t * tau + theta_s * (1 - tau), where <code>theta_{s,t}</code> are the parameters of the student and the teacher model, while <code>tau</code> is the teacher momentum. If <code>self.teacher_momentum_cosine_decay</code> is True, then the teacher momentum will follow a cosine scheduling from <code>self.teacher_momentum</code> to 1: tau = 1 - (1 - tau) * (cos(pi * t / T) + 1) / 2, where <code>t</code> is the current step and <code>T</code> is the max number of steps.</p> Source code in <code>quadra/modules/ssl/byol.py</code> <pre><code>def update_teacher(self):\n\"\"\"Update teacher given `self.teacher_momentum` by an exponential moving average\n    of the student parameters, that is: theta_t * tau + theta_s * (1 - tau), where\n    `theta_{s,t}` are the parameters of the student and the teacher model, while `tau` is the\n    teacher momentum. If `self.teacher_momentum_cosine_decay` is True, then the teacher\n    momentum will follow a cosine scheduling from `self.teacher_momentum` to 1:\n    tau = 1 - (1 - tau) * (cos(pi * t / T) + 1) / 2, where `t` is the current step and\n    `T` is the max number of steps.\n    \"\"\"\n    with torch.no_grad():\n        if self.teacher_momentum_cosine_decay:\n            teacher_momentum = (\n                1\n                - (1 - self.teacher_momentum)\n                * (math.cos(math.pi * self.trainer.global_step / self.max_steps) + 1)\n                / 2\n            )\n        else:\n            teacher_momentum = self.teacher_momentum\n        self.log(\"teacher_momentum\", teacher_momentum, prog_bar=True)\n        for student_ps, teacher_ps in zip(\n            list(self.model.parameters()) + list(self.student_projection_mlp.parameters()),\n            list(self.teacher.parameters()) + list(self.teacher_projection_mlp.parameters()),\n        ):\n            teacher_ps.data = teacher_ps.data * teacher_momentum + (1 - teacher_momentum) * student_ps.data\n</code></pre>"},{"location":"reference/quadra/modules/ssl/common.html","title":"common","text":""},{"location":"reference/quadra/modules/ssl/common.html#quadra.modules.ssl.common.BYOLPredictionHead","title":"<code>BYOLPredictionHead(input_dim, hidden_dim, output_dim)</code>","text":"<p>             Bases: <code>ProjectionHead</code></p> <p>Prediction head used for BYOL.</p> Source code in <code>quadra/modules/ssl/common.py</code> <pre><code>def __init__(self, input_dim: int, hidden_dim: int, output_dim: int):\n    super().__init__(\n        [\n            (\n                torch.nn.Linear(input_dim, hidden_dim, bias=False),\n                torch.nn.BatchNorm1d(hidden_dim),\n                torch.nn.ReLU(inplace=True),\n            ),\n            (torch.nn.Linear(hidden_dim, output_dim, bias=False), None, None),\n        ]\n    )\n</code></pre>"},{"location":"reference/quadra/modules/ssl/common.html#quadra.modules.ssl.common.BYOLProjectionHead","title":"<code>BYOLProjectionHead(input_dim, hidden_dim, output_dim)</code>","text":"<p>             Bases: <code>ProjectionHead</code></p> <p>Projection head used for BYOL.</p> Source code in <code>quadra/modules/ssl/common.py</code> <pre><code>def __init__(self, input_dim: int, hidden_dim: int, output_dim: int):\n    super().__init__(\n        [\n            (\n                torch.nn.Linear(input_dim, hidden_dim, bias=False),\n                torch.nn.BatchNorm1d(hidden_dim),\n                torch.nn.ReLU(inplace=True),\n            ),\n            (torch.nn.Linear(hidden_dim, output_dim, bias=False), None, None),\n        ]\n    )\n</code></pre>"},{"location":"reference/quadra/modules/ssl/common.html#quadra.modules.ssl.common.BarlowTwinsProjectionHead","title":"<code>BarlowTwinsProjectionHead(input_dim, hidden_dim, output_dim)</code>","text":"<p>             Bases: <code>ProjectionHead</code></p> <p>Projection head used for Barlow Twins. \"The projector network has three linear layers, each with 8192 output units. The first two layers of the projector are followed by a batch normalization layer and rectified linear units.\" https://arxiv.org/abs/2103.03230.</p> Source code in <code>quadra/modules/ssl/common.py</code> <pre><code>def __init__(self, input_dim: int, hidden_dim: int, output_dim: int):\n    super().__init__(\n        [\n            (\n                torch.nn.Linear(input_dim, hidden_dim, bias=False),\n                torch.nn.BatchNorm1d(hidden_dim),\n                torch.nn.ReLU(inplace=True),\n            ),\n            (\n                torch.nn.Linear(hidden_dim, hidden_dim, bias=False),\n                torch.nn.BatchNorm1d(hidden_dim),\n                torch.nn.ReLU(inplace=True),\n            ),\n            (torch.nn.Linear(hidden_dim, output_dim, bias=False), None, None),\n        ]\n    )\n</code></pre>"},{"location":"reference/quadra/modules/ssl/common.html#quadra.modules.ssl.common.DinoProjectionHead","title":"<code>DinoProjectionHead(input_dim, output_dim, hidden_dim, use_bn=False, norm_last_layer=True, num_layers=3, bottleneck_dim=256)</code>","text":"<p>             Bases: <code>Module</code></p> <p>Projection head used for Dino. This projection head does not have a batch norm layer.</p> <p>Parameters:</p> <ul> <li> input_dim             (<code>int</code>)         \u2013          <p>Input dimension for MLP head.</p> </li> <li> output_dim             (<code>int</code>)         \u2013          <p>Output dimension (projection dimension) for MLP head.</p> </li> <li> hidden_dim             (<code>int</code>)         \u2013          <p>Hidden dimension. Defaults to 512.</p> </li> <li> bottleneck_dim             (<code>int</code>, default:                 <code>256</code> )         \u2013          <p>Bottleneck dimension. Defaults to 256.</p> </li> <li> num_layers             (<code>int</code>, default:                 <code>3</code> )         \u2013          <p>Number of hidden layers used in MLP. Defaults to 3.</p> </li> <li> norm_last_layer             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Decides applying normalization before last layer. Defaults to False.</p> </li> </ul> Source code in <code>quadra/modules/ssl/common.py</code> <pre><code>def __init__(\n    self,\n    input_dim: int,\n    output_dim: int,\n    hidden_dim: int,\n    use_bn: bool = False,\n    norm_last_layer: bool = True,\n    num_layers: int = 3,\n    bottleneck_dim: int = 256,\n):\n    super().__init__()\n    num_layers = max(num_layers, 1)\n    self.mlp: nn.Linear | nn.Sequential\n    if num_layers == 1:\n        self.mlp = nn.Linear(input_dim, bottleneck_dim)\n    else:\n        layers: list[nn.Module] = [nn.Linear(input_dim, hidden_dim)]\n        if use_bn:\n            layers.append(nn.BatchNorm1d(hidden_dim))\n        layers.append(nn.GELU())\n        for _ in range(num_layers - 2):\n            layers.append(nn.Linear(hidden_dim, hidden_dim))\n            if use_bn:\n                layers.append(nn.BatchNorm1d(hidden_dim))\n            layers.append(nn.GELU())\n        layers.append(nn.Linear(hidden_dim, bottleneck_dim))\n        self.mlp = nn.Sequential(*layers)\n    self.apply(self._init_weights)\n    self.last_layer = nn.utils.weight_norm(nn.Linear(bottleneck_dim, output_dim, bias=False))\n    self.last_layer.weight_g.data.fill_(1)\n    if norm_last_layer:\n        self.last_layer.weight_g.requires_grad = False\n</code></pre>"},{"location":"reference/quadra/modules/ssl/common.html#quadra.modules.ssl.common.ExpanderReducer","title":"<code>ExpanderReducer(input_dim, hidden_dim, output_dim)</code>","text":"<p>             Bases: <code>ProjectionHead</code></p> <p>Expander followed by a reducer.</p> Source code in <code>quadra/modules/ssl/common.py</code> <pre><code>def __init__(self, input_dim: int, hidden_dim: int, output_dim: int):\n    super().__init__(\n        [\n            (\n                torch.nn.Linear(input_dim, hidden_dim, bias=False),\n                torch.nn.BatchNorm1d(hidden_dim),\n                torch.nn.ReLU(inplace=True),\n            ),\n            (\n                torch.nn.Linear(hidden_dim, output_dim, bias=False),\n                torch.nn.BatchNorm1d(output_dim, affine=False),\n                torch.nn.ReLU(inplace=True),\n            ),\n        ]\n    )\n</code></pre>"},{"location":"reference/quadra/modules/ssl/common.html#quadra.modules.ssl.common.MultiCropModel","title":"<code>MultiCropModel(backbone, head)</code>","text":"<p>             Bases: <code>Module</code></p> <p>MultiCrop model for DINO augmentation.</p> <p>It takes 2 global crops and N (possible) local crops as a single tensor.</p> <p>Parameters:</p> <ul> <li> backbone             (<code>Module</code>)         \u2013          <p>Backbone model.</p> </li> <li> head             (<code>Module</code>)         \u2013          <p>Head model.</p> </li> </ul> Source code in <code>quadra/modules/ssl/common.py</code> <pre><code>def __init__(self, backbone: nn.Module, head: nn.Module):\n    super().__init__()\n    self.backbone = backbone\n    self.head = head\n</code></pre>"},{"location":"reference/quadra/modules/ssl/common.html#quadra.modules.ssl.common.ProjectionHead","title":"<code>ProjectionHead(blocks)</code>","text":"<p>             Bases: <code>Module</code></p> <p>Base class for all projection and prediction heads.</p> <p>Parameters:</p> <ul> <li> blocks             (<code>list[tuple[Module | None, ...]]</code>)         \u2013          <p>List of tuples, each denoting one block of the projection head MLP. Each tuple reads (linear_layer, batch_norm_layer, non_linearity_layer). <code>batch_norm</code> layer can be possibly None, the same happens for <code>non_linearity_layer</code>.</p> </li> </ul> Source code in <code>quadra/modules/ssl/common.py</code> <pre><code>def __init__(self, blocks: list[tuple[torch.nn.Module | None, ...]]):\n    super().__init__()\n\n    layers: list[nn.Module] = []\n    for linear, batch_norm, non_linearity in blocks:\n        if linear:\n            layers.append(linear)\n        if batch_norm:\n            layers.append(batch_norm)\n        if non_linearity:\n            layers.append(non_linearity)\n    self.layers = torch.nn.Sequential(*layers)\n</code></pre>"},{"location":"reference/quadra/modules/ssl/common.html#quadra.modules.ssl.common.SimCLRPredictionHead","title":"<code>SimCLRPredictionHead(input_dim, hidden_dim, output_dim)</code>","text":"<p>             Bases: <code>ProjectionHead</code></p> <p>Prediction head used for SimCLR. \"We set g(h) = W(2)\u03c3(W(1)h), with the same input and output dimensionality (i.e. 2048).\" https://arxiv.org/abs/2002.05709.</p> Source code in <code>quadra/modules/ssl/common.py</code> <pre><code>def __init__(self, input_dim: int, hidden_dim: int, output_dim: int):\n    super().__init__(\n        [\n            (\n                torch.nn.Linear(input_dim, hidden_dim, bias=False),\n                torch.nn.BatchNorm1d(hidden_dim),\n                torch.nn.ReLU(inplace=True),\n            ),\n            (torch.nn.Linear(hidden_dim, output_dim, bias=False), None, None),\n        ]\n    )\n</code></pre>"},{"location":"reference/quadra/modules/ssl/common.html#quadra.modules.ssl.common.SimCLRProjectionHead","title":"<code>SimCLRProjectionHead(input_dim, hidden_dim, output_dim)</code>","text":"<p>             Bases: <code>ProjectionHead</code></p> <p>Projection head used for SimCLR. \"We use a MLP with one hidden layer to obtain zi = g(h) = W_2 * \u03c3(W_1 * h) where \u03c3 is a ReLU non-linearity.\" https://arxiv.org/abs/2002.05709.</p> Source code in <code>quadra/modules/ssl/common.py</code> <pre><code>def __init__(self, input_dim: int, hidden_dim: int, output_dim: int):\n    super().__init__(\n        [\n            (\n                torch.nn.Linear(input_dim, hidden_dim),\n                None,\n                torch.nn.ReLU(inplace=True),\n            ),\n            (torch.nn.Linear(hidden_dim, output_dim), None, None),\n        ]\n    )\n</code></pre>"},{"location":"reference/quadra/modules/ssl/common.html#quadra.modules.ssl.common.SimSiamPredictionHead","title":"<code>SimSiamPredictionHead(input_dim, hidden_dim, output_dim)</code>","text":"<p>             Bases: <code>ProjectionHead</code></p> <p>Prediction head used for SimSiam. \"The prediction MLP (h) has BN applied to its hidden fc layers. Its output fc does not have BN (...) or ReLU. This MLP has 2 layers.\" https://arxiv.org/abs/2011.10566.</p> Source code in <code>quadra/modules/ssl/common.py</code> <pre><code>def __init__(self, input_dim: int, hidden_dim: int, output_dim: int):\n    super().__init__(\n        [\n            (\n                torch.nn.Linear(input_dim, hidden_dim, bias=False),\n                torch.nn.BatchNorm1d(hidden_dim),\n                torch.nn.ReLU(inplace=True),\n            ),\n            (torch.nn.Linear(hidden_dim, output_dim, bias=False), None, None),\n        ]\n    )\n</code></pre>"},{"location":"reference/quadra/modules/ssl/common.html#quadra.modules.ssl.common.SimSiamProjectionHead","title":"<code>SimSiamProjectionHead(input_dim, hidden_dim, output_dim)</code>","text":"<p>             Bases: <code>ProjectionHead</code></p> <p>Projection head used for SimSiam. \"The projection MLP (in f) has BN applied to each fully-connected (fc) layer, including its output fc. Its output fc has no ReLU. The hidden fc is 2048-d. This MLP has 3 layers.\" https://arxiv.org/abs/2011.10566.</p> Source code in <code>quadra/modules/ssl/common.py</code> <pre><code>def __init__(self, input_dim: int, hidden_dim: int, output_dim: int):\n    super().__init__(\n        [\n            (\n                torch.nn.Linear(input_dim, hidden_dim, bias=False),\n                torch.nn.BatchNorm1d(hidden_dim),\n                torch.nn.ReLU(inplace=True),\n            ),\n            (\n                torch.nn.Linear(hidden_dim, hidden_dim, bias=False),\n                torch.nn.BatchNorm1d(hidden_dim, affine=False),\n                torch.nn.ReLU(inplace=True),\n            ),\n            (\n                torch.nn.Linear(hidden_dim, output_dim, bias=False),\n                torch.nn.BatchNorm1d(output_dim, affine=False),\n                None,\n            ),\n        ]\n    )\n</code></pre>"},{"location":"reference/quadra/modules/ssl/dino.html","title":"dino","text":""},{"location":"reference/quadra/modules/ssl/dino.html#quadra.modules.ssl.dino.Dino","title":"<code>Dino(student, teacher, student_projection_mlp, teacher_projection_mlp, criterion, freeze_last_layer=1, classifier=None, optimizer=None, lr_scheduler=None, lr_scheduler_interval='epoch', teacher_momentum=0.9995, teacher_momentum_cosine_decay=True)</code>","text":"<p>             Bases: <code>BYOL</code></p> <p>DINO pytorch-lightning module.</p> <p>Parameters:</p> <ul> <li> student         \u2013          <p>student model</p> </li> <li> teacher         \u2013          <p>teacher model</p> </li> <li> student_projection_mlp         \u2013          <p>student projection MLP</p> </li> <li> teacher_projection_mlp         \u2013          <p>teacher projection MLP</p> </li> <li> criterion         \u2013          <p>loss function</p> </li> <li> freeze_last_layer         \u2013          <p>number of layers to freeze in the student model. Default: 1</p> </li> <li> classifier             (<code>ClassifierMixin | None</code>, default:                 <code>None</code> )         \u2013          <p>Standard sklearn classifier</p> </li> <li> optimizer             (<code>Optimizer | None</code>, default:                 <code>None</code> )         \u2013          <p>optimizer of the training. If None a default Adam is used.</p> </li> <li> lr_scheduler             (<code>object | None</code>, default:                 <code>None</code> )         \u2013          <p>lr scheduler. If None a default ReduceLROnPlateau is used.</p> </li> <li> lr_scheduler_interval             (<code>str | None</code>, default:                 <code>'epoch'</code> )         \u2013          <p>interval at which the lr scheduler is updated.</p> </li> <li> teacher_momentum             (<code>float</code>, default:                 <code>0.9995</code> )         \u2013          <p>momentum of the teacher parameters</p> </li> <li> teacher_momentum_cosine_decay             (<code>bool | None</code>, default:                 <code>True</code> )         \u2013          <p>whether to use cosine decay for the teacher momentum</p> </li> </ul> Source code in <code>quadra/modules/ssl/dino.py</code> <pre><code>def __init__(\n    self,\n    student: nn.Module,\n    teacher: nn.Module,\n    student_projection_mlp: nn.Module,\n    teacher_projection_mlp: nn.Module,\n    criterion: nn.Module,\n    freeze_last_layer: int = 1,\n    classifier: sklearn.base.ClassifierMixin | None = None,\n    optimizer: Optimizer | None = None,\n    lr_scheduler: object | None = None,\n    lr_scheduler_interval: str | None = \"epoch\",\n    teacher_momentum: float = 0.9995,\n    teacher_momentum_cosine_decay: bool | None = True,\n):\n    super().__init__(\n        student=student,\n        teacher=teacher,\n        student_projection_mlp=student_projection_mlp,\n        student_prediction_mlp=nn.Identity(),\n        teacher_projection_mlp=teacher_projection_mlp,\n        criterion=criterion,\n        teacher_momentum=teacher_momentum,\n        teacher_momentum_cosine_decay=teacher_momentum_cosine_decay,\n        classifier=classifier,\n        optimizer=optimizer,\n        lr_scheduler=lr_scheduler,\n        lr_scheduler_interval=lr_scheduler_interval,\n    )\n    self.freeze_last_layer = freeze_last_layer\n</code></pre>"},{"location":"reference/quadra/modules/ssl/dino.html#quadra.modules.ssl.dino.Dino.cancel_gradients_last_layer","title":"<code>cancel_gradients_last_layer(epoch, freeze_last_layer)</code>","text":"<p>Zero out the gradient of the last layer, as specified in the paper.</p> <p>Parameters:</p> <ul> <li> epoch             (<code>int</code>)         \u2013          <p>current epoch</p> </li> <li> freeze_last_layer             (<code>int</code>)         \u2013          <p>maximum freeze epoch: if <code>epoch</code> &gt;= <code>freeze_last_layer</code> then the gradient of the last layer will not be freezed</p> </li> </ul> Source code in <code>quadra/modules/ssl/dino.py</code> <pre><code>def cancel_gradients_last_layer(self, epoch: int, freeze_last_layer: int):\n\"\"\"Zero out the gradient of the last layer, as specified in the paper.\n\n    Args:\n        epoch: current epoch\n        freeze_last_layer: maximum freeze epoch: if `epoch` &gt;= `freeze_last_layer`\n            then the gradient of the last layer will not be freezed\n    \"\"\"\n    if epoch &gt;= freeze_last_layer:\n        return\n    for n, p in self.student_projection_mlp.named_parameters():\n        if \"last_layer\" in n:\n            p.grad = None\n</code></pre>"},{"location":"reference/quadra/modules/ssl/dino.html#quadra.modules.ssl.dino.Dino.configure_gradient_clipping","title":"<code>configure_gradient_clipping(optimizer, gradient_clip_val=None, gradient_clip_algorithm=None)</code>","text":"<p>Configure gradient clipping for the optimizer.</p> Source code in <code>quadra/modules/ssl/dino.py</code> <pre><code>def configure_gradient_clipping(\n    self,\n    optimizer: Optimizer,\n    gradient_clip_val: int | float | None = None,\n    gradient_clip_algorithm: str | None = None,\n):\n\"\"\"Configure gradient clipping for the optimizer.\"\"\"\n    if gradient_clip_algorithm is not None and gradient_clip_val is not None:\n        clip_gradients(self.model, gradient_clip_val)\n        clip_gradients(self.student_projection_mlp, gradient_clip_val)\n    self.cancel_gradients_last_layer(self.current_epoch, self.freeze_last_layer)\n</code></pre>"},{"location":"reference/quadra/modules/ssl/dino.html#quadra.modules.ssl.dino.Dino.initialize_teacher","title":"<code>initialize_teacher()</code>","text":"<p>Initialize teacher from the state dict of the student one, checking also that student model requires greadient correctly.</p> Source code in <code>quadra/modules/ssl/dino.py</code> <pre><code>def initialize_teacher(self):\n\"\"\"Initialize teacher from the state dict of the student one,\n    checking also that student model requires greadient correctly.\n    \"\"\"\n    self.teacher_projection_mlp.load_state_dict(self.student_projection_mlp.state_dict())\n    for p in self.teacher_projection_mlp.parameters():\n        p.requires_grad = False\n\n    self.teacher.load_state_dict(self.model.state_dict())\n    for p in self.teacher.parameters():\n        p.requires_grad = False\n\n    all_frozen = True\n    for p in self.model.parameters():\n        all_frozen = all_frozen and (not p.requires_grad)\n\n    if all_frozen:\n        log.warning(\n            \"All parameters of the student model are frozen, the model will not be trained, automatically\"\n            \" unfreezing all the layers\"\n        )\n\n        for p in self.model.parameters():\n            p.requires_grad = True\n\n    for name, p in self.student_projection_mlp.named_parameters():\n        if name != \"last_layer.weight_g\":\n            assert p.requires_grad is True\n\n    self.teacher_initialized = True\n</code></pre>"},{"location":"reference/quadra/modules/ssl/dino.html#quadra.modules.ssl.dino.Dino.optimizer_step","title":"<code>optimizer_step(epoch, batch_idx, optimizer, optimizer_closure=None)</code>","text":"<p>Override optimizer step to update the teacher parameters.</p> Source code in <code>quadra/modules/ssl/dino.py</code> <pre><code>def optimizer_step(\n    self,\n    epoch: int,\n    batch_idx: int,\n    optimizer: Optimizer | LightningOptimizer,\n    optimizer_closure: Callable[[], Any] | None = None,\n) -&gt; None:\n\"\"\"Override optimizer step to update the teacher parameters.\"\"\"\n    super().optimizer_step(\n        epoch,\n        batch_idx,\n        optimizer,\n        optimizer_closure=optimizer_closure,\n    )\n    self.update_teacher()\n</code></pre>"},{"location":"reference/quadra/modules/ssl/dino.html#quadra.modules.ssl.dino.Dino.student_multicrop_forward","title":"<code>student_multicrop_forward(x)</code>","text":"<p>Student forward on the multicrop imges.</p> <p>Parameters:</p> <ul> <li> x             (<code>list[Tensor]</code>)         \u2013          <p>List of torch.Tensor containing multicropped augmented images</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>         \u2013          <p>torch.Tensor: a tensor of shape NxBxD, where N is the number crops corresponding to the length of the input list <code>x</code>, B is the batch size and D is the output dimension</p> </li> </ul> Source code in <code>quadra/modules/ssl/dino.py</code> <pre><code>def student_multicrop_forward(self, x: list[torch.Tensor]) -&gt; torch.Tensor:\n\"\"\"Student forward on the multicrop imges.\n\n    Args:\n        x: List of torch.Tensor containing multicropped augmented images\n\n    Returns:\n        torch.Tensor: a tensor of shape NxBxD, where N is the number crops\n            corresponding to the length of the input list `x`, B is the batch size\n            and D is the output dimension\n    \"\"\"\n    n_crops = len(x)\n    concatenated = torch.cat(x, dim=0)  # (n_samples * n_crops, C, H, W)\n    embedding = self.model(concatenated)  # (n_samples * n_crops, in_dim)\n    logits = self.student_projection_mlp(embedding)  # (n_samples * n_crops, out_dim)\n    chunks = logits.chunk(n_crops)  # n_crops * (n_samples, out_dim)\n    return chunks\n</code></pre>"},{"location":"reference/quadra/modules/ssl/dino.html#quadra.modules.ssl.dino.Dino.teacher_multicrop_forward","title":"<code>teacher_multicrop_forward(x)</code>","text":"<p>Teacher forward on the multicrop imges.</p> <p>Parameters:</p> <ul> <li> x             (<code>list[Tensor]</code>)         \u2013          <p>List of torch.Tensor containing multicropped augmented images</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>         \u2013          <p>torch.Tensor: a tensor of shape NxBxD, where N is the number crops corresponding to the length of the input list <code>x</code>, B is the batch size and D is the output dimension</p> </li> </ul> Source code in <code>quadra/modules/ssl/dino.py</code> <pre><code>def teacher_multicrop_forward(self, x: list[torch.Tensor]) -&gt; torch.Tensor:\n\"\"\"Teacher forward on the multicrop imges.\n\n    Args:\n        x: List of torch.Tensor containing multicropped augmented images\n\n    Returns:\n        torch.Tensor: a tensor of shape NxBxD, where N is the number crops\n            corresponding to the length of the input list `x`, B is the batch size\n            and D is the output dimension\n    \"\"\"\n    n_crops = len(x)\n    concatenated = torch.cat(x, dim=0)  # (n_samples * n_crops, C, H, W)\n    embedding = self.teacher(concatenated)  # (n_samples * n_crops, in_dim)\n    logits = self.teacher_projection_mlp(embedding)  # (n_samples * n_crops, out_dim)\n    chunks = logits.chunk(n_crops)  # n_crops * (n_samples, out_dim)\n    return chunks\n</code></pre>"},{"location":"reference/quadra/modules/ssl/hyperspherical.html","title":"hyperspherical","text":""},{"location":"reference/quadra/modules/ssl/hyperspherical.html#quadra.modules.ssl.hyperspherical.AlignLoss","title":"<code>AlignLoss</code>","text":"<p>             Bases: <code>Enum</code></p> <p>Align loss enum.</p>"},{"location":"reference/quadra/modules/ssl/hyperspherical.html#quadra.modules.ssl.hyperspherical.TLHyperspherical","title":"<code>TLHyperspherical(model, optimizer=None, lr_scheduler=None, align_weight=1, unifo_weight=1, classifier_weight=1, align_loss_type=AlignLoss.L2, classifier_loss=False, num_classes=None)</code>","text":"<p>             Bases: <code>BaseLightningModule</code></p> <p>Hyperspherical model: maps features extracted from a pretrained backbone into an hypersphere.</p> <p>Parameters:</p> <ul> <li> model             (<code>Module</code>)         \u2013          <p>Feature extractor as pytorch <code>torch.nn.Module</code></p> </li> <li> optimizer             (<code>Optimizer | None</code>, default:                 <code>None</code> )         \u2013          <p>optimizer of the training. If None a default Adam is used.</p> </li> <li> lr_scheduler             (<code>object | None</code>, default:                 <code>None</code> )         \u2013          <p>lr scheduler. If None a default ReduceLROnPlateau is used.</p> </li> <li> align_weight             (<code>float</code>, default:                 <code>1</code> )         \u2013          <p>Weight for the align loss component for the hyperspherical loss. Defaults to 1.</p> </li> <li> unifo_weight             (<code>float</code>, default:                 <code>1</code> )         \u2013          <p>Weight for the uniform loss component for the hyperspherical loss. Defaults to 1.</p> </li> <li> classifier_weight             (<code>float</code>, default:                 <code>1</code> )         \u2013          <p>Weight for the classifier loss component for the hyperspherical loss. Defaults to 1.</p> </li> <li> align_loss_type             (<code>AlignLoss</code>, default:                 <code>L2</code> )         \u2013          <p>Which type of align loss to use. Defaults to AlignLoss.L2.</p> </li> <li> classifier_loss             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to compute a classifier loss to 'enhance' the hyperpsherical loss with the classification loss. It True, model.classifier must be defined Defaults to False.</p> </li> <li> num_classes             (<code>int | None</code>, default:                 <code>None</code> )         \u2013          <p>Number of classes for a classification problem. Defaults to None.</p> </li> </ul> Source code in <code>quadra/modules/ssl/hyperspherical.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    optimizer: optim.Optimizer | None = None,\n    lr_scheduler: object | None = None,\n    align_weight: float = 1,\n    unifo_weight: float = 1,\n    classifier_weight: float = 1,\n    align_loss_type: AlignLoss = AlignLoss.L2,\n    classifier_loss: bool = False,\n    num_classes: int | None = None,\n):\n    super().__init__(model, optimizer, lr_scheduler)\n    self.align_loss_fun: (\n        Callable[[torch.Tensor, torch.Tensor, int], torch.Tensor]\n        | Callable[[torch.Tensor, torch.Tensor], torch.Tensor]\n    )\n    self.align_weight = align_weight\n    self.unifo_weight = unifo_weight\n    self.classifier_weight = classifier_weight\n    self.align_loss_type = align_loss_type\n    if align_loss_type == AlignLoss.L2:\n        self.align_loss_fun = loss.align_loss\n    elif align_loss_type == AlignLoss.COSINE:\n        self.align_loss_fun = loss.cosine_align_loss\n    else:\n        raise ValueError(\"The align loss must be one of 'AlignLoss.L2' (L2 distance) or AlignLoss.COSINE\")\n\n    if classifier_loss and model.classifier is None:\n        raise AssertionError(\"Classifier is not defined\")\n\n    self.classifier_loss = classifier_loss\n    self.num_classes = num_classes\n</code></pre>"},{"location":"reference/quadra/modules/ssl/idmm.html","title":"idmm","text":""},{"location":"reference/quadra/modules/ssl/idmm.html#quadra.modules.ssl.idmm.IDMM","title":"<code>IDMM(model, prediction_mlp, criterion, multiview_loss=True, mixup_fn=None, classifier=None, optimizer=None, lr_scheduler=None, lr_scheduler_interval='epoch')</code>","text":"<p>             Bases: <code>SSLModule</code></p> <p>IDMM model.</p> <p>Parameters:</p> <ul> <li> model             (<code>Module</code>)         \u2013          <p>backbone model</p> </li> <li> prediction_mlp             (<code>Module</code>)         \u2013          <p>student prediction MLP</p> </li> <li> criterion             (<code>Module</code>)         \u2013          <p>loss function</p> </li> <li> multiview_loss             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>whether to use the multiview loss as definied in https://arxiv.org/abs/2201.10728. Defaults to True.</p> </li> <li> mixup_fn             (<code>Mixup | None</code>, default:                 <code>None</code> )         \u2013          <p>the mixup/cutmix function to be applied to a batch of images. Defaults to None.</p> </li> <li> classifier             (<code>ClassifierMixin | None</code>, default:                 <code>None</code> )         \u2013          <p>Standard sklearn classifier</p> </li> <li> optimizer             (<code>Optimizer | None</code>, default:                 <code>None</code> )         \u2013          <p>optimizer of the training. If None a default Adam is used.</p> </li> <li> lr_scheduler             (<code>object | None</code>, default:                 <code>None</code> )         \u2013          <p>lr scheduler. If None a default ReduceLROnPlateau is used.</p> </li> <li> lr_scheduler_interval             (<code>str | None</code>, default:                 <code>'epoch'</code> )         \u2013          <p>interval at which the lr scheduler is updated.</p> </li> </ul> Source code in <code>quadra/modules/ssl/idmm.py</code> <pre><code>def __init__(\n    self,\n    model: torch.nn.Module,\n    prediction_mlp: torch.nn.Module,\n    criterion: torch.nn.Module,\n    multiview_loss: bool = True,\n    mixup_fn: timm.data.Mixup | None = None,\n    classifier: sklearn.base.ClassifierMixin | None = None,\n    optimizer: torch.optim.Optimizer | None = None,\n    lr_scheduler: object | None = None,\n    lr_scheduler_interval: str | None = \"epoch\",\n):\n    super().__init__(\n        model,\n        criterion,\n        classifier,\n        optimizer,\n        lr_scheduler,\n        lr_scheduler_interval,\n    )\n    # self.save_hyperparameters()\n    self.prediction_mlp = prediction_mlp\n    self.mixup_fn = mixup_fn\n    self.multiview_loss = multiview_loss\n</code></pre>"},{"location":"reference/quadra/modules/ssl/simclr.html","title":"simclr","text":""},{"location":"reference/quadra/modules/ssl/simclr.html#quadra.modules.ssl.simclr.SimCLR","title":"<code>SimCLR(model, projection_mlp, criterion, classifier=None, optimizer=None, lr_scheduler=None, lr_scheduler_interval='epoch')</code>","text":"<p>             Bases: <code>SSLModule</code></p> <p>SIMCLR class.</p> <p>Parameters:</p> <ul> <li> model             (<code>Module</code>)         \u2013          <p>Feature extractor as pytorch <code>torch.nn.Module</code></p> </li> <li> projection_mlp             (<code>Module</code>)         \u2013          <p>projection head as pytorch <code>torch.nn.Module</code></p> </li> <li> criterion             (<code>Module</code>)         \u2013          <p>SSL loss to be applied</p> </li> <li> classifier             (<code>ClassifierMixin | None</code>, default:                 <code>None</code> )         \u2013          <p>Standard sklearn classifier. Defaults to None.</p> </li> <li> optimizer             (<code>Optimizer | None</code>, default:                 <code>None</code> )         \u2013          <p>optimizer of the training. If None a default Adam is used. Defaults to None.</p> </li> <li> lr_scheduler             (<code>object | None</code>, default:                 <code>None</code> )         \u2013          <p>lr scheduler. If None a default ReduceLROnPlateau is used. Defaults to None.</p> </li> <li> lr_scheduler_interval             (<code>str | None</code>, default:                 <code>'epoch'</code> )         \u2013          <p>interval at which the lr scheduler is updated. Defaults to \"epoch\".</p> </li> </ul> Source code in <code>quadra/modules/ssl/simclr.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    projection_mlp: nn.Module,\n    criterion: torch.nn.Module,\n    classifier: sklearn.base.ClassifierMixin | None = None,\n    optimizer: torch.optim.Optimizer | None = None,\n    lr_scheduler: object | None = None,\n    lr_scheduler_interval: str | None = \"epoch\",\n):\n    super().__init__(\n        model,\n        criterion,\n        classifier,\n        optimizer,\n        lr_scheduler,\n        lr_scheduler_interval,\n    )\n    self.projection_mlp = projection_mlp\n</code></pre>"},{"location":"reference/quadra/modules/ssl/simclr.html#quadra.modules.ssl.simclr.SimCLR.training_step","title":"<code>training_step(batch, batch_idx)</code>","text":"<p>Parameters:</p> <ul> <li> batch             (<code>tuple[tuple[Tensor, Tensor], Tensor]</code>)         \u2013          <p>The batch of data</p> </li> <li> batch_idx             (<code>int</code>)         \u2013          <p>The index of the batch.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>         \u2013          <p>The computed loss</p> </li> </ul> Source code in <code>quadra/modules/ssl/simclr.py</code> <pre><code>def training_step(\n    self, batch: tuple[tuple[torch.Tensor, torch.Tensor], torch.Tensor], batch_idx: int\n) -&gt; torch.Tensor:\n\"\"\"Args:\n        batch: The batch of data\n        batch_idx: The index of the batch.\n\n    Returns:\n        The computed loss\n    \"\"\"\n    # pylint: disable=unused-argument\n    (im_x, im_y), _ = batch\n    emb_x = self(im_x)\n    emb_y = self(im_y)\n    loss = self.criterion(emb_x, emb_y)\n\n    self.log(\n        \"loss\",\n        loss,\n        on_epoch=True,\n        on_step=True,\n        logger=True,\n        prog_bar=True,\n    )\n    return loss\n</code></pre>"},{"location":"reference/quadra/modules/ssl/simsiam.html","title":"simsiam","text":""},{"location":"reference/quadra/modules/ssl/simsiam.html#quadra.modules.ssl.simsiam.SimSIAM","title":"<code>SimSIAM(model, projection_mlp, prediction_mlp, criterion, classifier=None, optimizer=None, lr_scheduler=None, lr_scheduler_interval='epoch')</code>","text":"<p>             Bases: <code>SSLModule</code></p> <p>SimSIAM model.</p> <p>Parameters:</p> <ul> <li> model             (<code>Module</code>)         \u2013          <p>Feature extractor as pytorch <code>torch.nn.Module</code></p> </li> <li> projection_mlp             (<code>Module</code>)         \u2013          <p>optional projection head as pytorch <code>torch.nn.Module</code></p> </li> <li> prediction_mlp             (<code>Module</code>)         \u2013          <p>optional predicition head as pytorch <code>torch.nn.Module</code></p> </li> <li> criterion             (<code>Module</code>)         \u2013          <p>loss to be applied.</p> </li> <li> classifier             (<code>ClassifierMixin | None</code>, default:                 <code>None</code> )         \u2013          <p>Standard sklearn classifier.</p> </li> <li> optimizer             (<code>Optimizer | None</code>, default:                 <code>None</code> )         \u2013          <p>optimizer of the training. If None a default Adam is used.</p> </li> <li> lr_scheduler             (<code>object | None</code>, default:                 <code>None</code> )         \u2013          <p>lr scheduler. If None a default ReduceLROnPlateau is used.</p> </li> <li> lr_scheduler_interval             (<code>str | None</code>, default:                 <code>'epoch'</code> )         \u2013          <p>interval at which the lr scheduler is updated.</p> </li> </ul> Source code in <code>quadra/modules/ssl/simsiam.py</code> <pre><code>def __init__(\n    self,\n    model: torch.nn.Module,\n    projection_mlp: torch.nn.Module,\n    prediction_mlp: torch.nn.Module,\n    criterion: torch.nn.Module,\n    classifier: sklearn.base.ClassifierMixin | None = None,\n    optimizer: torch.optim.Optimizer | None = None,\n    lr_scheduler: object | None = None,\n    lr_scheduler_interval: str | None = \"epoch\",\n):\n    super().__init__(\n        model,\n        criterion,\n        classifier,\n        optimizer,\n        lr_scheduler,\n        lr_scheduler_interval,\n    )\n    # self.save_hyperparameters()\n    self.projection_mlp = projection_mlp\n    self.prediction_mlp = prediction_mlp\n</code></pre>"},{"location":"reference/quadra/modules/ssl/vicreg.html","title":"vicreg","text":""},{"location":"reference/quadra/modules/ssl/vicreg.html#quadra.modules.ssl.vicreg.VICReg","title":"<code>VICReg(model, projection_mlp, criterion, classifier=None, optimizer=None, lr_scheduler=None, lr_scheduler_interval='epoch')</code>","text":"<p>             Bases: <code>SSLModule</code></p> <p>VICReg model.</p> <p>Parameters:</p> <ul> <li> model             (<code>Module</code>)         \u2013          <p>Network Module used for extract features</p> </li> <li> projection_mlp             (<code>Module</code>)         \u2013          <p>Module to project extracted features</p> </li> <li> criterion             (<code>Module</code>)         \u2013          <p>SSL loss to be applied.</p> </li> <li> classifier             (<code>ClassifierMixin | None</code>, default:                 <code>None</code> )         \u2013          <p>Standard sklearn classifier. Defaults to None.</p> </li> <li> optimizer             (<code>Optimizer | None</code>, default:                 <code>None</code> )         \u2013          <p>optimizer of the training. If None a default Adam is used. Defaults to None.</p> </li> <li> lr_scheduler             (<code>object | None</code>, default:                 <code>None</code> )         \u2013          <p>lr scheduler. If None a default ReduceLROnPlateau is used. Defaults to None.</p> </li> <li> lr_scheduler_interval             (<code>str | None</code>, default:                 <code>'epoch'</code> )         \u2013          <p>interval at which the lr scheduler is updated. Defaults to \"epoch\".</p> </li> </ul> Source code in <code>quadra/modules/ssl/vicreg.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    projection_mlp: nn.Module,\n    criterion: nn.Module,\n    classifier: sklearn.base.ClassifierMixin | None = None,\n    optimizer: optim.Optimizer | None = None,\n    lr_scheduler: object | None = None,\n    lr_scheduler_interval: str | None = \"epoch\",\n):\n    super().__init__(\n        model,\n        criterion,\n        classifier,\n        optimizer,\n        lr_scheduler,\n        lr_scheduler_interval,\n    )\n    # self.save_hyperparameters()\n    self.projection_mlp = projection_mlp\n    self.criterion = criterion\n</code></pre>"},{"location":"reference/quadra/optimizers/index.html","title":"optimizers","text":""},{"location":"reference/quadra/optimizers/index.html#quadra.optimizers.LARS","title":"<code>LARS(params, lr=required, momentum=0, dampening=0, weight_decay=0, nesterov=False, trust_coefficient=0.001, eps=1e-08)</code>","text":"<p>             Bases: <code>Optimizer</code></p> <p>Extends SGD in PyTorch with LARS scaling from the paper <code>Large batch training of Convolutional Networks &lt;https://arxiv.org/pdf/1708.03888.pdf&gt;</code>_.</p> <p>Parameters:</p> <ul> <li> params             (<code>list[Parameter]</code>)         \u2013          <p>iterable of parameters to optimize or dicts defining parameter groups</p> </li> <li> lr             (<code>_RequiredParameter</code>, default:                 <code>required</code> )         \u2013          <p>learning rate</p> </li> <li> momentum             (<code>float</code>, default:                 <code>0</code> )         \u2013          <p>momentum factor (default: 0)</p> </li> <li> weight_decay             (<code>float</code>, default:                 <code>0</code> )         \u2013          <p>weight decay (L2 penalty) (default: 0)</p> </li> <li> dampening             (<code>float</code>, default:                 <code>0</code> )         \u2013          <p>dampening for momentum (default: 0)</p> </li> <li> nesterov             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>enables Nesterov momentum (default: False)</p> </li> <li> trust_coefficient             (<code>float</code>, default:                 <code>0.001</code> )         \u2013          <p>trust coefficient for computing LR (default: 0.001)</p> </li> <li> eps             (<code>float</code>, default:                 <code>1e-08</code> )         \u2013          <p>eps for division denominator (default: 1e-8).</p> </li> </ul> Example <p>model = torch.nn.Linear(10, 1) input = torch.Tensor(10) target = torch.Tensor([1.]) loss_fn = lambda input, target: (input - target) ** 2</p> <p>.. note::     The application of momentum in the SGD part is modified according to     the PyTorch standards. LARS scaling fits into the equation in the     following fashion.</p> <pre><code>.. math::\n    \\begin{aligned}\n        g_{t+1} &amp; = \\text{lars_lr} * (\\beta * p_{t} + g_{t+1}), \\\\\n        v_{t+1} &amp; = \\\\mu * v_{t} + g_{t+1}, \\\\\n        p_{t+1} &amp; = p_{t} - \\text{lr} * v_{t+1},\n    \\\\end{aligned}\n\nwhere :math:`p`, :math:`g`, :math:`v`, :math:`\\\\mu` and :math:`\\beta` denote the\nparameters, gradient, velocity, momentum, and weight decay respectively.\nThe :math:`lars_lr` is defined by Eq. 6 in the paper.\nThe Nesterov version is analogously modified.\n</code></pre> <p>.. warning::     Parameters with weight decay set to 0 will automatically be excluded from     layer-wise LR scaling. This is to ensure consistency with papers like SimCLR     and BYOL.</p> Source code in <code>quadra/optimizers/lars.py</code> <pre><code>def __init__(\n    self,\n    params: list[Parameter],\n    lr: _RequiredParameter = required,\n    momentum: float = 0,\n    dampening: float = 0,\n    weight_decay: float = 0,\n    nesterov: bool = False,\n    trust_coefficient: float = 0.001,\n    eps: float = 1e-8,\n):\n    if lr is not required and lr &lt; 0.0:  # type: ignore[operator]\n        raise ValueError(f\"Invalid learning rate: {lr}\")\n    if momentum &lt; 0.0:\n        raise ValueError(f\"Invalid momentum value: {momentum}\")\n    if weight_decay &lt; 0.0:\n        raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n\n    defaults = {\n        \"lr\": lr,\n        \"momentum\": momentum,\n        \"dampening\": dampening,\n        \"weight_decay\": weight_decay,\n        \"nesterov\": nesterov,\n    }\n    if nesterov and (momentum &lt;= 0 or dampening != 0):\n        raise ValueError(\"Nesterov momentum requires a momentum and zero dampening\")\n\n    self.eps = eps\n    self.trust_coefficient = trust_coefficient\n\n    super().__init__(params, defaults)\n</code></pre>"},{"location":"reference/quadra/optimizers/index.html#quadra.optimizers.LARS--_1","title":"optimizers","text":"<p>optimizer = LARS(model.parameters(), lr=0.1, momentum=0.9) optimizer.zero_grad() loss_fn(model(input), target).backward() optimizer.step()</p>"},{"location":"reference/quadra/optimizers/index.html#quadra.optimizers.LARS.step","title":"<code>step(closure=None)</code>","text":"<p>Performs a single optimization step.</p> <p>Parameters:</p> <ul> <li> closure             (<code>Callable | None</code>, default:                 <code>None</code> )         \u2013          <p>A closure that reevaluates the model and returns the loss. Defaults to None.</p> </li> </ul> Source code in <code>quadra/optimizers/lars.py</code> <pre><code>@torch.no_grad()\ndef step(self, closure: Callable | None = None):\n\"\"\"Performs a single optimization step.\n\n    Args:\n        closure: A closure that reevaluates the model and returns the loss. Defaults to None.\n    \"\"\"\n    loss = None\n    if closure is not None:\n        with torch.enable_grad():\n            loss = closure()\n\n    # exclude scaling for params with 0 weight decay\n    for group in self.param_groups:\n        weight_decay = group[\"weight_decay\"]\n        momentum = group[\"momentum\"]\n        dampening = group[\"dampening\"]\n        nesterov = group[\"nesterov\"]\n\n        for p in group[\"params\"]:\n            if p.grad is None:\n                continue\n\n            d_p = p.grad\n            p_norm = torch.norm(p.data)\n            g_norm = torch.norm(p.grad.data)\n\n            # lars scaling + weight decay part\n            if weight_decay != 0 and p_norm != 0 and g_norm != 0:\n                lars_lr = p_norm / (g_norm + p_norm * weight_decay + self.eps)\n                lars_lr *= self.trust_coefficient\n\n                d_p = d_p.add(p, alpha=weight_decay)\n                d_p *= lars_lr\n\n            # sgd part\n            if momentum != 0:\n                param_state = self.state[p]\n                if \"momentum_buffer\" not in param_state:\n                    buf = param_state[\"momentum_buffer\"] = torch.clone(d_p).detach()\n                else:\n                    buf = param_state[\"momentum_buffer\"]\n                    buf.mul_(momentum).add_(d_p, alpha=1 - dampening)\n                if nesterov:\n                    d_p = d_p.add(buf, alpha=momentum)\n                else:\n                    d_p = buf\n\n            p.add_(d_p, alpha=-group[\"lr\"])\n\n    return loss\n</code></pre>"},{"location":"reference/quadra/optimizers/index.html#quadra.optimizers.SAM","title":"<code>SAM(params, base_optimizer, rho=0.05, adaptive=True, **kwargs)</code>","text":"<p>             Bases: <code>Optimizer</code></p> <p>PyTorch implementation of Sharpness-Aware-Minization paper: https://arxiv.org/abs/2010.01412 and https://arxiv.org/abs/2102.11600. Taken from: https://github.com/davda54/sam.</p> <p>Parameters:</p> <ul> <li> params             (<code>list[Parameter]</code>)         \u2013          <p>model parameters.</p> </li> <li> base_optimizer             (<code>Optimizer</code>)         \u2013          <p>optimizer to use.</p> </li> <li> rho             (<code>float</code>, default:                 <code>0.05</code> )         \u2013          <p>Postive float value used to scale the gradients.</p> </li> <li> adaptive             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Boolean flag indicating whether to use adaptive step update.</p> </li> <li> **kwargs             (<code>Any</code>, default:                 <code>{}</code> )         \u2013          <p>Additional parameters for the base optimizer.</p> </li> </ul> Source code in <code>quadra/optimizers/sam.py</code> <pre><code>def __init__(\n    self,\n    params: list[Parameter],\n    base_optimizer: torch.optim.Optimizer,\n    rho: float = 0.05,\n    adaptive: bool = True,\n    **kwargs: Any,\n):\n    assert rho &gt;= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n\n    defaults = {\"rho\": rho, \"adaptive\": adaptive, **kwargs}\n    super().__init__(params, defaults)\n\n    if callable(base_optimizer):\n        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n    else:\n        self.base_optimizer = base_optimizer\n    self.rho = rho\n    self.adaptive = adaptive\n    self.param_groups = self.base_optimizer.param_groups\n</code></pre>"},{"location":"reference/quadra/optimizers/index.html#quadra.optimizers.SAM.first_step","title":"<code>first_step(zero_grad=False)</code>","text":"<p>First step for SAM optimizer.</p> <p>Parameters:</p> <ul> <li> zero_grad             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Boolean flag indicating whether to zero the gradients.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>None</code>         \u2013          <p>None</p> </li> </ul> Source code in <code>quadra/optimizers/sam.py</code> <pre><code>@torch.no_grad()\ndef first_step(self, zero_grad: bool = False) -&gt; None:\n\"\"\"First step for SAM optimizer.\n\n    Args:\n        zero_grad: Boolean flag indicating whether to zero the gradients.\n\n    Returns:\n        None\n    \"\"\"\n    grad_norm = self._grad_norm()\n    for group in self.param_groups:\n        scale = self.rho / (grad_norm + 1e-12)\n\n        for p in group[\"params\"]:\n            if p.grad is None:\n                continue\n            e_w = (torch.pow(p, 2) if self.adaptive else 1.0) * p.grad * scale.to(p)\n            p.add_(e_w)  # climb to the local maximum \"w + e(w)\"\n            self.state[p][\"e_w\"] = e_w\n\n    if zero_grad:\n        self.zero_grad()\n</code></pre>"},{"location":"reference/quadra/optimizers/index.html#quadra.optimizers.SAM.second_step","title":"<code>second_step(zero_grad=False)</code>","text":"<p>Second step for SAM optimizer.</p> <p>Parameters:</p> <ul> <li> zero_grad             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Boolean flag indicating whether to zero the gradients.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>None</code>         \u2013          <p>None</p> </li> </ul> Source code in <code>quadra/optimizers/sam.py</code> <pre><code>@torch.no_grad()\ndef second_step(self, zero_grad: bool = False) -&gt; None:\n\"\"\"Second step for SAM optimizer.\n\n    Args:\n        zero_grad: Boolean flag indicating whether to zero the gradients.\n\n    Returns:\n        None\n\n    \"\"\"\n    for group in self.param_groups:\n        for p in group[\"params\"]:\n            if p.grad is None:\n                continue\n            p.sub_(self.state[p][\"e_w\"])  # get back to \"w\" from \"w + e(w)\"\n\n    self.base_optimizer.step()  # do the actual \"sharpness-aware\" update\n\n    if zero_grad:\n        self.zero_grad()\n</code></pre>"},{"location":"reference/quadra/optimizers/index.html#quadra.optimizers.SAM.step","title":"<code>step(closure=None)</code>","text":"<p>Step for SAM optimizer.</p> <p>Parameters:</p> <ul> <li> closure             (<code>Callable | None</code>, default:                 <code>None</code> )         \u2013          <p>The Optional closure for enable grad.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>None</code>         \u2013          <p>None</p> </li> </ul> Source code in <code>quadra/optimizers/sam.py</code> <pre><code>@torch.no_grad()\ndef step(self, closure: Callable | None = None) -&gt; None:  # type: ignore[override]\n\"\"\"Step for SAM optimizer.\n\n    Args:\n        closure: The Optional closure for enable grad.\n\n    Returns:\n        None\n\n    \"\"\"\n    if closure is not None:\n        closure = torch.enable_grad()(closure)\n\n    self.first_step(zero_grad=True)\n    if closure is not None:\n        closure()\n    self.second_step(zero_grad=False)\n</code></pre>"},{"location":"reference/quadra/optimizers/lars.html","title":"lars","text":"<ul> <li>https://arxiv.org/pdf/1708.03888.pdf</li> <li>https://github.com/pytorch/pytorch/blob/1.6/torch/optim/sgd.py.</li> </ul>"},{"location":"reference/quadra/optimizers/lars.html#quadra.optimizers.lars.LARS","title":"<code>LARS(params, lr=required, momentum=0, dampening=0, weight_decay=0, nesterov=False, trust_coefficient=0.001, eps=1e-08)</code>","text":"<p>             Bases: <code>Optimizer</code></p> <p>Extends SGD in PyTorch with LARS scaling from the paper <code>Large batch training of Convolutional Networks &lt;https://arxiv.org/pdf/1708.03888.pdf&gt;</code>_.</p> <p>Parameters:</p> <ul> <li> params             (<code>list[Parameter]</code>)         \u2013          <p>iterable of parameters to optimize or dicts defining parameter groups</p> </li> <li> lr             (<code>_RequiredParameter</code>, default:                 <code>required</code> )         \u2013          <p>learning rate</p> </li> <li> momentum             (<code>float</code>, default:                 <code>0</code> )         \u2013          <p>momentum factor (default: 0)</p> </li> <li> weight_decay             (<code>float</code>, default:                 <code>0</code> )         \u2013          <p>weight decay (L2 penalty) (default: 0)</p> </li> <li> dampening             (<code>float</code>, default:                 <code>0</code> )         \u2013          <p>dampening for momentum (default: 0)</p> </li> <li> nesterov             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>enables Nesterov momentum (default: False)</p> </li> <li> trust_coefficient             (<code>float</code>, default:                 <code>0.001</code> )         \u2013          <p>trust coefficient for computing LR (default: 0.001)</p> </li> <li> eps             (<code>float</code>, default:                 <code>1e-08</code> )         \u2013          <p>eps for division denominator (default: 1e-8).</p> </li> </ul> Example <p>model = torch.nn.Linear(10, 1) input = torch.Tensor(10) target = torch.Tensor([1.]) loss_fn = lambda input, target: (input - target) ** 2</p> <p>.. note::     The application of momentum in the SGD part is modified according to     the PyTorch standards. LARS scaling fits into the equation in the     following fashion.</p> <pre><code>.. math::\n    \\begin{aligned}\n        g_{t+1} &amp; = \\text{lars_lr} * (\\beta * p_{t} + g_{t+1}), \\\\\n        v_{t+1} &amp; = \\\\mu * v_{t} + g_{t+1}, \\\\\n        p_{t+1} &amp; = p_{t} - \\text{lr} * v_{t+1},\n    \\\\end{aligned}\n\nwhere :math:`p`, :math:`g`, :math:`v`, :math:`\\\\mu` and :math:`\\beta` denote the\nparameters, gradient, velocity, momentum, and weight decay respectively.\nThe :math:`lars_lr` is defined by Eq. 6 in the paper.\nThe Nesterov version is analogously modified.\n</code></pre> <p>.. warning::     Parameters with weight decay set to 0 will automatically be excluded from     layer-wise LR scaling. This is to ensure consistency with papers like SimCLR     and BYOL.</p> Source code in <code>quadra/optimizers/lars.py</code> <pre><code>def __init__(\n    self,\n    params: list[Parameter],\n    lr: _RequiredParameter = required,\n    momentum: float = 0,\n    dampening: float = 0,\n    weight_decay: float = 0,\n    nesterov: bool = False,\n    trust_coefficient: float = 0.001,\n    eps: float = 1e-8,\n):\n    if lr is not required and lr &lt; 0.0:  # type: ignore[operator]\n        raise ValueError(f\"Invalid learning rate: {lr}\")\n    if momentum &lt; 0.0:\n        raise ValueError(f\"Invalid momentum value: {momentum}\")\n    if weight_decay &lt; 0.0:\n        raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n\n    defaults = {\n        \"lr\": lr,\n        \"momentum\": momentum,\n        \"dampening\": dampening,\n        \"weight_decay\": weight_decay,\n        \"nesterov\": nesterov,\n    }\n    if nesterov and (momentum &lt;= 0 or dampening != 0):\n        raise ValueError(\"Nesterov momentum requires a momentum and zero dampening\")\n\n    self.eps = eps\n    self.trust_coefficient = trust_coefficient\n\n    super().__init__(params, defaults)\n</code></pre>"},{"location":"reference/quadra/optimizers/lars.html#quadra.optimizers.lars.LARS--_1","title":"lars","text":"<p>optimizer = LARS(model.parameters(), lr=0.1, momentum=0.9) optimizer.zero_grad() loss_fn(model(input), target).backward() optimizer.step()</p>"},{"location":"reference/quadra/optimizers/lars.html#quadra.optimizers.lars.LARS.step","title":"<code>step(closure=None)</code>","text":"<p>Performs a single optimization step.</p> <p>Parameters:</p> <ul> <li> closure             (<code>Callable | None</code>, default:                 <code>None</code> )         \u2013          <p>A closure that reevaluates the model and returns the loss. Defaults to None.</p> </li> </ul> Source code in <code>quadra/optimizers/lars.py</code> <pre><code>@torch.no_grad()\ndef step(self, closure: Callable | None = None):\n\"\"\"Performs a single optimization step.\n\n    Args:\n        closure: A closure that reevaluates the model and returns the loss. Defaults to None.\n    \"\"\"\n    loss = None\n    if closure is not None:\n        with torch.enable_grad():\n            loss = closure()\n\n    # exclude scaling for params with 0 weight decay\n    for group in self.param_groups:\n        weight_decay = group[\"weight_decay\"]\n        momentum = group[\"momentum\"]\n        dampening = group[\"dampening\"]\n        nesterov = group[\"nesterov\"]\n\n        for p in group[\"params\"]:\n            if p.grad is None:\n                continue\n\n            d_p = p.grad\n            p_norm = torch.norm(p.data)\n            g_norm = torch.norm(p.grad.data)\n\n            # lars scaling + weight decay part\n            if weight_decay != 0 and p_norm != 0 and g_norm != 0:\n                lars_lr = p_norm / (g_norm + p_norm * weight_decay + self.eps)\n                lars_lr *= self.trust_coefficient\n\n                d_p = d_p.add(p, alpha=weight_decay)\n                d_p *= lars_lr\n\n            # sgd part\n            if momentum != 0:\n                param_state = self.state[p]\n                if \"momentum_buffer\" not in param_state:\n                    buf = param_state[\"momentum_buffer\"] = torch.clone(d_p).detach()\n                else:\n                    buf = param_state[\"momentum_buffer\"]\n                    buf.mul_(momentum).add_(d_p, alpha=1 - dampening)\n                if nesterov:\n                    d_p = d_p.add(buf, alpha=momentum)\n                else:\n                    d_p = buf\n\n            p.add_(d_p, alpha=-group[\"lr\"])\n\n    return loss\n</code></pre>"},{"location":"reference/quadra/optimizers/sam.html","title":"sam","text":""},{"location":"reference/quadra/optimizers/sam.html#quadra.optimizers.sam.SAM","title":"<code>SAM(params, base_optimizer, rho=0.05, adaptive=True, **kwargs)</code>","text":"<p>             Bases: <code>Optimizer</code></p> <p>PyTorch implementation of Sharpness-Aware-Minization paper: https://arxiv.org/abs/2010.01412 and https://arxiv.org/abs/2102.11600. Taken from: https://github.com/davda54/sam.</p> <p>Parameters:</p> <ul> <li> params             (<code>list[Parameter]</code>)         \u2013          <p>model parameters.</p> </li> <li> base_optimizer             (<code>Optimizer</code>)         \u2013          <p>optimizer to use.</p> </li> <li> rho             (<code>float</code>, default:                 <code>0.05</code> )         \u2013          <p>Postive float value used to scale the gradients.</p> </li> <li> adaptive             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Boolean flag indicating whether to use adaptive step update.</p> </li> <li> **kwargs             (<code>Any</code>, default:                 <code>{}</code> )         \u2013          <p>Additional parameters for the base optimizer.</p> </li> </ul> Source code in <code>quadra/optimizers/sam.py</code> <pre><code>def __init__(\n    self,\n    params: list[Parameter],\n    base_optimizer: torch.optim.Optimizer,\n    rho: float = 0.05,\n    adaptive: bool = True,\n    **kwargs: Any,\n):\n    assert rho &gt;= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n\n    defaults = {\"rho\": rho, \"adaptive\": adaptive, **kwargs}\n    super().__init__(params, defaults)\n\n    if callable(base_optimizer):\n        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n    else:\n        self.base_optimizer = base_optimizer\n    self.rho = rho\n    self.adaptive = adaptive\n    self.param_groups = self.base_optimizer.param_groups\n</code></pre>"},{"location":"reference/quadra/optimizers/sam.html#quadra.optimizers.sam.SAM.first_step","title":"<code>first_step(zero_grad=False)</code>","text":"<p>First step for SAM optimizer.</p> <p>Parameters:</p> <ul> <li> zero_grad             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Boolean flag indicating whether to zero the gradients.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>None</code>         \u2013          <p>None</p> </li> </ul> Source code in <code>quadra/optimizers/sam.py</code> <pre><code>@torch.no_grad()\ndef first_step(self, zero_grad: bool = False) -&gt; None:\n\"\"\"First step for SAM optimizer.\n\n    Args:\n        zero_grad: Boolean flag indicating whether to zero the gradients.\n\n    Returns:\n        None\n    \"\"\"\n    grad_norm = self._grad_norm()\n    for group in self.param_groups:\n        scale = self.rho / (grad_norm + 1e-12)\n\n        for p in group[\"params\"]:\n            if p.grad is None:\n                continue\n            e_w = (torch.pow(p, 2) if self.adaptive else 1.0) * p.grad * scale.to(p)\n            p.add_(e_w)  # climb to the local maximum \"w + e(w)\"\n            self.state[p][\"e_w\"] = e_w\n\n    if zero_grad:\n        self.zero_grad()\n</code></pre>"},{"location":"reference/quadra/optimizers/sam.html#quadra.optimizers.sam.SAM.second_step","title":"<code>second_step(zero_grad=False)</code>","text":"<p>Second step for SAM optimizer.</p> <p>Parameters:</p> <ul> <li> zero_grad             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Boolean flag indicating whether to zero the gradients.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>None</code>         \u2013          <p>None</p> </li> </ul> Source code in <code>quadra/optimizers/sam.py</code> <pre><code>@torch.no_grad()\ndef second_step(self, zero_grad: bool = False) -&gt; None:\n\"\"\"Second step for SAM optimizer.\n\n    Args:\n        zero_grad: Boolean flag indicating whether to zero the gradients.\n\n    Returns:\n        None\n\n    \"\"\"\n    for group in self.param_groups:\n        for p in group[\"params\"]:\n            if p.grad is None:\n                continue\n            p.sub_(self.state[p][\"e_w\"])  # get back to \"w\" from \"w + e(w)\"\n\n    self.base_optimizer.step()  # do the actual \"sharpness-aware\" update\n\n    if zero_grad:\n        self.zero_grad()\n</code></pre>"},{"location":"reference/quadra/optimizers/sam.html#quadra.optimizers.sam.SAM.step","title":"<code>step(closure=None)</code>","text":"<p>Step for SAM optimizer.</p> <p>Parameters:</p> <ul> <li> closure             (<code>Callable | None</code>, default:                 <code>None</code> )         \u2013          <p>The Optional closure for enable grad.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>None</code>         \u2013          <p>None</p> </li> </ul> Source code in <code>quadra/optimizers/sam.py</code> <pre><code>@torch.no_grad()\ndef step(self, closure: Callable | None = None) -&gt; None:  # type: ignore[override]\n\"\"\"Step for SAM optimizer.\n\n    Args:\n        closure: The Optional closure for enable grad.\n\n    Returns:\n        None\n\n    \"\"\"\n    if closure is not None:\n        closure = torch.enable_grad()(closure)\n\n    self.first_step(zero_grad=True)\n    if closure is not None:\n        closure()\n    self.second_step(zero_grad=False)\n</code></pre>"},{"location":"reference/quadra/schedulers/index.html","title":"schedulers","text":""},{"location":"reference/quadra/schedulers/index.html#quadra.schedulers.CosineAnnealingWithLinearWarmUp","title":"<code>CosineAnnealingWithLinearWarmUp(optimizer, batch_size, total_epochs, init_lr=(0.01), lr_scale=256.0, linear_warmup_epochs=10, lr_reduce_factor=0.001, len_loader=None, scheduler_interval='epoch')</code>","text":"<p>             Bases: <code>LearningRateScheduler</code></p> <p>Cosine learning rate scheduler with linear warmup.</p> <p>Parameters:</p> <ul> <li> optimizer             (<code>Optimizer</code>)         \u2013          <p>optimizer for which the learning rate has to be optimized. If your are using this scheduler, than you have set the learning rate of the optimizer to 0</p> </li> <li> batch_size             (<code>int</code>)         \u2013          <p>global batch size of the data loader. For more information please take a look at https://pytorch-lightning.readthedocs.io/en/latest/advanced/multi_gpu.html?highlight=batch%20size#batch-size</p> </li> <li> total_epochs             (<code>int</code>)         \u2013          <p>the total number of epochs</p> </li> <li> init_lr             (<code>tuple[float, ...]</code>, default:                 <code>(0.01)</code> )         \u2013          <p>The initial learning rate, one for every <code>param_group</code>. Mind that the learning rate it's linearly scaled by <code>batch_size</code> / <code>lr_scale</code>, as specified by https://arxiv.org/abs/1706.02677. Defaults to 0.01.</p> </li> <li> lr_scale             (<code>float</code>, default:                 <code>256.0</code> )         \u2013          <p>the learning rate scheduler. Mind that the learning rate it's linearly scaled by <code>batch_size</code> / <code>lr_scale</code> as specified by https://arxiv.org/abs/1706.02677. Defaults to 256.</p> </li> <li> linear_warmup_epochs             (<code>int</code>, default:                 <code>10</code> )         \u2013          <p>how many epochs for the initial linear learning rate scaling. Defaults to 10.</p> </li> <li> lr_reduce_factor             (<code>float</code>, default:                 <code>0.001</code> )         \u2013          <p>factor to be multiplied by scaled lr (init_lr * batch_size / lr_scale) to avoid reaching 0 lr at the end of training.</p> </li> <li> len_loader             (<code>int | None</code>, default:                 <code>None</code> )         \u2013          <p>number of batches in a given dataloader. Remind that the <code>len_loader</code> must be divided by total number of gpus used during the training. If one specifies the <code>len_loader</code> parameter, then the unit measure for the lr update will be in steps (number of batches), not in epochs. Defaults to None.</p> </li> <li> scheduler_interval             (<code>str</code>, default:                 <code>'epoch'</code> )         \u2013          <p>'step' or 'epoch'. If 'step' then the scheduler expects 'len_loader' to be not None. Defaults to <code>epoch</code>.</p> </li> </ul> Source code in <code>quadra/schedulers/warmup.py</code> <pre><code>def __init__(\n    self,\n    optimizer: torch.optim.Optimizer,\n    batch_size: int,\n    total_epochs: int,\n    init_lr: tuple[float, ...] = (0.01,),\n    lr_scale: float = 256.0,\n    linear_warmup_epochs: int = 10,\n    lr_reduce_factor: float = 0.001,\n    len_loader: int | None = None,\n    scheduler_interval: str = \"epoch\",\n) -&gt; None:\n    super().__init__(optimizer, init_lr)\n    assert batch_size &gt; 0\n    assert total_epochs &gt; 0\n    assert lr_scale != 0\n    assert linear_warmup_epochs &gt;= 0\n    assert lr_reduce_factor &gt; 0\n    assert scheduler_interval.lower() in [\"step\", \"epoch\"]\n    self.batch_size = batch_size\n    self.total_epochs = total_epochs\n    self.linear_warmup_epochs = linear_warmup_epochs\n    self.base_lr_scale = self.batch_size / lr_scale\n    self.updates_counter = 1\n    self.init_lr = tuple(lr * self.base_lr_scale for lr in init_lr)\n    self.lr = self.init_lr\n    self.lr_reduce_factor = lr_reduce_factor\n\n    self.scheduler_interval = scheduler_interval.lower()\n    if self.scheduler_interval == \"step\":\n        assert len_loader is not None and len_loader &gt; 0\n        self.total_epochs = len_loader * self.total_epochs\n        self.linear_warmup_epochs = len_loader * self.linear_warmup_epochs\n</code></pre>"},{"location":"reference/quadra/schedulers/index.html#quadra.schedulers.CosineAnnealingWithLinearWarmUp.step","title":"<code>step()</code>","text":"<p>Update the learning rate for the current step.</p> Source code in <code>quadra/schedulers/warmup.py</code> <pre><code>def step(self):\n\"\"\"Update the learning rate for the current step.\"\"\"\n    self.lr = cosine_annealing_with_warmup(\n        self.init_lr,\n        self.updates_counter,\n        self.total_epochs,\n        self.linear_warmup_epochs,\n        self.lr_reduce_factor,\n    )\n    self.set_lr(self.lr)\n    self.updates_counter += 1\n    return self.lr\n</code></pre>"},{"location":"reference/quadra/schedulers/base.html","title":"base","text":""},{"location":"reference/quadra/schedulers/base.html#quadra.schedulers.base.LearningRateScheduler","title":"<code>LearningRateScheduler(optimizer, init_lr)</code>","text":"<p>             Bases: <code>_LRScheduler</code></p> <p>Provides inteface of learning rate scheduler.</p> Note <p>Do not use this class directly, use one of the sub classes.</p> Source code in <code>quadra/schedulers/base.py</code> <pre><code>def __init__(self, optimizer: Optimizer, init_lr: tuple[float, ...]):\n    # pylint: disable=super-init-not-called\n    self.optimizer = optimizer\n    self.init_lr = init_lr\n</code></pre>"},{"location":"reference/quadra/schedulers/base.html#quadra.schedulers.base.LearningRateScheduler.get_lr","title":"<code>get_lr()</code>","text":"<p>Get the current learning rate if the optimizer is available.</p> Source code in <code>quadra/schedulers/base.py</code> <pre><code>def get_lr(self):\n\"\"\"Get the current learning rate if the optimizer is available.\"\"\"\n    if self.optimizer is not None:\n        for g in self.optimizer.param_groups:\n            return g[\"lr\"]\n\n    return None\n</code></pre>"},{"location":"reference/quadra/schedulers/base.html#quadra.schedulers.base.LearningRateScheduler.set_lr","title":"<code>set_lr(lr)</code>","text":"<p>Set the learning rate for the optimizer.</p> Source code in <code>quadra/schedulers/base.py</code> <pre><code>def set_lr(self, lr: tuple[float, ...]):\n\"\"\"Set the learning rate for the optimizer.\"\"\"\n    if self.optimizer is not None:\n        for i, g in enumerate(self.optimizer.param_groups):\n            if \"fix_lr\" in g and g[\"fix_lr\"]:\n                if len(lr) == 1:\n                    lr_to_set = self.init_lr[0]\n                else:\n                    lr_to_set = self.init_lr[i]\n            elif len(lr) == 1:\n                lr_to_set = lr[0]\n            else:\n                lr_to_set = lr[i]\n            g[\"lr\"] = lr_to_set\n</code></pre>"},{"location":"reference/quadra/schedulers/base.html#quadra.schedulers.base.LearningRateScheduler.step","title":"<code>step(*args, **kwargs)</code>","text":"<p>Base method, must be implemented by the sub classes.</p> Source code in <code>quadra/schedulers/base.py</code> <pre><code>def step(self, *args, **kwargs):\n\"\"\"Base method, must be implemented by the sub classes.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/quadra/schedulers/warmup.html","title":"warmup","text":""},{"location":"reference/quadra/schedulers/warmup.html#quadra.schedulers.warmup.CosineAnnealingWithLinearWarmUp","title":"<code>CosineAnnealingWithLinearWarmUp(optimizer, batch_size, total_epochs, init_lr=(0.01), lr_scale=256.0, linear_warmup_epochs=10, lr_reduce_factor=0.001, len_loader=None, scheduler_interval='epoch')</code>","text":"<p>             Bases: <code>LearningRateScheduler</code></p> <p>Cosine learning rate scheduler with linear warmup.</p> <p>Parameters:</p> <ul> <li> optimizer             (<code>Optimizer</code>)         \u2013          <p>optimizer for which the learning rate has to be optimized. If your are using this scheduler, than you have set the learning rate of the optimizer to 0</p> </li> <li> batch_size             (<code>int</code>)         \u2013          <p>global batch size of the data loader. For more information please take a look at https://pytorch-lightning.readthedocs.io/en/latest/advanced/multi_gpu.html?highlight=batch%20size#batch-size</p> </li> <li> total_epochs             (<code>int</code>)         \u2013          <p>the total number of epochs</p> </li> <li> init_lr             (<code>tuple[float, ...]</code>, default:                 <code>(0.01)</code> )         \u2013          <p>The initial learning rate, one for every <code>param_group</code>. Mind that the learning rate it's linearly scaled by <code>batch_size</code> / <code>lr_scale</code>, as specified by https://arxiv.org/abs/1706.02677. Defaults to 0.01.</p> </li> <li> lr_scale             (<code>float</code>, default:                 <code>256.0</code> )         \u2013          <p>the learning rate scheduler. Mind that the learning rate it's linearly scaled by <code>batch_size</code> / <code>lr_scale</code> as specified by https://arxiv.org/abs/1706.02677. Defaults to 256.</p> </li> <li> linear_warmup_epochs             (<code>int</code>, default:                 <code>10</code> )         \u2013          <p>how many epochs for the initial linear learning rate scaling. Defaults to 10.</p> </li> <li> lr_reduce_factor             (<code>float</code>, default:                 <code>0.001</code> )         \u2013          <p>factor to be multiplied by scaled lr (init_lr * batch_size / lr_scale) to avoid reaching 0 lr at the end of training.</p> </li> <li> len_loader             (<code>int | None</code>, default:                 <code>None</code> )         \u2013          <p>number of batches in a given dataloader. Remind that the <code>len_loader</code> must be divided by total number of gpus used during the training. If one specifies the <code>len_loader</code> parameter, then the unit measure for the lr update will be in steps (number of batches), not in epochs. Defaults to None.</p> </li> <li> scheduler_interval             (<code>str</code>, default:                 <code>'epoch'</code> )         \u2013          <p>'step' or 'epoch'. If 'step' then the scheduler expects 'len_loader' to be not None. Defaults to <code>epoch</code>.</p> </li> </ul> Source code in <code>quadra/schedulers/warmup.py</code> <pre><code>def __init__(\n    self,\n    optimizer: torch.optim.Optimizer,\n    batch_size: int,\n    total_epochs: int,\n    init_lr: tuple[float, ...] = (0.01,),\n    lr_scale: float = 256.0,\n    linear_warmup_epochs: int = 10,\n    lr_reduce_factor: float = 0.001,\n    len_loader: int | None = None,\n    scheduler_interval: str = \"epoch\",\n) -&gt; None:\n    super().__init__(optimizer, init_lr)\n    assert batch_size &gt; 0\n    assert total_epochs &gt; 0\n    assert lr_scale != 0\n    assert linear_warmup_epochs &gt;= 0\n    assert lr_reduce_factor &gt; 0\n    assert scheduler_interval.lower() in [\"step\", \"epoch\"]\n    self.batch_size = batch_size\n    self.total_epochs = total_epochs\n    self.linear_warmup_epochs = linear_warmup_epochs\n    self.base_lr_scale = self.batch_size / lr_scale\n    self.updates_counter = 1\n    self.init_lr = tuple(lr * self.base_lr_scale for lr in init_lr)\n    self.lr = self.init_lr\n    self.lr_reduce_factor = lr_reduce_factor\n\n    self.scheduler_interval = scheduler_interval.lower()\n    if self.scheduler_interval == \"step\":\n        assert len_loader is not None and len_loader &gt; 0\n        self.total_epochs = len_loader * self.total_epochs\n        self.linear_warmup_epochs = len_loader * self.linear_warmup_epochs\n</code></pre>"},{"location":"reference/quadra/schedulers/warmup.html#quadra.schedulers.warmup.CosineAnnealingWithLinearWarmUp.step","title":"<code>step()</code>","text":"<p>Update the learning rate for the current step.</p> Source code in <code>quadra/schedulers/warmup.py</code> <pre><code>def step(self):\n\"\"\"Update the learning rate for the current step.\"\"\"\n    self.lr = cosine_annealing_with_warmup(\n        self.init_lr,\n        self.updates_counter,\n        self.total_epochs,\n        self.linear_warmup_epochs,\n        self.lr_reduce_factor,\n    )\n    self.set_lr(self.lr)\n    self.updates_counter += 1\n    return self.lr\n</code></pre>"},{"location":"reference/quadra/schedulers/warmup.html#quadra.schedulers.warmup.cosine_annealing_with_warmup","title":"<code>cosine_annealing_with_warmup(init_lrs, step, total_steps, warmup_steps, lr_reduce_factor=0.001)</code>","text":"<p>Cosine learning rate scheduler with linear warmup helper function.</p> <p>Parameters:</p> <ul> <li> init_lrs             (<code>list[float]</code>)         \u2013          <p>The initial learning rate, one for every <code>param_group</code>.</p> </li> <li> step             (<code>int</code>)         \u2013          <p>the current step</p> </li> <li> total_steps             (<code>int</code>)         \u2013          <p>the total steps</p> </li> <li> warmup_steps             (<code>int</code>)         \u2013          <p>total linear warmup steps</p> </li> <li> lr_reduce_factor             (<code>float</code>, default:                 <code>0.001</code> )         \u2013          <p>reduce factor for the initial learning rate. This is used to set the minimum learning rate as <code>init_lr[i] * lr_reduce_factor</code> Defaults to 0.001.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[float]</code>         \u2013          <p>Annealed learning rate for this <code>step</code></p> </li> </ul> Source code in <code>quadra/schedulers/warmup.py</code> <pre><code>def cosine_annealing_with_warmup(\n    init_lrs: list[float],\n    step: int,\n    total_steps: int,\n    warmup_steps: int,\n    lr_reduce_factor: float = 0.001,\n) -&gt; list[float]:\n\"\"\"Cosine learning rate scheduler with linear warmup helper function.\n\n    Args:\n        init_lrs: The initial learning rate, one for every `param_group`.\n        step: the current step\n        total_steps: the total steps\n        warmup_steps: total linear warmup steps\n        lr_reduce_factor: reduce factor for the initial learning\n            rate. This is used to set the minimum learning rate as\n            `init_lr[i] * lr_reduce_factor`\n            Defaults to 0.001.\n\n    Returns:\n        Annealed learning rate for this `step`\n    \"\"\"\n    lrs = []\n    for init_lr in init_lrs:\n        if step &lt; warmup_steps:\n            lr = init_lr * step / warmup_steps\n        else:\n            step -= warmup_steps\n            total_steps -= warmup_steps\n            q = 0.5 * (1 + math.cos(math.pi * step / total_steps))\n            end_lr = init_lr * lr_reduce_factor\n            lr = init_lr * q + end_lr * (1 - q)\n        lrs.append(lr)\n    return lrs\n</code></pre>"},{"location":"reference/quadra/tasks/index.html","title":"tasks","text":""},{"location":"reference/quadra/tasks/index.html#quadra.tasks.AnomalibDetection","title":"<code>AnomalibDetection(config, module_function, checkpoint_path=None, run_test=True, report=True)</code>","text":"<p>             Bases: <code>Generic[AnomalyDataModuleT]</code>, <code>LightningTask[AnomalyDataModuleT]</code></p> <p>Anomaly Detection Task.</p> <p>Parameters:</p> <ul> <li> config             (<code>DictConfig</code>)         \u2013          <p>The experiment configuration</p> </li> <li> module_function             (<code>DictConfig</code>)         \u2013          <p>The function that instantiates the module and model</p> </li> <li> checkpoint_path             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>The path to the checkpoint to load the model from. Defaults to None.</p> </li> <li> run_test             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Whether to run the test after training. Defaults to False.</p> </li> <li> report             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Whether to report the results. Defaults to False.</p> </li> </ul> Source code in <code>quadra/tasks/anomaly.py</code> <pre><code>def __init__(\n    self,\n    config: DictConfig,\n    module_function: DictConfig,\n    checkpoint_path: str | None = None,\n    run_test: bool = True,\n    report: bool = True,\n):\n    super().__init__(\n        config=config,\n        checkpoint_path=checkpoint_path,\n        run_test=run_test,\n        report=report,\n    )\n    self._module: AnomalyModule\n    self.module_function = module_function\n    self.export_folder = \"deployment_model\"\n    self.report_path = \"\"\n    self.test_results: list[dict] | None = None\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.AnomalibDetection.module","title":"<code>module: AnomalyModule</code>  <code>property</code> <code>writable</code>","text":"<p>Get the module.</p>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.AnomalibDetection.export","title":"<code>export()</code>","text":"<p>Export model for production.</p> Source code in <code>quadra/tasks/anomaly.py</code> <pre><code>def export(self) -&gt; None:\n\"\"\"Export model for production.\"\"\"\n    if self.config.trainer.get(\"fast_dev_run\"):\n        log.warning(\"Skipping export since fast_dev_run is enabled\")\n        return\n\n    model = self.module.model\n\n    input_shapes = self.config.export.input_shapes\n\n    half_precision = \"16\" in self.trainer.precision\n\n    model_json, export_paths = export_model(\n        config=self.config,\n        model=model,\n        export_folder=self.export_folder,\n        half_precision=half_precision,\n        input_shapes=input_shapes,\n        idx_to_class={0: \"good\", 1: \"defect\"},\n    )\n\n    if len(export_paths) == 0:\n        return\n\n    model_json[\"image_threshold\"] = np.round(self.module.image_threshold.value.item(), 3)\n    model_json[\"pixel_threshold\"] = np.round(self.module.pixel_threshold.value.item(), 3)\n    model_json[\"anomaly_method\"] = self.config.model.model.name\n\n    with open(os.path.join(self.export_folder, \"model.json\"), \"w\") as f:\n        json.dump(model_json, f, cls=utils.HydraEncoder)\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.AnomalibDetection.generate_report","title":"<code>generate_report()</code>","text":"<p>Generate a report for the task and try to upload artifacts.</p> Source code in <code>quadra/tasks/anomaly.py</code> <pre><code>def generate_report(self):\n\"\"\"Generate a report for the task and try to upload artifacts.\"\"\"\n    self._generate_report()\n    self._upload_artifacts()\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.AnomalibDetection.prepare","title":"<code>prepare()</code>","text":"<p>Prepare the task.</p> Source code in <code>quadra/tasks/anomaly.py</code> <pre><code>def prepare(self) -&gt; None:\n\"\"\"Prepare the task.\"\"\"\n    super().prepare()\n    self.module = self.config.model\n    self.module.model = ModelSignatureWrapper(self.module.model)\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.AnomalibDetection.test","title":"<code>test()</code>","text":"<p>Lightning test.</p> Source code in <code>quadra/tasks/anomaly.py</code> <pre><code>def test(self) -&gt; Any:\n\"\"\"Lightning test.\"\"\"\n    self.test_results = super().test()\n    return self.test_results\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.Classification","title":"<code>Classification(config, output, checkpoint_path=None, lr_multiplier=None, gradcam=False, report=False, run_test=False)</code>","text":"<p>             Bases: <code>Generic[ClassificationDataModuleT]</code>, <code>LightningTask[ClassificationDataModuleT]</code></p> <p>Classification Task.</p> <p>Parameters:</p> <ul> <li> config             (<code>DictConfig</code>)         \u2013          <p>The experiment configuration</p> </li> <li> output             (<code>DictConfig</code>)         \u2013          <p>The otuput configuration.</p> </li> <li> gradcam             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to compute gradcams</p> </li> <li> checkpoint_path             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>The path to the checkpoint to load the model from. Defaults to None.</p> </li> <li> lr_multiplier             (<code>float | None</code>, default:                 <code>None</code> )         \u2013          <p>The multiplier for the backbone learning rate. Defaults to None.</p> </li> <li> output             (<code>DictConfig</code>)         \u2013          <p>The ouput configuration (under task config). It contains the bool \"example\" to generate figs of discordant/concordant predictions.</p> </li> <li> report             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to generate a report containing the results after test phase</p> </li> <li> run_test             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to run the test phase.</p> </li> </ul> Source code in <code>quadra/tasks/classification.py</code> <pre><code>def __init__(\n    self,\n    config: DictConfig,\n    output: DictConfig,\n    checkpoint_path: str | None = None,\n    lr_multiplier: float | None = None,\n    gradcam: bool = False,\n    report: bool = False,\n    run_test: bool = False,\n):\n    super().__init__(\n        config=config,\n        checkpoint_path=checkpoint_path,\n        run_test=run_test,\n        report=report,\n    )\n    self.output = output\n    self.gradcam = gradcam\n    self._lr_multiplier = lr_multiplier\n    self._pre_classifier: nn.Module\n    self._classifier: nn.Module\n    self._model: nn.Module\n    self._optimizer: torch.optim.Optimizer\n    self._scheduler: torch.optim.lr_scheduler._LRScheduler\n    self.model_json: dict[str, Any] | None = None\n    self.export_folder: str = \"deployment_model\"\n    self.deploy_info_file: str = \"model.json\"\n    self.report_confmat: pd.DataFrame\n    self.best_model_path: str | None = None\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.Classification.len_train_dataloader","title":"<code>len_train_dataloader: int</code>  <code>property</code>","text":"<p>Get the length of the train dataloader.</p>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.Classification.optimizer","title":"<code>optimizer: torch.optim.Optimizer</code>  <code>property</code> <code>writable</code>","text":"<p>Get the optimizer.</p>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.Classification.scheduler","title":"<code>scheduler: torch.optim.lr_scheduler._LRScheduler</code>  <code>property</code> <code>writable</code>","text":"<p>Get the scheduler.</p>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.Classification.export","title":"<code>export()</code>","text":"<p>Generate deployment models for the task.</p> Source code in <code>quadra/tasks/classification.py</code> <pre><code>def export(self) -&gt; None:\n\"\"\"Generate deployment models for the task.\"\"\"\n    if self.datamodule.class_to_idx is None:\n        log.warning(\n            \"No `class_to_idx` found in the datamodule, class information will not be saved in the model.json\"\n        )\n        idx_to_class = {}\n    else:\n        idx_to_class = {v: k for k, v in self.datamodule.class_to_idx.items()}\n\n    # Get best model!\n    if self.best_model_path is not None:\n        log.info(\"Saving deployment model for %s checkpoint\", self.best_model_path)\n\n        module = self.module.__class__.load_from_checkpoint(\n            self.best_model_path,\n            model=self.module.model,\n            optimizer=self.optimizer,\n            lr_scheduler=self.scheduler,\n            criterion=self.module.criterion,\n            gradcam=False,\n        )\n    else:\n        log.warning(\"No checkpoint callback found in the trainer, exporting the last model weights\")\n        module = self.module\n\n    input_shapes = self.config.export.input_shapes\n\n    # TODO: What happens if we have 64 precision?\n    half_precision = \"16\" in self.trainer.precision\n\n    self.model_json, export_paths = export_model(\n        config=self.config,\n        model=module.model,\n        export_folder=self.export_folder,\n        half_precision=half_precision,\n        input_shapes=input_shapes,\n        idx_to_class=idx_to_class,\n    )\n\n    if len(export_paths) == 0:\n        return\n\n    with open(os.path.join(self.export_folder, self.deploy_info_file), \"w\") as f:\n        json.dump(self.model_json, f)\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.Classification.freeze_layers_by_name","title":"<code>freeze_layers_by_name(freeze_parameters_name)</code>","text":"<p>Freeze layers specified in freeze_parameters_name.</p> <p>Parameters:</p> <ul> <li> freeze_parameters_name             (<code>list[str]</code>)         \u2013          <p>Layers that will be frozen during training.</p> </li> </ul> Source code in <code>quadra/tasks/classification.py</code> <pre><code>def freeze_layers_by_name(self, freeze_parameters_name: list[str]):\n\"\"\"Freeze layers specified in freeze_parameters_name.\n\n    Args:\n        freeze_parameters_name: Layers that will be frozen during training.\n\n    \"\"\"\n    count_frozen = 0\n    for name, param in self.model.named_parameters():\n        if any(x in name.split(\".\")[1] for x in freeze_parameters_name):\n            log.debug(\"Freezing layer %s\", name)\n            param.requires_grad = False\n\n        if not param.requires_grad:\n            count_frozen += 1\n\n    log.info(\"Frozen %d parameters\", count_frozen)\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.Classification.freeze_parameters_by_index","title":"<code>freeze_parameters_by_index(freeze_parameters_index)</code>","text":"<p>Freeze parameters specified in freeze_parameters_name.</p> <p>Parameters:</p> <ul> <li> freeze_parameters_index             (<code>list[int]</code>)         \u2013          <p>Indices of parameters that will be frozen during training.</p> </li> </ul> Source code in <code>quadra/tasks/classification.py</code> <pre><code>def freeze_parameters_by_index(self, freeze_parameters_index: list[int]):\n\"\"\"Freeze parameters specified in freeze_parameters_name.\n\n    Args:\n        freeze_parameters_index: Indices of parameters that will be frozen during training.\n\n    \"\"\"\n    if getattr(self.config.backbone, \"freeze_parameters_name\", None) is not None:\n        log.warning(\n            \"Please be aware that some of the model's parameters have already been frozen using \\\n            the specified freeze_parameters_name. You are combining these two actions.\"\n        )\n    count_frozen = 0\n    for i, (name, param) in enumerate(self.model.named_parameters()):\n        if i in freeze_parameters_index:\n            log.debug(\"Freezing layer %s\", name)\n            param.requires_grad = False\n\n        if not param.requires_grad:\n            count_frozen += 1\n\n    log.info(\"Frozen %d parameters\", count_frozen)\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.Classification.generate_report","title":"<code>generate_report()</code>","text":"<p>Generate a report for the task.</p> Source code in <code>quadra/tasks/classification.py</code> <pre><code>def generate_report(self) -&gt; None:\n\"\"\"Generate a report for the task.\"\"\"\n    if self.datamodule.class_to_idx is None:\n        log.warning(\"No `class_to_idx` found in the datamodule, report will not be generated\")\n        return\n\n    if isinstance(self.datamodule, MultilabelClassificationDataModule):\n        log.warning(\"Report generation is not supported for multilabel classification tasks at the moment.\")\n        return\n\n    log.info(\"Generating report!\")\n    if not self.run_test or self.config.trainer.get(\"fast_dev_run\"):\n        self.datamodule.setup(stage=\"test\")\n\n    # Deepcopy to remove the inference mode from gradients causing issues when loading checkpoints\n    # TODO: Why deepcopy of module model removes ModelSignatureWrapper?\n    self.module.model.instance = deepcopy(self.module.model.instance)\n    if \"16\" in self.trainer.precision:\n        log.warning(\"Gradcam is currently not supported with half precision, it will be disabled\")\n        self.module.gradcam = False\n        self.gradcam = False\n\n    predictions_outputs = self.trainer.predict(\n        model=self.module, datamodule=self.datamodule, ckpt_path=self.best_model_path\n    )\n    if not predictions_outputs:\n        log.warning(\"There is no prediction to generate the report. Skipping report generation.\")\n        return\n    all_outputs = [x[0] for x in predictions_outputs]\n    all_probs = [x[2] for x in predictions_outputs]\n    if not all_outputs or not all_probs:\n        log.warning(\"There is no prediction to generate the report. Skipping report generation.\")\n        return\n    all_outputs = [item for sublist in all_outputs for item in sublist]\n    all_probs = [item for sublist in all_probs for item in sublist]\n    all_targets = [target.tolist() for im, target in self.datamodule.test_dataloader()]\n    all_targets = [item for sublist in all_targets for item in sublist]\n\n    if self.module.gradcam:\n        grayscale_cams = [x[1] for x in predictions_outputs]\n        grayscale_cams = [item for sublist in grayscale_cams for item in sublist]\n        grayscale_cams = np.stack(grayscale_cams)  # N x H x W\n    else:\n        grayscale_cams = None\n\n    # creating confusion matrix\n    idx_to_class = {v: k for k, v in self.datamodule.class_to_idx.items()}\n    _, self.report_confmat, accuracy = get_results(\n        test_labels=all_targets,\n        pred_labels=all_outputs,\n        idx_to_labels=idx_to_class,\n    )\n    output_folder_test = \"test\"\n    test_dataloader = self.datamodule.test_dataloader()\n    test_dataset = cast(ImageClassificationListDataset, test_dataloader.dataset)\n    self.res = pd.DataFrame(\n        {\n            \"sample\": list(test_dataset.x),\n            \"real_label\": all_targets,\n            \"pred_label\": all_outputs,\n            \"probability\": all_probs,\n        }\n    )\n    os.makedirs(output_folder_test, exist_ok=True)\n    save_classification_result(\n        results=self.res,\n        output_folder=output_folder_test,\n        confmat=self.report_confmat,\n        accuracy=accuracy,\n        test_dataloader=self.datamodule.test_dataloader(),\n        config=self.config,\n        output=self.output,\n        grayscale_cams=grayscale_cams,\n    )\n\n    if len(self.logger) &gt; 0:\n        mflow_logger = get_mlflow_logger(trainer=self.trainer)\n        tensorboard_logger = utils.get_tensorboard_logger(trainer=self.trainer)\n        artifacts = glob.glob(os.path.join(output_folder_test, \"**/*\"), recursive=True)\n        if self.config.core.get(\"upload_artifacts\") and len(artifacts) &gt; 0:\n            if mflow_logger is not None:\n                log.info(\"Uploading artifacts to MLFlow\")\n                for a in artifacts:\n                    if os.path.isdir(a):\n                        continue\n\n                    dirname = Path(a).parent.name\n                    mflow_logger.experiment.log_artifact(\n                        run_id=mflow_logger.run_id,\n                        local_path=a,\n                        artifact_path=os.path.join(\"classification_output\", dirname),\n                    )\n            if tensorboard_logger is not None:\n                log.info(\"Uploading artifacts to Tensorboard\")\n                for a in artifacts:\n                    if os.path.isdir(a):\n                        continue\n\n                    ext = os.path.splitext(a)[1].lower()\n\n                    if ext in [\".png\", \".jpg\", \".jpeg\", \".bmp\", \".tiff\", \".tif\", \".gif\"]:\n                        try:\n                            img = cv2.imread(a)\n                            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n                        except cv2.error:\n                            log.info(\"Could not upload artifact image %s\", a)\n                            continue\n                        output_path = os.path.sep.join(a.split(os.path.sep)[-2:])\n                        tensorboard_logger.experiment.add_image(output_path, img, 0, dataformats=\"HWC\")\n                    else:\n                        utils.upload_file_tensorboard(a, tensorboard_logger)\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.Classification.module","title":"<code>module(module_config)</code>","text":"<p>Set the module of the model.</p> Source code in <code>quadra/tasks/classification.py</code> <pre><code>@LightningTask.module.setter\ndef module(self, module_config):  # noqa: F811\n\"\"\"Set the module of the model.\"\"\"\n    module = hydra.utils.instantiate(\n        module_config,\n        model=self.model,\n        optimizer=self.optimizer,\n        lr_scheduler=self.scheduler,\n        gradcam=self.gradcam,\n    )\n    if self.checkpoint_path is not None:\n        log.info(\"Loading model from lightning checkpoint: %s\", self.checkpoint_path)\n        module = module.__class__.load_from_checkpoint(\n            self.checkpoint_path,\n            model=self.model,\n            optimizer=self.optimizer,\n            lr_scheduler=self.scheduler,\n            criterion=module.criterion,\n            gradcam=self.gradcam,\n        )\n    self._module = module\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.Classification.prepare","title":"<code>prepare()</code>","text":"<p>Prepare the experiment.</p> Source code in <code>quadra/tasks/classification.py</code> <pre><code>def prepare(self) -&gt; None:\n\"\"\"Prepare the experiment.\"\"\"\n    super().prepare()\n    self.model = self.config.model\n    self.optimizer = self.config.optimizer\n    self.scheduler = self.config.scheduler\n    self.module = self.config.model.module\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.Classification.test","title":"<code>test()</code>","text":"<p>Test the model.</p> Source code in <code>quadra/tasks/classification.py</code> <pre><code>def test(self) -&gt; None:\n\"\"\"Test the model.\"\"\"\n    if not self.config.trainer.get(\"fast_dev_run\"):\n        log.info(\"Starting testing!\")\n        self.trainer.test(datamodule=self.datamodule, model=self.module, ckpt_path=self.best_model_path)\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.Classification.train","title":"<code>train()</code>","text":"<p>Train the model.</p> Source code in <code>quadra/tasks/classification.py</code> <pre><code>def train(self):\n\"\"\"Train the model.\"\"\"\n    super().train()\n    if (\n        self.trainer.checkpoint_callback is not None\n        and hasattr(self.trainer.checkpoint_callback, \"best_model_path\")\n        and self.trainer.checkpoint_callback.best_model_path is not None\n        and len(self.trainer.checkpoint_callback.best_model_path) &gt; 0\n    ):\n        self.best_model_path = self.trainer.checkpoint_callback.best_model_path\n        log.info(\"Loading best epoch weights...\")\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.ClassificationEvaluation","title":"<code>ClassificationEvaluation(config, output, model_path, report=True, gradcam=False, device=None)</code>","text":"<p>             Bases: <code>Evaluation[ClassificationDataModuleT]</code></p> <p>Perform a test on an imported Classification pytorch model.</p> <p>Parameters:</p> <ul> <li> config             (<code>DictConfig</code>)         \u2013          <p>Task configuration</p> </li> <li> output             (<code>DictConfig</code>)         \u2013          <p>Configuration for the output</p> </li> <li> model_path             (<code>str</code>)         \u2013          <p>Path to pytorch .pt model file</p> </li> <li> report             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Whether to generate the report of the predictions</p> </li> <li> gradcam             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to compute gradcams</p> </li> <li> device             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>Device to use for evaluation. If None, the device is automatically determined</p> </li> </ul> Source code in <code>quadra/tasks/classification.py</code> <pre><code>def __init__(\n    self,\n    config: DictConfig,\n    output: DictConfig,\n    model_path: str,\n    report: bool = True,\n    gradcam: bool = False,\n    device: str | None = None,\n):\n    super().__init__(config=config, model_path=model_path, device=device)\n    self.report_path = \"test_output\"\n    self.output = output\n    self.report = report\n    self.gradcam = gradcam\n    self.cam: GradCAM\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.ClassificationEvaluation.deployment_model","title":"<code>deployment_model: BaseEvaluationModel</code>  <code>property</code> <code>writable</code>","text":"<p>Deployment model.</p>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.ClassificationEvaluation.execute","title":"<code>execute()</code>","text":"<p>Execute the evaluation.</p> Source code in <code>quadra/tasks/classification.py</code> <pre><code>def execute(self) -&gt; None:\n\"\"\"Execute the evaluation.\"\"\"\n    self.prepare()\n    self.test()\n    if self.report:\n        self.generate_report()\n    self.finalize()\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.ClassificationEvaluation.generate_report","title":"<code>generate_report()</code>","text":"<p>Generate a report for the task.</p> Source code in <code>quadra/tasks/classification.py</code> <pre><code>def generate_report(self) -&gt; None:\n\"\"\"Generate a report for the task.\"\"\"\n    log.info(\"Generating report!\")\n    os.makedirs(self.report_path, exist_ok=True)\n\n    save_classification_result(\n        results=self.metadata[\"test_results\"],\n        output_folder=self.report_path,\n        confmat=self.metadata[\"test_confusion_matrix\"],\n        accuracy=self.metadata[\"test_accuracy\"],\n        test_dataloader=self.datamodule.test_dataloader(),\n        config=self.config,\n        output=self.output,\n        grayscale_cams=self.metadata[\"grayscale_cams\"],\n    )\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.ClassificationEvaluation.get_classifier","title":"<code>get_classifier(model_config)</code>","text":"<p>Instantiate the classifier from the config.</p> Source code in <code>quadra/tasks/classification.py</code> <pre><code>def get_classifier(self, model_config: DictConfig) -&gt; nn.Module:\n\"\"\"Instantiate the classifier from the config.\"\"\"\n    if \"classifier\" in model_config:\n        log.info(\"Instantiating classifier &lt;%s&gt;\", model_config.classifier[\"_target_\"])\n        return hydra.utils.instantiate(\n            model_config.classifier, out_features=len(self.model_data[\"classes\"]), _convert_=\"partial\"\n        )\n\n    raise ValueError(\"A `classifier` definition must be specified in the config\")\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.ClassificationEvaluation.get_pre_classifier","title":"<code>get_pre_classifier(model_config)</code>","text":"<p>Instantiate the pre-classifier from the config.</p> Source code in <code>quadra/tasks/classification.py</code> <pre><code>def get_pre_classifier(self, model_config: DictConfig) -&gt; nn.Module:\n\"\"\"Instantiate the pre-classifier from the config.\"\"\"\n    if \"pre_classifier\" in model_config and model_config.pre_classifier is not None:\n        log.info(\"Instantiating pre_classifier &lt;%s&gt;\", model_config.pre_classifier[\"_target_\"])\n        pre_classifier = hydra.utils.instantiate(model_config.pre_classifier, _convert_=\"partial\")\n    else:\n        log.info(\"No pre-classifier found in config: instantiate a torch.nn.Identity instead\")\n        pre_classifier = nn.Identity()\n\n    return pre_classifier\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.ClassificationEvaluation.get_torch_model","title":"<code>get_torch_model(model_config)</code>","text":"<p>Instantiate the torch model from the config.</p> Source code in <code>quadra/tasks/classification.py</code> <pre><code>def get_torch_model(self, model_config: DictConfig) -&gt; nn.Module:\n\"\"\"Instantiate the torch model from the config.\"\"\"\n    pre_classifier = self.get_pre_classifier(model_config)\n    classifier = self.get_classifier(model_config)\n    log.info(\"Instantiating backbone &lt;%s&gt;\", model_config.model[\"_target_\"])\n\n    return hydra.utils.instantiate(\n        model_config.model, classifier=classifier, pre_classifier=pre_classifier, _convert_=\"partial\"\n    )\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.ClassificationEvaluation.prepare","title":"<code>prepare()</code>","text":"<p>Prepare the evaluation.</p> Source code in <code>quadra/tasks/classification.py</code> <pre><code>def prepare(self) -&gt; None:\n\"\"\"Prepare the evaluation.\"\"\"\n    super().prepare()\n    self.datamodule = self.config.datamodule\n    self.datamodule.class_to_idx = {v: int(k) for k, v in self.model_data[\"classes\"].items()}\n    self.datamodule.num_classes = len(self.datamodule.class_to_idx)\n\n    # prepare_data() must be explicitly called because there is no training\n    self.datamodule.prepare_data()\n    self.datamodule.setup(stage=\"test\")\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.ClassificationEvaluation.prepare_gradcam","title":"<code>prepare_gradcam()</code>","text":"<p>Initializing gradcam for the predictions.</p> Source code in <code>quadra/tasks/classification.py</code> <pre><code>def prepare_gradcam(self) -&gt; None:\n\"\"\"Initializing gradcam for the predictions.\"\"\"\n    if not hasattr(self.deployment_model.model, \"features_extractor\"):\n        log.warning(\"Gradcam not implemented for this backbone, it will not be computed\")\n        self.gradcam = False\n        return\n\n    if isinstance(self.deployment_model.model.features_extractor, timm.models.resnet.ResNet):\n        target_layers = [cast(BaseNetworkBuilder, self.deployment_model.model).features_extractor.layer4[-1]]\n        self.cam = GradCAM(\n            model=self.deployment_model.model,\n            target_layers=target_layers,\n        )\n        for p in self.deployment_model.model.features_extractor.layer4[-1].parameters():\n            p.requires_grad = True\n    elif is_vision_transformer(cast(BaseNetworkBuilder, self.deployment_model.model).features_extractor):\n        self.grad_rollout = VitAttentionGradRollout(cast(nn.Module, self.deployment_model.model))\n    else:\n        log.warning(\"Gradcam not implemented for this backbone, it will not be computed\")\n        self.gradcam = False\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.ClassificationEvaluation.test","title":"<code>test()</code>","text":"<p>Perform test.</p> Source code in <code>quadra/tasks/classification.py</code> <pre><code>@automatic_datamodule_batch_size(batch_size_attribute_name=\"batch_size\")\ndef test(self) -&gt; None:\n\"\"\"Perform test.\"\"\"\n    log.info(\"Running test\")\n    test_dataloader = self.datamodule.test_dataloader()\n\n    image_labels = []\n    probabilities = []\n    predicted_classes = []\n    grayscale_cams_list = []\n\n    if self.gradcam:\n        self.prepare_gradcam()\n\n    with torch.set_grad_enabled(self.gradcam):\n        for batch_item in tqdm(test_dataloader):\n            im, target = batch_item\n            im = im.to(device=self.device, dtype=self.deployment_model.model_dtype).detach()\n\n            if self.gradcam:\n                # When gradcam is used we need to remove gradients\n                outputs = self.deployment_model(im).detach()\n            else:\n                outputs = self.deployment_model(im)\n\n            probs = torch.softmax(outputs, dim=1)\n            preds = torch.max(probs, dim=1).indices\n\n            probabilities.append(probs.tolist())\n            predicted_classes.append(preds.tolist())\n            image_labels.extend(target.tolist())\n            if self.gradcam and hasattr(self.deployment_model.model, \"features_extractor\"):\n                with torch.inference_mode(False):\n                    im = im.clone()\n                    if isinstance(self.deployment_model.model.features_extractor, timm.models.resnet.ResNet):\n                        grayscale_cam = self.cam(input_tensor=im, targets=None)\n                        grayscale_cams_list.append(torch.from_numpy(grayscale_cam))\n                    elif is_vision_transformer(\n                        cast(BaseNetworkBuilder, self.deployment_model.model).features_extractor\n                    ):\n                        grayscale_cam_low_res = self.grad_rollout(input_tensor=im, targets_list=preds.tolist())\n                        orig_shape = grayscale_cam_low_res.shape\n                        new_shape = (orig_shape[0], im.shape[2], im.shape[3])\n                        zoom_factors = tuple(np.array(new_shape) / np.array(orig_shape))\n                        grayscale_cam = ndimage.zoom(grayscale_cam_low_res, zoom_factors, order=1)\n                        grayscale_cams_list.append(torch.from_numpy(grayscale_cam))\n\n    grayscale_cams: torch.Tensor | None = None\n    if self.gradcam:\n        grayscale_cams = torch.cat(grayscale_cams_list, dim=0)\n\n    predicted_classes = [item for sublist in predicted_classes for item in sublist]\n    probabilities = [max(item) for sublist in probabilities for item in sublist]\n    if self.datamodule.class_to_idx is not None:\n        idx_to_class = {v: k for k, v in self.datamodule.class_to_idx.items()}\n\n    _, pd_cm, test_accuracy = get_results(\n        test_labels=image_labels,\n        pred_labels=predicted_classes,\n        idx_to_labels=idx_to_class,\n    )\n\n    res = pd.DataFrame(\n        {\n            \"sample\": list(test_dataloader.dataset.x),  # type: ignore[attr-defined]\n            \"real_label\": image_labels,\n            \"pred_label\": predicted_classes,\n            \"probability\": probabilities,\n        }\n    )\n\n    log.info(\"Avg classification accuracy: %s\", test_accuracy)\n\n    self.res = pd.DataFrame(\n        {\n            \"sample\": list(test_dataloader.dataset.x),  # type: ignore[attr-defined]\n            \"real_label\": image_labels,\n            \"pred_label\": predicted_classes,\n            \"probability\": probabilities,\n        }\n    )\n\n    # save results\n    self.metadata[\"test_confusion_matrix\"] = pd_cm\n    self.metadata[\"test_accuracy\"] = test_accuracy\n    self.metadata[\"predictions\"] = predicted_classes\n    self.metadata[\"test_results\"] = res\n    self.metadata[\"probabilities\"] = probabilities\n    self.metadata[\"test_labels\"] = image_labels\n    self.metadata[\"grayscale_cams\"] = grayscale_cams\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.Evaluation","title":"<code>Evaluation(config, model_path, device=None)</code>","text":"<p>             Bases: <code>Generic[DataModuleT]</code>, <code>Task[DataModuleT]</code></p> <p>Base Evaluation Task with deployment models.</p> <p>Parameters:</p> <ul> <li> config             (<code>DictConfig</code>)         \u2013          <p>The experiment configuration</p> </li> <li> model_path             (<code>str</code>)         \u2013          <p>The model path.</p> </li> <li> device             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>Device to use for evaluation. If None, the device is automatically determined.</p> </li> </ul> Source code in <code>quadra/tasks/base.py</code> <pre><code>def __init__(\n    self,\n    config: DictConfig,\n    model_path: str,\n    device: str | None = None,\n):\n    super().__init__(config=config)\n\n    if device is None:\n        self.device = utils.get_device()\n    else:\n        self.device = device\n\n    self.config = config\n    self.model_data: dict[str, Any]\n    self.model_path = model_path\n    self._deployment_model: BaseEvaluationModel\n    self.deployment_model_type: str\n    self.model_info_filename = \"model.json\"\n    self.report_path = \"\"\n    self.metadata = {\"report_files\": []}\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.Evaluation.deployment_model","title":"<code>deployment_model: BaseEvaluationModel</code>  <code>property</code> <code>writable</code>","text":"<p>Deployment model.</p>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.Evaluation.prepare","title":"<code>prepare()</code>","text":"<p>Prepare the evaluation.</p> Source code in <code>quadra/tasks/base.py</code> <pre><code>def prepare(self) -&gt; None:\n\"\"\"Prepare the evaluation.\"\"\"\n    with open(os.path.join(Path(self.model_path).parent, self.model_info_filename)) as f:\n        self.model_data = json.load(f)\n\n    if not isinstance(self.model_data, dict):\n        raise ValueError(\"Model info file is not a valid json\")\n\n    for input_size in self.model_data[\"input_size\"]:\n        if len(input_size) != 3:\n            continue\n\n        # Adjust the transform for 2D models (CxHxW)\n        # We assume that each input size has the same height and width\n        if input_size[1] != self.config.transforms.input_height:\n            log.warning(\n                f\"Input height of the model ({input_size[1]}) is different from the one specified \"\n                + f\"in the config ({self.config.transforms.input_height}). Fixing the config.\"\n            )\n            self.config.transforms.input_height = input_size[1]\n\n        if input_size[2] != self.config.transforms.input_width:\n            log.warning(\n                f\"Input width of the model ({input_size[2]}) is different from the one specified \"\n                + f\"in the config ({self.config.transforms.input_width}). Fixing the config.\"\n            )\n            self.config.transforms.input_width = input_size[2]\n\n    self.deployment_model = self.model_path  # type: ignore[assignment]\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.LightningTask","title":"<code>LightningTask(config, checkpoint_path=None, run_test=False, report=False)</code>","text":"<p>             Bases: <code>Generic[DataModuleT]</code>, <code>Task[DataModuleT]</code></p> <p>Base Experiment Task.</p> <p>Parameters:</p> <ul> <li> config             (<code>DictConfig</code>)         \u2013          <p>The experiment configuration</p> </li> <li> checkpoint_path             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>The path to the checkpoint to load the model from. Defaults to None.</p> </li> <li> run_test             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to run the test after training. Defaults to False.</p> </li> <li> report             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to generate a report. Defaults to False.</p> </li> </ul> Source code in <code>quadra/tasks/base.py</code> <pre><code>def __init__(\n    self,\n    config: DictConfig,\n    checkpoint_path: str | None = None,\n    run_test: bool = False,\n    report: bool = False,\n):\n    super().__init__(config=config)\n    self.checkpoint_path = checkpoint_path\n    self.run_test = run_test\n    self.report = report\n    self._module: LightningModule\n    self._devices: int | list[int]\n    self._callbacks: list[Callback]\n    self._logger: list[Logger]\n    self._trainer: Trainer\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.LightningTask.callbacks","title":"<code>callbacks: list[Callback]</code>  <code>property</code> <code>writable</code>","text":"<p>List[Callback]: The callbacks.</p>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.LightningTask.devices","title":"<code>devices: int | list[int]</code>  <code>property</code> <code>writable</code>","text":"<p>List[int]: The devices ids.</p>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.LightningTask.logger","title":"<code>logger: list[Logger]</code>  <code>property</code> <code>writable</code>","text":"<p>List[Logger]: The loggers.</p>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.LightningTask.module","title":"<code>module: LightningModule</code>  <code>property</code> <code>writable</code>","text":""},{"location":"reference/quadra/tasks/index.html#quadra.tasks.LightningTask.trainer","title":"<code>trainer: Trainer</code>  <code>property</code> <code>writable</code>","text":""},{"location":"reference/quadra/tasks/index.html#quadra.tasks.LightningTask.add_callback","title":"<code>add_callback(callback)</code>","text":"<p>Add a callback to the trainer.</p> <p>Parameters:</p> <ul> <li> callback             (<code>Callback</code>)         \u2013          <p>The callback to add</p> </li> </ul> Source code in <code>quadra/tasks/base.py</code> <pre><code>def add_callback(self, callback: Callback):\n\"\"\"Add a callback to the trainer.\n\n    Args:\n        callback: The callback to add\n    \"\"\"\n    if hasattr(self.trainer, \"callbacks\") and isinstance(self.trainer.callbacks, list):\n        self.trainer.callbacks.append(callback)\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.LightningTask.execute","title":"<code>execute()</code>","text":"<p>Execute the experiment and all the steps.</p> Source code in <code>quadra/tasks/base.py</code> <pre><code>def execute(self) -&gt; None:\n\"\"\"Execute the experiment and all the steps.\"\"\"\n    self.prepare()\n    self.train()\n    if self.run_test:\n        self.test()\n    if self.config.export is not None and len(self.config.export.types) &gt; 0:\n        self.export()\n    if self.report:\n        self.generate_report()\n    self.finalize()\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.LightningTask.finalize","title":"<code>finalize()</code>","text":"<p>Finalize the experiment.</p> Source code in <code>quadra/tasks/base.py</code> <pre><code>def finalize(self) -&gt; None:\n\"\"\"Finalize the experiment.\"\"\"\n    super().finalize()\n    utils.finish(\n        config=self.config,\n        module=self.module,\n        datamodule=self.datamodule,\n        trainer=self.trainer,\n        callbacks=self.callbacks,\n        logger=self.logger,\n        export_folder=self.export_folder,\n    )\n\n    if (\n        not self.config.trainer.get(\"fast_dev_run\")\n        and self.trainer.checkpoint_callback is not None\n        and hasattr(self.trainer.checkpoint_callback, \"best_model_path\")\n    ):\n        log.info(\"Best model ckpt: %s\", self.trainer.checkpoint_callback.best_model_path)\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.LightningTask.prepare","title":"<code>prepare()</code>","text":"<p>Prepare the experiment.</p> Source code in <code>quadra/tasks/base.py</code> <pre><code>def prepare(self) -&gt; None:\n\"\"\"Prepare the experiment.\"\"\"\n    super().prepare()\n\n    # First setup loggers since some callbacks might need logger setup correctly.\n    if \"logger\" in self.config:\n        self.logger = self.config.logger\n\n    if \"callbacks\" in self.config:\n        self.callbacks = self.config.callbacks\n\n    self.devices = self.config.trainer.devices\n    self.trainer = self.config.trainer\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.LightningTask.test","title":"<code>test()</code>","text":"<p>Test the model.</p> Source code in <code>quadra/tasks/base.py</code> <pre><code>def test(self) -&gt; Any:\n\"\"\"Test the model.\"\"\"\n    log.info(\"Starting testing!\")\n\n    best_model = None\n    if (\n        self.trainer.checkpoint_callback is not None\n        and hasattr(self.trainer.checkpoint_callback, \"best_model_path\")\n        and self.trainer.checkpoint_callback.best_model_path is not None\n        and len(self.trainer.checkpoint_callback.best_model_path) &gt; 0\n    ):\n        best_model = self.trainer.checkpoint_callback.best_model_path\n\n    if best_model is None:\n        log.warning(\n            \"No best checkpoint model found, using last weights for test, this might lead to worse results, \"\n            \"consider using a checkpoint callback.\"\n        )\n\n    return self.trainer.test(model=self.module, datamodule=self.datamodule, ckpt_path=best_model)\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.LightningTask.train","title":"<code>train()</code>","text":"<p>Train the model.</p> Source code in <code>quadra/tasks/base.py</code> <pre><code>def train(self) -&gt; None:\n\"\"\"Train the model.\"\"\"\n    log.info(\"Starting training!\")\n    utils.log_hyperparameters(\n        config=self.config,\n        model=self.module,\n        trainer=self.trainer,\n    )\n\n    self.trainer.fit(model=self.module, datamodule=self.datamodule)\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.PatchSklearnClassification","title":"<code>PatchSklearnClassification(config, output, device, automatic_batch_size, half_precision=False)</code>","text":"<p>             Bases: <code>Task[PatchSklearnClassificationDataModule]</code></p> <p>Patch classification using torch backbone for feature extraction and sklearn to learn a linear classifier.</p> <p>Parameters:</p> <ul> <li> config             (<code>DictConfig</code>)         \u2013          <p>The experiment configuration</p> </li> <li> device             (<code>str</code>)         \u2013          <p>The device to use</p> </li> <li> output             (<code>DictConfig</code>)         \u2013          <p>Dictionary defining which kind of outputs to generate. Defaults to None.</p> </li> <li> automatic_batch_size             (<code>DictConfig</code>)         \u2013          <p>Whether to automatically find the largest batch size that fits in memory.</p> </li> <li> half_precision             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to use half precision.</p> </li> </ul> Source code in <code>quadra/tasks/patch.py</code> <pre><code>def __init__(\n    self,\n    config: DictConfig,\n    output: DictConfig,\n    device: str,\n    automatic_batch_size: DictConfig,\n    half_precision: bool = False,\n):\n    super().__init__(config=config)\n    self.device: str = device\n    self.output: DictConfig = output\n    self.return_polygon: bool = True\n    self.reconstruction_results: dict[str, Any]\n    self._backbone: ModelSignatureWrapper\n    self._trainer: SklearnClassificationTrainer\n    self._model: ClassifierMixin\n    self.metadata: dict[str, Any] = {\n        \"test_confusion_matrix\": [],\n        \"test_accuracy\": [],\n        \"test_results\": [],\n        \"test_labels\": [],\n    }\n    self.export_folder: str = \"deployment_model\"\n    self.automatic_batch_size = automatic_batch_size\n    self.half_precision = half_precision\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.PatchSklearnClassification.backbone","title":"<code>backbone: ModelSignatureWrapper</code>  <code>property</code> <code>writable</code>","text":""},{"location":"reference/quadra/tasks/index.html#quadra.tasks.PatchSklearnClassification.model","title":"<code>model: ClassifierMixin</code>  <code>property</code> <code>writable</code>","text":"<p>sklearn.base.ClassifierMixin: The model.</p>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.PatchSklearnClassification.trainer","title":"<code>trainer: SklearnClassificationTrainer</code>  <code>property</code> <code>writable</code>","text":""},{"location":"reference/quadra/tasks/index.html#quadra.tasks.PatchSklearnClassification.execute","title":"<code>execute()</code>","text":"<p>Execute the experiment and all the steps.</p> Source code in <code>quadra/tasks/patch.py</code> <pre><code>def execute(self) -&gt; None:\n\"\"\"Execute the experiment and all the steps.\"\"\"\n    self.prepare()\n    self.train()\n    if self.output.report:\n        self.generate_report()\n    if self.config.export is not None and len(self.config.export.types) &gt; 0:\n        self.export()\n    self.finalize()\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.PatchSklearnClassification.export","title":"<code>export()</code>","text":"<p>Generate deployment model for the task.</p> Source code in <code>quadra/tasks/patch.py</code> <pre><code>def export(self) -&gt; None:\n\"\"\"Generate deployment model for the task.\"\"\"\n    input_shapes = self.config.export.input_shapes\n\n    idx_to_class = {v: k for k, v in self.datamodule.class_to_idx.items()}\n\n    model_json, export_paths = export_model(\n        config=self.config,\n        model=self.backbone,\n        export_folder=self.export_folder,\n        half_precision=self.half_precision,\n        input_shapes=input_shapes,\n        idx_to_class=idx_to_class,\n        pytorch_model_type=\"backbone\",\n    )\n\n    if len(export_paths) &gt; 0:\n        dataset_info = self.datamodule.info\n\n        horizontal_patches = dataset_info.patch_number[1] if dataset_info.patch_number is not None else None\n        vertical_patches = dataset_info.patch_number[0] if dataset_info.patch_number is not None else None\n        patch_height = dataset_info.patch_size[0] if dataset_info.patch_size is not None else None\n        patch_width = dataset_info.patch_size[1] if dataset_info.patch_size is not None else None\n        overlap = dataset_info.overlap\n\n        model_json.update(\n            {\n                \"horizontal_patches\": horizontal_patches,\n                \"vertical_patches\": vertical_patches,\n                \"patch_height\": patch_height,\n                \"patch_width\": patch_width,\n                \"overlap\": overlap,\n                \"reconstruction_method\": self.output.reconstruction_method,\n                \"class_to_skip\": self.datamodule.class_to_skip_training,\n            }\n        )\n\n        with open(os.path.join(self.export_folder, \"model.json\"), \"w\") as f:\n            json.dump(model_json, f, cls=utils.HydraEncoder)\n\n    dump(self.model, os.path.join(self.export_folder, \"classifier.joblib\"))\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.PatchSklearnClassification.generate_report","title":"<code>generate_report()</code>","text":"<p>Generate the report for the task.</p> Source code in <code>quadra/tasks/patch.py</code> <pre><code>def generate_report(self) -&gt; None:\n\"\"\"Generate the report for the task.\"\"\"\n    log.info(\"Generating report!\")\n    os.makedirs(self.output.folder, exist_ok=True)\n\n    c_matrix = self.metadata[\"test_confusion_matrix\"]\n    idx_to_class = {v: k for k, v in self.datamodule.class_to_idx.items()}\n\n    datamodule: PatchSklearnClassificationDataModule = self.datamodule\n    val_img_info: list[PatchDatasetFileFormat] = datamodule.info.val_files\n    for img_info in val_img_info:\n        if not os.path.isabs(img_info.image_path):\n            img_info.image_path = os.path.join(datamodule.data_path, img_info.image_path)\n        if img_info.mask_path is not None and not os.path.isabs(img_info.mask_path):\n            img_info.mask_path = os.path.join(datamodule.data_path, img_info.mask_path)\n\n    false_region_bad, false_region_good, true_region_bad, reconstructions = compute_patch_metrics(\n        test_img_info=val_img_info,\n        test_results=self.metadata[\"test_results\"],\n        patch_num_h=datamodule.info.patch_number[0] if datamodule.info.patch_number is not None else None,\n        patch_num_w=datamodule.info.patch_number[1] if datamodule.info.patch_number is not None else None,\n        patch_h=datamodule.info.patch_size[0] if datamodule.info.patch_size is not None else None,\n        patch_w=datamodule.info.patch_size[1] if datamodule.info.patch_size is not None else None,\n        overlap=datamodule.info.overlap,\n        idx_to_class=idx_to_class,\n        return_polygon=self.return_polygon,\n        patch_reconstruction_method=self.output.reconstruction_method,\n        annotated_good=datamodule.info.annotated_good,\n    )\n\n    self.reconstruction_results = {\n        \"false_region_bad\": false_region_bad,\n        \"false_region_good\": false_region_good,\n        \"true_region_bad\": true_region_bad,\n        \"reconstructions\": reconstructions,\n        \"reconstructions_type\": \"polygon\" if self.return_polygon else \"rle\",\n        \"patch_reconstruction_method\": self.output.reconstruction_method,\n    }\n\n    with open(\"reconstruction_results.json\", \"w\") as f:\n        json.dump(\n            self.reconstruction_results,\n            f,\n            cls=RleEncoder,\n        )\n\n    if hasattr(self.datamodule, \"class_to_skip_training\") and self.datamodule.class_to_skip_training is not None:\n        ignore_classes = [self.datamodule.class_to_idx[x] for x in self.datamodule.class_to_skip_training]\n    else:\n        ignore_classes = None\n    val_dataloader = self.datamodule.val_dataloader()\n    save_classification_result(\n        results=self.metadata[\"test_results\"],\n        output_folder=self.output.folder,\n        confusion_matrix=c_matrix,\n        accuracy=self.metadata[\"test_accuracy\"],\n        test_dataloader=val_dataloader,\n        config=self.config,\n        output=self.output,\n        reconstructions=reconstructions,\n        ignore_classes=ignore_classes,\n    )\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.PatchSklearnClassification.prepare","title":"<code>prepare()</code>","text":"<p>Prepare the experiment.</p> Source code in <code>quadra/tasks/patch.py</code> <pre><code>def prepare(self) -&gt; None:\n\"\"\"Prepare the experiment.\"\"\"\n    self.datamodule = self.config.datamodule\n    self.backbone = self.config.backbone\n    self.model = self.config.model\n\n    if not self.automatic_batch_size.disable and self.device != \"cpu\":\n        self.datamodule.batch_size = automatic_batch_size_computation(\n            datamodule=self.datamodule,\n            backbone=self.backbone,\n            starting_batch_size=self.automatic_batch_size.starting_batch_size,\n        )\n\n    self.trainer = self.config.trainer\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.PatchSklearnClassification.train","title":"<code>train()</code>","text":"<p>Train the model.</p> Source code in <code>quadra/tasks/patch.py</code> <pre><code>def train(self) -&gt; None:\n\"\"\"Train the model.\"\"\"\n    log.info(\"Starting training...!\")\n    # prepare_data() must be explicitly called if the task does not include a lightining training\n    self.datamodule.prepare_data()\n    self.datamodule.setup(stage=\"fit\")\n    class_to_keep = None\n    if hasattr(self.datamodule, \"class_to_skip_training\") and self.datamodule.class_to_skip_training is not None:\n        class_to_keep = [x for x in self.datamodule.class_to_idx if x not in self.datamodule.class_to_skip_training]\n\n    self.model = self.config.model\n    self.trainer.change_classifier(self.model)\n    train_dataloader = self.datamodule.train_dataloader()\n    val_dataloader = self.datamodule.val_dataloader()\n    train_dataset = cast(PatchSklearnClassificationTrainDataset, train_dataloader.dataset)\n    self.trainer.fit(train_dataloader=train_dataloader)\n    _, pd_cm, accuracy, res, _ = self.trainer.test(\n        test_dataloader=val_dataloader,\n        class_to_keep=class_to_keep,\n        idx_to_class=train_dataset.idx_to_class,\n        predict_proba=True,\n    )\n\n    # save results\n    self.metadata[\"test_confusion_matrix\"] = pd_cm\n    self.metadata[\"test_accuracy\"] = accuracy\n    self.metadata[\"test_results\"] = res\n    self.metadata[\"test_labels\"] = [\n        train_dataset.idx_to_class[i] if i != -1 else \"N/A\" for i in res[\"real_label\"].unique().tolist()\n    ]\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.PatchSklearnTestClassification","title":"<code>PatchSklearnTestClassification(config, output, model_path, device='cpu')</code>","text":"<p>             Bases: <code>Evaluation[PatchSklearnClassificationDataModule]</code></p> <p>Perform a test of an already trained classification model.</p> <p>Parameters:</p> <ul> <li> config             (<code>DictConfig</code>)         \u2013          <p>The experiment configuration</p> </li> <li> output             (<code>DictConfig</code>)         \u2013          <p>where to save resultss</p> </li> <li> model_path             (<code>str</code>)         \u2013          <p>path to trained model from PatchSklearnClassification task.</p> </li> <li> device             (<code>str</code>, default:                 <code>'cpu'</code> )         \u2013          <p>the device where to run the model (cuda or cpu). Defaults to 'cpu'.</p> </li> </ul> Source code in <code>quadra/tasks/patch.py</code> <pre><code>def __init__(\n    self,\n    config: DictConfig,\n    output: DictConfig,\n    model_path: str,\n    device: str = \"cpu\",\n):\n    super().__init__(config=config, model_path=model_path, device=device)\n    self.output = output\n    self._backbone: BaseEvaluationModel\n    self._classifier: ClassifierMixin\n    self.class_to_idx: dict[str, int]\n    self.idx_to_class: dict[int, str]\n    self.metadata: dict[str, Any] = {\n        \"test_confusion_matrix\": None,\n        \"test_accuracy\": None,\n        \"test_results\": None,\n        \"test_labels\": None,\n    }\n    self.class_to_skip: list[str] = []\n    self.reconstruction_results: dict[str, Any]\n    self.return_polygon: bool = True\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.PatchSklearnTestClassification.backbone","title":"<code>backbone: BaseEvaluationModel</code>  <code>property</code> <code>writable</code>","text":""},{"location":"reference/quadra/tasks/index.html#quadra.tasks.PatchSklearnTestClassification.classifier","title":"<code>classifier: ClassifierMixin</code>  <code>property</code> <code>writable</code>","text":""},{"location":"reference/quadra/tasks/index.html#quadra.tasks.PatchSklearnTestClassification.deployment_model","title":"<code>deployment_model</code>  <code>property</code> <code>writable</code>","text":"<p>Deployment model.</p>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.PatchSklearnTestClassification.trainer","title":"<code>trainer: SklearnClassificationTrainer</code>  <code>property</code> <code>writable</code>","text":""},{"location":"reference/quadra/tasks/index.html#quadra.tasks.PatchSklearnTestClassification.execute","title":"<code>execute()</code>","text":"<p>Execute the experiment and all the steps.</p> Source code in <code>quadra/tasks/patch.py</code> <pre><code>def execute(self) -&gt; None:\n\"\"\"Execute the experiment and all the steps.\"\"\"\n    self.prepare()\n    self.test()\n    if self.output.report:\n        self.generate_report()\n    self.finalize()\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.PatchSklearnTestClassification.generate_report","title":"<code>generate_report()</code>","text":"<p>Generate a report for the task.</p> Source code in <code>quadra/tasks/patch.py</code> <pre><code>def generate_report(self) -&gt; None:\n\"\"\"Generate a report for the task.\"\"\"\n    log.info(\"Generating report!\")\n    os.makedirs(self.output.folder, exist_ok=True)\n\n    c_matrix = self.metadata[\"test_confusion_matrix\"]\n    idx_to_class = {v: k for k, v in self.datamodule.class_to_idx.items()}\n\n    datamodule: PatchSklearnClassificationDataModule = self.datamodule\n    test_img_info = datamodule.info.test_files\n    for img_info in test_img_info:\n        if not os.path.isabs(img_info.image_path):\n            img_info.image_path = os.path.join(datamodule.data_path, img_info.image_path)\n        if img_info.mask_path is not None and not os.path.isabs(img_info.mask_path):\n            img_info.mask_path = os.path.join(datamodule.data_path, img_info.mask_path)\n\n    false_region_bad, false_region_good, true_region_bad, reconstructions = compute_patch_metrics(\n        test_img_info=test_img_info,\n        test_results=self.metadata[\"test_results\"],\n        patch_num_h=datamodule.info.patch_number[0] if datamodule.info.patch_number is not None else None,\n        patch_num_w=datamodule.info.patch_number[1] if datamodule.info.patch_number is not None else None,\n        patch_h=datamodule.info.patch_size[0] if datamodule.info.patch_size is not None else None,\n        patch_w=datamodule.info.patch_size[1] if datamodule.info.patch_size is not None else None,\n        overlap=datamodule.info.overlap,\n        idx_to_class=idx_to_class,\n        return_polygon=self.return_polygon,\n        patch_reconstruction_method=self.output.reconstruction_method,\n        annotated_good=datamodule.info.annotated_good,\n    )\n\n    self.reconstruction_results = {\n        \"false_region_bad\": false_region_bad,\n        \"false_region_good\": false_region_good,\n        \"true_region_bad\": true_region_bad,\n        \"reconstructions\": reconstructions,\n        \"reconstructions_type\": \"polygon\" if self.return_polygon else \"rle\",\n        \"patch_reconstruction_method\": self.output.reconstruction_method,\n    }\n\n    with open(\"reconstruction_results.json\", \"w\") as f:\n        json.dump(\n            self.reconstruction_results,\n            f,\n            cls=RleEncoder,\n        )\n\n    if self.class_to_skip is not None:\n        ignore_classes = [datamodule.class_to_idx[x] for x in self.class_to_skip]\n    else:\n        ignore_classes = None\n    test_dataloader = self.datamodule.test_dataloader()\n    save_classification_result(\n        results=self.metadata[\"test_results\"],\n        output_folder=self.output.folder,\n        confusion_matrix=c_matrix,\n        accuracy=self.metadata[\"test_accuracy\"],\n        test_dataloader=test_dataloader,\n        config=self.config,\n        output=self.output,\n        reconstructions=reconstructions,\n        ignore_classes=ignore_classes,\n    )\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.PatchSklearnTestClassification.prepare","title":"<code>prepare()</code>","text":"<p>Prepare the experiment.</p> Source code in <code>quadra/tasks/patch.py</code> <pre><code>def prepare(self) -&gt; None:\n\"\"\"Prepare the experiment.\"\"\"\n    super().prepare()\n\n    idx_to_class = {}\n    class_to_idx = {}\n    for k, v in self.model_data[\"classes\"].items():\n        idx_to_class[int(k)] = v\n        class_to_idx[v] = int(k)\n\n    self.idx_to_class = idx_to_class\n    self.class_to_idx = class_to_idx\n    self.config.datamodule.class_to_idx = class_to_idx\n\n    self.datamodule = self.config.datamodule\n    # Configure trainer\n    self.trainer = self.config.trainer\n\n    # prepare_data() must be explicitly called because there is no lightning training\n    self.datamodule.prepare_data()\n    self.datamodule.setup(stage=\"test\")\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.PatchSklearnTestClassification.test","title":"<code>test()</code>","text":"<p>Run the test.</p> Source code in <code>quadra/tasks/patch.py</code> <pre><code>@automatic_datamodule_batch_size(batch_size_attribute_name=\"batch_size\")\ndef test(self) -&gt; None:\n\"\"\"Run the test.\"\"\"\n    test_dataloader = self.datamodule.test_dataloader()\n\n    self.class_to_skip = self.model_data[\"class_to_skip\"] if hasattr(self.model_data, \"class_to_skip\") else None\n    class_to_keep = None\n\n    if self.class_to_skip is not None:\n        class_to_keep = [x for x in self.datamodule.class_to_idx if x not in self.class_to_skip]\n    _, pd_cm, accuracy, res, _ = self.trainer.test(\n        test_dataloader=test_dataloader,\n        idx_to_class=self.idx_to_class,\n        predict_proba=True,\n        class_to_keep=class_to_keep,\n    )\n\n    # save results\n    self.metadata[\"test_confusion_matrix\"] = pd_cm\n    self.metadata[\"test_accuracy\"] = accuracy\n    self.metadata[\"test_results\"] = res\n    self.metadata[\"test_labels\"] = [\n        self.idx_to_class[i] if i != -1 else \"N/A\" for i in res[\"real_label\"].unique().tolist()\n    ]\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.PlaceholderTask","title":"<code>PlaceholderTask</code>","text":"<p>             Bases: <code>Task</code></p> <p>Placeholder task.</p>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.PlaceholderTask.execute","title":"<code>execute()</code>","text":"<p>Execute the task and all the steps.</p> Source code in <code>quadra/tasks/base.py</code> <pre><code>def execute(self) -&gt; None:\n\"\"\"Execute the task and all the steps.\"\"\"\n    log.info(\"Running Placeholder Task.\")\n    log.info(\"Quadra Version: %s\", str(get_version()))\n    log.info(\"If you are reading this, it means that library is installed correctly!\")\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.SSL","title":"<code>SSL(config, run_test=False, report=False, checkpoint_path=None)</code>","text":"<p>             Bases: <code>LightningTask</code></p> <p>SSL Task.</p> <p>Parameters:</p> <ul> <li> config             (<code>DictConfig</code>)         \u2013          <p>The experiment configuration</p> </li> <li> checkpoint_path             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>The path to the checkpoint to load the model from Defaults to None</p> </li> <li> report             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to create the report</p> </li> <li> run_test             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to run final test</p> </li> </ul> Source code in <code>quadra/tasks/ssl.py</code> <pre><code>def __init__(\n    self,\n    config: DictConfig,\n    run_test: bool = False,\n    report: bool = False,\n    checkpoint_path: str | None = None,\n):\n    super().__init__(\n        config=config,\n        checkpoint_path=checkpoint_path,\n        run_test=run_test,\n        report=report,\n    )\n    self._backbone: nn.Module\n    self._optimizer: torch.optim.Optimizer\n    self._lr_scheduler: torch.optim.lr_scheduler._LRScheduler\n    self.export_folder = \"deployment_model\"\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.SSL.optimizer","title":"<code>optimizer: torch.optim.Optimizer</code>  <code>property</code> <code>writable</code>","text":"<p>Get the optimizer.</p>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.SSL.scheduler","title":"<code>scheduler: torch.optim.lr_scheduler._LRScheduler</code>  <code>property</code> <code>writable</code>","text":"<p>Get the scheduler.</p>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.SSL.export","title":"<code>export()</code>","text":"<p>Deploy a model ready for production.</p> Source code in <code>quadra/tasks/ssl.py</code> <pre><code>def export(self) -&gt; None:\n\"\"\"Deploy a model ready for production.\"\"\"\n    half_precision = \"16\" in self.trainer.precision\n\n    input_shapes = self.config.export.input_shapes\n\n    model_json, export_paths = export_model(\n        config=self.config,\n        model=self.module.model,\n        export_folder=self.export_folder,\n        half_precision=half_precision,\n        input_shapes=input_shapes,\n        idx_to_class=None,\n    )\n\n    if len(export_paths) == 0:\n        return\n\n    with open(os.path.join(self.export_folder, \"model.json\"), \"w\") as f:\n        json.dump(model_json, f, cls=utils.HydraEncoder)\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.SSL.learnable_parameters","title":"<code>learnable_parameters()</code>","text":"<p>Get the learnable parameters.</p> Source code in <code>quadra/tasks/ssl.py</code> <pre><code>def learnable_parameters(self) -&gt; list[nn.Parameter]:\n\"\"\"Get the learnable parameters.\"\"\"\n    raise NotImplementedError(\"This method must be implemented by the subclass\")\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.SSL.test","title":"<code>test()</code>","text":"<p>Test the model.</p> Source code in <code>quadra/tasks/ssl.py</code> <pre><code>def test(self) -&gt; None:\n\"\"\"Test the model.\"\"\"\n    if self.run_test and not self.config.trainer.get(\"fast_dev_run\"):\n        log.info(\"Starting testing!\")\n        log.info(\"Using last epoch's weights for testing.\")\n        self.trainer.test(datamodule=self.datamodule, model=self.module, ckpt_path=None)\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.Segmentation","title":"<code>Segmentation(config, num_viz_samples=5, checkpoint_path=None, run_test=False, evaluate=None, report=False)</code>","text":"<p>             Bases: <code>Generic[SegmentationDataModuleT]</code>, <code>LightningTask[SegmentationDataModuleT]</code></p> <p>Task for segmentation.</p> <p>Parameters:</p> <ul> <li> config             (<code>DictConfig</code>)         \u2013          <p>Config object</p> </li> <li> num_viz_samples             (<code>int</code>, default:                 <code>5</code> )         \u2013          <p>Number of samples to visualize. Defaults to 5.</p> </li> <li> checkpoint_path             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>Path to the checkpoint to load the model from. Defaults to None.</p> </li> <li> run_test             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>If True, run test after training. Defaults to False.</p> </li> <li> evaluate             (<code>DictConfig | None</code>, default:                 <code>None</code> )         \u2013          <p>Dict with evaluation parameters. Defaults to None.</p> </li> <li> report             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>If True, create report after training. Defaults to False.</p> </li> </ul> Source code in <code>quadra/tasks/segmentation.py</code> <pre><code>def __init__(\n    self,\n    config: DictConfig,\n    num_viz_samples: int = 5,\n    checkpoint_path: str | None = None,\n    run_test: bool = False,\n    evaluate: DictConfig | None = None,\n    report: bool = False,\n):\n    super().__init__(\n        config=config,\n        checkpoint_path=checkpoint_path,\n        run_test=run_test,\n        report=report,\n    )\n    self.evaluate = evaluate\n    self.num_viz_samples = num_viz_samples\n    self.export_folder: str = \"deployment_model\"\n    self.exported_model_path: str | None = None\n    if self.evaluate and any(self.evaluate.values()):\n        if (\n            self.config.export is None\n            or len(self.config.export.types) == 0\n            or \"torchscript\" not in self.config.export.types\n        ):\n            log.info(\n                \"Evaluation is enabled, but training does not export a deployment model. Automatically export the \"\n                \"model as torchscript.\"\n            )\n            if self.config.export is None:\n                self.config.export = DictConfig({\"types\": [\"torchscript\"]})\n            else:\n                self.config.export.types.append(\"torchscript\")\n\n        if not self.report:\n            log.info(\"Evaluation is enabled, but reporting is disabled. Enabling reporting automatically.\")\n            self.report = True\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.Segmentation.module","title":"<code>module: SegmentationModel</code>  <code>property</code> <code>writable</code>","text":"<p>Get the module.</p>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.Segmentation.export","title":"<code>export()</code>","text":"<p>Generate a deployment model for the task.</p> Source code in <code>quadra/tasks/segmentation.py</code> <pre><code>def export(self) -&gt; None:\n\"\"\"Generate a deployment model for the task.\"\"\"\n    log.info(\"Exporting model ready for deployment\")\n\n    # Get best model!\n    if (\n        self.trainer.checkpoint_callback is not None\n        and hasattr(self.trainer.checkpoint_callback, \"best_model_path\")\n        and self.trainer.checkpoint_callback.best_model_path is not None\n        and len(self.trainer.checkpoint_callback.best_model_path) &gt; 0\n    ):\n        best_model_path = self.trainer.checkpoint_callback.best_model_path\n        log.info(\"Loaded best model from %s\", best_model_path)\n\n        module = self.module.__class__.load_from_checkpoint(\n            best_model_path,\n            model=self.module.model,\n            loss_fun=None,\n            optimizer=self.module.optimizer,\n            lr_scheduler=self.module.schedulers,\n        )\n    else:\n        log.warning(\"No checkpoint callback found in the trainer, exporting the last model weights\")\n        module = self.module\n\n    if \"idx_to_class\" not in self.config.datamodule:\n        log.info(\"No idx_to_class key\")\n        idx_to_class = {0: \"good\", 1: \"bad\"}  # TODO: Why is this the default value?\n    else:\n        log.info(\"idx_to_class is present\")\n        idx_to_class = self.config.datamodule.idx_to_class\n\n    if self.config.export is None:\n        raise ValueError(\n            \"No export type specified. This should not happen, please check if you have set \"\n            \"the export_type or assign it to a default value.\"\n        )\n\n    half_precision = \"16\" in self.trainer.precision\n\n    input_shapes = self.config.export.input_shapes\n\n    model_json, export_paths = export_model(\n        config=self.config,\n        model=module.model,\n        export_folder=self.export_folder,\n        half_precision=half_precision,\n        input_shapes=input_shapes,\n        idx_to_class=idx_to_class,\n    )\n\n    if len(export_paths) == 0:\n        return\n\n    # Pick one model for evaluation, it should be independent of the export type as the model is wrapped\n    self.exported_model_path = next(iter(export_paths.values()))\n\n    with open(os.path.join(self.export_folder, \"model.json\"), \"w\") as f:\n        json.dump(model_json, f, cls=utils.HydraEncoder)\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.Segmentation.generate_report","title":"<code>generate_report()</code>","text":"<p>Generate a report for the task.</p> Source code in <code>quadra/tasks/segmentation.py</code> <pre><code>def generate_report(self) -&gt; None:\n\"\"\"Generate a report for the task.\"\"\"\n    if self.evaluate is not None:\n        log.info(\"Generating evaluation report!\")\n        eval_tasks: list[SegmentationEvaluation] = []\n        if self.evaluate.analysis:\n            if self.exported_model_path is None:\n                raise ValueError(\n                    \"Exported model path is not set yet but the task tries to do an analysis evaluation\"\n                )\n            eval_task = SegmentationAnalysisEvaluation(\n                config=self.config,\n                model_path=self.exported_model_path,\n            )\n            eval_tasks.append(eval_task)\n        for task in eval_tasks:\n            task.execute()\n\n        if len(self.logger) &gt; 0:\n            mflow_logger = get_mlflow_logger(trainer=self.trainer)\n            tensorboard_logger = utils.get_tensorboard_logger(trainer=self.trainer)\n\n            if mflow_logger is not None and self.config.core.get(\"upload_artifacts\"):\n                for task in eval_tasks:\n                    for file in task.metadata[\"report_files\"]:\n                        mflow_logger.experiment.log_artifact(\n                            run_id=mflow_logger.run_id, local_path=file, artifact_path=task.report_path\n                        )\n\n            if tensorboard_logger is not None and self.config.core.get(\"upload_artifacts\"):\n                for task in eval_tasks:\n                    for file in task.metadata[\"report_files\"]:\n                        ext = os.path.splitext(file)[1].lower()\n\n                        if ext in [\".png\", \".jpg\", \".jpeg\", \".bmp\", \".tiff\", \".tif\", \".gif\"]:\n                            try:\n                                img = cv2.imread(file)\n                                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n                            except cv2.error:\n                                log.info(\"Could not upload artifact image %s\", file)\n                                continue\n\n                            tensorboard_logger.experiment.add_image(\n                                os.path.basename(file), img, 0, dataformats=\"HWC\"\n                            )\n                        else:\n                            utils.upload_file_tensorboard(file, tensorboard_logger)\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.Segmentation.prepare","title":"<code>prepare()</code>","text":"<p>Prepare the task.</p> Source code in <code>quadra/tasks/segmentation.py</code> <pre><code>def prepare(self) -&gt; None:\n\"\"\"Prepare the task.\"\"\"\n    super().prepare()\n    self.module = self.config.model\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.SegmentationAnalysisEvaluation","title":"<code>SegmentationAnalysisEvaluation(config, model_path, device=None)</code>","text":"<p>             Bases: <code>SegmentationEvaluation</code></p> <p>Segmentation Analysis Evaluation Task Args:     config: The experiment configuration     model_path: The model path.     device: Device to use for evaluation. If None, the device is automatically determined.</p> Source code in <code>quadra/tasks/segmentation.py</code> <pre><code>def __init__(\n    self,\n    config: DictConfig,\n    model_path: str,\n    device: str | None = None,\n):\n    super().__init__(config=config, model_path=model_path, device=device)\n    self.test_output: dict[str, Any] = {}\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.SegmentationAnalysisEvaluation.generate_report","title":"<code>generate_report()</code>","text":"<p>Generate a report.</p> Source code in <code>quadra/tasks/segmentation.py</code> <pre><code>def generate_report(self) -&gt; None:\n\"\"\"Generate a report.\"\"\"\n    log.info(\"Generating analysis report\")\n\n    for stage, output in self.test_output.items():\n        image_mean = OmegaConf.to_container(self.config.transforms.mean)\n        if not isinstance(image_mean, list) or any(not isinstance(x, (int, float)) for x in image_mean):\n            raise ValueError(\"Image mean is not a list of float or integer values, please check your config\")\n        image_std = OmegaConf.to_container(self.config.transforms.std)\n        if not isinstance(image_std, list) or any(not isinstance(x, (int, float)) for x in image_std):\n            raise ValueError(\"Image std is not a list of float or integer values, please check your config\")\n        reports = create_mask_report(\n            stage=stage,\n            output=output,\n            report_path=\"analysis_report\",\n            mean=image_mean,\n            std=image_std,\n            analysis=True,\n            nb_samples=10,\n            apply_sigmoid=True,\n            show_orj_predictions=True,\n        )\n        self.metadata[\"report_files\"].extend(reports)\n        log.info(\"%s analysis report completed.\", stage)\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.SegmentationAnalysisEvaluation.prepare","title":"<code>prepare()</code>","text":"<p>Prepare the evaluation task.</p> Source code in <code>quadra/tasks/segmentation.py</code> <pre><code>def prepare(self) -&gt; None:\n\"\"\"Prepare the evaluation task.\"\"\"\n    super().prepare()\n    self.datamodule.setup(stage=\"fit\")\n    self.datamodule.setup(stage=\"test\")\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.SegmentationAnalysisEvaluation.test","title":"<code>test()</code>","text":"<p>Run testing.</p> Source code in <code>quadra/tasks/segmentation.py</code> <pre><code>@automatic_datamodule_batch_size(batch_size_attribute_name=\"batch_size\")\ndef test(self) -&gt; None:\n\"\"\"Run testing.\"\"\"\n    log.info(\"Starting inference for analysis.\")\n\n    stages: list[str] = []\n    dataloaders: list[torch.utils.data.DataLoader] = []\n\n    # if self.datamodule.train_dataset_available:\n    #     stages.append(\"train\")\n    #     dataloaders.append(self.datamodule.train_dataloader())\n    #     if self.datamodule.val_dataset_available:\n    #         stages.append(\"val\")\n    #         dataloaders.append(self.datamodule.val_dataloader())\n\n    if self.datamodule.test_dataset_available:\n        stages.append(\"test\")\n        dataloaders.append(self.datamodule.test_dataloader())\n    for stage, dataloader in zip(stages, dataloaders):\n        log.info(\"Running inference on %s set with batch size: %d\", stage, dataloader.batch_size)\n        image_list, mask_list, mask_pred_list, label_list = [], [], [], []\n        for batch in dataloader:\n            images, masks, labels = batch\n            images = images.to(device=self.device, dtype=self.deployment_model.model_dtype)\n            if len(masks.shape) == 3:  # BxHxW -&gt; Bx1xHxW\n                masks = masks.unsqueeze(1)\n            with torch.no_grad():\n                image_list.append(images)\n                mask_list.append(masks)\n                mask_pred_list.append(self.deployment_model(images.to(self.device)))\n                label_list.append(labels)\n\n        output = {\n            \"image\": torch.cat(image_list, dim=0),\n            \"mask\": torch.cat(mask_list, dim=0),\n            \"label\": torch.cat(label_list, dim=0),\n            \"mask_pred\": torch.cat(mask_pred_list, dim=0),\n        }\n        self.test_output[stage] = output\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.SegmentationAnalysisEvaluation.train","title":"<code>train()</code>","text":"<p>Skip training.</p> Source code in <code>quadra/tasks/segmentation.py</code> <pre><code>def train(self) -&gt; None:\n\"\"\"Skip training.\"\"\"\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.SegmentationEvaluation","title":"<code>SegmentationEvaluation(config, model_path, device='cpu')</code>","text":"<p>             Bases: <code>Evaluation[SegmentationDataModuleT]</code></p> <p>Segmentation Evaluation Task with deployment models.</p> <p>Parameters:</p> <ul> <li> config             (<code>DictConfig</code>)         \u2013          <p>The experiment configuration</p> </li> <li> model_path             (<code>str</code>)         \u2013          <p>The experiment path.</p> </li> <li> device             (<code>str | None</code>, default:                 <code>'cpu'</code> )         \u2013          <p>Device to use for evaluation. If None, the device is automatically determined.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>           \u2013          <p>If the model path is not provided</p> </li> </ul> Source code in <code>quadra/tasks/segmentation.py</code> <pre><code>def __init__(\n    self,\n    config: DictConfig,\n    model_path: str,\n    device: str | None = \"cpu\",\n):\n    super().__init__(config=config, model_path=model_path, device=device)\n    self.config = config\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.SegmentationEvaluation.inference","title":"<code>inference(dataloader, deployment_model, device)</code>","text":"<p>Run inference on the dataloader and return the output.</p> <p>Parameters:</p> <ul> <li> dataloader             (<code>DataLoader</code>)         \u2013          <p>The dataloader to run inference on</p> </li> <li> deployment_model             (<code>BaseEvaluationModel</code>)         \u2013          <p>The deployment model to use</p> </li> <li> device             (<code>device</code>)         \u2013          <p>The device to run inference on</p> </li> </ul> Source code in <code>quadra/tasks/segmentation.py</code> <pre><code>@torch.no_grad()\ndef inference(\n    self, dataloader: DataLoader, deployment_model: BaseEvaluationModel, device: torch.device\n) -&gt; dict[str, torch.Tensor]:\n\"\"\"Run inference on the dataloader and return the output.\n\n    Args:\n        dataloader: The dataloader to run inference on\n        deployment_model: The deployment model to use\n        device: The device to run inference on\n    \"\"\"\n    image_list, mask_list, mask_pred_list, label_list = [], [], [], []\n    for batch in dataloader:\n        images, masks, labels = batch\n        images = images.to(device)\n        masks = masks.to(device)\n        labels = labels.to(device)\n        image_list.append(images.cpu())\n        mask_list.append(masks.cpu())\n        mask_pred_list.append(deployment_model(images.to(device)).cpu())\n        label_list.append(labels.cpu())\n    output = {\n        \"image\": torch.cat(image_list, dim=0),\n        \"mask\": torch.cat(mask_list, dim=0),\n        \"label\": torch.cat(label_list, dim=0),\n        \"mask_pred\": torch.cat(mask_pred_list, dim=0),\n    }\n    return output\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.SegmentationEvaluation.prepare","title":"<code>prepare()</code>","text":"<p>Prepare the evaluation.</p> Source code in <code>quadra/tasks/segmentation.py</code> <pre><code>def prepare(self) -&gt; None:\n\"\"\"Prepare the evaluation.\"\"\"\n    super().prepare()\n    # TODO: Why we propagate mean and std only in Segmentation?\n    self.config.transforms.mean = self.model_data[\"mean\"]\n    self.config.transforms.std = self.model_data[\"std\"]\n    # Setup datamodule\n    if hasattr(self.config.datamodule, \"idx_to_class\"):\n        idx_to_class = self.model_data[\"classes\"]  # dict {index: class}\n        self.config.datamodule.idx_to_class = idx_to_class\n    self.datamodule = self.config.datamodule\n    # prepare_data() must be explicitly called because there is no lightning training\n    self.datamodule.prepare_data()\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.SegmentationEvaluation.save_config","title":"<code>save_config()</code>","text":"<p>Skip saving the config.</p> Source code in <code>quadra/tasks/segmentation.py</code> <pre><code>def save_config(self) -&gt; None:\n\"\"\"Skip saving the config.\"\"\"\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.SklearnClassification","title":"<code>SklearnClassification(config, output, device, automatic_batch_size, save_model_summary=False, half_precision=False, gradcam=False)</code>","text":"<p>             Bases: <code>Generic[SklearnClassificationDataModuleT]</code>, <code>Task[SklearnClassificationDataModuleT]</code></p> <p>Sklearn classification task.</p> <p>Parameters:</p> <ul> <li> config             (<code>DictConfig</code>)         \u2013          <p>The experiment configuration</p> </li> <li> device             (<code>str</code>)         \u2013          <p>The device to use. Defaults to None.</p> </li> <li> output             (<code>DictConfig</code>)         \u2013          <p>Dictionary defining which kind of outputs to generate. Defaults to None.</p> </li> <li> automatic_batch_size             (<code>DictConfig</code>)         \u2013          <p>Whether to automatically find the largest batch size that fits in memory.</p> </li> <li> save_model_summary             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to save a model_summary.txt file containing the model summary.</p> </li> <li> half_precision             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to use half precision during training.</p> </li> <li> gradcam             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to compute gradcams for test results.</p> </li> </ul> Source code in <code>quadra/tasks/classification.py</code> <pre><code>def __init__(\n    self,\n    config: DictConfig,\n    output: DictConfig,\n    device: str,\n    automatic_batch_size: DictConfig,\n    save_model_summary: bool = False,\n    half_precision: bool = False,\n    gradcam: bool = False,\n):\n    super().__init__(config=config)\n\n    self._device = device\n    self.output = output\n    self._backbone: ModelSignatureWrapper\n    self._trainer: SklearnClassificationTrainer\n    self._model: ClassifierMixin\n    self.metadata: dict[str, Any] = {\n        \"test_confusion_matrix\": [],\n        \"test_accuracy\": [],\n        \"test_results\": [],\n        \"test_labels\": [],\n        \"cams\": [],\n    }\n    self.export_folder = \"deployment_model\"\n    self.deploy_info_file = \"model.json\"\n    self.train_dataloader_list: list[torch.utils.data.DataLoader] = []\n    self.test_dataloader_list: list[torch.utils.data.DataLoader] = []\n    self.automatic_batch_size = automatic_batch_size\n    self.save_model_summary = save_model_summary\n    self.half_precision = half_precision\n    self.gradcam = gradcam\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.SklearnClassification.backbone","title":"<code>backbone: ModelSignatureWrapper</code>  <code>property</code> <code>writable</code>","text":""},{"location":"reference/quadra/tasks/index.html#quadra.tasks.SklearnClassification.model","title":"<code>model: ClassifierMixin</code>  <code>property</code> <code>writable</code>","text":"<p>sklearn.base.ClassifierMixin: The model.</p>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.SklearnClassification.trainer","title":"<code>trainer: SklearnClassificationTrainer</code>  <code>property</code> <code>writable</code>","text":""},{"location":"reference/quadra/tasks/index.html#quadra.tasks.SklearnClassification.execute","title":"<code>execute()</code>","text":"<p>Execute the experiment and all the steps.</p> Source code in <code>quadra/tasks/classification.py</code> <pre><code>def execute(self) -&gt; None:\n\"\"\"Execute the experiment and all the steps.\"\"\"\n    self.prepare()\n    self.train()\n    if self.output.report:\n        self.generate_report()\n    self.train_full_data()\n    if self.config.export is not None and len(self.config.export.types) &gt; 0:\n        self.export()\n    if self.output.test_full_data:\n        self.test_full_data()\n    self.finalize()\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.SklearnClassification.export","title":"<code>export()</code>","text":"<p>Generate deployment model for the task.</p> Source code in <code>quadra/tasks/classification.py</code> <pre><code>def export(self) -&gt; None:\n\"\"\"Generate deployment model for the task.\"\"\"\n    if self.config.export is None or len(self.config.export.types) == 0:\n        log.info(\"No export type specified skipping export\")\n        return\n\n    input_shapes = self.config.export.input_shapes\n\n    idx_to_class = {v: k for k, v in self.datamodule.full_dataset.class_to_idx.items()}\n\n    model_json, export_paths = export_model(\n        config=self.config,\n        model=self.backbone,\n        export_folder=self.export_folder,\n        half_precision=self.half_precision,\n        input_shapes=input_shapes,\n        idx_to_class=idx_to_class,\n        pytorch_model_type=\"backbone\",\n    )\n\n    dump(self.model, os.path.join(self.export_folder, \"classifier.joblib\"))\n\n    if len(export_paths) &gt; 0:\n        with open(os.path.join(self.export_folder, self.deploy_info_file), \"w\") as f:\n            json.dump(model_json, f)\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.SklearnClassification.extract_model_summary","title":"<code>extract_model_summary(feature_extractor, dl)</code>","text":"<p>Given a dataloader and a PyTorch model, use torchinfo to extract a summary of the model and save it to a file.</p> <p>Parameters:</p> <ul> <li> dl             (<code>DataLoader</code>)         \u2013          <p>PyTorch dataloader</p> </li> <li> feature_extractor             (<code>Module | BaseEvaluationModel</code>)         \u2013          <p>PyTorch backbone</p> </li> </ul> Source code in <code>quadra/tasks/classification.py</code> <pre><code>def extract_model_summary(\n    self, feature_extractor: torch.nn.Module | BaseEvaluationModel, dl: torch.utils.data.DataLoader\n) -&gt; None:\n\"\"\"Given a dataloader and a PyTorch model, use torchinfo to extract a summary of the model and save it\n    to a file.\n\n    Args:\n        dl: PyTorch dataloader\n        feature_extractor: PyTorch backbone\n    \"\"\"\n    if isinstance(feature_extractor, (TorchEvaluationModel, TorchscriptEvaluationModel)):\n        # TODO: I'm not sure torchinfo supports torchscript models\n        # If we are working with torch based evaluation models we need to extract the model\n        feature_extractor = feature_extractor.model\n\n    for b in tqdm(dl):\n        x1, _ = b\n\n        if hasattr(feature_extractor, \"parameters\"):\n            # Move input to the correct device\n            parameter = next(feature_extractor.parameters())\n            x1 = x1.to(parameter.device).to(parameter.dtype)\n            x1 = x1[0].unsqueeze(0)  # Remove batch dimension\n\n            model_info = None\n\n            try:\n                try:\n                    # TODO: Do we want to print the summary to the console as well?\n                    model_info = summary(feature_extractor, input_data=(x1), verbose=0)  # type: ignore[arg-type]\n                except Exception:\n                    log.warning(\n                        \"Failed to retrieve model summary using input data information, retrieving only \"\n                        \"parameters information\"\n                    )\n                    model_info = summary(feature_extractor, verbose=0)  # type: ignore[arg-type]\n            except Exception as e:\n                # If for some reason the summary fails we don't want to stop the training\n                log.warning(\"Failed to retrieve model summary: %s\", e)\n\n            if model_info is not None:\n                with open(\"model_summary.txt\", \"w\") as f:\n                    f.write(str(model_info))\n        else:\n            log.warning(\"Failed to retrieve model summary, current model has no parameters\")\n\n        break\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.SklearnClassification.generate_report","title":"<code>generate_report()</code>","text":"<p>Generate report for the task.</p> Source code in <code>quadra/tasks/classification.py</code> <pre><code>def generate_report(self) -&gt; None:\n\"\"\"Generate report for the task.\"\"\"\n    log.info(\"Generating report!\")\n\n    cm_list = []\n\n    for count in range(len(self.metadata[\"test_accuracy\"])):\n        current_output_folder = f\"{self.output.folder}_{count}\"\n        os.makedirs(current_output_folder, exist_ok=True)\n\n        c_matrix = self.metadata[\"test_confusion_matrix\"][count]\n        cm_list.append(c_matrix)\n        save_classification_result(\n            results=self.metadata[\"test_results\"][count],\n            output_folder=current_output_folder,\n            confmat=c_matrix,\n            accuracy=self.metadata[\"test_accuracy\"][count],\n            test_dataloader=self.test_dataloader_list[count],\n            config=self.config,\n            output=self.output,\n            grayscale_cams=self.metadata[\"cams\"][count],\n        )\n    final_confusion_matrix = sum(cm_list)\n\n    self.metadata[\"final_confusion_matrix\"] = final_confusion_matrix\n    # Save final conf matrix\n    final_folder = f\"{self.output.folder}\"\n    os.makedirs(final_folder, exist_ok=True)\n    disp = ConfusionMatrixDisplay(\n        confusion_matrix=np.array(final_confusion_matrix),\n        display_labels=[x.replace(\"pred:\", \"\") for x in final_confusion_matrix.columns.to_list()],\n    )\n    disp.plot(include_values=True, cmap=plt.cm.Greens, ax=None, colorbar=False, xticks_rotation=90)\n    plt.title(f\"Confusion Matrix (Accuracy: {(self.metadata['test_accuracy'][count] * 100):.2f}%)\")\n    plt.savefig(os.path.join(final_folder, \"test_confusion_matrix.png\"), bbox_inches=\"tight\", pad_inches=0, dpi=300)\n    plt.close()\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.SklearnClassification.prepare","title":"<code>prepare()</code>","text":"<p>Prepare the experiment.</p> Source code in <code>quadra/tasks/classification.py</code> <pre><code>def prepare(self) -&gt; None:\n\"\"\"Prepare the experiment.\"\"\"\n    self.datamodule = self.config.datamodule\n\n    self.backbone = self.config.backbone\n\n    self.model = self.config.model\n\n    # prepare_data() must be explicitly called if the task does not include a lightining training\n    self.datamodule.prepare_data()\n    self.datamodule.setup(stage=\"fit\")\n\n    self.trainer = self.config.trainer\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.SklearnClassification.test","title":"<code>test()</code>","text":"<p>Skip test phase.</p> Source code in <code>quadra/tasks/classification.py</code> <pre><code>def test(self) -&gt; None:\n\"\"\"Skip test phase.\"\"\"\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.SklearnClassification.test_full_data","title":"<code>test_full_data()</code>","text":"<p>Test model trained on full dataset.</p> Source code in <code>quadra/tasks/classification.py</code> <pre><code>@typing.no_type_check\n@automatic_datamodule_batch_size(batch_size_attribute_name=\"batch_size\")\ndef test_full_data(self) -&gt; None:\n\"\"\"Test model trained on full dataset.\"\"\"\n    self.config.datamodule.class_to_idx = self.datamodule.full_dataset.class_to_idx\n    self.config.datamodule.phase = \"test\"\n    idx_to_class = self.datamodule.full_dataset.idx_to_class\n    self.datamodule.setup(\"test\")\n    test_dataloader = self.datamodule.test_dataloader()\n\n    if len(self.datamodule.data[\"samples\"]) == 0:\n        log.info(\"No test data, skipping test\")\n        return\n\n    # Put backbone on the correct device as it may be moved after export\n    self.backbone.to(self.device)\n    _, pd_cm, accuracy, res, cams = self.trainer.test(\n        test_dataloader=test_dataloader, idx_to_class=idx_to_class, predict_proba=True, gradcam=self.gradcam\n    )\n\n    output_folder_test = \"test\"\n\n    os.makedirs(output_folder_test, exist_ok=True)\n\n    save_classification_result(\n        results=res,\n        output_folder=output_folder_test,\n        confmat=pd_cm,\n        accuracy=accuracy,\n        test_dataloader=test_dataloader,\n        config=self.config,\n        output=self.output,\n        grayscale_cams=cams,\n    )\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.SklearnClassification.train","title":"<code>train()</code>","text":"<p>Train the model.</p> Source code in <code>quadra/tasks/classification.py</code> <pre><code>@typing.no_type_check\n@automatic_datamodule_batch_size(batch_size_attribute_name=\"batch_size\")\ndef train(self) -&gt; None:\n\"\"\"Train the model.\"\"\"\n    log.info(\"Starting training...!\")\n    all_features = None\n    all_labels = None\n\n    class_to_keep = None\n\n    self.train_dataloader_list = list(self.datamodule.train_dataloader())\n    self.test_dataloader_list = list(self.datamodule.val_dataloader())\n\n    if hasattr(self.datamodule, \"class_to_keep_training\") and self.datamodule.class_to_keep_training is not None:\n        class_to_keep = self.datamodule.class_to_keep_training\n\n    if self.save_model_summary:\n        self.extract_model_summary(feature_extractor=self.backbone, dl=self.datamodule.full_dataloader())\n\n    if hasattr(self.datamodule, \"cache\") and self.datamodule.cache:\n        if self.config.trainer.iteration_over_training != 1:\n            raise AttributeError(\"Cache is only supported when iteration over training is set to 1\")\n\n        full_dataloader = self.datamodule.full_dataloader()\n        all_features, all_labels, _ = get_feature(\n            feature_extractor=self.backbone, dl=full_dataloader, iteration_over_training=1\n        )\n\n        sorted_indices = np.argsort(full_dataloader.dataset.x)\n        all_features = all_features[sorted_indices]\n        all_labels = all_labels[sorted_indices]\n\n    # cycle over all train/test split\n    for train_dataloader, test_dataloader in zip(self.train_dataloader_list, self.test_dataloader_list):\n        # Reinit classifier\n        self.model = self.config.model\n        self.trainer.change_classifier(self.model)\n\n        # Train on current training set\n        if all_features is not None and all_labels is not None:\n            # Find which are the indices used to pass from the sorted list of string to the disordered one\n            sorted_indices = np.argsort(np.concatenate([train_dataloader.dataset.x, test_dataloader.dataset.x]))\n            revese_sorted_indices = np.argsort(sorted_indices)\n\n            # Use these indices to correctly match the extracted features with the new file order\n            all_features_sorted = all_features[revese_sorted_indices]\n            all_labels_sorted = all_labels[revese_sorted_indices]\n\n            train_len = len(train_dataloader.dataset.x)\n\n            self.trainer.fit(\n                train_features=all_features_sorted[0:train_len], train_labels=all_labels_sorted[0:train_len]\n            )\n\n            _, pd_cm, accuracy, res, cams = self.trainer.test(\n                test_dataloader=test_dataloader,\n                test_features=all_features_sorted[train_len:],\n                test_labels=all_labels_sorted[train_len:],\n                class_to_keep=class_to_keep,\n                idx_to_class=train_dataloader.dataset.idx_to_class,\n                predict_proba=True,\n                gradcam=self.gradcam,\n            )\n        else:\n            self.trainer.fit(train_dataloader=train_dataloader)\n            _, pd_cm, accuracy, res, cams = self.trainer.test(\n                test_dataloader=test_dataloader,\n                class_to_keep=class_to_keep,\n                idx_to_class=train_dataloader.dataset.idx_to_class,\n                predict_proba=True,\n                gradcam=self.gradcam,\n            )\n\n        # save results\n        self.metadata[\"test_confusion_matrix\"].append(pd_cm)\n        self.metadata[\"test_accuracy\"].append(accuracy)\n        self.metadata[\"test_results\"].append(res)\n        self.metadata[\"test_labels\"].append(\n            [\n                train_dataloader.dataset.idx_to_class[i] if i != -1 else \"N/A\"\n                for i in res[\"real_label\"].unique().tolist()\n            ]\n        )\n        self.metadata[\"cams\"].append(cams)\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.SklearnClassification.train_full_data","title":"<code>train_full_data()</code>","text":"<p>Train the model on train + validation.</p> Source code in <code>quadra/tasks/classification.py</code> <pre><code>@automatic_datamodule_batch_size(batch_size_attribute_name=\"batch_size\")\ndef train_full_data(self):\n\"\"\"Train the model on train + validation.\"\"\"\n    # Reinit classifier\n    self.model = self.config.model\n    self.trainer.change_classifier(self.model)\n\n    self.trainer.fit(train_dataloader=self.datamodule.full_dataloader())\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.SklearnTestClassification","title":"<code>SklearnTestClassification(config, output, model_path, device, gradcam=False, **kwargs)</code>","text":"<p>             Bases: <code>Evaluation[SklearnClassificationDataModuleT]</code></p> <p>Perform a test using an imported SklearnClassification pytorch model.</p> <p>Parameters:</p> <ul> <li> config             (<code>DictConfig</code>)         \u2013          <p>The experiment configuration</p> </li> <li> output             (<code>DictConfig</code>)         \u2013          <p>where to save results</p> </li> <li> model_path             (<code>str</code>)         \u2013          <p>path to trained model generated from SklearnClassification task.</p> </li> <li> device             (<code>str</code>)         \u2013          <p>the device where to run the model (cuda or cpu)</p> </li> <li> gradcam             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to compute gradcams</p> </li> <li> **kwargs             (<code>Any</code>, default:                 <code>{}</code> )         \u2013          <p>Additional arguments to pass to the task</p> </li> </ul> Source code in <code>quadra/tasks/classification.py</code> <pre><code>def __init__(\n    self,\n    config: DictConfig,\n    output: DictConfig,\n    model_path: str,\n    device: str,\n    gradcam: bool = False,\n    **kwargs: Any,\n):\n    super().__init__(config=config, model_path=model_path, device=device, **kwargs)\n    self.gradcam = gradcam\n    self.output = output\n    self._backbone: BaseEvaluationModel\n    self._classifier: ClassifierMixin\n    self.class_to_idx: dict[str, int]\n    self.idx_to_class: dict[int, str]\n    self.test_dataloader: torch.utils.data.DataLoader\n    self.metadata: dict[str, Any] = {\n        \"test_confusion_matrix\": None,\n        \"test_accuracy\": None,\n        \"test_results\": None,\n        \"test_labels\": None,\n        \"cams\": None,\n    }\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.SklearnTestClassification.backbone","title":"<code>backbone: BaseEvaluationModel</code>  <code>property</code> <code>writable</code>","text":""},{"location":"reference/quadra/tasks/index.html#quadra.tasks.SklearnTestClassification.classifier","title":"<code>classifier: ClassifierMixin</code>  <code>property</code> <code>writable</code>","text":""},{"location":"reference/quadra/tasks/index.html#quadra.tasks.SklearnTestClassification.deployment_model","title":"<code>deployment_model</code>  <code>property</code> <code>writable</code>","text":"<p>Deployment model.</p>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.SklearnTestClassification.trainer","title":"<code>trainer: SklearnClassificationTrainer</code>  <code>property</code> <code>writable</code>","text":""},{"location":"reference/quadra/tasks/index.html#quadra.tasks.SklearnTestClassification.execute","title":"<code>execute()</code>","text":"<p>Execute the experiment and all the steps.</p> Source code in <code>quadra/tasks/classification.py</code> <pre><code>def execute(self) -&gt; None:\n\"\"\"Execute the experiment and all the steps.\"\"\"\n    self.prepare()\n    self.test()\n    if self.output.report:\n        self.generate_report()\n    self.finalize()\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.SklearnTestClassification.generate_report","title":"<code>generate_report()</code>","text":"<p>Generate a report for the task.</p> Source code in <code>quadra/tasks/classification.py</code> <pre><code>def generate_report(self) -&gt; None:\n\"\"\"Generate a report for the task.\"\"\"\n    log.info(\"Generating report!\")\n    os.makedirs(self.output.folder, exist_ok=True)\n    save_classification_result(\n        results=self.metadata[\"test_results\"],\n        output_folder=self.output.folder,\n        confmat=self.metadata[\"test_confusion_matrix\"],\n        accuracy=self.metadata[\"test_accuracy\"],\n        test_dataloader=self.test_dataloader,\n        config=self.config,\n        output=self.output,\n        grayscale_cams=self.metadata[\"cams\"],\n    )\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.SklearnTestClassification.prepare","title":"<code>prepare()</code>","text":"<p>Prepare the experiment.</p> Source code in <code>quadra/tasks/classification.py</code> <pre><code>def prepare(self) -&gt; None:\n\"\"\"Prepare the experiment.\"\"\"\n    super().prepare()\n\n    idx_to_class = {}\n    class_to_idx = {}\n    for k, v in self.model_data[\"classes\"].items():\n        idx_to_class[int(k)] = v\n        class_to_idx[v] = int(k)\n\n    self.idx_to_class = idx_to_class\n    self.class_to_idx = class_to_idx\n\n    self.config.datamodule.class_to_idx = class_to_idx\n\n    self.datamodule = self.config.datamodule\n    # prepare_data() must be explicitly called because there is no lightning training\n    self.datamodule.prepare_data()\n    self.datamodule.setup(stage=\"test\")\n\n    # Configure trainer\n    self.trainer = self.config.trainer\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.SklearnTestClassification.test","title":"<code>test()</code>","text":"<p>Run the test.</p> Source code in <code>quadra/tasks/classification.py</code> <pre><code>@automatic_datamodule_batch_size(batch_size_attribute_name=\"batch_size\")\ndef test(self) -&gt; None:\n\"\"\"Run the test.\"\"\"\n    self.test_dataloader = self.datamodule.test_dataloader()\n\n    _, pd_cm, accuracy, res, cams = self.trainer.test(\n        test_dataloader=self.test_dataloader,\n        idx_to_class=self.idx_to_class,\n        predict_proba=True,\n        gradcam=self.gradcam,\n    )\n\n    # save results\n    self.metadata[\"test_confusion_matrix\"] = pd_cm\n    self.metadata[\"test_accuracy\"] = accuracy\n    self.metadata[\"test_results\"] = res\n    self.metadata[\"test_labels\"] = [\n        self.idx_to_class[i] if i != -1 else \"N/A\" for i in res[\"real_label\"].unique().tolist()\n    ]\n    self.metadata[\"cams\"] = cams\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.Task","title":"<code>Task(config)</code>","text":"<p>             Bases: <code>Generic[DataModuleT]</code></p> <p>Base Experiment Task.</p> <p>Parameters:</p> <ul> <li> config             (<code>DictConfig</code>)         \u2013          <p>The experiment configuration.</p> </li> </ul> Source code in <code>quadra/tasks/base.py</code> <pre><code>def __init__(self, config: DictConfig):\n    self.config = config\n    self.export_folder: str = \"deployment_model\"\n    self._datamodule: DataModuleT\n    self.metadata: dict[str, Any]\n    self.save_config()\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.Task.datamodule","title":"<code>datamodule: DataModuleT</code>  <code>property</code> <code>writable</code>","text":""},{"location":"reference/quadra/tasks/index.html#quadra.tasks.Task.execute","title":"<code>execute()</code>","text":"<p>Execute the experiment and all the steps.</p> Source code in <code>quadra/tasks/base.py</code> <pre><code>def execute(self) -&gt; None:\n\"\"\"Execute the experiment and all the steps.\"\"\"\n    self.prepare()\n    self.train()\n    self.test()\n    if self.config.export is not None and len(self.config.export.types) &gt; 0:\n        self.export()\n    self.generate_report()\n    self.finalize()\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.Task.export","title":"<code>export()</code>","text":"<p>Export model for production.</p> Source code in <code>quadra/tasks/base.py</code> <pre><code>def export(self) -&gt; None:\n\"\"\"Export model for production.\"\"\"\n    log.info(\"Export model for production not implemented for this task!\")\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.Task.finalize","title":"<code>finalize()</code>","text":"<p>Finalize the experiment.</p> Source code in <code>quadra/tasks/base.py</code> <pre><code>def finalize(self) -&gt; None:\n\"\"\"Finalize the experiment.\"\"\"\n    log.info(\"Results are saved in %s\", os.getcwd())\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.Task.generate_report","title":"<code>generate_report()</code>","text":"<p>Generate a report.</p> Source code in <code>quadra/tasks/base.py</code> <pre><code>def generate_report(self) -&gt; None:\n\"\"\"Generate a report.\"\"\"\n    log.info(\"Report generation not implemented for this task!\")\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.Task.prepare","title":"<code>prepare()</code>","text":"<p>Prepare the experiment.</p> Source code in <code>quadra/tasks/base.py</code> <pre><code>def prepare(self) -&gt; None:\n\"\"\"Prepare the experiment.\"\"\"\n    self.datamodule = self.config.datamodule\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.Task.save_config","title":"<code>save_config()</code>","text":"<p>Save the experiment configuration when running an Hydra experiment.</p> Source code in <code>quadra/tasks/base.py</code> <pre><code>def save_config(self) -&gt; None:\n\"\"\"Save the experiment configuration when running an Hydra experiment.\"\"\"\n    if HydraConfig.initialized():\n        with open(\"config_resolved.yaml\", \"w\") as fp:\n            OmegaConf.save(config=OmegaConf.to_container(self.config, resolve=True), f=fp.name)\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.Task.test","title":"<code>test()</code>","text":"<p>Test the model.</p> Source code in <code>quadra/tasks/base.py</code> <pre><code>def test(self) -&gt; Any:\n\"\"\"Test the model.\"\"\"\n    log.info(\"Testing not implemented for this task!\")\n</code></pre>"},{"location":"reference/quadra/tasks/index.html#quadra.tasks.Task.train","title":"<code>train()</code>","text":"<p>Train the model.</p> Source code in <code>quadra/tasks/base.py</code> <pre><code>def train(self) -&gt; Any:\n\"\"\"Train the model.\"\"\"\n    log.info(\"Training not implemented for this task!\")\n</code></pre>"},{"location":"reference/quadra/tasks/anomaly.html","title":"anomaly","text":""},{"location":"reference/quadra/tasks/anomaly.html#quadra.tasks.anomaly.AnomalibDetection","title":"<code>AnomalibDetection(config, module_function, checkpoint_path=None, run_test=True, report=True)</code>","text":"<p>             Bases: <code>Generic[AnomalyDataModuleT]</code>, <code>LightningTask[AnomalyDataModuleT]</code></p> <p>Anomaly Detection Task.</p> <p>Parameters:</p> <ul> <li> config             (<code>DictConfig</code>)         \u2013          <p>The experiment configuration</p> </li> <li> module_function             (<code>DictConfig</code>)         \u2013          <p>The function that instantiates the module and model</p> </li> <li> checkpoint_path             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>The path to the checkpoint to load the model from. Defaults to None.</p> </li> <li> run_test             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Whether to run the test after training. Defaults to False.</p> </li> <li> report             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Whether to report the results. Defaults to False.</p> </li> </ul> Source code in <code>quadra/tasks/anomaly.py</code> <pre><code>def __init__(\n    self,\n    config: DictConfig,\n    module_function: DictConfig,\n    checkpoint_path: str | None = None,\n    run_test: bool = True,\n    report: bool = True,\n):\n    super().__init__(\n        config=config,\n        checkpoint_path=checkpoint_path,\n        run_test=run_test,\n        report=report,\n    )\n    self._module: AnomalyModule\n    self.module_function = module_function\n    self.export_folder = \"deployment_model\"\n    self.report_path = \"\"\n    self.test_results: list[dict] | None = None\n</code></pre>"},{"location":"reference/quadra/tasks/anomaly.html#quadra.tasks.anomaly.AnomalibDetection.module","title":"<code>module: AnomalyModule</code>  <code>property</code> <code>writable</code>","text":"<p>Get the module.</p>"},{"location":"reference/quadra/tasks/anomaly.html#quadra.tasks.anomaly.AnomalibDetection.export","title":"<code>export()</code>","text":"<p>Export model for production.</p> Source code in <code>quadra/tasks/anomaly.py</code> <pre><code>def export(self) -&gt; None:\n\"\"\"Export model for production.\"\"\"\n    if self.config.trainer.get(\"fast_dev_run\"):\n        log.warning(\"Skipping export since fast_dev_run is enabled\")\n        return\n\n    model = self.module.model\n\n    input_shapes = self.config.export.input_shapes\n\n    half_precision = \"16\" in self.trainer.precision\n\n    model_json, export_paths = export_model(\n        config=self.config,\n        model=model,\n        export_folder=self.export_folder,\n        half_precision=half_precision,\n        input_shapes=input_shapes,\n        idx_to_class={0: \"good\", 1: \"defect\"},\n    )\n\n    if len(export_paths) == 0:\n        return\n\n    model_json[\"image_threshold\"] = np.round(self.module.image_threshold.value.item(), 3)\n    model_json[\"pixel_threshold\"] = np.round(self.module.pixel_threshold.value.item(), 3)\n    model_json[\"anomaly_method\"] = self.config.model.model.name\n\n    with open(os.path.join(self.export_folder, \"model.json\"), \"w\") as f:\n        json.dump(model_json, f, cls=utils.HydraEncoder)\n</code></pre>"},{"location":"reference/quadra/tasks/anomaly.html#quadra.tasks.anomaly.AnomalibDetection.generate_report","title":"<code>generate_report()</code>","text":"<p>Generate a report for the task and try to upload artifacts.</p> Source code in <code>quadra/tasks/anomaly.py</code> <pre><code>def generate_report(self):\n\"\"\"Generate a report for the task and try to upload artifacts.\"\"\"\n    self._generate_report()\n    self._upload_artifacts()\n</code></pre>"},{"location":"reference/quadra/tasks/anomaly.html#quadra.tasks.anomaly.AnomalibDetection.prepare","title":"<code>prepare()</code>","text":"<p>Prepare the task.</p> Source code in <code>quadra/tasks/anomaly.py</code> <pre><code>def prepare(self) -&gt; None:\n\"\"\"Prepare the task.\"\"\"\n    super().prepare()\n    self.module = self.config.model\n    self.module.model = ModelSignatureWrapper(self.module.model)\n</code></pre>"},{"location":"reference/quadra/tasks/anomaly.html#quadra.tasks.anomaly.AnomalibDetection.test","title":"<code>test()</code>","text":"<p>Lightning test.</p> Source code in <code>quadra/tasks/anomaly.py</code> <pre><code>def test(self) -&gt; Any:\n\"\"\"Lightning test.\"\"\"\n    self.test_results = super().test()\n    return self.test_results\n</code></pre>"},{"location":"reference/quadra/tasks/anomaly.html#quadra.tasks.anomaly.AnomalibEvaluation","title":"<code>AnomalibEvaluation(config, model_path, use_training_threshold=False, device=None, training_threshold_type=None)</code>","text":"<p>             Bases: <code>Evaluation[AnomalyDataModule]</code></p> <p>Evaluation task for Anomalib.</p> <p>Parameters:</p> <ul> <li> config             (<code>DictConfig</code>)         \u2013          <p>Task configuration</p> </li> <li> model_path             (<code>str</code>)         \u2013          <p>Path to the model folder that contains an exported model</p> </li> <li> use_training_threshold             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to use the training threshold for the evaluation or use the one that maximizes the F1 score on the test set.</p> </li> <li> device             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>Device to use for evaluation. If None, the device is automatically determined.</p> </li> </ul> Source code in <code>quadra/tasks/anomaly.py</code> <pre><code>def __init__(\n    self,\n    config: DictConfig,\n    model_path: str,\n    use_training_threshold: bool = False,\n    device: str | None = None,\n    training_threshold_type: Literal[\"image\", \"pixel\"] | None = None,\n):\n    super().__init__(config=config, model_path=model_path, device=device)\n\n    self.use_training_threshold = use_training_threshold\n\n    if training_threshold_type is not None and training_threshold_type not in [\"image\", \"pixel\"]:\n        raise ValueError(\"Training threshold type must be either image or pixel\")\n\n    if training_threshold_type is None and use_training_threshold:\n        log.warning(\"Using training threshold but no training threshold type is provided, defaulting to image\")\n        training_threshold_type = \"image\"\n\n    self.training_threshold_type = training_threshold_type\n</code></pre>"},{"location":"reference/quadra/tasks/anomaly.html#quadra.tasks.anomaly.AnomalibEvaluation.execute","title":"<code>execute()</code>","text":"<p>Execute the evaluation.</p> Source code in <code>quadra/tasks/anomaly.py</code> <pre><code>def execute(self) -&gt; None:\n\"\"\"Execute the evaluation.\"\"\"\n    self.prepare()\n    self.test()\n    self.generate_report()\n    self.finalize()\n</code></pre>"},{"location":"reference/quadra/tasks/anomaly.html#quadra.tasks.anomaly.AnomalibEvaluation.generate_report","title":"<code>generate_report()</code>","text":"<p>Generate report.</p> Source code in <code>quadra/tasks/anomaly.py</code> <pre><code>def generate_report(self) -&gt; None:\n\"\"\"Generate report.\"\"\"\n    log.info(\"Generating report\")\n    if len(self.report_path) &gt; 0:\n        os.makedirs(self.report_path, exist_ok=True)\n\n    # TODO: We currently don't use anomaly for segmentation, so the pixel threshold handling is not properly\n    # implemented and we produce as output only a single threshold.\n    training_threshold = self.model_data[f\"{self.training_threshold_type}_threshold\"]\n    optimal_threshold = self.metadata[\"threshold\"]\n\n    normalized_optimal_threshold = cast(float, normalize_anomaly_score(optimal_threshold, training_threshold))\n\n    os.makedirs(os.path.join(self.report_path, \"predictions\"), exist_ok=True)\n    os.makedirs(os.path.join(self.report_path, \"heatmaps\"), exist_ok=True)\n\n    anomaly_scores = self.metadata[\"anomaly_scores\"].cpu().numpy()\n    anomaly_scores = normalize_anomaly_score(anomaly_scores, training_threshold)\n\n    if not isinstance(anomaly_scores, np.ndarray):\n        raise ValueError(\"Anomaly scores must be a numpy array\")\n\n    good_scores = anomaly_scores[np.where(np.array(self.metadata[\"image_labels\"]) == 0)]\n    defect_scores = anomaly_scores[np.where(np.array(self.metadata[\"image_labels\"]) == 1)]\n\n    count_overlapping_scores = 0\n\n    if len(good_scores) != 0 and len(defect_scores) != 0 and defect_scores.min() &lt;= good_scores.max():\n        count_overlapping_scores = len(\n            np.where((anomaly_scores &gt;= defect_scores.min()) &amp; (anomaly_scores &lt;= good_scores.max()))[0]\n        )\n\n    plot_cumulative_histogram(good_scores, defect_scores, normalized_optimal_threshold, self.report_path)\n\n    json_output = {\n        \"observations\": [],\n        \"threshold\": np.round(normalized_optimal_threshold, 3),\n        \"unnormalized_threshold\": np.round(optimal_threshold, 3),\n        \"f1_score\": np.round(self.metadata[\"optimal_f1\"], 3),\n        \"metrics\": {\n            \"overlapping_scores\": count_overlapping_scores,\n        },\n    }\n\n    tg, fb, fg, tb = 0, 0, 0, 0\n\n    mask_area = None\n    crop_area = None\n\n    if hasattr(self.datamodule, \"valid_area_mask\") and self.datamodule.valid_area_mask is not None:\n        mask_area = cv2.imread(self.datamodule.valid_area_mask, 0)\n        mask_area = (mask_area &gt; 0).astype(np.uint8)  # type: ignore[operator]\n\n    if hasattr(self.datamodule, \"crop_area\") and self.datamodule.crop_area is not None:\n        crop_area = self.datamodule.crop_area\n\n    anomaly_maps = normalize_anomaly_score(self.metadata[\"anomaly_maps\"], training_threshold)\n\n    if not isinstance(anomaly_maps, torch.Tensor):\n        raise ValueError(\"Anomaly maps must be a tensor\")\n\n    for img_path, gt_label, anomaly_score, anomaly_map in tqdm(\n        zip(\n            self.metadata[\"image_paths\"],\n            self.metadata[\"image_labels\"],\n            anomaly_scores,\n            anomaly_maps,\n        ),\n        total=len(self.metadata[\"image_paths\"]),\n    ):\n        img = cv2.imread(img_path, 0)\n        if mask_area is not None:\n            img = img * mask_area  # type: ignore[operator]\n\n        if crop_area is not None:\n            img = img[crop_area[1] : crop_area[3], crop_area[0] : crop_area[2]]\n\n        output_mask = (anomaly_map &gt;= normalized_optimal_threshold).cpu().numpy().squeeze().astype(np.uint8)\n        output_mask_label = os.path.basename(os.path.dirname(img_path))\n        output_mask_name = os.path.splitext(os.path.basename(img_path))[0] + \".png\"\n        pred_label = int(anomaly_score &gt;= normalized_optimal_threshold)\n\n        json_output[\"observations\"].append(\n            {\n                \"image_path\": os.path.dirname(img_path),\n                \"file_name\": os.path.basename(img_path),\n                \"expectation\": gt_label if gt_label != -1 else \"\",\n                \"prediction\": pred_label,\n                \"prediction_mask\": os.path.join(\"predictions\", output_mask_label, output_mask_name),\n                \"prediction_heatmap\": os.path.join(\"heatmaps\", output_mask_label, output_mask_name),\n                \"is_correct\": pred_label == gt_label if gt_label != -1 else True,\n                \"anomaly_score\": f\"{anomaly_score.item():.3f}\",\n            }\n        )\n\n        if gt_label == 0 and pred_label == 0:\n            tg += 1\n        elif gt_label == 0 and pred_label == 1:\n            fb += 1\n        elif gt_label == 1 and pred_label == 0:\n            fg += 1\n        elif gt_label == 1 and pred_label == 1:\n            tb += 1\n\n        output_mask = output_mask * 255\n        output_mask = cv2.resize(output_mask, (img.shape[1], img.shape[0]))\n        output_prediction_folder = os.path.join(self.report_path, \"predictions\", output_mask_label)\n        os.makedirs(output_prediction_folder, exist_ok=True)\n        cv2.imwrite(os.path.join(output_prediction_folder, output_mask_name), output_mask)\n\n        # Normalize the map and rescale it to 0-1 range\n        # In this case we are saying that the anomaly map is in the range [normalized_th - 50, normalized_th + 50]\n        # This allow to have a stronger color for the anomalies and a lighter one for really normal regions\n        # It's also independent from the max or min anomaly score!\n        normalized_map: MapOrValue = (anomaly_map - (normalized_optimal_threshold - 50)) / 100\n\n        if isinstance(normalized_map, torch.Tensor):\n            normalized_map = normalized_map.cpu().numpy().squeeze()\n\n        normalized_map = np.clip(normalized_map, 0, 1)\n        output_heatmap = anomaly_map_to_color_map(normalized_map, normalize=False)\n        output_heatmap = cv2.resize(output_heatmap, (img.shape[1], img.shape[0]))\n\n        output_heatmap_folder = os.path.join(self.report_path, \"heatmaps\", output_mask_label)\n        os.makedirs(output_heatmap_folder, exist_ok=True)\n\n        cv2.imwrite(\n            os.path.join(output_heatmap_folder, output_mask_name),\n            cv2.cvtColor(output_heatmap, cv2.COLOR_RGB2BGR),\n        )\n\n    json_output[\"metrics\"][\"confusion_matrix\"] = {\n        \"class_labels\": [\"normal\", \"anomaly\"],\n        \"matrix\": [\n            [tg, fb],\n            [fg, tb],\n        ],\n    }\n\n    with open(os.path.join(self.report_path, \"anomaly_test_output.json\"), \"w\") as f:\n        json.dump(json_output, f)\n</code></pre>"},{"location":"reference/quadra/tasks/anomaly.html#quadra.tasks.anomaly.AnomalibEvaluation.prepare","title":"<code>prepare()</code>","text":"<p>Prepare the evaluation.</p> Source code in <code>quadra/tasks/anomaly.py</code> <pre><code>def prepare(self) -&gt; None:\n\"\"\"Prepare the evaluation.\"\"\"\n    super().prepare()\n    self.datamodule = self.config.datamodule\n    # prepare_data() must be explicitly called because there is no lightning training\n    self.datamodule.prepare_data()\n    self.datamodule.setup(stage=\"test\")\n</code></pre>"},{"location":"reference/quadra/tasks/anomaly.html#quadra.tasks.anomaly.AnomalibEvaluation.test","title":"<code>test()</code>","text":"<p>Perform test.</p> Source code in <code>quadra/tasks/anomaly.py</code> <pre><code>@automatic_datamodule_batch_size(batch_size_attribute_name=\"test_batch_size\")\ndef test(self) -&gt; None:\n\"\"\"Perform test.\"\"\"\n    log.info(\"Running test\")\n    test_dataloader = self.datamodule.test_dataloader()\n\n    optimal_f1 = OptimalF1(num_classes=None, pos_label=1)  # type: ignore[arg-type]\n\n    anomaly_scores = []\n    anomaly_maps = []\n    image_labels = []\n    image_paths = []\n\n    with torch.no_grad():\n        for batch_item in tqdm(test_dataloader):\n            batch_images = batch_item[\"image\"]\n            batch_labels = batch_item[\"label\"]\n            image_labels.extend(batch_labels.tolist())\n            image_paths.extend(batch_item[\"image_path\"])\n            batch_images = batch_images.to(device=self.device, dtype=self.deployment_model.model_dtype)\n            if self.model_data.get(\"anomaly_method\") == \"efficientad\":\n                model_output = self.deployment_model(batch_images, None)\n            else:\n                model_output = self.deployment_model(batch_images)\n            anomaly_map, anomaly_score = model_output[0], model_output[1]\n            anomaly_map = anomaly_map.cpu()\n            anomaly_score = anomaly_score.cpu()\n            known_labels = torch.where(batch_labels != -1)[0]\n            if len(known_labels) &gt; 0:\n                # Skip computing F1 score for images without gt\n                optimal_f1.update(anomaly_score[known_labels], batch_labels[known_labels])\n            anomaly_scores.append(anomaly_score)\n            anomaly_maps.append(anomaly_map)\n\n    anomaly_scores = torch.cat(anomaly_scores)\n    anomaly_maps = torch.cat(anomaly_maps)\n\n    if any(x != -1 for x in image_labels):\n        if self.use_training_threshold:\n            _image_labels = torch.tensor(image_labels)\n            threshold = torch.tensor(float(self.model_data[f\"{self.training_threshold_type}_threshold\"]))\n            known_labels = torch.where(_image_labels != -1)[0]\n\n            _image_labels = _image_labels[known_labels]\n            _anomaly_scores = anomaly_scores[known_labels]\n\n            pred_labels = (_anomaly_scores &gt;= threshold).long()\n\n            optimal_f1_score = torch.tensor(f1_score(_image_labels, pred_labels))\n        else:\n            optimal_f1_score = optimal_f1.compute()\n            threshold = optimal_f1.threshold\n    else:\n        log.warning(\"No ground truth available during evaluation, use training image threshold for reporting\")\n        optimal_f1_score = torch.tensor(0)\n        threshold = torch.tensor(float(self.model_data[\"image_threshold\"]))\n\n    log.info(\"Computed F1 score: %s\", optimal_f1_score.item())\n    self.metadata[\"anomaly_scores\"] = anomaly_scores\n    self.metadata[\"anomaly_maps\"] = anomaly_maps\n    self.metadata[\"image_labels\"] = image_labels\n    self.metadata[\"image_paths\"] = image_paths\n    self.metadata[\"threshold\"] = threshold.item()\n    self.metadata[\"optimal_f1\"] = optimal_f1_score.item()\n</code></pre>"},{"location":"reference/quadra/tasks/base.html","title":"base","text":""},{"location":"reference/quadra/tasks/base.html#quadra.tasks.base.Evaluation","title":"<code>Evaluation(config, model_path, device=None)</code>","text":"<p>             Bases: <code>Generic[DataModuleT]</code>, <code>Task[DataModuleT]</code></p> <p>Base Evaluation Task with deployment models.</p> <p>Parameters:</p> <ul> <li> config             (<code>DictConfig</code>)         \u2013          <p>The experiment configuration</p> </li> <li> model_path             (<code>str</code>)         \u2013          <p>The model path.</p> </li> <li> device             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>Device to use for evaluation. If None, the device is automatically determined.</p> </li> </ul> Source code in <code>quadra/tasks/base.py</code> <pre><code>def __init__(\n    self,\n    config: DictConfig,\n    model_path: str,\n    device: str | None = None,\n):\n    super().__init__(config=config)\n\n    if device is None:\n        self.device = utils.get_device()\n    else:\n        self.device = device\n\n    self.config = config\n    self.model_data: dict[str, Any]\n    self.model_path = model_path\n    self._deployment_model: BaseEvaluationModel\n    self.deployment_model_type: str\n    self.model_info_filename = \"model.json\"\n    self.report_path = \"\"\n    self.metadata = {\"report_files\": []}\n</code></pre>"},{"location":"reference/quadra/tasks/base.html#quadra.tasks.base.Evaluation.deployment_model","title":"<code>deployment_model: BaseEvaluationModel</code>  <code>property</code> <code>writable</code>","text":"<p>Deployment model.</p>"},{"location":"reference/quadra/tasks/base.html#quadra.tasks.base.Evaluation.prepare","title":"<code>prepare()</code>","text":"<p>Prepare the evaluation.</p> Source code in <code>quadra/tasks/base.py</code> <pre><code>def prepare(self) -&gt; None:\n\"\"\"Prepare the evaluation.\"\"\"\n    with open(os.path.join(Path(self.model_path).parent, self.model_info_filename)) as f:\n        self.model_data = json.load(f)\n\n    if not isinstance(self.model_data, dict):\n        raise ValueError(\"Model info file is not a valid json\")\n\n    for input_size in self.model_data[\"input_size\"]:\n        if len(input_size) != 3:\n            continue\n\n        # Adjust the transform for 2D models (CxHxW)\n        # We assume that each input size has the same height and width\n        if input_size[1] != self.config.transforms.input_height:\n            log.warning(\n                f\"Input height of the model ({input_size[1]}) is different from the one specified \"\n                + f\"in the config ({self.config.transforms.input_height}). Fixing the config.\"\n            )\n            self.config.transforms.input_height = input_size[1]\n\n        if input_size[2] != self.config.transforms.input_width:\n            log.warning(\n                f\"Input width of the model ({input_size[2]}) is different from the one specified \"\n                + f\"in the config ({self.config.transforms.input_width}). Fixing the config.\"\n            )\n            self.config.transforms.input_width = input_size[2]\n\n    self.deployment_model = self.model_path  # type: ignore[assignment]\n</code></pre>"},{"location":"reference/quadra/tasks/base.html#quadra.tasks.base.LightningTask","title":"<code>LightningTask(config, checkpoint_path=None, run_test=False, report=False)</code>","text":"<p>             Bases: <code>Generic[DataModuleT]</code>, <code>Task[DataModuleT]</code></p> <p>Base Experiment Task.</p> <p>Parameters:</p> <ul> <li> config             (<code>DictConfig</code>)         \u2013          <p>The experiment configuration</p> </li> <li> checkpoint_path             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>The path to the checkpoint to load the model from. Defaults to None.</p> </li> <li> run_test             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to run the test after training. Defaults to False.</p> </li> <li> report             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to generate a report. Defaults to False.</p> </li> </ul> Source code in <code>quadra/tasks/base.py</code> <pre><code>def __init__(\n    self,\n    config: DictConfig,\n    checkpoint_path: str | None = None,\n    run_test: bool = False,\n    report: bool = False,\n):\n    super().__init__(config=config)\n    self.checkpoint_path = checkpoint_path\n    self.run_test = run_test\n    self.report = report\n    self._module: LightningModule\n    self._devices: int | list[int]\n    self._callbacks: list[Callback]\n    self._logger: list[Logger]\n    self._trainer: Trainer\n</code></pre>"},{"location":"reference/quadra/tasks/base.html#quadra.tasks.base.LightningTask.callbacks","title":"<code>callbacks: list[Callback]</code>  <code>property</code> <code>writable</code>","text":"<p>List[Callback]: The callbacks.</p>"},{"location":"reference/quadra/tasks/base.html#quadra.tasks.base.LightningTask.devices","title":"<code>devices: int | list[int]</code>  <code>property</code> <code>writable</code>","text":"<p>List[int]: The devices ids.</p>"},{"location":"reference/quadra/tasks/base.html#quadra.tasks.base.LightningTask.logger","title":"<code>logger: list[Logger]</code>  <code>property</code> <code>writable</code>","text":"<p>List[Logger]: The loggers.</p>"},{"location":"reference/quadra/tasks/base.html#quadra.tasks.base.LightningTask.module","title":"<code>module: LightningModule</code>  <code>property</code> <code>writable</code>","text":""},{"location":"reference/quadra/tasks/base.html#quadra.tasks.base.LightningTask.trainer","title":"<code>trainer: Trainer</code>  <code>property</code> <code>writable</code>","text":""},{"location":"reference/quadra/tasks/base.html#quadra.tasks.base.LightningTask.add_callback","title":"<code>add_callback(callback)</code>","text":"<p>Add a callback to the trainer.</p> <p>Parameters:</p> <ul> <li> callback             (<code>Callback</code>)         \u2013          <p>The callback to add</p> </li> </ul> Source code in <code>quadra/tasks/base.py</code> <pre><code>def add_callback(self, callback: Callback):\n\"\"\"Add a callback to the trainer.\n\n    Args:\n        callback: The callback to add\n    \"\"\"\n    if hasattr(self.trainer, \"callbacks\") and isinstance(self.trainer.callbacks, list):\n        self.trainer.callbacks.append(callback)\n</code></pre>"},{"location":"reference/quadra/tasks/base.html#quadra.tasks.base.LightningTask.execute","title":"<code>execute()</code>","text":"<p>Execute the experiment and all the steps.</p> Source code in <code>quadra/tasks/base.py</code> <pre><code>def execute(self) -&gt; None:\n\"\"\"Execute the experiment and all the steps.\"\"\"\n    self.prepare()\n    self.train()\n    if self.run_test:\n        self.test()\n    if self.config.export is not None and len(self.config.export.types) &gt; 0:\n        self.export()\n    if self.report:\n        self.generate_report()\n    self.finalize()\n</code></pre>"},{"location":"reference/quadra/tasks/base.html#quadra.tasks.base.LightningTask.finalize","title":"<code>finalize()</code>","text":"<p>Finalize the experiment.</p> Source code in <code>quadra/tasks/base.py</code> <pre><code>def finalize(self) -&gt; None:\n\"\"\"Finalize the experiment.\"\"\"\n    super().finalize()\n    utils.finish(\n        config=self.config,\n        module=self.module,\n        datamodule=self.datamodule,\n        trainer=self.trainer,\n        callbacks=self.callbacks,\n        logger=self.logger,\n        export_folder=self.export_folder,\n    )\n\n    if (\n        not self.config.trainer.get(\"fast_dev_run\")\n        and self.trainer.checkpoint_callback is not None\n        and hasattr(self.trainer.checkpoint_callback, \"best_model_path\")\n    ):\n        log.info(\"Best model ckpt: %s\", self.trainer.checkpoint_callback.best_model_path)\n</code></pre>"},{"location":"reference/quadra/tasks/base.html#quadra.tasks.base.LightningTask.prepare","title":"<code>prepare()</code>","text":"<p>Prepare the experiment.</p> Source code in <code>quadra/tasks/base.py</code> <pre><code>def prepare(self) -&gt; None:\n\"\"\"Prepare the experiment.\"\"\"\n    super().prepare()\n\n    # First setup loggers since some callbacks might need logger setup correctly.\n    if \"logger\" in self.config:\n        self.logger = self.config.logger\n\n    if \"callbacks\" in self.config:\n        self.callbacks = self.config.callbacks\n\n    self.devices = self.config.trainer.devices\n    self.trainer = self.config.trainer\n</code></pre>"},{"location":"reference/quadra/tasks/base.html#quadra.tasks.base.LightningTask.test","title":"<code>test()</code>","text":"<p>Test the model.</p> Source code in <code>quadra/tasks/base.py</code> <pre><code>def test(self) -&gt; Any:\n\"\"\"Test the model.\"\"\"\n    log.info(\"Starting testing!\")\n\n    best_model = None\n    if (\n        self.trainer.checkpoint_callback is not None\n        and hasattr(self.trainer.checkpoint_callback, \"best_model_path\")\n        and self.trainer.checkpoint_callback.best_model_path is not None\n        and len(self.trainer.checkpoint_callback.best_model_path) &gt; 0\n    ):\n        best_model = self.trainer.checkpoint_callback.best_model_path\n\n    if best_model is None:\n        log.warning(\n            \"No best checkpoint model found, using last weights for test, this might lead to worse results, \"\n            \"consider using a checkpoint callback.\"\n        )\n\n    return self.trainer.test(model=self.module, datamodule=self.datamodule, ckpt_path=best_model)\n</code></pre>"},{"location":"reference/quadra/tasks/base.html#quadra.tasks.base.LightningTask.train","title":"<code>train()</code>","text":"<p>Train the model.</p> Source code in <code>quadra/tasks/base.py</code> <pre><code>def train(self) -&gt; None:\n\"\"\"Train the model.\"\"\"\n    log.info(\"Starting training!\")\n    utils.log_hyperparameters(\n        config=self.config,\n        model=self.module,\n        trainer=self.trainer,\n    )\n\n    self.trainer.fit(model=self.module, datamodule=self.datamodule)\n</code></pre>"},{"location":"reference/quadra/tasks/base.html#quadra.tasks.base.PlaceholderTask","title":"<code>PlaceholderTask</code>","text":"<p>             Bases: <code>Task</code></p> <p>Placeholder task.</p>"},{"location":"reference/quadra/tasks/base.html#quadra.tasks.base.PlaceholderTask.execute","title":"<code>execute()</code>","text":"<p>Execute the task and all the steps.</p> Source code in <code>quadra/tasks/base.py</code> <pre><code>def execute(self) -&gt; None:\n\"\"\"Execute the task and all the steps.\"\"\"\n    log.info(\"Running Placeholder Task.\")\n    log.info(\"Quadra Version: %s\", str(get_version()))\n    log.info(\"If you are reading this, it means that library is installed correctly!\")\n</code></pre>"},{"location":"reference/quadra/tasks/base.html#quadra.tasks.base.Task","title":"<code>Task(config)</code>","text":"<p>             Bases: <code>Generic[DataModuleT]</code></p> <p>Base Experiment Task.</p> <p>Parameters:</p> <ul> <li> config             (<code>DictConfig</code>)         \u2013          <p>The experiment configuration.</p> </li> </ul> Source code in <code>quadra/tasks/base.py</code> <pre><code>def __init__(self, config: DictConfig):\n    self.config = config\n    self.export_folder: str = \"deployment_model\"\n    self._datamodule: DataModuleT\n    self.metadata: dict[str, Any]\n    self.save_config()\n</code></pre>"},{"location":"reference/quadra/tasks/base.html#quadra.tasks.base.Task.datamodule","title":"<code>datamodule: DataModuleT</code>  <code>property</code> <code>writable</code>","text":""},{"location":"reference/quadra/tasks/base.html#quadra.tasks.base.Task.execute","title":"<code>execute()</code>","text":"<p>Execute the experiment and all the steps.</p> Source code in <code>quadra/tasks/base.py</code> <pre><code>def execute(self) -&gt; None:\n\"\"\"Execute the experiment and all the steps.\"\"\"\n    self.prepare()\n    self.train()\n    self.test()\n    if self.config.export is not None and len(self.config.export.types) &gt; 0:\n        self.export()\n    self.generate_report()\n    self.finalize()\n</code></pre>"},{"location":"reference/quadra/tasks/base.html#quadra.tasks.base.Task.export","title":"<code>export()</code>","text":"<p>Export model for production.</p> Source code in <code>quadra/tasks/base.py</code> <pre><code>def export(self) -&gt; None:\n\"\"\"Export model for production.\"\"\"\n    log.info(\"Export model for production not implemented for this task!\")\n</code></pre>"},{"location":"reference/quadra/tasks/base.html#quadra.tasks.base.Task.finalize","title":"<code>finalize()</code>","text":"<p>Finalize the experiment.</p> Source code in <code>quadra/tasks/base.py</code> <pre><code>def finalize(self) -&gt; None:\n\"\"\"Finalize the experiment.\"\"\"\n    log.info(\"Results are saved in %s\", os.getcwd())\n</code></pre>"},{"location":"reference/quadra/tasks/base.html#quadra.tasks.base.Task.generate_report","title":"<code>generate_report()</code>","text":"<p>Generate a report.</p> Source code in <code>quadra/tasks/base.py</code> <pre><code>def generate_report(self) -&gt; None:\n\"\"\"Generate a report.\"\"\"\n    log.info(\"Report generation not implemented for this task!\")\n</code></pre>"},{"location":"reference/quadra/tasks/base.html#quadra.tasks.base.Task.prepare","title":"<code>prepare()</code>","text":"<p>Prepare the experiment.</p> Source code in <code>quadra/tasks/base.py</code> <pre><code>def prepare(self) -&gt; None:\n\"\"\"Prepare the experiment.\"\"\"\n    self.datamodule = self.config.datamodule\n</code></pre>"},{"location":"reference/quadra/tasks/base.html#quadra.tasks.base.Task.save_config","title":"<code>save_config()</code>","text":"<p>Save the experiment configuration when running an Hydra experiment.</p> Source code in <code>quadra/tasks/base.py</code> <pre><code>def save_config(self) -&gt; None:\n\"\"\"Save the experiment configuration when running an Hydra experiment.\"\"\"\n    if HydraConfig.initialized():\n        with open(\"config_resolved.yaml\", \"w\") as fp:\n            OmegaConf.save(config=OmegaConf.to_container(self.config, resolve=True), f=fp.name)\n</code></pre>"},{"location":"reference/quadra/tasks/base.html#quadra.tasks.base.Task.test","title":"<code>test()</code>","text":"<p>Test the model.</p> Source code in <code>quadra/tasks/base.py</code> <pre><code>def test(self) -&gt; Any:\n\"\"\"Test the model.\"\"\"\n    log.info(\"Testing not implemented for this task!\")\n</code></pre>"},{"location":"reference/quadra/tasks/base.html#quadra.tasks.base.Task.train","title":"<code>train()</code>","text":"<p>Train the model.</p> Source code in <code>quadra/tasks/base.py</code> <pre><code>def train(self) -&gt; Any:\n\"\"\"Train the model.\"\"\"\n    log.info(\"Training not implemented for this task!\")\n</code></pre>"},{"location":"reference/quadra/tasks/classification.html","title":"classification","text":""},{"location":"reference/quadra/tasks/classification.html#quadra.tasks.classification.Classification","title":"<code>Classification(config, output, checkpoint_path=None, lr_multiplier=None, gradcam=False, report=False, run_test=False)</code>","text":"<p>             Bases: <code>Generic[ClassificationDataModuleT]</code>, <code>LightningTask[ClassificationDataModuleT]</code></p> <p>Classification Task.</p> <p>Parameters:</p> <ul> <li> config             (<code>DictConfig</code>)         \u2013          <p>The experiment configuration</p> </li> <li> output             (<code>DictConfig</code>)         \u2013          <p>The otuput configuration.</p> </li> <li> gradcam             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to compute gradcams</p> </li> <li> checkpoint_path             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>The path to the checkpoint to load the model from. Defaults to None.</p> </li> <li> lr_multiplier             (<code>float | None</code>, default:                 <code>None</code> )         \u2013          <p>The multiplier for the backbone learning rate. Defaults to None.</p> </li> <li> output             (<code>DictConfig</code>)         \u2013          <p>The ouput configuration (under task config). It contains the bool \"example\" to generate figs of discordant/concordant predictions.</p> </li> <li> report             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to generate a report containing the results after test phase</p> </li> <li> run_test             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to run the test phase.</p> </li> </ul> Source code in <code>quadra/tasks/classification.py</code> <pre><code>def __init__(\n    self,\n    config: DictConfig,\n    output: DictConfig,\n    checkpoint_path: str | None = None,\n    lr_multiplier: float | None = None,\n    gradcam: bool = False,\n    report: bool = False,\n    run_test: bool = False,\n):\n    super().__init__(\n        config=config,\n        checkpoint_path=checkpoint_path,\n        run_test=run_test,\n        report=report,\n    )\n    self.output = output\n    self.gradcam = gradcam\n    self._lr_multiplier = lr_multiplier\n    self._pre_classifier: nn.Module\n    self._classifier: nn.Module\n    self._model: nn.Module\n    self._optimizer: torch.optim.Optimizer\n    self._scheduler: torch.optim.lr_scheduler._LRScheduler\n    self.model_json: dict[str, Any] | None = None\n    self.export_folder: str = \"deployment_model\"\n    self.deploy_info_file: str = \"model.json\"\n    self.report_confmat: pd.DataFrame\n    self.best_model_path: str | None = None\n</code></pre>"},{"location":"reference/quadra/tasks/classification.html#quadra.tasks.classification.Classification.len_train_dataloader","title":"<code>len_train_dataloader: int</code>  <code>property</code>","text":"<p>Get the length of the train dataloader.</p>"},{"location":"reference/quadra/tasks/classification.html#quadra.tasks.classification.Classification.optimizer","title":"<code>optimizer: torch.optim.Optimizer</code>  <code>property</code> <code>writable</code>","text":"<p>Get the optimizer.</p>"},{"location":"reference/quadra/tasks/classification.html#quadra.tasks.classification.Classification.scheduler","title":"<code>scheduler: torch.optim.lr_scheduler._LRScheduler</code>  <code>property</code> <code>writable</code>","text":"<p>Get the scheduler.</p>"},{"location":"reference/quadra/tasks/classification.html#quadra.tasks.classification.Classification.export","title":"<code>export()</code>","text":"<p>Generate deployment models for the task.</p> Source code in <code>quadra/tasks/classification.py</code> <pre><code>def export(self) -&gt; None:\n\"\"\"Generate deployment models for the task.\"\"\"\n    if self.datamodule.class_to_idx is None:\n        log.warning(\n            \"No `class_to_idx` found in the datamodule, class information will not be saved in the model.json\"\n        )\n        idx_to_class = {}\n    else:\n        idx_to_class = {v: k for k, v in self.datamodule.class_to_idx.items()}\n\n    # Get best model!\n    if self.best_model_path is not None:\n        log.info(\"Saving deployment model for %s checkpoint\", self.best_model_path)\n\n        module = self.module.__class__.load_from_checkpoint(\n            self.best_model_path,\n            model=self.module.model,\n            optimizer=self.optimizer,\n            lr_scheduler=self.scheduler,\n            criterion=self.module.criterion,\n            gradcam=False,\n        )\n    else:\n        log.warning(\"No checkpoint callback found in the trainer, exporting the last model weights\")\n        module = self.module\n\n    input_shapes = self.config.export.input_shapes\n\n    # TODO: What happens if we have 64 precision?\n    half_precision = \"16\" in self.trainer.precision\n\n    self.model_json, export_paths = export_model(\n        config=self.config,\n        model=module.model,\n        export_folder=self.export_folder,\n        half_precision=half_precision,\n        input_shapes=input_shapes,\n        idx_to_class=idx_to_class,\n    )\n\n    if len(export_paths) == 0:\n        return\n\n    with open(os.path.join(self.export_folder, self.deploy_info_file), \"w\") as f:\n        json.dump(self.model_json, f)\n</code></pre>"},{"location":"reference/quadra/tasks/classification.html#quadra.tasks.classification.Classification.freeze_layers_by_name","title":"<code>freeze_layers_by_name(freeze_parameters_name)</code>","text":"<p>Freeze layers specified in freeze_parameters_name.</p> <p>Parameters:</p> <ul> <li> freeze_parameters_name             (<code>list[str]</code>)         \u2013          <p>Layers that will be frozen during training.</p> </li> </ul> Source code in <code>quadra/tasks/classification.py</code> <pre><code>def freeze_layers_by_name(self, freeze_parameters_name: list[str]):\n\"\"\"Freeze layers specified in freeze_parameters_name.\n\n    Args:\n        freeze_parameters_name: Layers that will be frozen during training.\n\n    \"\"\"\n    count_frozen = 0\n    for name, param in self.model.named_parameters():\n        if any(x in name.split(\".\")[1] for x in freeze_parameters_name):\n            log.debug(\"Freezing layer %s\", name)\n            param.requires_grad = False\n\n        if not param.requires_grad:\n            count_frozen += 1\n\n    log.info(\"Frozen %d parameters\", count_frozen)\n</code></pre>"},{"location":"reference/quadra/tasks/classification.html#quadra.tasks.classification.Classification.freeze_parameters_by_index","title":"<code>freeze_parameters_by_index(freeze_parameters_index)</code>","text":"<p>Freeze parameters specified in freeze_parameters_name.</p> <p>Parameters:</p> <ul> <li> freeze_parameters_index             (<code>list[int]</code>)         \u2013          <p>Indices of parameters that will be frozen during training.</p> </li> </ul> Source code in <code>quadra/tasks/classification.py</code> <pre><code>def freeze_parameters_by_index(self, freeze_parameters_index: list[int]):\n\"\"\"Freeze parameters specified in freeze_parameters_name.\n\n    Args:\n        freeze_parameters_index: Indices of parameters that will be frozen during training.\n\n    \"\"\"\n    if getattr(self.config.backbone, \"freeze_parameters_name\", None) is not None:\n        log.warning(\n            \"Please be aware that some of the model's parameters have already been frozen using \\\n            the specified freeze_parameters_name. You are combining these two actions.\"\n        )\n    count_frozen = 0\n    for i, (name, param) in enumerate(self.model.named_parameters()):\n        if i in freeze_parameters_index:\n            log.debug(\"Freezing layer %s\", name)\n            param.requires_grad = False\n\n        if not param.requires_grad:\n            count_frozen += 1\n\n    log.info(\"Frozen %d parameters\", count_frozen)\n</code></pre>"},{"location":"reference/quadra/tasks/classification.html#quadra.tasks.classification.Classification.generate_report","title":"<code>generate_report()</code>","text":"<p>Generate a report for the task.</p> Source code in <code>quadra/tasks/classification.py</code> <pre><code>def generate_report(self) -&gt; None:\n\"\"\"Generate a report for the task.\"\"\"\n    if self.datamodule.class_to_idx is None:\n        log.warning(\"No `class_to_idx` found in the datamodule, report will not be generated\")\n        return\n\n    if isinstance(self.datamodule, MultilabelClassificationDataModule):\n        log.warning(\"Report generation is not supported for multilabel classification tasks at the moment.\")\n        return\n\n    log.info(\"Generating report!\")\n    if not self.run_test or self.config.trainer.get(\"fast_dev_run\"):\n        self.datamodule.setup(stage=\"test\")\n\n    # Deepcopy to remove the inference mode from gradients causing issues when loading checkpoints\n    # TODO: Why deepcopy of module model removes ModelSignatureWrapper?\n    self.module.model.instance = deepcopy(self.module.model.instance)\n    if \"16\" in self.trainer.precision:\n        log.warning(\"Gradcam is currently not supported with half precision, it will be disabled\")\n        self.module.gradcam = False\n        self.gradcam = False\n\n    predictions_outputs = self.trainer.predict(\n        model=self.module, datamodule=self.datamodule, ckpt_path=self.best_model_path\n    )\n    if not predictions_outputs:\n        log.warning(\"There is no prediction to generate the report. Skipping report generation.\")\n        return\n    all_outputs = [x[0] for x in predictions_outputs]\n    all_probs = [x[2] for x in predictions_outputs]\n    if not all_outputs or not all_probs:\n        log.warning(\"There is no prediction to generate the report. Skipping report generation.\")\n        return\n    all_outputs = [item for sublist in all_outputs for item in sublist]\n    all_probs = [item for sublist in all_probs for item in sublist]\n    all_targets = [target.tolist() for im, target in self.datamodule.test_dataloader()]\n    all_targets = [item for sublist in all_targets for item in sublist]\n\n    if self.module.gradcam:\n        grayscale_cams = [x[1] for x in predictions_outputs]\n        grayscale_cams = [item for sublist in grayscale_cams for item in sublist]\n        grayscale_cams = np.stack(grayscale_cams)  # N x H x W\n    else:\n        grayscale_cams = None\n\n    # creating confusion matrix\n    idx_to_class = {v: k for k, v in self.datamodule.class_to_idx.items()}\n    _, self.report_confmat, accuracy = get_results(\n        test_labels=all_targets,\n        pred_labels=all_outputs,\n        idx_to_labels=idx_to_class,\n    )\n    output_folder_test = \"test\"\n    test_dataloader = self.datamodule.test_dataloader()\n    test_dataset = cast(ImageClassificationListDataset, test_dataloader.dataset)\n    self.res = pd.DataFrame(\n        {\n            \"sample\": list(test_dataset.x),\n            \"real_label\": all_targets,\n            \"pred_label\": all_outputs,\n            \"probability\": all_probs,\n        }\n    )\n    os.makedirs(output_folder_test, exist_ok=True)\n    save_classification_result(\n        results=self.res,\n        output_folder=output_folder_test,\n        confmat=self.report_confmat,\n        accuracy=accuracy,\n        test_dataloader=self.datamodule.test_dataloader(),\n        config=self.config,\n        output=self.output,\n        grayscale_cams=grayscale_cams,\n    )\n\n    if len(self.logger) &gt; 0:\n        mflow_logger = get_mlflow_logger(trainer=self.trainer)\n        tensorboard_logger = utils.get_tensorboard_logger(trainer=self.trainer)\n        artifacts = glob.glob(os.path.join(output_folder_test, \"**/*\"), recursive=True)\n        if self.config.core.get(\"upload_artifacts\") and len(artifacts) &gt; 0:\n            if mflow_logger is not None:\n                log.info(\"Uploading artifacts to MLFlow\")\n                for a in artifacts:\n                    if os.path.isdir(a):\n                        continue\n\n                    dirname = Path(a).parent.name\n                    mflow_logger.experiment.log_artifact(\n                        run_id=mflow_logger.run_id,\n                        local_path=a,\n                        artifact_path=os.path.join(\"classification_output\", dirname),\n                    )\n            if tensorboard_logger is not None:\n                log.info(\"Uploading artifacts to Tensorboard\")\n                for a in artifacts:\n                    if os.path.isdir(a):\n                        continue\n\n                    ext = os.path.splitext(a)[1].lower()\n\n                    if ext in [\".png\", \".jpg\", \".jpeg\", \".bmp\", \".tiff\", \".tif\", \".gif\"]:\n                        try:\n                            img = cv2.imread(a)\n                            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n                        except cv2.error:\n                            log.info(\"Could not upload artifact image %s\", a)\n                            continue\n                        output_path = os.path.sep.join(a.split(os.path.sep)[-2:])\n                        tensorboard_logger.experiment.add_image(output_path, img, 0, dataformats=\"HWC\")\n                    else:\n                        utils.upload_file_tensorboard(a, tensorboard_logger)\n</code></pre>"},{"location":"reference/quadra/tasks/classification.html#quadra.tasks.classification.Classification.module","title":"<code>module(module_config)</code>","text":"<p>Set the module of the model.</p> Source code in <code>quadra/tasks/classification.py</code> <pre><code>@LightningTask.module.setter\ndef module(self, module_config):  # noqa: F811\n\"\"\"Set the module of the model.\"\"\"\n    module = hydra.utils.instantiate(\n        module_config,\n        model=self.model,\n        optimizer=self.optimizer,\n        lr_scheduler=self.scheduler,\n        gradcam=self.gradcam,\n    )\n    if self.checkpoint_path is not None:\n        log.info(\"Loading model from lightning checkpoint: %s\", self.checkpoint_path)\n        module = module.__class__.load_from_checkpoint(\n            self.checkpoint_path,\n            model=self.model,\n            optimizer=self.optimizer,\n            lr_scheduler=self.scheduler,\n            criterion=module.criterion,\n            gradcam=self.gradcam,\n        )\n    self._module = module\n</code></pre>"},{"location":"reference/quadra/tasks/classification.html#quadra.tasks.classification.Classification.prepare","title":"<code>prepare()</code>","text":"<p>Prepare the experiment.</p> Source code in <code>quadra/tasks/classification.py</code> <pre><code>def prepare(self) -&gt; None:\n\"\"\"Prepare the experiment.\"\"\"\n    super().prepare()\n    self.model = self.config.model\n    self.optimizer = self.config.optimizer\n    self.scheduler = self.config.scheduler\n    self.module = self.config.model.module\n</code></pre>"},{"location":"reference/quadra/tasks/classification.html#quadra.tasks.classification.Classification.test","title":"<code>test()</code>","text":"<p>Test the model.</p> Source code in <code>quadra/tasks/classification.py</code> <pre><code>def test(self) -&gt; None:\n\"\"\"Test the model.\"\"\"\n    if not self.config.trainer.get(\"fast_dev_run\"):\n        log.info(\"Starting testing!\")\n        self.trainer.test(datamodule=self.datamodule, model=self.module, ckpt_path=self.best_model_path)\n</code></pre>"},{"location":"reference/quadra/tasks/classification.html#quadra.tasks.classification.Classification.train","title":"<code>train()</code>","text":"<p>Train the model.</p> Source code in <code>quadra/tasks/classification.py</code> <pre><code>def train(self):\n\"\"\"Train the model.\"\"\"\n    super().train()\n    if (\n        self.trainer.checkpoint_callback is not None\n        and hasattr(self.trainer.checkpoint_callback, \"best_model_path\")\n        and self.trainer.checkpoint_callback.best_model_path is not None\n        and len(self.trainer.checkpoint_callback.best_model_path) &gt; 0\n    ):\n        self.best_model_path = self.trainer.checkpoint_callback.best_model_path\n        log.info(\"Loading best epoch weights...\")\n</code></pre>"},{"location":"reference/quadra/tasks/classification.html#quadra.tasks.classification.ClassificationEvaluation","title":"<code>ClassificationEvaluation(config, output, model_path, report=True, gradcam=False, device=None)</code>","text":"<p>             Bases: <code>Evaluation[ClassificationDataModuleT]</code></p> <p>Perform a test on an imported Classification pytorch model.</p> <p>Parameters:</p> <ul> <li> config             (<code>DictConfig</code>)         \u2013          <p>Task configuration</p> </li> <li> output             (<code>DictConfig</code>)         \u2013          <p>Configuration for the output</p> </li> <li> model_path             (<code>str</code>)         \u2013          <p>Path to pytorch .pt model file</p> </li> <li> report             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Whether to generate the report of the predictions</p> </li> <li> gradcam             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to compute gradcams</p> </li> <li> device             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>Device to use for evaluation. If None, the device is automatically determined</p> </li> </ul> Source code in <code>quadra/tasks/classification.py</code> <pre><code>def __init__(\n    self,\n    config: DictConfig,\n    output: DictConfig,\n    model_path: str,\n    report: bool = True,\n    gradcam: bool = False,\n    device: str | None = None,\n):\n    super().__init__(config=config, model_path=model_path, device=device)\n    self.report_path = \"test_output\"\n    self.output = output\n    self.report = report\n    self.gradcam = gradcam\n    self.cam: GradCAM\n</code></pre>"},{"location":"reference/quadra/tasks/classification.html#quadra.tasks.classification.ClassificationEvaluation.deployment_model","title":"<code>deployment_model: BaseEvaluationModel</code>  <code>property</code> <code>writable</code>","text":"<p>Deployment model.</p>"},{"location":"reference/quadra/tasks/classification.html#quadra.tasks.classification.ClassificationEvaluation.execute","title":"<code>execute()</code>","text":"<p>Execute the evaluation.</p> Source code in <code>quadra/tasks/classification.py</code> <pre><code>def execute(self) -&gt; None:\n\"\"\"Execute the evaluation.\"\"\"\n    self.prepare()\n    self.test()\n    if self.report:\n        self.generate_report()\n    self.finalize()\n</code></pre>"},{"location":"reference/quadra/tasks/classification.html#quadra.tasks.classification.ClassificationEvaluation.generate_report","title":"<code>generate_report()</code>","text":"<p>Generate a report for the task.</p> Source code in <code>quadra/tasks/classification.py</code> <pre><code>def generate_report(self) -&gt; None:\n\"\"\"Generate a report for the task.\"\"\"\n    log.info(\"Generating report!\")\n    os.makedirs(self.report_path, exist_ok=True)\n\n    save_classification_result(\n        results=self.metadata[\"test_results\"],\n        output_folder=self.report_path,\n        confmat=self.metadata[\"test_confusion_matrix\"],\n        accuracy=self.metadata[\"test_accuracy\"],\n        test_dataloader=self.datamodule.test_dataloader(),\n        config=self.config,\n        output=self.output,\n        grayscale_cams=self.metadata[\"grayscale_cams\"],\n    )\n</code></pre>"},{"location":"reference/quadra/tasks/classification.html#quadra.tasks.classification.ClassificationEvaluation.get_classifier","title":"<code>get_classifier(model_config)</code>","text":"<p>Instantiate the classifier from the config.</p> Source code in <code>quadra/tasks/classification.py</code> <pre><code>def get_classifier(self, model_config: DictConfig) -&gt; nn.Module:\n\"\"\"Instantiate the classifier from the config.\"\"\"\n    if \"classifier\" in model_config:\n        log.info(\"Instantiating classifier &lt;%s&gt;\", model_config.classifier[\"_target_\"])\n        return hydra.utils.instantiate(\n            model_config.classifier, out_features=len(self.model_data[\"classes\"]), _convert_=\"partial\"\n        )\n\n    raise ValueError(\"A `classifier` definition must be specified in the config\")\n</code></pre>"},{"location":"reference/quadra/tasks/classification.html#quadra.tasks.classification.ClassificationEvaluation.get_pre_classifier","title":"<code>get_pre_classifier(model_config)</code>","text":"<p>Instantiate the pre-classifier from the config.</p> Source code in <code>quadra/tasks/classification.py</code> <pre><code>def get_pre_classifier(self, model_config: DictConfig) -&gt; nn.Module:\n\"\"\"Instantiate the pre-classifier from the config.\"\"\"\n    if \"pre_classifier\" in model_config and model_config.pre_classifier is not None:\n        log.info(\"Instantiating pre_classifier &lt;%s&gt;\", model_config.pre_classifier[\"_target_\"])\n        pre_classifier = hydra.utils.instantiate(model_config.pre_classifier, _convert_=\"partial\")\n    else:\n        log.info(\"No pre-classifier found in config: instantiate a torch.nn.Identity instead\")\n        pre_classifier = nn.Identity()\n\n    return pre_classifier\n</code></pre>"},{"location":"reference/quadra/tasks/classification.html#quadra.tasks.classification.ClassificationEvaluation.get_torch_model","title":"<code>get_torch_model(model_config)</code>","text":"<p>Instantiate the torch model from the config.</p> Source code in <code>quadra/tasks/classification.py</code> <pre><code>def get_torch_model(self, model_config: DictConfig) -&gt; nn.Module:\n\"\"\"Instantiate the torch model from the config.\"\"\"\n    pre_classifier = self.get_pre_classifier(model_config)\n    classifier = self.get_classifier(model_config)\n    log.info(\"Instantiating backbone &lt;%s&gt;\", model_config.model[\"_target_\"])\n\n    return hydra.utils.instantiate(\n        model_config.model, classifier=classifier, pre_classifier=pre_classifier, _convert_=\"partial\"\n    )\n</code></pre>"},{"location":"reference/quadra/tasks/classification.html#quadra.tasks.classification.ClassificationEvaluation.prepare","title":"<code>prepare()</code>","text":"<p>Prepare the evaluation.</p> Source code in <code>quadra/tasks/classification.py</code> <pre><code>def prepare(self) -&gt; None:\n\"\"\"Prepare the evaluation.\"\"\"\n    super().prepare()\n    self.datamodule = self.config.datamodule\n    self.datamodule.class_to_idx = {v: int(k) for k, v in self.model_data[\"classes\"].items()}\n    self.datamodule.num_classes = len(self.datamodule.class_to_idx)\n\n    # prepare_data() must be explicitly called because there is no training\n    self.datamodule.prepare_data()\n    self.datamodule.setup(stage=\"test\")\n</code></pre>"},{"location":"reference/quadra/tasks/classification.html#quadra.tasks.classification.ClassificationEvaluation.prepare_gradcam","title":"<code>prepare_gradcam()</code>","text":"<p>Initializing gradcam for the predictions.</p> Source code in <code>quadra/tasks/classification.py</code> <pre><code>def prepare_gradcam(self) -&gt; None:\n\"\"\"Initializing gradcam for the predictions.\"\"\"\n    if not hasattr(self.deployment_model.model, \"features_extractor\"):\n        log.warning(\"Gradcam not implemented for this backbone, it will not be computed\")\n        self.gradcam = False\n        return\n\n    if isinstance(self.deployment_model.model.features_extractor, timm.models.resnet.ResNet):\n        target_layers = [cast(BaseNetworkBuilder, self.deployment_model.model).features_extractor.layer4[-1]]\n        self.cam = GradCAM(\n            model=self.deployment_model.model,\n            target_layers=target_layers,\n        )\n        for p in self.deployment_model.model.features_extractor.layer4[-1].parameters():\n            p.requires_grad = True\n    elif is_vision_transformer(cast(BaseNetworkBuilder, self.deployment_model.model).features_extractor):\n        self.grad_rollout = VitAttentionGradRollout(cast(nn.Module, self.deployment_model.model))\n    else:\n        log.warning(\"Gradcam not implemented for this backbone, it will not be computed\")\n        self.gradcam = False\n</code></pre>"},{"location":"reference/quadra/tasks/classification.html#quadra.tasks.classification.ClassificationEvaluation.test","title":"<code>test()</code>","text":"<p>Perform test.</p> Source code in <code>quadra/tasks/classification.py</code> <pre><code>@automatic_datamodule_batch_size(batch_size_attribute_name=\"batch_size\")\ndef test(self) -&gt; None:\n\"\"\"Perform test.\"\"\"\n    log.info(\"Running test\")\n    test_dataloader = self.datamodule.test_dataloader()\n\n    image_labels = []\n    probabilities = []\n    predicted_classes = []\n    grayscale_cams_list = []\n\n    if self.gradcam:\n        self.prepare_gradcam()\n\n    with torch.set_grad_enabled(self.gradcam):\n        for batch_item in tqdm(test_dataloader):\n            im, target = batch_item\n            im = im.to(device=self.device, dtype=self.deployment_model.model_dtype).detach()\n\n            if self.gradcam:\n                # When gradcam is used we need to remove gradients\n                outputs = self.deployment_model(im).detach()\n            else:\n                outputs = self.deployment_model(im)\n\n            probs = torch.softmax(outputs, dim=1)\n            preds = torch.max(probs, dim=1).indices\n\n            probabilities.append(probs.tolist())\n            predicted_classes.append(preds.tolist())\n            image_labels.extend(target.tolist())\n            if self.gradcam and hasattr(self.deployment_model.model, \"features_extractor\"):\n                with torch.inference_mode(False):\n                    im = im.clone()\n                    if isinstance(self.deployment_model.model.features_extractor, timm.models.resnet.ResNet):\n                        grayscale_cam = self.cam(input_tensor=im, targets=None)\n                        grayscale_cams_list.append(torch.from_numpy(grayscale_cam))\n                    elif is_vision_transformer(\n                        cast(BaseNetworkBuilder, self.deployment_model.model).features_extractor\n                    ):\n                        grayscale_cam_low_res = self.grad_rollout(input_tensor=im, targets_list=preds.tolist())\n                        orig_shape = grayscale_cam_low_res.shape\n                        new_shape = (orig_shape[0], im.shape[2], im.shape[3])\n                        zoom_factors = tuple(np.array(new_shape) / np.array(orig_shape))\n                        grayscale_cam = ndimage.zoom(grayscale_cam_low_res, zoom_factors, order=1)\n                        grayscale_cams_list.append(torch.from_numpy(grayscale_cam))\n\n    grayscale_cams: torch.Tensor | None = None\n    if self.gradcam:\n        grayscale_cams = torch.cat(grayscale_cams_list, dim=0)\n\n    predicted_classes = [item for sublist in predicted_classes for item in sublist]\n    probabilities = [max(item) for sublist in probabilities for item in sublist]\n    if self.datamodule.class_to_idx is not None:\n        idx_to_class = {v: k for k, v in self.datamodule.class_to_idx.items()}\n\n    _, pd_cm, test_accuracy = get_results(\n        test_labels=image_labels,\n        pred_labels=predicted_classes,\n        idx_to_labels=idx_to_class,\n    )\n\n    res = pd.DataFrame(\n        {\n            \"sample\": list(test_dataloader.dataset.x),  # type: ignore[attr-defined]\n            \"real_label\": image_labels,\n            \"pred_label\": predicted_classes,\n            \"probability\": probabilities,\n        }\n    )\n\n    log.info(\"Avg classification accuracy: %s\", test_accuracy)\n\n    self.res = pd.DataFrame(\n        {\n            \"sample\": list(test_dataloader.dataset.x),  # type: ignore[attr-defined]\n            \"real_label\": image_labels,\n            \"pred_label\": predicted_classes,\n            \"probability\": probabilities,\n        }\n    )\n\n    # save results\n    self.metadata[\"test_confusion_matrix\"] = pd_cm\n    self.metadata[\"test_accuracy\"] = test_accuracy\n    self.metadata[\"predictions\"] = predicted_classes\n    self.metadata[\"test_results\"] = res\n    self.metadata[\"probabilities\"] = probabilities\n    self.metadata[\"test_labels\"] = image_labels\n    self.metadata[\"grayscale_cams\"] = grayscale_cams\n</code></pre>"},{"location":"reference/quadra/tasks/classification.html#quadra.tasks.classification.SklearnClassification","title":"<code>SklearnClassification(config, output, device, automatic_batch_size, save_model_summary=False, half_precision=False, gradcam=False)</code>","text":"<p>             Bases: <code>Generic[SklearnClassificationDataModuleT]</code>, <code>Task[SklearnClassificationDataModuleT]</code></p> <p>Sklearn classification task.</p> <p>Parameters:</p> <ul> <li> config             (<code>DictConfig</code>)         \u2013          <p>The experiment configuration</p> </li> <li> device             (<code>str</code>)         \u2013          <p>The device to use. Defaults to None.</p> </li> <li> output             (<code>DictConfig</code>)         \u2013          <p>Dictionary defining which kind of outputs to generate. Defaults to None.</p> </li> <li> automatic_batch_size             (<code>DictConfig</code>)         \u2013          <p>Whether to automatically find the largest batch size that fits in memory.</p> </li> <li> save_model_summary             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to save a model_summary.txt file containing the model summary.</p> </li> <li> half_precision             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to use half precision during training.</p> </li> <li> gradcam             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to compute gradcams for test results.</p> </li> </ul> Source code in <code>quadra/tasks/classification.py</code> <pre><code>def __init__(\n    self,\n    config: DictConfig,\n    output: DictConfig,\n    device: str,\n    automatic_batch_size: DictConfig,\n    save_model_summary: bool = False,\n    half_precision: bool = False,\n    gradcam: bool = False,\n):\n    super().__init__(config=config)\n\n    self._device = device\n    self.output = output\n    self._backbone: ModelSignatureWrapper\n    self._trainer: SklearnClassificationTrainer\n    self._model: ClassifierMixin\n    self.metadata: dict[str, Any] = {\n        \"test_confusion_matrix\": [],\n        \"test_accuracy\": [],\n        \"test_results\": [],\n        \"test_labels\": [],\n        \"cams\": [],\n    }\n    self.export_folder = \"deployment_model\"\n    self.deploy_info_file = \"model.json\"\n    self.train_dataloader_list: list[torch.utils.data.DataLoader] = []\n    self.test_dataloader_list: list[torch.utils.data.DataLoader] = []\n    self.automatic_batch_size = automatic_batch_size\n    self.save_model_summary = save_model_summary\n    self.half_precision = half_precision\n    self.gradcam = gradcam\n</code></pre>"},{"location":"reference/quadra/tasks/classification.html#quadra.tasks.classification.SklearnClassification.backbone","title":"<code>backbone: ModelSignatureWrapper</code>  <code>property</code> <code>writable</code>","text":""},{"location":"reference/quadra/tasks/classification.html#quadra.tasks.classification.SklearnClassification.model","title":"<code>model: ClassifierMixin</code>  <code>property</code> <code>writable</code>","text":"<p>sklearn.base.ClassifierMixin: The model.</p>"},{"location":"reference/quadra/tasks/classification.html#quadra.tasks.classification.SklearnClassification.trainer","title":"<code>trainer: SklearnClassificationTrainer</code>  <code>property</code> <code>writable</code>","text":""},{"location":"reference/quadra/tasks/classification.html#quadra.tasks.classification.SklearnClassification.execute","title":"<code>execute()</code>","text":"<p>Execute the experiment and all the steps.</p> Source code in <code>quadra/tasks/classification.py</code> <pre><code>def execute(self) -&gt; None:\n\"\"\"Execute the experiment and all the steps.\"\"\"\n    self.prepare()\n    self.train()\n    if self.output.report:\n        self.generate_report()\n    self.train_full_data()\n    if self.config.export is not None and len(self.config.export.types) &gt; 0:\n        self.export()\n    if self.output.test_full_data:\n        self.test_full_data()\n    self.finalize()\n</code></pre>"},{"location":"reference/quadra/tasks/classification.html#quadra.tasks.classification.SklearnClassification.export","title":"<code>export()</code>","text":"<p>Generate deployment model for the task.</p> Source code in <code>quadra/tasks/classification.py</code> <pre><code>def export(self) -&gt; None:\n\"\"\"Generate deployment model for the task.\"\"\"\n    if self.config.export is None or len(self.config.export.types) == 0:\n        log.info(\"No export type specified skipping export\")\n        return\n\n    input_shapes = self.config.export.input_shapes\n\n    idx_to_class = {v: k for k, v in self.datamodule.full_dataset.class_to_idx.items()}\n\n    model_json, export_paths = export_model(\n        config=self.config,\n        model=self.backbone,\n        export_folder=self.export_folder,\n        half_precision=self.half_precision,\n        input_shapes=input_shapes,\n        idx_to_class=idx_to_class,\n        pytorch_model_type=\"backbone\",\n    )\n\n    dump(self.model, os.path.join(self.export_folder, \"classifier.joblib\"))\n\n    if len(export_paths) &gt; 0:\n        with open(os.path.join(self.export_folder, self.deploy_info_file), \"w\") as f:\n            json.dump(model_json, f)\n</code></pre>"},{"location":"reference/quadra/tasks/classification.html#quadra.tasks.classification.SklearnClassification.extract_model_summary","title":"<code>extract_model_summary(feature_extractor, dl)</code>","text":"<p>Given a dataloader and a PyTorch model, use torchinfo to extract a summary of the model and save it to a file.</p> <p>Parameters:</p> <ul> <li> dl             (<code>DataLoader</code>)         \u2013          <p>PyTorch dataloader</p> </li> <li> feature_extractor             (<code>Module | BaseEvaluationModel</code>)         \u2013          <p>PyTorch backbone</p> </li> </ul> Source code in <code>quadra/tasks/classification.py</code> <pre><code>def extract_model_summary(\n    self, feature_extractor: torch.nn.Module | BaseEvaluationModel, dl: torch.utils.data.DataLoader\n) -&gt; None:\n\"\"\"Given a dataloader and a PyTorch model, use torchinfo to extract a summary of the model and save it\n    to a file.\n\n    Args:\n        dl: PyTorch dataloader\n        feature_extractor: PyTorch backbone\n    \"\"\"\n    if isinstance(feature_extractor, (TorchEvaluationModel, TorchscriptEvaluationModel)):\n        # TODO: I'm not sure torchinfo supports torchscript models\n        # If we are working with torch based evaluation models we need to extract the model\n        feature_extractor = feature_extractor.model\n\n    for b in tqdm(dl):\n        x1, _ = b\n\n        if hasattr(feature_extractor, \"parameters\"):\n            # Move input to the correct device\n            parameter = next(feature_extractor.parameters())\n            x1 = x1.to(parameter.device).to(parameter.dtype)\n            x1 = x1[0].unsqueeze(0)  # Remove batch dimension\n\n            model_info = None\n\n            try:\n                try:\n                    # TODO: Do we want to print the summary to the console as well?\n                    model_info = summary(feature_extractor, input_data=(x1), verbose=0)  # type: ignore[arg-type]\n                except Exception:\n                    log.warning(\n                        \"Failed to retrieve model summary using input data information, retrieving only \"\n                        \"parameters information\"\n                    )\n                    model_info = summary(feature_extractor, verbose=0)  # type: ignore[arg-type]\n            except Exception as e:\n                # If for some reason the summary fails we don't want to stop the training\n                log.warning(\"Failed to retrieve model summary: %s\", e)\n\n            if model_info is not None:\n                with open(\"model_summary.txt\", \"w\") as f:\n                    f.write(str(model_info))\n        else:\n            log.warning(\"Failed to retrieve model summary, current model has no parameters\")\n\n        break\n</code></pre>"},{"location":"reference/quadra/tasks/classification.html#quadra.tasks.classification.SklearnClassification.generate_report","title":"<code>generate_report()</code>","text":"<p>Generate report for the task.</p> Source code in <code>quadra/tasks/classification.py</code> <pre><code>def generate_report(self) -&gt; None:\n\"\"\"Generate report for the task.\"\"\"\n    log.info(\"Generating report!\")\n\n    cm_list = []\n\n    for count in range(len(self.metadata[\"test_accuracy\"])):\n        current_output_folder = f\"{self.output.folder}_{count}\"\n        os.makedirs(current_output_folder, exist_ok=True)\n\n        c_matrix = self.metadata[\"test_confusion_matrix\"][count]\n        cm_list.append(c_matrix)\n        save_classification_result(\n            results=self.metadata[\"test_results\"][count],\n            output_folder=current_output_folder,\n            confmat=c_matrix,\n            accuracy=self.metadata[\"test_accuracy\"][count],\n            test_dataloader=self.test_dataloader_list[count],\n            config=self.config,\n            output=self.output,\n            grayscale_cams=self.metadata[\"cams\"][count],\n        )\n    final_confusion_matrix = sum(cm_list)\n\n    self.metadata[\"final_confusion_matrix\"] = final_confusion_matrix\n    # Save final conf matrix\n    final_folder = f\"{self.output.folder}\"\n    os.makedirs(final_folder, exist_ok=True)\n    disp = ConfusionMatrixDisplay(\n        confusion_matrix=np.array(final_confusion_matrix),\n        display_labels=[x.replace(\"pred:\", \"\") for x in final_confusion_matrix.columns.to_list()],\n    )\n    disp.plot(include_values=True, cmap=plt.cm.Greens, ax=None, colorbar=False, xticks_rotation=90)\n    plt.title(f\"Confusion Matrix (Accuracy: {(self.metadata['test_accuracy'][count] * 100):.2f}%)\")\n    plt.savefig(os.path.join(final_folder, \"test_confusion_matrix.png\"), bbox_inches=\"tight\", pad_inches=0, dpi=300)\n    plt.close()\n</code></pre>"},{"location":"reference/quadra/tasks/classification.html#quadra.tasks.classification.SklearnClassification.prepare","title":"<code>prepare()</code>","text":"<p>Prepare the experiment.</p> Source code in <code>quadra/tasks/classification.py</code> <pre><code>def prepare(self) -&gt; None:\n\"\"\"Prepare the experiment.\"\"\"\n    self.datamodule = self.config.datamodule\n\n    self.backbone = self.config.backbone\n\n    self.model = self.config.model\n\n    # prepare_data() must be explicitly called if the task does not include a lightining training\n    self.datamodule.prepare_data()\n    self.datamodule.setup(stage=\"fit\")\n\n    self.trainer = self.config.trainer\n</code></pre>"},{"location":"reference/quadra/tasks/classification.html#quadra.tasks.classification.SklearnClassification.test","title":"<code>test()</code>","text":"<p>Skip test phase.</p> Source code in <code>quadra/tasks/classification.py</code> <pre><code>def test(self) -&gt; None:\n\"\"\"Skip test phase.\"\"\"\n</code></pre>"},{"location":"reference/quadra/tasks/classification.html#quadra.tasks.classification.SklearnClassification.test_full_data","title":"<code>test_full_data()</code>","text":"<p>Test model trained on full dataset.</p> Source code in <code>quadra/tasks/classification.py</code> <pre><code>@typing.no_type_check\n@automatic_datamodule_batch_size(batch_size_attribute_name=\"batch_size\")\ndef test_full_data(self) -&gt; None:\n\"\"\"Test model trained on full dataset.\"\"\"\n    self.config.datamodule.class_to_idx = self.datamodule.full_dataset.class_to_idx\n    self.config.datamodule.phase = \"test\"\n    idx_to_class = self.datamodule.full_dataset.idx_to_class\n    self.datamodule.setup(\"test\")\n    test_dataloader = self.datamodule.test_dataloader()\n\n    if len(self.datamodule.data[\"samples\"]) == 0:\n        log.info(\"No test data, skipping test\")\n        return\n\n    # Put backbone on the correct device as it may be moved after export\n    self.backbone.to(self.device)\n    _, pd_cm, accuracy, res, cams = self.trainer.test(\n        test_dataloader=test_dataloader, idx_to_class=idx_to_class, predict_proba=True, gradcam=self.gradcam\n    )\n\n    output_folder_test = \"test\"\n\n    os.makedirs(output_folder_test, exist_ok=True)\n\n    save_classification_result(\n        results=res,\n        output_folder=output_folder_test,\n        confmat=pd_cm,\n        accuracy=accuracy,\n        test_dataloader=test_dataloader,\n        config=self.config,\n        output=self.output,\n        grayscale_cams=cams,\n    )\n</code></pre>"},{"location":"reference/quadra/tasks/classification.html#quadra.tasks.classification.SklearnClassification.train","title":"<code>train()</code>","text":"<p>Train the model.</p> Source code in <code>quadra/tasks/classification.py</code> <pre><code>@typing.no_type_check\n@automatic_datamodule_batch_size(batch_size_attribute_name=\"batch_size\")\ndef train(self) -&gt; None:\n\"\"\"Train the model.\"\"\"\n    log.info(\"Starting training...!\")\n    all_features = None\n    all_labels = None\n\n    class_to_keep = None\n\n    self.train_dataloader_list = list(self.datamodule.train_dataloader())\n    self.test_dataloader_list = list(self.datamodule.val_dataloader())\n\n    if hasattr(self.datamodule, \"class_to_keep_training\") and self.datamodule.class_to_keep_training is not None:\n        class_to_keep = self.datamodule.class_to_keep_training\n\n    if self.save_model_summary:\n        self.extract_model_summary(feature_extractor=self.backbone, dl=self.datamodule.full_dataloader())\n\n    if hasattr(self.datamodule, \"cache\") and self.datamodule.cache:\n        if self.config.trainer.iteration_over_training != 1:\n            raise AttributeError(\"Cache is only supported when iteration over training is set to 1\")\n\n        full_dataloader = self.datamodule.full_dataloader()\n        all_features, all_labels, _ = get_feature(\n            feature_extractor=self.backbone, dl=full_dataloader, iteration_over_training=1\n        )\n\n        sorted_indices = np.argsort(full_dataloader.dataset.x)\n        all_features = all_features[sorted_indices]\n        all_labels = all_labels[sorted_indices]\n\n    # cycle over all train/test split\n    for train_dataloader, test_dataloader in zip(self.train_dataloader_list, self.test_dataloader_list):\n        # Reinit classifier\n        self.model = self.config.model\n        self.trainer.change_classifier(self.model)\n\n        # Train on current training set\n        if all_features is not None and all_labels is not None:\n            # Find which are the indices used to pass from the sorted list of string to the disordered one\n            sorted_indices = np.argsort(np.concatenate([train_dataloader.dataset.x, test_dataloader.dataset.x]))\n            revese_sorted_indices = np.argsort(sorted_indices)\n\n            # Use these indices to correctly match the extracted features with the new file order\n            all_features_sorted = all_features[revese_sorted_indices]\n            all_labels_sorted = all_labels[revese_sorted_indices]\n\n            train_len = len(train_dataloader.dataset.x)\n\n            self.trainer.fit(\n                train_features=all_features_sorted[0:train_len], train_labels=all_labels_sorted[0:train_len]\n            )\n\n            _, pd_cm, accuracy, res, cams = self.trainer.test(\n                test_dataloader=test_dataloader,\n                test_features=all_features_sorted[train_len:],\n                test_labels=all_labels_sorted[train_len:],\n                class_to_keep=class_to_keep,\n                idx_to_class=train_dataloader.dataset.idx_to_class,\n                predict_proba=True,\n                gradcam=self.gradcam,\n            )\n        else:\n            self.trainer.fit(train_dataloader=train_dataloader)\n            _, pd_cm, accuracy, res, cams = self.trainer.test(\n                test_dataloader=test_dataloader,\n                class_to_keep=class_to_keep,\n                idx_to_class=train_dataloader.dataset.idx_to_class,\n                predict_proba=True,\n                gradcam=self.gradcam,\n            )\n\n        # save results\n        self.metadata[\"test_confusion_matrix\"].append(pd_cm)\n        self.metadata[\"test_accuracy\"].append(accuracy)\n        self.metadata[\"test_results\"].append(res)\n        self.metadata[\"test_labels\"].append(\n            [\n                train_dataloader.dataset.idx_to_class[i] if i != -1 else \"N/A\"\n                for i in res[\"real_label\"].unique().tolist()\n            ]\n        )\n        self.metadata[\"cams\"].append(cams)\n</code></pre>"},{"location":"reference/quadra/tasks/classification.html#quadra.tasks.classification.SklearnClassification.train_full_data","title":"<code>train_full_data()</code>","text":"<p>Train the model on train + validation.</p> Source code in <code>quadra/tasks/classification.py</code> <pre><code>@automatic_datamodule_batch_size(batch_size_attribute_name=\"batch_size\")\ndef train_full_data(self):\n\"\"\"Train the model on train + validation.\"\"\"\n    # Reinit classifier\n    self.model = self.config.model\n    self.trainer.change_classifier(self.model)\n\n    self.trainer.fit(train_dataloader=self.datamodule.full_dataloader())\n</code></pre>"},{"location":"reference/quadra/tasks/classification.html#quadra.tasks.classification.SklearnTestClassification","title":"<code>SklearnTestClassification(config, output, model_path, device, gradcam=False, **kwargs)</code>","text":"<p>             Bases: <code>Evaluation[SklearnClassificationDataModuleT]</code></p> <p>Perform a test using an imported SklearnClassification pytorch model.</p> <p>Parameters:</p> <ul> <li> config             (<code>DictConfig</code>)         \u2013          <p>The experiment configuration</p> </li> <li> output             (<code>DictConfig</code>)         \u2013          <p>where to save results</p> </li> <li> model_path             (<code>str</code>)         \u2013          <p>path to trained model generated from SklearnClassification task.</p> </li> <li> device             (<code>str</code>)         \u2013          <p>the device where to run the model (cuda or cpu)</p> </li> <li> gradcam             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to compute gradcams</p> </li> <li> **kwargs             (<code>Any</code>, default:                 <code>{}</code> )         \u2013          <p>Additional arguments to pass to the task</p> </li> </ul> Source code in <code>quadra/tasks/classification.py</code> <pre><code>def __init__(\n    self,\n    config: DictConfig,\n    output: DictConfig,\n    model_path: str,\n    device: str,\n    gradcam: bool = False,\n    **kwargs: Any,\n):\n    super().__init__(config=config, model_path=model_path, device=device, **kwargs)\n    self.gradcam = gradcam\n    self.output = output\n    self._backbone: BaseEvaluationModel\n    self._classifier: ClassifierMixin\n    self.class_to_idx: dict[str, int]\n    self.idx_to_class: dict[int, str]\n    self.test_dataloader: torch.utils.data.DataLoader\n    self.metadata: dict[str, Any] = {\n        \"test_confusion_matrix\": None,\n        \"test_accuracy\": None,\n        \"test_results\": None,\n        \"test_labels\": None,\n        \"cams\": None,\n    }\n</code></pre>"},{"location":"reference/quadra/tasks/classification.html#quadra.tasks.classification.SklearnTestClassification.backbone","title":"<code>backbone: BaseEvaluationModel</code>  <code>property</code> <code>writable</code>","text":""},{"location":"reference/quadra/tasks/classification.html#quadra.tasks.classification.SklearnTestClassification.classifier","title":"<code>classifier: ClassifierMixin</code>  <code>property</code> <code>writable</code>","text":""},{"location":"reference/quadra/tasks/classification.html#quadra.tasks.classification.SklearnTestClassification.deployment_model","title":"<code>deployment_model</code>  <code>property</code> <code>writable</code>","text":"<p>Deployment model.</p>"},{"location":"reference/quadra/tasks/classification.html#quadra.tasks.classification.SklearnTestClassification.trainer","title":"<code>trainer: SklearnClassificationTrainer</code>  <code>property</code> <code>writable</code>","text":""},{"location":"reference/quadra/tasks/classification.html#quadra.tasks.classification.SklearnTestClassification.execute","title":"<code>execute()</code>","text":"<p>Execute the experiment and all the steps.</p> Source code in <code>quadra/tasks/classification.py</code> <pre><code>def execute(self) -&gt; None:\n\"\"\"Execute the experiment and all the steps.\"\"\"\n    self.prepare()\n    self.test()\n    if self.output.report:\n        self.generate_report()\n    self.finalize()\n</code></pre>"},{"location":"reference/quadra/tasks/classification.html#quadra.tasks.classification.SklearnTestClassification.generate_report","title":"<code>generate_report()</code>","text":"<p>Generate a report for the task.</p> Source code in <code>quadra/tasks/classification.py</code> <pre><code>def generate_report(self) -&gt; None:\n\"\"\"Generate a report for the task.\"\"\"\n    log.info(\"Generating report!\")\n    os.makedirs(self.output.folder, exist_ok=True)\n    save_classification_result(\n        results=self.metadata[\"test_results\"],\n        output_folder=self.output.folder,\n        confmat=self.metadata[\"test_confusion_matrix\"],\n        accuracy=self.metadata[\"test_accuracy\"],\n        test_dataloader=self.test_dataloader,\n        config=self.config,\n        output=self.output,\n        grayscale_cams=self.metadata[\"cams\"],\n    )\n</code></pre>"},{"location":"reference/quadra/tasks/classification.html#quadra.tasks.classification.SklearnTestClassification.prepare","title":"<code>prepare()</code>","text":"<p>Prepare the experiment.</p> Source code in <code>quadra/tasks/classification.py</code> <pre><code>def prepare(self) -&gt; None:\n\"\"\"Prepare the experiment.\"\"\"\n    super().prepare()\n\n    idx_to_class = {}\n    class_to_idx = {}\n    for k, v in self.model_data[\"classes\"].items():\n        idx_to_class[int(k)] = v\n        class_to_idx[v] = int(k)\n\n    self.idx_to_class = idx_to_class\n    self.class_to_idx = class_to_idx\n\n    self.config.datamodule.class_to_idx = class_to_idx\n\n    self.datamodule = self.config.datamodule\n    # prepare_data() must be explicitly called because there is no lightning training\n    self.datamodule.prepare_data()\n    self.datamodule.setup(stage=\"test\")\n\n    # Configure trainer\n    self.trainer = self.config.trainer\n</code></pre>"},{"location":"reference/quadra/tasks/classification.html#quadra.tasks.classification.SklearnTestClassification.test","title":"<code>test()</code>","text":"<p>Run the test.</p> Source code in <code>quadra/tasks/classification.py</code> <pre><code>@automatic_datamodule_batch_size(batch_size_attribute_name=\"batch_size\")\ndef test(self) -&gt; None:\n\"\"\"Run the test.\"\"\"\n    self.test_dataloader = self.datamodule.test_dataloader()\n\n    _, pd_cm, accuracy, res, cams = self.trainer.test(\n        test_dataloader=self.test_dataloader,\n        idx_to_class=self.idx_to_class,\n        predict_proba=True,\n        gradcam=self.gradcam,\n    )\n\n    # save results\n    self.metadata[\"test_confusion_matrix\"] = pd_cm\n    self.metadata[\"test_accuracy\"] = accuracy\n    self.metadata[\"test_results\"] = res\n    self.metadata[\"test_labels\"] = [\n        self.idx_to_class[i] if i != -1 else \"N/A\" for i in res[\"real_label\"].unique().tolist()\n    ]\n    self.metadata[\"cams\"] = cams\n</code></pre>"},{"location":"reference/quadra/tasks/patch.html","title":"patch","text":""},{"location":"reference/quadra/tasks/patch.html#quadra.tasks.patch.PatchSklearnClassification","title":"<code>PatchSklearnClassification(config, output, device, automatic_batch_size, half_precision=False)</code>","text":"<p>             Bases: <code>Task[PatchSklearnClassificationDataModule]</code></p> <p>Patch classification using torch backbone for feature extraction and sklearn to learn a linear classifier.</p> <p>Parameters:</p> <ul> <li> config             (<code>DictConfig</code>)         \u2013          <p>The experiment configuration</p> </li> <li> device             (<code>str</code>)         \u2013          <p>The device to use</p> </li> <li> output             (<code>DictConfig</code>)         \u2013          <p>Dictionary defining which kind of outputs to generate. Defaults to None.</p> </li> <li> automatic_batch_size             (<code>DictConfig</code>)         \u2013          <p>Whether to automatically find the largest batch size that fits in memory.</p> </li> <li> half_precision             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to use half precision.</p> </li> </ul> Source code in <code>quadra/tasks/patch.py</code> <pre><code>def __init__(\n    self,\n    config: DictConfig,\n    output: DictConfig,\n    device: str,\n    automatic_batch_size: DictConfig,\n    half_precision: bool = False,\n):\n    super().__init__(config=config)\n    self.device: str = device\n    self.output: DictConfig = output\n    self.return_polygon: bool = True\n    self.reconstruction_results: dict[str, Any]\n    self._backbone: ModelSignatureWrapper\n    self._trainer: SklearnClassificationTrainer\n    self._model: ClassifierMixin\n    self.metadata: dict[str, Any] = {\n        \"test_confusion_matrix\": [],\n        \"test_accuracy\": [],\n        \"test_results\": [],\n        \"test_labels\": [],\n    }\n    self.export_folder: str = \"deployment_model\"\n    self.automatic_batch_size = automatic_batch_size\n    self.half_precision = half_precision\n</code></pre>"},{"location":"reference/quadra/tasks/patch.html#quadra.tasks.patch.PatchSklearnClassification.backbone","title":"<code>backbone: ModelSignatureWrapper</code>  <code>property</code> <code>writable</code>","text":""},{"location":"reference/quadra/tasks/patch.html#quadra.tasks.patch.PatchSklearnClassification.model","title":"<code>model: ClassifierMixin</code>  <code>property</code> <code>writable</code>","text":"<p>sklearn.base.ClassifierMixin: The model.</p>"},{"location":"reference/quadra/tasks/patch.html#quadra.tasks.patch.PatchSklearnClassification.trainer","title":"<code>trainer: SklearnClassificationTrainer</code>  <code>property</code> <code>writable</code>","text":""},{"location":"reference/quadra/tasks/patch.html#quadra.tasks.patch.PatchSklearnClassification.execute","title":"<code>execute()</code>","text":"<p>Execute the experiment and all the steps.</p> Source code in <code>quadra/tasks/patch.py</code> <pre><code>def execute(self) -&gt; None:\n\"\"\"Execute the experiment and all the steps.\"\"\"\n    self.prepare()\n    self.train()\n    if self.output.report:\n        self.generate_report()\n    if self.config.export is not None and len(self.config.export.types) &gt; 0:\n        self.export()\n    self.finalize()\n</code></pre>"},{"location":"reference/quadra/tasks/patch.html#quadra.tasks.patch.PatchSklearnClassification.export","title":"<code>export()</code>","text":"<p>Generate deployment model for the task.</p> Source code in <code>quadra/tasks/patch.py</code> <pre><code>def export(self) -&gt; None:\n\"\"\"Generate deployment model for the task.\"\"\"\n    input_shapes = self.config.export.input_shapes\n\n    idx_to_class = {v: k for k, v in self.datamodule.class_to_idx.items()}\n\n    model_json, export_paths = export_model(\n        config=self.config,\n        model=self.backbone,\n        export_folder=self.export_folder,\n        half_precision=self.half_precision,\n        input_shapes=input_shapes,\n        idx_to_class=idx_to_class,\n        pytorch_model_type=\"backbone\",\n    )\n\n    if len(export_paths) &gt; 0:\n        dataset_info = self.datamodule.info\n\n        horizontal_patches = dataset_info.patch_number[1] if dataset_info.patch_number is not None else None\n        vertical_patches = dataset_info.patch_number[0] if dataset_info.patch_number is not None else None\n        patch_height = dataset_info.patch_size[0] if dataset_info.patch_size is not None else None\n        patch_width = dataset_info.patch_size[1] if dataset_info.patch_size is not None else None\n        overlap = dataset_info.overlap\n\n        model_json.update(\n            {\n                \"horizontal_patches\": horizontal_patches,\n                \"vertical_patches\": vertical_patches,\n                \"patch_height\": patch_height,\n                \"patch_width\": patch_width,\n                \"overlap\": overlap,\n                \"reconstruction_method\": self.output.reconstruction_method,\n                \"class_to_skip\": self.datamodule.class_to_skip_training,\n            }\n        )\n\n        with open(os.path.join(self.export_folder, \"model.json\"), \"w\") as f:\n            json.dump(model_json, f, cls=utils.HydraEncoder)\n\n    dump(self.model, os.path.join(self.export_folder, \"classifier.joblib\"))\n</code></pre>"},{"location":"reference/quadra/tasks/patch.html#quadra.tasks.patch.PatchSklearnClassification.generate_report","title":"<code>generate_report()</code>","text":"<p>Generate the report for the task.</p> Source code in <code>quadra/tasks/patch.py</code> <pre><code>def generate_report(self) -&gt; None:\n\"\"\"Generate the report for the task.\"\"\"\n    log.info(\"Generating report!\")\n    os.makedirs(self.output.folder, exist_ok=True)\n\n    c_matrix = self.metadata[\"test_confusion_matrix\"]\n    idx_to_class = {v: k for k, v in self.datamodule.class_to_idx.items()}\n\n    datamodule: PatchSklearnClassificationDataModule = self.datamodule\n    val_img_info: list[PatchDatasetFileFormat] = datamodule.info.val_files\n    for img_info in val_img_info:\n        if not os.path.isabs(img_info.image_path):\n            img_info.image_path = os.path.join(datamodule.data_path, img_info.image_path)\n        if img_info.mask_path is not None and not os.path.isabs(img_info.mask_path):\n            img_info.mask_path = os.path.join(datamodule.data_path, img_info.mask_path)\n\n    false_region_bad, false_region_good, true_region_bad, reconstructions = compute_patch_metrics(\n        test_img_info=val_img_info,\n        test_results=self.metadata[\"test_results\"],\n        patch_num_h=datamodule.info.patch_number[0] if datamodule.info.patch_number is not None else None,\n        patch_num_w=datamodule.info.patch_number[1] if datamodule.info.patch_number is not None else None,\n        patch_h=datamodule.info.patch_size[0] if datamodule.info.patch_size is not None else None,\n        patch_w=datamodule.info.patch_size[1] if datamodule.info.patch_size is not None else None,\n        overlap=datamodule.info.overlap,\n        idx_to_class=idx_to_class,\n        return_polygon=self.return_polygon,\n        patch_reconstruction_method=self.output.reconstruction_method,\n        annotated_good=datamodule.info.annotated_good,\n    )\n\n    self.reconstruction_results = {\n        \"false_region_bad\": false_region_bad,\n        \"false_region_good\": false_region_good,\n        \"true_region_bad\": true_region_bad,\n        \"reconstructions\": reconstructions,\n        \"reconstructions_type\": \"polygon\" if self.return_polygon else \"rle\",\n        \"patch_reconstruction_method\": self.output.reconstruction_method,\n    }\n\n    with open(\"reconstruction_results.json\", \"w\") as f:\n        json.dump(\n            self.reconstruction_results,\n            f,\n            cls=RleEncoder,\n        )\n\n    if hasattr(self.datamodule, \"class_to_skip_training\") and self.datamodule.class_to_skip_training is not None:\n        ignore_classes = [self.datamodule.class_to_idx[x] for x in self.datamodule.class_to_skip_training]\n    else:\n        ignore_classes = None\n    val_dataloader = self.datamodule.val_dataloader()\n    save_classification_result(\n        results=self.metadata[\"test_results\"],\n        output_folder=self.output.folder,\n        confusion_matrix=c_matrix,\n        accuracy=self.metadata[\"test_accuracy\"],\n        test_dataloader=val_dataloader,\n        config=self.config,\n        output=self.output,\n        reconstructions=reconstructions,\n        ignore_classes=ignore_classes,\n    )\n</code></pre>"},{"location":"reference/quadra/tasks/patch.html#quadra.tasks.patch.PatchSklearnClassification.prepare","title":"<code>prepare()</code>","text":"<p>Prepare the experiment.</p> Source code in <code>quadra/tasks/patch.py</code> <pre><code>def prepare(self) -&gt; None:\n\"\"\"Prepare the experiment.\"\"\"\n    self.datamodule = self.config.datamodule\n    self.backbone = self.config.backbone\n    self.model = self.config.model\n\n    if not self.automatic_batch_size.disable and self.device != \"cpu\":\n        self.datamodule.batch_size = automatic_batch_size_computation(\n            datamodule=self.datamodule,\n            backbone=self.backbone,\n            starting_batch_size=self.automatic_batch_size.starting_batch_size,\n        )\n\n    self.trainer = self.config.trainer\n</code></pre>"},{"location":"reference/quadra/tasks/patch.html#quadra.tasks.patch.PatchSklearnClassification.train","title":"<code>train()</code>","text":"<p>Train the model.</p> Source code in <code>quadra/tasks/patch.py</code> <pre><code>def train(self) -&gt; None:\n\"\"\"Train the model.\"\"\"\n    log.info(\"Starting training...!\")\n    # prepare_data() must be explicitly called if the task does not include a lightining training\n    self.datamodule.prepare_data()\n    self.datamodule.setup(stage=\"fit\")\n    class_to_keep = None\n    if hasattr(self.datamodule, \"class_to_skip_training\") and self.datamodule.class_to_skip_training is not None:\n        class_to_keep = [x for x in self.datamodule.class_to_idx if x not in self.datamodule.class_to_skip_training]\n\n    self.model = self.config.model\n    self.trainer.change_classifier(self.model)\n    train_dataloader = self.datamodule.train_dataloader()\n    val_dataloader = self.datamodule.val_dataloader()\n    train_dataset = cast(PatchSklearnClassificationTrainDataset, train_dataloader.dataset)\n    self.trainer.fit(train_dataloader=train_dataloader)\n    _, pd_cm, accuracy, res, _ = self.trainer.test(\n        test_dataloader=val_dataloader,\n        class_to_keep=class_to_keep,\n        idx_to_class=train_dataset.idx_to_class,\n        predict_proba=True,\n    )\n\n    # save results\n    self.metadata[\"test_confusion_matrix\"] = pd_cm\n    self.metadata[\"test_accuracy\"] = accuracy\n    self.metadata[\"test_results\"] = res\n    self.metadata[\"test_labels\"] = [\n        train_dataset.idx_to_class[i] if i != -1 else \"N/A\" for i in res[\"real_label\"].unique().tolist()\n    ]\n</code></pre>"},{"location":"reference/quadra/tasks/patch.html#quadra.tasks.patch.PatchSklearnTestClassification","title":"<code>PatchSklearnTestClassification(config, output, model_path, device='cpu')</code>","text":"<p>             Bases: <code>Evaluation[PatchSklearnClassificationDataModule]</code></p> <p>Perform a test of an already trained classification model.</p> <p>Parameters:</p> <ul> <li> config             (<code>DictConfig</code>)         \u2013          <p>The experiment configuration</p> </li> <li> output             (<code>DictConfig</code>)         \u2013          <p>where to save resultss</p> </li> <li> model_path             (<code>str</code>)         \u2013          <p>path to trained model from PatchSklearnClassification task.</p> </li> <li> device             (<code>str</code>, default:                 <code>'cpu'</code> )         \u2013          <p>the device where to run the model (cuda or cpu). Defaults to 'cpu'.</p> </li> </ul> Source code in <code>quadra/tasks/patch.py</code> <pre><code>def __init__(\n    self,\n    config: DictConfig,\n    output: DictConfig,\n    model_path: str,\n    device: str = \"cpu\",\n):\n    super().__init__(config=config, model_path=model_path, device=device)\n    self.output = output\n    self._backbone: BaseEvaluationModel\n    self._classifier: ClassifierMixin\n    self.class_to_idx: dict[str, int]\n    self.idx_to_class: dict[int, str]\n    self.metadata: dict[str, Any] = {\n        \"test_confusion_matrix\": None,\n        \"test_accuracy\": None,\n        \"test_results\": None,\n        \"test_labels\": None,\n    }\n    self.class_to_skip: list[str] = []\n    self.reconstruction_results: dict[str, Any]\n    self.return_polygon: bool = True\n</code></pre>"},{"location":"reference/quadra/tasks/patch.html#quadra.tasks.patch.PatchSklearnTestClassification.backbone","title":"<code>backbone: BaseEvaluationModel</code>  <code>property</code> <code>writable</code>","text":""},{"location":"reference/quadra/tasks/patch.html#quadra.tasks.patch.PatchSklearnTestClassification.classifier","title":"<code>classifier: ClassifierMixin</code>  <code>property</code> <code>writable</code>","text":""},{"location":"reference/quadra/tasks/patch.html#quadra.tasks.patch.PatchSklearnTestClassification.deployment_model","title":"<code>deployment_model</code>  <code>property</code> <code>writable</code>","text":"<p>Deployment model.</p>"},{"location":"reference/quadra/tasks/patch.html#quadra.tasks.patch.PatchSklearnTestClassification.trainer","title":"<code>trainer: SklearnClassificationTrainer</code>  <code>property</code> <code>writable</code>","text":""},{"location":"reference/quadra/tasks/patch.html#quadra.tasks.patch.PatchSklearnTestClassification.execute","title":"<code>execute()</code>","text":"<p>Execute the experiment and all the steps.</p> Source code in <code>quadra/tasks/patch.py</code> <pre><code>def execute(self) -&gt; None:\n\"\"\"Execute the experiment and all the steps.\"\"\"\n    self.prepare()\n    self.test()\n    if self.output.report:\n        self.generate_report()\n    self.finalize()\n</code></pre>"},{"location":"reference/quadra/tasks/patch.html#quadra.tasks.patch.PatchSklearnTestClassification.generate_report","title":"<code>generate_report()</code>","text":"<p>Generate a report for the task.</p> Source code in <code>quadra/tasks/patch.py</code> <pre><code>def generate_report(self) -&gt; None:\n\"\"\"Generate a report for the task.\"\"\"\n    log.info(\"Generating report!\")\n    os.makedirs(self.output.folder, exist_ok=True)\n\n    c_matrix = self.metadata[\"test_confusion_matrix\"]\n    idx_to_class = {v: k for k, v in self.datamodule.class_to_idx.items()}\n\n    datamodule: PatchSklearnClassificationDataModule = self.datamodule\n    test_img_info = datamodule.info.test_files\n    for img_info in test_img_info:\n        if not os.path.isabs(img_info.image_path):\n            img_info.image_path = os.path.join(datamodule.data_path, img_info.image_path)\n        if img_info.mask_path is not None and not os.path.isabs(img_info.mask_path):\n            img_info.mask_path = os.path.join(datamodule.data_path, img_info.mask_path)\n\n    false_region_bad, false_region_good, true_region_bad, reconstructions = compute_patch_metrics(\n        test_img_info=test_img_info,\n        test_results=self.metadata[\"test_results\"],\n        patch_num_h=datamodule.info.patch_number[0] if datamodule.info.patch_number is not None else None,\n        patch_num_w=datamodule.info.patch_number[1] if datamodule.info.patch_number is not None else None,\n        patch_h=datamodule.info.patch_size[0] if datamodule.info.patch_size is not None else None,\n        patch_w=datamodule.info.patch_size[1] if datamodule.info.patch_size is not None else None,\n        overlap=datamodule.info.overlap,\n        idx_to_class=idx_to_class,\n        return_polygon=self.return_polygon,\n        patch_reconstruction_method=self.output.reconstruction_method,\n        annotated_good=datamodule.info.annotated_good,\n    )\n\n    self.reconstruction_results = {\n        \"false_region_bad\": false_region_bad,\n        \"false_region_good\": false_region_good,\n        \"true_region_bad\": true_region_bad,\n        \"reconstructions\": reconstructions,\n        \"reconstructions_type\": \"polygon\" if self.return_polygon else \"rle\",\n        \"patch_reconstruction_method\": self.output.reconstruction_method,\n    }\n\n    with open(\"reconstruction_results.json\", \"w\") as f:\n        json.dump(\n            self.reconstruction_results,\n            f,\n            cls=RleEncoder,\n        )\n\n    if self.class_to_skip is not None:\n        ignore_classes = [datamodule.class_to_idx[x] for x in self.class_to_skip]\n    else:\n        ignore_classes = None\n    test_dataloader = self.datamodule.test_dataloader()\n    save_classification_result(\n        results=self.metadata[\"test_results\"],\n        output_folder=self.output.folder,\n        confusion_matrix=c_matrix,\n        accuracy=self.metadata[\"test_accuracy\"],\n        test_dataloader=test_dataloader,\n        config=self.config,\n        output=self.output,\n        reconstructions=reconstructions,\n        ignore_classes=ignore_classes,\n    )\n</code></pre>"},{"location":"reference/quadra/tasks/patch.html#quadra.tasks.patch.PatchSklearnTestClassification.prepare","title":"<code>prepare()</code>","text":"<p>Prepare the experiment.</p> Source code in <code>quadra/tasks/patch.py</code> <pre><code>def prepare(self) -&gt; None:\n\"\"\"Prepare the experiment.\"\"\"\n    super().prepare()\n\n    idx_to_class = {}\n    class_to_idx = {}\n    for k, v in self.model_data[\"classes\"].items():\n        idx_to_class[int(k)] = v\n        class_to_idx[v] = int(k)\n\n    self.idx_to_class = idx_to_class\n    self.class_to_idx = class_to_idx\n    self.config.datamodule.class_to_idx = class_to_idx\n\n    self.datamodule = self.config.datamodule\n    # Configure trainer\n    self.trainer = self.config.trainer\n\n    # prepare_data() must be explicitly called because there is no lightning training\n    self.datamodule.prepare_data()\n    self.datamodule.setup(stage=\"test\")\n</code></pre>"},{"location":"reference/quadra/tasks/patch.html#quadra.tasks.patch.PatchSklearnTestClassification.test","title":"<code>test()</code>","text":"<p>Run the test.</p> Source code in <code>quadra/tasks/patch.py</code> <pre><code>@automatic_datamodule_batch_size(batch_size_attribute_name=\"batch_size\")\ndef test(self) -&gt; None:\n\"\"\"Run the test.\"\"\"\n    test_dataloader = self.datamodule.test_dataloader()\n\n    self.class_to_skip = self.model_data[\"class_to_skip\"] if hasattr(self.model_data, \"class_to_skip\") else None\n    class_to_keep = None\n\n    if self.class_to_skip is not None:\n        class_to_keep = [x for x in self.datamodule.class_to_idx if x not in self.class_to_skip]\n    _, pd_cm, accuracy, res, _ = self.trainer.test(\n        test_dataloader=test_dataloader,\n        idx_to_class=self.idx_to_class,\n        predict_proba=True,\n        class_to_keep=class_to_keep,\n    )\n\n    # save results\n    self.metadata[\"test_confusion_matrix\"] = pd_cm\n    self.metadata[\"test_accuracy\"] = accuracy\n    self.metadata[\"test_results\"] = res\n    self.metadata[\"test_labels\"] = [\n        self.idx_to_class[i] if i != -1 else \"N/A\" for i in res[\"real_label\"].unique().tolist()\n    ]\n</code></pre>"},{"location":"reference/quadra/tasks/segmentation.html","title":"segmentation","text":""},{"location":"reference/quadra/tasks/segmentation.html#quadra.tasks.segmentation.Segmentation","title":"<code>Segmentation(config, num_viz_samples=5, checkpoint_path=None, run_test=False, evaluate=None, report=False)</code>","text":"<p>             Bases: <code>Generic[SegmentationDataModuleT]</code>, <code>LightningTask[SegmentationDataModuleT]</code></p> <p>Task for segmentation.</p> <p>Parameters:</p> <ul> <li> config             (<code>DictConfig</code>)         \u2013          <p>Config object</p> </li> <li> num_viz_samples             (<code>int</code>, default:                 <code>5</code> )         \u2013          <p>Number of samples to visualize. Defaults to 5.</p> </li> <li> checkpoint_path             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>Path to the checkpoint to load the model from. Defaults to None.</p> </li> <li> run_test             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>If True, run test after training. Defaults to False.</p> </li> <li> evaluate             (<code>DictConfig | None</code>, default:                 <code>None</code> )         \u2013          <p>Dict with evaluation parameters. Defaults to None.</p> </li> <li> report             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>If True, create report after training. Defaults to False.</p> </li> </ul> Source code in <code>quadra/tasks/segmentation.py</code> <pre><code>def __init__(\n    self,\n    config: DictConfig,\n    num_viz_samples: int = 5,\n    checkpoint_path: str | None = None,\n    run_test: bool = False,\n    evaluate: DictConfig | None = None,\n    report: bool = False,\n):\n    super().__init__(\n        config=config,\n        checkpoint_path=checkpoint_path,\n        run_test=run_test,\n        report=report,\n    )\n    self.evaluate = evaluate\n    self.num_viz_samples = num_viz_samples\n    self.export_folder: str = \"deployment_model\"\n    self.exported_model_path: str | None = None\n    if self.evaluate and any(self.evaluate.values()):\n        if (\n            self.config.export is None\n            or len(self.config.export.types) == 0\n            or \"torchscript\" not in self.config.export.types\n        ):\n            log.info(\n                \"Evaluation is enabled, but training does not export a deployment model. Automatically export the \"\n                \"model as torchscript.\"\n            )\n            if self.config.export is None:\n                self.config.export = DictConfig({\"types\": [\"torchscript\"]})\n            else:\n                self.config.export.types.append(\"torchscript\")\n\n        if not self.report:\n            log.info(\"Evaluation is enabled, but reporting is disabled. Enabling reporting automatically.\")\n            self.report = True\n</code></pre>"},{"location":"reference/quadra/tasks/segmentation.html#quadra.tasks.segmentation.Segmentation.module","title":"<code>module: SegmentationModel</code>  <code>property</code> <code>writable</code>","text":"<p>Get the module.</p>"},{"location":"reference/quadra/tasks/segmentation.html#quadra.tasks.segmentation.Segmentation.export","title":"<code>export()</code>","text":"<p>Generate a deployment model for the task.</p> Source code in <code>quadra/tasks/segmentation.py</code> <pre><code>def export(self) -&gt; None:\n\"\"\"Generate a deployment model for the task.\"\"\"\n    log.info(\"Exporting model ready for deployment\")\n\n    # Get best model!\n    if (\n        self.trainer.checkpoint_callback is not None\n        and hasattr(self.trainer.checkpoint_callback, \"best_model_path\")\n        and self.trainer.checkpoint_callback.best_model_path is not None\n        and len(self.trainer.checkpoint_callback.best_model_path) &gt; 0\n    ):\n        best_model_path = self.trainer.checkpoint_callback.best_model_path\n        log.info(\"Loaded best model from %s\", best_model_path)\n\n        module = self.module.__class__.load_from_checkpoint(\n            best_model_path,\n            model=self.module.model,\n            loss_fun=None,\n            optimizer=self.module.optimizer,\n            lr_scheduler=self.module.schedulers,\n        )\n    else:\n        log.warning(\"No checkpoint callback found in the trainer, exporting the last model weights\")\n        module = self.module\n\n    if \"idx_to_class\" not in self.config.datamodule:\n        log.info(\"No idx_to_class key\")\n        idx_to_class = {0: \"good\", 1: \"bad\"}  # TODO: Why is this the default value?\n    else:\n        log.info(\"idx_to_class is present\")\n        idx_to_class = self.config.datamodule.idx_to_class\n\n    if self.config.export is None:\n        raise ValueError(\n            \"No export type specified. This should not happen, please check if you have set \"\n            \"the export_type or assign it to a default value.\"\n        )\n\n    half_precision = \"16\" in self.trainer.precision\n\n    input_shapes = self.config.export.input_shapes\n\n    model_json, export_paths = export_model(\n        config=self.config,\n        model=module.model,\n        export_folder=self.export_folder,\n        half_precision=half_precision,\n        input_shapes=input_shapes,\n        idx_to_class=idx_to_class,\n    )\n\n    if len(export_paths) == 0:\n        return\n\n    # Pick one model for evaluation, it should be independent of the export type as the model is wrapped\n    self.exported_model_path = next(iter(export_paths.values()))\n\n    with open(os.path.join(self.export_folder, \"model.json\"), \"w\") as f:\n        json.dump(model_json, f, cls=utils.HydraEncoder)\n</code></pre>"},{"location":"reference/quadra/tasks/segmentation.html#quadra.tasks.segmentation.Segmentation.generate_report","title":"<code>generate_report()</code>","text":"<p>Generate a report for the task.</p> Source code in <code>quadra/tasks/segmentation.py</code> <pre><code>def generate_report(self) -&gt; None:\n\"\"\"Generate a report for the task.\"\"\"\n    if self.evaluate is not None:\n        log.info(\"Generating evaluation report!\")\n        eval_tasks: list[SegmentationEvaluation] = []\n        if self.evaluate.analysis:\n            if self.exported_model_path is None:\n                raise ValueError(\n                    \"Exported model path is not set yet but the task tries to do an analysis evaluation\"\n                )\n            eval_task = SegmentationAnalysisEvaluation(\n                config=self.config,\n                model_path=self.exported_model_path,\n            )\n            eval_tasks.append(eval_task)\n        for task in eval_tasks:\n            task.execute()\n\n        if len(self.logger) &gt; 0:\n            mflow_logger = get_mlflow_logger(trainer=self.trainer)\n            tensorboard_logger = utils.get_tensorboard_logger(trainer=self.trainer)\n\n            if mflow_logger is not None and self.config.core.get(\"upload_artifacts\"):\n                for task in eval_tasks:\n                    for file in task.metadata[\"report_files\"]:\n                        mflow_logger.experiment.log_artifact(\n                            run_id=mflow_logger.run_id, local_path=file, artifact_path=task.report_path\n                        )\n\n            if tensorboard_logger is not None and self.config.core.get(\"upload_artifacts\"):\n                for task in eval_tasks:\n                    for file in task.metadata[\"report_files\"]:\n                        ext = os.path.splitext(file)[1].lower()\n\n                        if ext in [\".png\", \".jpg\", \".jpeg\", \".bmp\", \".tiff\", \".tif\", \".gif\"]:\n                            try:\n                                img = cv2.imread(file)\n                                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n                            except cv2.error:\n                                log.info(\"Could not upload artifact image %s\", file)\n                                continue\n\n                            tensorboard_logger.experiment.add_image(\n                                os.path.basename(file), img, 0, dataformats=\"HWC\"\n                            )\n                        else:\n                            utils.upload_file_tensorboard(file, tensorboard_logger)\n</code></pre>"},{"location":"reference/quadra/tasks/segmentation.html#quadra.tasks.segmentation.Segmentation.prepare","title":"<code>prepare()</code>","text":"<p>Prepare the task.</p> Source code in <code>quadra/tasks/segmentation.py</code> <pre><code>def prepare(self) -&gt; None:\n\"\"\"Prepare the task.\"\"\"\n    super().prepare()\n    self.module = self.config.model\n</code></pre>"},{"location":"reference/quadra/tasks/segmentation.html#quadra.tasks.segmentation.SegmentationAnalysisEvaluation","title":"<code>SegmentationAnalysisEvaluation(config, model_path, device=None)</code>","text":"<p>             Bases: <code>SegmentationEvaluation</code></p> <p>Segmentation Analysis Evaluation Task Args:     config: The experiment configuration     model_path: The model path.     device: Device to use for evaluation. If None, the device is automatically determined.</p> Source code in <code>quadra/tasks/segmentation.py</code> <pre><code>def __init__(\n    self,\n    config: DictConfig,\n    model_path: str,\n    device: str | None = None,\n):\n    super().__init__(config=config, model_path=model_path, device=device)\n    self.test_output: dict[str, Any] = {}\n</code></pre>"},{"location":"reference/quadra/tasks/segmentation.html#quadra.tasks.segmentation.SegmentationAnalysisEvaluation.generate_report","title":"<code>generate_report()</code>","text":"<p>Generate a report.</p> Source code in <code>quadra/tasks/segmentation.py</code> <pre><code>def generate_report(self) -&gt; None:\n\"\"\"Generate a report.\"\"\"\n    log.info(\"Generating analysis report\")\n\n    for stage, output in self.test_output.items():\n        image_mean = OmegaConf.to_container(self.config.transforms.mean)\n        if not isinstance(image_mean, list) or any(not isinstance(x, (int, float)) for x in image_mean):\n            raise ValueError(\"Image mean is not a list of float or integer values, please check your config\")\n        image_std = OmegaConf.to_container(self.config.transforms.std)\n        if not isinstance(image_std, list) or any(not isinstance(x, (int, float)) for x in image_std):\n            raise ValueError(\"Image std is not a list of float or integer values, please check your config\")\n        reports = create_mask_report(\n            stage=stage,\n            output=output,\n            report_path=\"analysis_report\",\n            mean=image_mean,\n            std=image_std,\n            analysis=True,\n            nb_samples=10,\n            apply_sigmoid=True,\n            show_orj_predictions=True,\n        )\n        self.metadata[\"report_files\"].extend(reports)\n        log.info(\"%s analysis report completed.\", stage)\n</code></pre>"},{"location":"reference/quadra/tasks/segmentation.html#quadra.tasks.segmentation.SegmentationAnalysisEvaluation.prepare","title":"<code>prepare()</code>","text":"<p>Prepare the evaluation task.</p> Source code in <code>quadra/tasks/segmentation.py</code> <pre><code>def prepare(self) -&gt; None:\n\"\"\"Prepare the evaluation task.\"\"\"\n    super().prepare()\n    self.datamodule.setup(stage=\"fit\")\n    self.datamodule.setup(stage=\"test\")\n</code></pre>"},{"location":"reference/quadra/tasks/segmentation.html#quadra.tasks.segmentation.SegmentationAnalysisEvaluation.test","title":"<code>test()</code>","text":"<p>Run testing.</p> Source code in <code>quadra/tasks/segmentation.py</code> <pre><code>@automatic_datamodule_batch_size(batch_size_attribute_name=\"batch_size\")\ndef test(self) -&gt; None:\n\"\"\"Run testing.\"\"\"\n    log.info(\"Starting inference for analysis.\")\n\n    stages: list[str] = []\n    dataloaders: list[torch.utils.data.DataLoader] = []\n\n    # if self.datamodule.train_dataset_available:\n    #     stages.append(\"train\")\n    #     dataloaders.append(self.datamodule.train_dataloader())\n    #     if self.datamodule.val_dataset_available:\n    #         stages.append(\"val\")\n    #         dataloaders.append(self.datamodule.val_dataloader())\n\n    if self.datamodule.test_dataset_available:\n        stages.append(\"test\")\n        dataloaders.append(self.datamodule.test_dataloader())\n    for stage, dataloader in zip(stages, dataloaders):\n        log.info(\"Running inference on %s set with batch size: %d\", stage, dataloader.batch_size)\n        image_list, mask_list, mask_pred_list, label_list = [], [], [], []\n        for batch in dataloader:\n            images, masks, labels = batch\n            images = images.to(device=self.device, dtype=self.deployment_model.model_dtype)\n            if len(masks.shape) == 3:  # BxHxW -&gt; Bx1xHxW\n                masks = masks.unsqueeze(1)\n            with torch.no_grad():\n                image_list.append(images)\n                mask_list.append(masks)\n                mask_pred_list.append(self.deployment_model(images.to(self.device)))\n                label_list.append(labels)\n\n        output = {\n            \"image\": torch.cat(image_list, dim=0),\n            \"mask\": torch.cat(mask_list, dim=0),\n            \"label\": torch.cat(label_list, dim=0),\n            \"mask_pred\": torch.cat(mask_pred_list, dim=0),\n        }\n        self.test_output[stage] = output\n</code></pre>"},{"location":"reference/quadra/tasks/segmentation.html#quadra.tasks.segmentation.SegmentationAnalysisEvaluation.train","title":"<code>train()</code>","text":"<p>Skip training.</p> Source code in <code>quadra/tasks/segmentation.py</code> <pre><code>def train(self) -&gt; None:\n\"\"\"Skip training.\"\"\"\n</code></pre>"},{"location":"reference/quadra/tasks/segmentation.html#quadra.tasks.segmentation.SegmentationEvaluation","title":"<code>SegmentationEvaluation(config, model_path, device='cpu')</code>","text":"<p>             Bases: <code>Evaluation[SegmentationDataModuleT]</code></p> <p>Segmentation Evaluation Task with deployment models.</p> <p>Parameters:</p> <ul> <li> config             (<code>DictConfig</code>)         \u2013          <p>The experiment configuration</p> </li> <li> model_path             (<code>str</code>)         \u2013          <p>The experiment path.</p> </li> <li> device             (<code>str | None</code>, default:                 <code>'cpu'</code> )         \u2013          <p>Device to use for evaluation. If None, the device is automatically determined.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>           \u2013          <p>If the model path is not provided</p> </li> </ul> Source code in <code>quadra/tasks/segmentation.py</code> <pre><code>def __init__(\n    self,\n    config: DictConfig,\n    model_path: str,\n    device: str | None = \"cpu\",\n):\n    super().__init__(config=config, model_path=model_path, device=device)\n    self.config = config\n</code></pre>"},{"location":"reference/quadra/tasks/segmentation.html#quadra.tasks.segmentation.SegmentationEvaluation.inference","title":"<code>inference(dataloader, deployment_model, device)</code>","text":"<p>Run inference on the dataloader and return the output.</p> <p>Parameters:</p> <ul> <li> dataloader             (<code>DataLoader</code>)         \u2013          <p>The dataloader to run inference on</p> </li> <li> deployment_model             (<code>BaseEvaluationModel</code>)         \u2013          <p>The deployment model to use</p> </li> <li> device             (<code>device</code>)         \u2013          <p>The device to run inference on</p> </li> </ul> Source code in <code>quadra/tasks/segmentation.py</code> <pre><code>@torch.no_grad()\ndef inference(\n    self, dataloader: DataLoader, deployment_model: BaseEvaluationModel, device: torch.device\n) -&gt; dict[str, torch.Tensor]:\n\"\"\"Run inference on the dataloader and return the output.\n\n    Args:\n        dataloader: The dataloader to run inference on\n        deployment_model: The deployment model to use\n        device: The device to run inference on\n    \"\"\"\n    image_list, mask_list, mask_pred_list, label_list = [], [], [], []\n    for batch in dataloader:\n        images, masks, labels = batch\n        images = images.to(device)\n        masks = masks.to(device)\n        labels = labels.to(device)\n        image_list.append(images.cpu())\n        mask_list.append(masks.cpu())\n        mask_pred_list.append(deployment_model(images.to(device)).cpu())\n        label_list.append(labels.cpu())\n    output = {\n        \"image\": torch.cat(image_list, dim=0),\n        \"mask\": torch.cat(mask_list, dim=0),\n        \"label\": torch.cat(label_list, dim=0),\n        \"mask_pred\": torch.cat(mask_pred_list, dim=0),\n    }\n    return output\n</code></pre>"},{"location":"reference/quadra/tasks/segmentation.html#quadra.tasks.segmentation.SegmentationEvaluation.prepare","title":"<code>prepare()</code>","text":"<p>Prepare the evaluation.</p> Source code in <code>quadra/tasks/segmentation.py</code> <pre><code>def prepare(self) -&gt; None:\n\"\"\"Prepare the evaluation.\"\"\"\n    super().prepare()\n    # TODO: Why we propagate mean and std only in Segmentation?\n    self.config.transforms.mean = self.model_data[\"mean\"]\n    self.config.transforms.std = self.model_data[\"std\"]\n    # Setup datamodule\n    if hasattr(self.config.datamodule, \"idx_to_class\"):\n        idx_to_class = self.model_data[\"classes\"]  # dict {index: class}\n        self.config.datamodule.idx_to_class = idx_to_class\n    self.datamodule = self.config.datamodule\n    # prepare_data() must be explicitly called because there is no lightning training\n    self.datamodule.prepare_data()\n</code></pre>"},{"location":"reference/quadra/tasks/segmentation.html#quadra.tasks.segmentation.SegmentationEvaluation.save_config","title":"<code>save_config()</code>","text":"<p>Skip saving the config.</p> Source code in <code>quadra/tasks/segmentation.py</code> <pre><code>def save_config(self) -&gt; None:\n\"\"\"Skip saving the config.\"\"\"\n</code></pre>"},{"location":"reference/quadra/tasks/ssl.html","title":"ssl","text":""},{"location":"reference/quadra/tasks/ssl.html#quadra.tasks.ssl.BYOL","title":"<code>BYOL(config, checkpoint_path=None, run_test=False, **kwargs)</code>","text":"<p>             Bases: <code>SSL</code></p> <p>BYOL model as a pytorch_lightning.LightningModule.</p> <p>Parameters:</p> <ul> <li> config             (<code>DictConfig</code>)         \u2013          <p>the main config</p> </li> <li> checkpoint_path             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>if a checkpoint is specified, then it will return a trained model, with weights loaded from the checkpoint path specified. Defaults to None.</p> </li> <li> run_test             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to run final test</p> </li> <li> **kwargs             (<code>Any</code>, default:                 <code>{}</code> )         \u2013          <p>Keyword arguments</p> </li> </ul> Source code in <code>quadra/tasks/ssl.py</code> <pre><code>def __init__(\n    self,\n    config: DictConfig,\n    checkpoint_path: str | None = None,\n    run_test: bool = False,\n    **kwargs: Any,\n):\n    super().__init__(\n        config=config,\n        checkpoint_path=checkpoint_path,\n        run_test=run_test,\n        **kwargs,\n    )\n    self.student_model: nn.Module\n    self.teacher_model: nn.Module\n    self.student_projection_mlp: nn.Module\n    self.student_prediction_mlp: nn.Module\n    self.teacher_projection_mlp: nn.Module\n</code></pre>"},{"location":"reference/quadra/tasks/ssl.html#quadra.tasks.ssl.BYOL.learnable_parameters","title":"<code>learnable_parameters()</code>","text":"<p>Get the learnable parameters.</p> Source code in <code>quadra/tasks/ssl.py</code> <pre><code>def learnable_parameters(self) -&gt; list[nn.Parameter]:\n\"\"\"Get the learnable parameters.\"\"\"\n    return list(\n        list(self.student_model.parameters())\n        + list(self.student_projection_mlp.parameters())\n        + list(self.student_prediction_mlp.parameters()),\n    )\n</code></pre>"},{"location":"reference/quadra/tasks/ssl.html#quadra.tasks.ssl.BYOL.prepare","title":"<code>prepare()</code>","text":"<p>Prepare the experiment.</p> Source code in <code>quadra/tasks/ssl.py</code> <pre><code>def prepare(self) -&gt; None:\n\"\"\"Prepare the experiment.\"\"\"\n    super().prepare()\n    self.student_model = hydra.utils.instantiate(self.config.model.student)\n    self.teacher_model = hydra.utils.instantiate(self.config.model.student)\n    self.student_projection_mlp = hydra.utils.instantiate(self.config.model.projection_mlp)\n    self.student_prediction_mlp = hydra.utils.instantiate(self.config.model.prediction_mlp)\n    self.teacher_projection_mlp = hydra.utils.instantiate(self.config.model.projection_mlp)\n    self.optimizer = self.config.optimizer\n    self.scheduler = self.config.scheduler\n    self.module = self.config.model.module\n</code></pre>"},{"location":"reference/quadra/tasks/ssl.html#quadra.tasks.ssl.Barlow","title":"<code>Barlow(config, checkpoint_path=None, run_test=False)</code>","text":"<p>             Bases: <code>SimCLR</code></p> <p>Barlow model as a pytorch_lightning.LightningModule.</p> <p>Parameters:</p> <ul> <li> config             (<code>DictConfig</code>)         \u2013          <p>the main config</p> </li> <li> checkpoint_path             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>if a checkpoint is specified, then it will return a trained model, with weights loaded from the checkpoint path specified. Defaults to None.</p> </li> <li> run_test             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to run final test</p> </li> </ul> Source code in <code>quadra/tasks/ssl.py</code> <pre><code>def __init__(\n    self,\n    config: DictConfig,\n    checkpoint_path: str | None = None,\n    run_test: bool = False,\n):\n    super().__init__(config=config, checkpoint_path=checkpoint_path, run_test=run_test)\n</code></pre>"},{"location":"reference/quadra/tasks/ssl.html#quadra.tasks.ssl.Barlow.prepare","title":"<code>prepare()</code>","text":"<p>Prepare the experiment.</p> Source code in <code>quadra/tasks/ssl.py</code> <pre><code>def prepare(self) -&gt; None:\n\"\"\"Prepare the experiment.\"\"\"\n    super(SimCLR, self).prepare()\n    self.backbone = hydra.utils.instantiate(self.config.model.model)\n\n    with open_dict(self.config):\n        self.config.model.projection_mlp.hidden_dim = (\n            self.config.model.projection_mlp.hidden_dim * self.config.model.projection_mlp_mult\n        )\n        self.config.model.projection_mlp.output_dim = (\n            self.config.model.projection_mlp.output_dim * self.config.model.projection_mlp_mult\n        )\n    self.projection_mlp = hydra.utils.instantiate(self.config.model.projection_mlp)\n    self.optimizer = self.config.optimizer\n    self.scheduler = self.config.scheduler\n    self.module = self.config.model.module\n</code></pre>"},{"location":"reference/quadra/tasks/ssl.html#quadra.tasks.ssl.DINO","title":"<code>DINO(config, checkpoint_path=None, run_test=False)</code>","text":"<p>             Bases: <code>SSL</code></p> <p>DINO model as a pytorch_lightning.LightningModule.</p> <p>Parameters:</p> <ul> <li> config             (<code>DictConfig</code>)         \u2013          <p>the main config</p> </li> <li> checkpoint_path             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>if a checkpoint is specified, then it will return a trained model, with weights loaded from the checkpoint path specified. Defaults to None.</p> </li> <li> run_test             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to run final test</p> </li> </ul> Source code in <code>quadra/tasks/ssl.py</code> <pre><code>def __init__(\n    self,\n    config: DictConfig,\n    checkpoint_path: str | None = None,\n    run_test: bool = False,\n):\n    super().__init__(config=config, checkpoint_path=checkpoint_path, run_test=run_test)\n    self.student_model: nn.Module\n    self.teacher_model: nn.Module\n    self.student_projection_mlp: nn.Module\n    self.teacher_projection_mlp: nn.Module\n</code></pre>"},{"location":"reference/quadra/tasks/ssl.html#quadra.tasks.ssl.DINO.learnable_parameters","title":"<code>learnable_parameters()</code>","text":"<p>Get the learnable parameters.</p> Source code in <code>quadra/tasks/ssl.py</code> <pre><code>def learnable_parameters(self) -&gt; list[nn.Parameter]:\n\"\"\"Get the learnable parameters.\"\"\"\n    return list(\n        list(self.student_model.parameters()) + list(self.student_projection_mlp.parameters()),\n    )\n</code></pre>"},{"location":"reference/quadra/tasks/ssl.html#quadra.tasks.ssl.DINO.prepare","title":"<code>prepare()</code>","text":"<p>Prepare the experiment.</p> Source code in <code>quadra/tasks/ssl.py</code> <pre><code>def prepare(self) -&gt; None:\n\"\"\"Prepare the experiment.\"\"\"\n    super().prepare()\n    self.student_model = cast(nn.Module, hydra.utils.instantiate(self.config.model.student))\n    self.teacher_model = cast(nn.Module, hydra.utils.instantiate(self.config.model.student))\n    self.student_projection_mlp = cast(nn.Module, hydra.utils.instantiate(self.config.model.student_projection_mlp))\n    self.teacher_projection_mlp = cast(nn.Module, hydra.utils.instantiate(self.config.model.teacher_projection_mlp))\n    self.optimizer = self.config.optimizer\n    self.scheduler = self.config.scheduler\n    self.module = self.config.model.module\n</code></pre>"},{"location":"reference/quadra/tasks/ssl.html#quadra.tasks.ssl.EmbeddingVisualization","title":"<code>EmbeddingVisualization(config, model_path, report_folder='embeddings', embedding_image_size=None)</code>","text":"<p>             Bases: <code>Task</code></p> <p>Visualization task for learned embeddings.</p> <p>Parameters:</p> <ul> <li> config             (<code>DictConfig</code>)         \u2013          <p>The loaded experiment config</p> </li> <li> model_path             (<code>str</code>)         \u2013          <p>The path to a deployment model</p> </li> <li> report_folder             (<code>str</code>, default:                 <code>'embeddings'</code> )         \u2013          <p>Where to save the embeddings</p> </li> <li> embedding_image_size             (<code>int | None</code>, default:                 <code>None</code> )         \u2013          <p>If not None rescale the images associated with the embeddings, tensorboard will save on disk a large sprite containing all the images in a matrix fashion, if the dimension of this sprite is too big it's not possible to load it in the browser. Rescaling the output image from the model input size to something smaller can solve this issue. The field is an int to always rescale to a squared image.</p> </li> </ul> Source code in <code>quadra/tasks/ssl.py</code> <pre><code>def __init__(\n    self,\n    config: DictConfig,\n    model_path: str,\n    report_folder: str = \"embeddings\",\n    embedding_image_size: int | None = None,\n):\n    super().__init__(config=config)\n\n    self.config = config\n    self.metadata = {\n        \"report_files\": [],\n    }\n    self.model_path = model_path\n    self._device = utils.get_device()\n    log.info(\"Current device: %s\", self._device)\n\n    self.report_folder = report_folder\n    if self.model_path is None:\n        raise ValueError(\n            \"Model path cannot be found!, please specify it in the config or pass it as an argument for\"\n            \" evaluation\"\n        )\n    self.embeddings_path = os.path.join(self.model_path, self.report_folder)\n    if not os.path.exists(self.embeddings_path):\n        os.makedirs(self.embeddings_path)\n    self.embedding_writer = SummaryWriter(self.embeddings_path)\n    self.writer_step = 0  # for tensorboard\n    self.embedding_image_size = embedding_image_size\n    self._deployment_model: BaseEvaluationModel\n    self.deployment_model_type: str\n</code></pre>"},{"location":"reference/quadra/tasks/ssl.html#quadra.tasks.ssl.EmbeddingVisualization.deployment_model","title":"<code>deployment_model</code>  <code>property</code> <code>writable</code>","text":"<p>Get the deployment model.</p>"},{"location":"reference/quadra/tasks/ssl.html#quadra.tasks.ssl.EmbeddingVisualization.prepare","title":"<code>prepare()</code>","text":"<p>Prepare the evaluation.</p> Source code in <code>quadra/tasks/ssl.py</code> <pre><code>def prepare(self) -&gt; None:\n\"\"\"Prepare the evaluation.\"\"\"\n    super().prepare()\n    self.deployment_model = self.model_path\n</code></pre>"},{"location":"reference/quadra/tasks/ssl.html#quadra.tasks.ssl.EmbeddingVisualization.test","title":"<code>test()</code>","text":"<p>Run embeddings extraction.</p> Source code in <code>quadra/tasks/ssl.py</code> <pre><code>@torch.no_grad()\ndef test(self) -&gt; None:\n\"\"\"Run embeddings extraction.\"\"\"\n    self.datamodule.setup(\"fit\")\n    idx_to_class = self.datamodule.val_dataset.idx_to_class\n    self.datamodule.setup(\"test\")\n    dataloader = self.datamodule.test_dataloader()\n    images = []\n    metadata: list[tuple[int, str, str]] = []\n    embeddings = []\n    std = torch.tensor(self.config.transforms.std).view(1, -1, 1, 1)\n    mean = torch.tensor(self.config.transforms.mean).view(1, -1, 1, 1)\n    dl = self.datamodule.test_dataloader()\n    counter = 0\n\n    is_half_precision = False\n    for param in self.deployment_model.parameters():\n        if param.dtype == torch.half:\n            is_half_precision = True\n        break\n\n    for batch in tqdm(dataloader):\n        im, target = batch\n        if is_half_precision:\n            im = im.half()\n\n        x = self.deployment_model(im.to(self.device))\n        targets = [int(t.item()) for t in target]\n        class_names = [idx_to_class[t.item()] for t in target]\n        file_paths = [s[0] for s in dl.dataset.samples[counter : counter + len(im)]]\n        embeddings.append(x.cpu())\n        im = im * std\n        im += mean\n\n        if self.embedding_image_size is not None:\n            im = interpolate(im, self.embedding_image_size)\n\n        images.append(im.cpu())\n        metadata.extend(zip(targets, class_names, file_paths))\n        counter += len(im)\n    images = torch.cat(images, dim=0)\n    embeddings = torch.cat(embeddings, dim=0)\n    self.embedding_writer.add_embedding(\n        embeddings,\n        metadata=metadata,\n        label_img=images,\n        global_step=self.writer_step,\n        metadata_header=[\"class\", \"class_name\", \"path\"],\n    )\n</code></pre>"},{"location":"reference/quadra/tasks/ssl.html#quadra.tasks.ssl.SSL","title":"<code>SSL(config, run_test=False, report=False, checkpoint_path=None)</code>","text":"<p>             Bases: <code>LightningTask</code></p> <p>SSL Task.</p> <p>Parameters:</p> <ul> <li> config             (<code>DictConfig</code>)         \u2013          <p>The experiment configuration</p> </li> <li> checkpoint_path             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>The path to the checkpoint to load the model from Defaults to None</p> </li> <li> report             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to create the report</p> </li> <li> run_test             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to run final test</p> </li> </ul> Source code in <code>quadra/tasks/ssl.py</code> <pre><code>def __init__(\n    self,\n    config: DictConfig,\n    run_test: bool = False,\n    report: bool = False,\n    checkpoint_path: str | None = None,\n):\n    super().__init__(\n        config=config,\n        checkpoint_path=checkpoint_path,\n        run_test=run_test,\n        report=report,\n    )\n    self._backbone: nn.Module\n    self._optimizer: torch.optim.Optimizer\n    self._lr_scheduler: torch.optim.lr_scheduler._LRScheduler\n    self.export_folder = \"deployment_model\"\n</code></pre>"},{"location":"reference/quadra/tasks/ssl.html#quadra.tasks.ssl.SSL.optimizer","title":"<code>optimizer: torch.optim.Optimizer</code>  <code>property</code> <code>writable</code>","text":"<p>Get the optimizer.</p>"},{"location":"reference/quadra/tasks/ssl.html#quadra.tasks.ssl.SSL.scheduler","title":"<code>scheduler: torch.optim.lr_scheduler._LRScheduler</code>  <code>property</code> <code>writable</code>","text":"<p>Get the scheduler.</p>"},{"location":"reference/quadra/tasks/ssl.html#quadra.tasks.ssl.SSL.export","title":"<code>export()</code>","text":"<p>Deploy a model ready for production.</p> Source code in <code>quadra/tasks/ssl.py</code> <pre><code>def export(self) -&gt; None:\n\"\"\"Deploy a model ready for production.\"\"\"\n    half_precision = \"16\" in self.trainer.precision\n\n    input_shapes = self.config.export.input_shapes\n\n    model_json, export_paths = export_model(\n        config=self.config,\n        model=self.module.model,\n        export_folder=self.export_folder,\n        half_precision=half_precision,\n        input_shapes=input_shapes,\n        idx_to_class=None,\n    )\n\n    if len(export_paths) == 0:\n        return\n\n    with open(os.path.join(self.export_folder, \"model.json\"), \"w\") as f:\n        json.dump(model_json, f, cls=utils.HydraEncoder)\n</code></pre>"},{"location":"reference/quadra/tasks/ssl.html#quadra.tasks.ssl.SSL.learnable_parameters","title":"<code>learnable_parameters()</code>","text":"<p>Get the learnable parameters.</p> Source code in <code>quadra/tasks/ssl.py</code> <pre><code>def learnable_parameters(self) -&gt; list[nn.Parameter]:\n\"\"\"Get the learnable parameters.\"\"\"\n    raise NotImplementedError(\"This method must be implemented by the subclass\")\n</code></pre>"},{"location":"reference/quadra/tasks/ssl.html#quadra.tasks.ssl.SSL.test","title":"<code>test()</code>","text":"<p>Test the model.</p> Source code in <code>quadra/tasks/ssl.py</code> <pre><code>def test(self) -&gt; None:\n\"\"\"Test the model.\"\"\"\n    if self.run_test and not self.config.trainer.get(\"fast_dev_run\"):\n        log.info(\"Starting testing!\")\n        log.info(\"Using last epoch's weights for testing.\")\n        self.trainer.test(datamodule=self.datamodule, model=self.module, ckpt_path=None)\n</code></pre>"},{"location":"reference/quadra/tasks/ssl.html#quadra.tasks.ssl.SimCLR","title":"<code>SimCLR(config, checkpoint_path=None, run_test=False)</code>","text":"<p>             Bases: <code>SSL</code></p> <p>SimCLR model as a pytorch_lightning.LightningModule.</p> <p>Parameters:</p> <ul> <li> config             (<code>DictConfig</code>)         \u2013          <p>the main config</p> </li> <li> checkpoint_path             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>if a checkpoint is specified, then it will return a trained model, with weights loaded from the checkpoint path specified. Defaults to None.</p> </li> <li> run_test             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to run final test</p> </li> </ul> Source code in <code>quadra/tasks/ssl.py</code> <pre><code>def __init__(\n    self,\n    config: DictConfig,\n    checkpoint_path: str | None = None,\n    run_test: bool = False,\n):\n    super().__init__(config=config, checkpoint_path=checkpoint_path, run_test=run_test)\n    self.backbone: nn.Module\n    self.projection_mlp: nn.Module\n</code></pre>"},{"location":"reference/quadra/tasks/ssl.html#quadra.tasks.ssl.SimCLR.learnable_parameters","title":"<code>learnable_parameters()</code>","text":"<p>Get the learnable parameters.</p> Source code in <code>quadra/tasks/ssl.py</code> <pre><code>def learnable_parameters(self) -&gt; list[nn.Parameter]:\n\"\"\"Get the learnable parameters.\"\"\"\n    return list(self.backbone.parameters()) + list(self.projection_mlp.parameters())\n</code></pre>"},{"location":"reference/quadra/tasks/ssl.html#quadra.tasks.ssl.SimCLR.prepare","title":"<code>prepare()</code>","text":"<p>Prepare the experiment.</p> Source code in <code>quadra/tasks/ssl.py</code> <pre><code>def prepare(self) -&gt; None:\n\"\"\"Prepare the experiment.\"\"\"\n    super().prepare()\n    self.backbone = hydra.utils.instantiate(self.config.model.model)\n    self.projection_mlp = hydra.utils.instantiate(self.config.model.projection_mlp)\n    self.optimizer = self.config.optimizer\n    self.scheduler = self.config.scheduler\n    self.module = self.config.model.module\n</code></pre>"},{"location":"reference/quadra/tasks/ssl.html#quadra.tasks.ssl.Simsiam","title":"<code>Simsiam(config, checkpoint_path=None, run_test=False)</code>","text":"<p>             Bases: <code>SSL</code></p> <p>Simsiam model as a pytorch_lightning.LightningModule.</p> <p>Parameters:</p> <ul> <li> config             (<code>DictConfig</code>)         \u2013          <p>the main config</p> </li> <li> checkpoint_path             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>if a checkpoint is specified, then it will return a trained model, with weights loaded from the checkpoint path specified. Defaults to None.</p> </li> <li> run_test             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to run final test</p> </li> </ul> Source code in <code>quadra/tasks/ssl.py</code> <pre><code>def __init__(\n    self,\n    config: DictConfig,\n    checkpoint_path: str | None = None,\n    run_test: bool = False,\n):\n    super().__init__(config=config, checkpoint_path=checkpoint_path, run_test=run_test)\n    self.backbone: nn.Module\n    self.projection_mlp: nn.Module\n    self.prediction_mlp: nn.Module\n</code></pre>"},{"location":"reference/quadra/tasks/ssl.html#quadra.tasks.ssl.Simsiam.module","title":"<code>module: LightningModule</code>  <code>property</code> <code>writable</code>","text":"<p>Get the module of the model.</p>"},{"location":"reference/quadra/tasks/ssl.html#quadra.tasks.ssl.Simsiam.learnable_parameters","title":"<code>learnable_parameters()</code>","text":"<p>Get the learnable parameters.</p> Source code in <code>quadra/tasks/ssl.py</code> <pre><code>def learnable_parameters(self) -&gt; list[nn.Parameter]:\n\"\"\"Get the learnable parameters.\"\"\"\n    return list(\n        list(self.backbone.parameters())\n        + list(self.projection_mlp.parameters())\n        + list(self.prediction_mlp.parameters()),\n    )\n</code></pre>"},{"location":"reference/quadra/tasks/ssl.html#quadra.tasks.ssl.Simsiam.prepare","title":"<code>prepare()</code>","text":"<p>Prepare the experiment.</p> Source code in <code>quadra/tasks/ssl.py</code> <pre><code>def prepare(self) -&gt; None:\n\"\"\"Prepare the experiment.\"\"\"\n    super().prepare()\n    self.backbone = hydra.utils.instantiate(self.config.model.model)\n    self.projection_mlp = hydra.utils.instantiate(self.config.model.projection_mlp)\n    self.prediction_mlp = hydra.utils.instantiate(self.config.model.prediction_mlp)\n    self.optimizer = self.config.optimizer\n    self.scheduler = self.config.scheduler\n    self.module = self.config.model.module\n</code></pre>"},{"location":"reference/quadra/trainers/index.html","title":"Trainers","text":"<p>Here are defined custom trainers that can be used to replace Pytorch Lightning. For example for classification we have implemented a trainer that uses the <code>scikit-learn</code> library to train a classifier on top of a torch feature extractor.</p>"},{"location":"reference/quadra/trainers/index.html#python-files","title":"Python Files","text":"<ul> <li>classification.py </li> </ul>"},{"location":"reference/quadra/trainers/classification.html","title":"classification","text":""},{"location":"reference/quadra/trainers/classification.html#quadra.trainers.classification.SklearnClassificationTrainer","title":"<code>SklearnClassificationTrainer(input_shape, backbone, random_state=42, classifier=LogisticRegression, iteration_over_training=1)</code>","text":"<p>Class to configure and run a classification using torch for feature extraction and sklearn to fit a classifier.</p> <p>Parameters:</p> <ul> <li> input_shape             (<code>list</code>)         \u2013          <p>[H, W, C]</p> </li> <li> random_state             (<code>int</code>, default:                 <code>42</code> )         \u2013          <p>seed to fix randomness</p> </li> <li> classifier             (<code>ClassifierMixin</code>, default:                 <code>LogisticRegression</code> )         \u2013          <p>classification model</p> </li> <li> iteration_over_training             (<code>int</code>, default:                 <code>1</code> )         \u2013          <p>the number of iteration over training during feature extraction</p> </li> <li> backbone             (<code>Module</code>)         \u2013          <p>the feature extractor</p> </li> </ul> Source code in <code>quadra/trainers/classification.py</code> <pre><code>def __init__(\n    self,\n    input_shape: list,\n    backbone: torch.nn.Module,\n    random_state: int = 42,\n    classifier: ClassifierMixin = LogisticRegression,\n    iteration_over_training: int = 1,\n) -&gt; None:\n    super().__init__()\n\n    try:\n        self.classifier = classifier(max_iter=1e4, random_state=random_state)\n    except Exception:\n        self.classifier = classifier\n\n    self.input_shape = input_shape\n    self.random_state = random_state\n    self.iteration_over_training = iteration_over_training\n    self.backbone = backbone\n</code></pre>"},{"location":"reference/quadra/trainers/classification.html#quadra.trainers.classification.SklearnClassificationTrainer.change_backbone","title":"<code>change_backbone(backbone)</code>","text":"<p>Update feature extractor.</p> Source code in <code>quadra/trainers/classification.py</code> <pre><code>def change_backbone(self, backbone: torch.nn.Module):\n\"\"\"Update feature extractor.\"\"\"\n    self.backbone = backbone\n    self.backbone.eval()\n</code></pre>"},{"location":"reference/quadra/trainers/classification.html#quadra.trainers.classification.SklearnClassificationTrainer.change_classifier","title":"<code>change_classifier(classifier)</code>","text":"<p>Update classifier.</p> Source code in <code>quadra/trainers/classification.py</code> <pre><code>def change_classifier(self, classifier: ClassifierMixin):\n\"\"\"Update classifier.\"\"\"\n    self.classifier = classifier\n</code></pre>"},{"location":"reference/quadra/trainers/classification.html#quadra.trainers.classification.SklearnClassificationTrainer.fit","title":"<code>fit(train_dataloader=None, train_features=None, train_labels=None)</code>","text":"<p>Fit classifier on training set.</p> Source code in <code>quadra/trainers/classification.py</code> <pre><code>def fit(\n    self,\n    train_dataloader: DataLoader | None = None,\n    train_features: ndarray | None = None,\n    train_labels: ndarray | None = None,\n):\n\"\"\"Fit classifier on training set.\"\"\"\n    # Extract feature\n    if self.backbone is None:\n        raise AssertionError(\"You must set a model before running execution\")\n\n    if train_dataloader is not None:  # train_features is None or train_labels is None:\n        log.info(\"Extracting features from training set\")\n        train_features, train_labels, _ = get_feature(\n            feature_extractor=self.backbone,\n            dl=train_dataloader,\n            iteration_over_training=self.iteration_over_training,\n            gradcam=False,\n        )\n    else:\n        log.info(\"Using cached features for training set\")\n        # With the current implementation cached features are not sorted\n        # Even though it doesn't seem to change anything\n        if train_features is None or train_labels is None:\n            raise AssertionError(\"Train features and labels must be provided when using cached data\")\n        permuted_indices = np.random.RandomState(seed=self.random_state).permutation(train_features.shape[0])\n        train_features = train_features[permuted_indices]\n        train_labels = train_labels[permuted_indices]\n\n    log.info(\"Fitting classifier on %d features\", len(train_features))  # type: ignore[arg-type]\n    self.classifier.fit(train_features, train_labels)\n</code></pre>"},{"location":"reference/quadra/trainers/classification.html#quadra.trainers.classification.SklearnClassificationTrainer.test","title":"<code>test(test_dataloader, test_labels=None, test_features=None, class_to_keep=None, idx_to_class=None, predict_proba=True, gradcam=False)</code>","text":"<p>Test classifier on test set.</p> <p>Parameters:</p> <ul> <li> test_dataloader             (<code>DataLoader</code>)         \u2013          <p>Test dataloader</p> </li> <li> test_labels             (<code>ndarray | None</code>, default:                 <code>None</code> )         \u2013          <p>test labels</p> </li> <li> test_features             (<code>ndarray | None</code>, default:                 <code>None</code> )         \u2013          <p>Optional test features used when cache data is available</p> </li> <li> class_to_keep             (<code>list[int] | None</code>, default:                 <code>None</code> )         \u2013          <p>list of class to keep</p> </li> <li> idx_to_class             (<code>dict[int, str] | None</code>, default:                 <code>None</code> )         \u2013          <p>dictionary mapping class index to class name</p> </li> <li> predict_proba             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>if True, predict also probability for each test image</p> </li> <li> gradcam             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to compute gradcam</p> </li> </ul> <p>Returns:</p> <ul> <li> cl_rep (            <code>tuple[str | dict, DataFrame, float, DataFrame, ndarray | None] | tuple[None, None, None, DataFrame, ndarray | None]</code> )        \u2013          <p>Classification report</p> </li> <li> pd_cm (            <code>tuple[str | dict, DataFrame, float, DataFrame, ndarray | None] | tuple[None, None, None, DataFrame, ndarray | None]</code> )        \u2013          <p>Confusion matrix dataframe</p> </li> <li> accuracy (            <code>tuple[str | dict, DataFrame, float, DataFrame, ndarray | None] | tuple[None, None, None, DataFrame, ndarray | None]</code> )        \u2013          <p>Test accuracy</p> </li> <li> res (            <code>tuple[str | dict, DataFrame, float, DataFrame, ndarray | None] | tuple[None, None, None, DataFrame, ndarray | None]</code> )        \u2013          <p>Test results</p> </li> <li> cams (            <code>tuple[str | dict, DataFrame, float, DataFrame, ndarray | None] | tuple[None, None, None, DataFrame, ndarray | None]</code> )        \u2013          <p>Gradcams</p> </li> </ul> Source code in <code>quadra/trainers/classification.py</code> <pre><code>def test(\n    self,\n    test_dataloader: DataLoader,\n    test_labels: ndarray | None = None,\n    test_features: ndarray | None = None,\n    class_to_keep: list[int] | None = None,\n    idx_to_class: dict[int, str] | None = None,\n    predict_proba: bool = True,\n    gradcam: bool = False,\n) -&gt; (\n    tuple[str | dict, DataFrame, float, DataFrame, np.ndarray | None]\n    | tuple[None, None, None, DataFrame, np.ndarray | None]\n):\n\"\"\"Test classifier on test set.\n\n    Args:\n        test_dataloader: Test dataloader\n        test_labels: test labels\n        test_features: Optional test features used when cache data is available\n        class_to_keep: list of class to keep\n        idx_to_class: dictionary mapping class index to class name\n        predict_proba: if True, predict also probability for each test image\n        gradcam: Whether to compute gradcam\n\n    Returns:\n        cl_rep: Classification report\n        pd_cm: Confusion matrix dataframe\n        accuracy: Test accuracy\n        res: Test results\n        cams: Gradcams\n    \"\"\"\n    cams = None\n    # Extract feature\n    if test_features is None:\n        log.info(\"Extracting features from test set\")\n        test_features, final_test_labels, cams = get_feature(\n            feature_extractor=self.backbone,\n            dl=test_dataloader,\n            gradcam=gradcam,\n            classifier=self.classifier,\n            input_shape=(self.input_shape[2], self.input_shape[0], self.input_shape[1]),\n        )\n    else:\n        if test_labels is None:\n            raise ValueError(\"Test labels must be provided when using cached data\")\n        log.info(\"Using cached features for test set\")\n        final_test_labels = test_labels\n\n    # Run classifier\n    log.info(\"Predict classifier on test set\")\n    test_prediction_label = self.classifier.predict(test_features)\n    if predict_proba:\n        test_probability = self.classifier.predict_proba(test_features)\n        test_probability = test_probability.max(axis=1)\n\n    if class_to_keep is not None:\n        if idx_to_class is None:\n            raise ValueError(\"You must provide `idx_to_class` and `test_labels` when using `class_to_keep`\")\n        filtered_test_labels = [int(x) if idx_to_class[x] in class_to_keep else -1 for x in final_test_labels]\n    else:\n        filtered_test_labels = cast(list[int], final_test_labels.tolist())\n\n    if not hasattr(test_dataloader.dataset, \"x\"):\n        raise ValueError(\"Current dataset doesn't provide an `x` attribute\")\n\n    res = pd.DataFrame(\n        {\n            \"sample\": list(test_dataloader.dataset.x),\n            \"real_label\": final_test_labels,\n            \"pred_label\": test_prediction_label,\n        }\n    )\n\n    if not all(t == -1 for t in filtered_test_labels):\n        test_real_label_cm = np.array(filtered_test_labels)\n        if cams is not None:\n            cams = cams[test_real_label_cm != -1]  # TODO: Is class_to_keep still used?\n        pred_labels_cm = np.array(test_prediction_label)[test_real_label_cm != -1]\n        test_real_label_cm = test_real_label_cm[test_real_label_cm != -1].astype(pred_labels_cm.dtype)\n        cl_rep, pd_cm, accuracy = get_results(test_real_label_cm, pred_labels_cm, idx_to_class)\n\n        if predict_proba:\n            res[\"probability\"] = test_probability\n\n        return cl_rep, pd_cm, accuracy, res, cams\n\n    return None, None, None, res, cams\n</code></pre>"},{"location":"reference/quadra/utils/index.html","title":"utils","text":""},{"location":"reference/quadra/utils/index.html#submodules","title":"Submodules","text":"<ul> <li>patch</li> <li>tests</li> </ul>"},{"location":"reference/quadra/utils/index.html#python-files","title":"Python Files","text":"<ul> <li>resolver.py</li> <li>mlflow.py</li> <li>anomaly.py</li> <li>deprecation.py</li> <li>segmentation.py</li> <li>classification.py</li> <li>logger.py</li> <li>validator.py</li> <li>utils.py</li> <li>evaluation.py</li> <li>visualization.py</li> <li>model_manager.py</li> <li>models.py</li> <li>imaging.py</li> <li>export.py</li> <li>vit_explainability.py </li> </ul>"},{"location":"reference/quadra/utils/anomaly.html","title":"anomaly","text":"<p>Anomaly Score Normalization Callback that uses min-max normalization.</p>"},{"location":"reference/quadra/utils/anomaly.html#quadra.utils.anomaly.ThresholdNormalizationCallback","title":"<code>ThresholdNormalizationCallback(threshold_type='pixel')</code>","text":"<p>             Bases: <code>Callback</code></p> <p>Callback that normalizes the image-level and pixel-level anomaly scores dividing by the threshold value.</p> <p>Parameters:</p> <ul> <li> threshold_type             (<code>str</code>, default:                 <code>'pixel'</code> )         \u2013          <p>Threshold used to normalize pixel level anomaly scores, either image or pixel (default)</p> </li> </ul> Source code in <code>quadra/utils/anomaly.py</code> <pre><code>def __init__(self, threshold_type: str = \"pixel\"):\n    super().__init__()\n    self.threshold_type = threshold_type\n</code></pre>"},{"location":"reference/quadra/utils/anomaly.html#quadra.utils.anomaly.ThresholdNormalizationCallback.on_predict_batch_end","title":"<code>on_predict_batch_end(trainer, pl_module, outputs, batch, batch_idx, dataloader_idx=0)</code>","text":"<p>Called when the predict batch ends, normalizes the predicted scores and anomaly maps.</p> Source code in <code>quadra/utils/anomaly.py</code> <pre><code>def on_predict_batch_end(\n    self,\n    trainer: pl.Trainer,\n    pl_module: AnomalyModule,\n    outputs: Any,\n    batch: Any,\n    batch_idx: int,\n    dataloader_idx: int = 0,\n) -&gt; None:\n\"\"\"Called when the predict batch ends, normalizes the predicted scores and anomaly maps.\"\"\"\n    del trainer, batch, batch_idx, dataloader_idx  # These variables are not used.\n\n    self._normalize_batch(outputs, pl_module)\n</code></pre>"},{"location":"reference/quadra/utils/anomaly.html#quadra.utils.anomaly.ThresholdNormalizationCallback.on_test_batch_end","title":"<code>on_test_batch_end(trainer, pl_module, outputs, batch, batch_idx, dataloader_idx=0)</code>","text":"<p>Called when the test batch ends, normalizes the predicted scores and anomaly maps.</p> Source code in <code>quadra/utils/anomaly.py</code> <pre><code>def on_test_batch_end(\n    self,\n    trainer: pl.Trainer,\n    pl_module: AnomalyModule,\n    outputs: STEP_OUTPUT | None,\n    batch: Any,\n    batch_idx: int,\n    dataloader_idx: int = 0,\n) -&gt; None:\n\"\"\"Called when the test batch ends, normalizes the predicted scores and anomaly maps.\"\"\"\n    del trainer, batch, batch_idx, dataloader_idx  # These variables are not used.\n\n    self._normalize_batch(outputs, pl_module)\n</code></pre>"},{"location":"reference/quadra/utils/anomaly.html#quadra.utils.anomaly.ThresholdNormalizationCallback.on_test_start","title":"<code>on_test_start(trainer, pl_module)</code>","text":"<p>Called when the test begins.</p> Source code in <code>quadra/utils/anomaly.py</code> <pre><code>def on_test_start(self, trainer: pl.Trainer, pl_module: AnomalyModule) -&gt; None:\n\"\"\"Called when the test begins.\"\"\"\n    del trainer  # `trainer` variable is not used.\n\n    for metric in (pl_module.image_metrics, pl_module.pixel_metrics):\n        if metric is not None:\n            metric.set_threshold(100.0)\n</code></pre>"},{"location":"reference/quadra/utils/anomaly.html#quadra.utils.anomaly.normalize_anomaly_score","title":"<code>normalize_anomaly_score(raw_score, threshold)</code>","text":"<p>Normalize anomaly score value or map based on threshold.</p> <p>Parameters:</p> <ul> <li> raw_score             (<code>MapOrValue</code>)         \u2013          <p>Raw anomaly score valure or map</p> </li> <li> threshold             (<code>float</code>)         \u2013          <p>Threshold for anomaly detection</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>MapOrValue</code>         \u2013          <p>Normalized anomaly score value or map clipped between 0 and 1000</p> </li> </ul> Source code in <code>quadra/utils/anomaly.py</code> <pre><code>def normalize_anomaly_score(raw_score: MapOrValue, threshold: float) -&gt; MapOrValue:\n\"\"\"Normalize anomaly score value or map based on threshold.\n\n    Args:\n        raw_score: Raw anomaly score valure or map\n        threshold: Threshold for anomaly detection\n\n    Returns:\n        Normalized anomaly score value or map clipped between 0 and 1000\n    \"\"\"\n    if threshold &gt; 0:\n        normalized_score = (raw_score / threshold) * 100.0\n    elif threshold == 0:\n        # TODO: Is this the best way to handle this case?\n        normalized_score = (raw_score + 1) * 100.0\n    else:\n        normalized_score = 200.0 - ((raw_score / threshold) * 100.0)\n\n    if isinstance(normalized_score, torch.Tensor):\n        return torch.clamp(normalized_score, 0.0, 1000.0)\n\n    return np.clip(normalized_score, 0.0, 1000.0)\n</code></pre>"},{"location":"reference/quadra/utils/classification.html","title":"classification","text":""},{"location":"reference/quadra/utils/classification.html#quadra.utils.classification.automatic_batch_size_computation","title":"<code>automatic_batch_size_computation(datamodule, backbone, starting_batch_size)</code>","text":"<p>Find the optimal batch size for feature extraction. This algorithm works from the largest batch size possible and divide by 2 until it finds the largest batch size that fits in memory.</p> <p>Parameters:</p> <ul> <li> datamodule             (<code>SklearnClassificationDataModule | PatchSklearnClassificationDataModule</code>)         \u2013          <p>Datamodule used for feature extraction</p> </li> <li> backbone             (<code>ModelSignatureWrapper</code>)         \u2013          <p>Backbone used for feature extraction</p> </li> <li> starting_batch_size             (<code>int</code>)         \u2013          <p>Starting batch size to use for the search</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>int</code>         \u2013          <p>Optimal batch size</p> </li> </ul> Source code in <code>quadra/utils/classification.py</code> <pre><code>def automatic_batch_size_computation(\n    datamodule: SklearnClassificationDataModule | PatchSklearnClassificationDataModule,\n    backbone: ModelSignatureWrapper,\n    starting_batch_size: int,\n) -&gt; int:\n\"\"\"Find the optimal batch size for feature extraction. This algorithm works from the largest batch size possible\n    and divide by 2 until it finds the largest batch size that fits in memory.\n\n    Args:\n        datamodule: Datamodule used for feature extraction\n        backbone: Backbone used for feature extraction\n        starting_batch_size: Starting batch size to use for the search\n\n    Returns:\n        Optimal batch size\n    \"\"\"\n    log.info(\"Finding optimal batch size...\")\n    optimal = False\n    batch_size = starting_batch_size\n\n    while not optimal:\n        datamodule.batch_size = batch_size\n        base_dataloader = datamodule.train_dataloader()\n\n        if isinstance(base_dataloader, Sequence):\n            base_dataloader = base_dataloader[0]\n\n        if len(base_dataloader) == 1:\n            # If it fits in memory this is the largest batch size possible\n            # If it crashes restart with the previous batch size // 2\n            datamodule.batch_size = len(base_dataloader.dataset)  # type: ignore[arg-type]\n            # New restarting batch size is the largest closest power of 2 to the dataset size, it will be divided by 2\n            batch_size = 2 ** math.ceil(math.log2(datamodule.batch_size))\n            base_dataloader = datamodule.train_dataloader()\n            if isinstance(base_dataloader, Sequence):\n                base_dataloader = base_dataloader[0]\n            optimal = True\n\n        try:\n            log.info(\"Trying batch size: %d\", datamodule.batch_size)\n            _ = get_feature(feature_extractor=backbone, dl=base_dataloader, iteration_over_training=1, limit_batches=1)\n        except RuntimeError as e:\n            if batch_size &gt; 1:\n                batch_size = batch_size // 2\n                optimal = False\n                continue\n\n            log.error(\"Unable to run the model with batch size 1\")\n            raise e\n\n        log.info(\"Found optimal batch size: %d\", datamodule.batch_size)\n        optimal = True\n\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\n    return datamodule.batch_size\n</code></pre>"},{"location":"reference/quadra/utils/classification.html#quadra.utils.classification.filter_with_file","title":"<code>filter_with_file(list_of_full_paths, file_path, root_path)</code>","text":"<p>Filter a list of items using a file containing the items to keep. Paths inside file should be relative to root_path not absolute to avoid user related issues.</p> <p>Parameters:</p> <ul> <li> list_of_full_paths             (<code>list[str]</code>)         \u2013          <p>list of items to filter</p> </li> <li> file_path             (<code>str</code>)         \u2013          <p>path to the file containing the items to keep</p> </li> <li> root_path             (<code>str</code>)         \u2013          <p>root path of the dataset</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[str]</code>         \u2013          <p>list of items to keep</p> </li> <li> <code>list[bool]</code>         \u2013          <p>the mask list to apply different lists later.</p> </li> </ul> Source code in <code>quadra/utils/classification.py</code> <pre><code>def filter_with_file(list_of_full_paths: list[str], file_path: str, root_path: str) -&gt; tuple[list[str], list[bool]]:\n\"\"\"Filter a list of items using a file containing the items to keep. Paths inside file\n    should be relative to root_path not absolute to avoid user related issues.\n\n    Args:\n        list_of_full_paths: list of items to filter\n        file_path: path to the file containing the items to keep\n        root_path: root path of the dataset\n\n    Returns:\n        list of items to keep\n        the mask list to apply different lists later.\n    \"\"\"\n    filtered_full_paths = []\n    filter_mask = []\n\n    with open(file_path) as f:\n        for relative_path in f.read().splitlines():\n            full_path = os.path.join(root_path, relative_path)\n            if full_path in list_of_full_paths:\n                filtered_full_paths.append(full_path)\n                filter_mask.append(True)\n            else:\n                filter_mask.append(False)\n\n    return filtered_full_paths, filter_mask\n</code></pre>"},{"location":"reference/quadra/utils/classification.html#quadra.utils.classification.find_images_and_targets","title":"<code>find_images_and_targets(folder, types=None, class_to_idx=None, leaf_name_only=True, sort=True, exclude_filter=None, include_filter=None, label_map=None)</code>","text":"<p>Given a folder, extract the absolute path of all the files with a valid extension. Then assign a label based on subfolder name.</p> <p>Parameters:</p> <ul> <li> folder             (<code>str</code>)         \u2013          <p>path to main folder</p> </li> <li> types             (<code>list | None</code>, default:                 <code>None</code> )         \u2013          <p>valid file extentions</p> </li> <li> class_to_idx             (<code>dict[str, int] | None</code>, default:                 <code>None</code> )         \u2013          <p>dictionary of conversion btw folder name and index. Only file whose label is in dictionary key list will be considered. If None all files will be considered and a custom conversion is created.</p> </li> <li> leaf_name_only             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>if True use only the leaf folder name as label, otherwise use the full path</p> </li> <li> sort             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>if True sort the images and labels based on the image name</p> </li> <li> exclude_filter             (<code>list | None</code>, default:                 <code>None</code> )         \u2013          <p>list of string filter to be used to exclude images. If None no filter will be applied.</p> </li> <li> include_filter             (<code>list | None</code>, default:                 <code>None</code> )         \u2013          <p>list of string filder to be used to include images. Only images that satisfied at list one of the filter will be included.</p> </li> <li> label_map             (<code>dict[str, Any] | None</code>, default:                 <code>None</code> )         \u2013          <p>dictionary of conversion btw folder name and label.</p> </li> </ul> Source code in <code>quadra/utils/classification.py</code> <pre><code>def find_images_and_targets(\n    folder: str,\n    types: list | None = None,\n    class_to_idx: dict[str, int] | None = None,\n    leaf_name_only: bool = True,\n    sort: bool = True,\n    exclude_filter: list | None = None,\n    include_filter: list | None = None,\n    label_map: dict[str, Any] | None = None,\n) -&gt; tuple[np.ndarray, np.ndarray, dict]:\n\"\"\"Given a folder, extract the absolute path of all the files with a valid extension.\n    Then assign a label based on subfolder name.\n\n    Args:\n        folder: path to main folder\n        types: valid file extentions\n        class_to_idx: dictionary of conversion btw folder name and index.\n            Only file whose label is in dictionary key list will be considered. If None all files will\n            be considered and a custom conversion is created.\n        leaf_name_only: if True use only the leaf folder name as label, otherwise use the full path\n        sort: if True sort the images and labels based on the image name\n        exclude_filter: list of string filter to be used to exclude images.\n            If None no filter will be applied.\n        include_filter: list of string filder to be used to include images.\n            Only images that satisfied at list one of the filter will be included.\n        label_map: dictionary of conversion btw folder name and label.\n    \"\"\"\n    if types is None:\n        types = [\".png\", \".jpg\", \".jpeg\", \".bmp\"]\n    labels = []\n    filenames = []\n\n    for root, _, files in os.walk(folder, topdown=False, followlinks=True):\n        if root != folder:\n            rel_path = os.path.relpath(root, folder)\n        else:\n            rel_path = \"\"\n\n        if leaf_name_only:\n            label = os.path.basename(rel_path)\n        else:\n            aa = rel_path.split(os.path.sep)\n            if len(aa) == 2:\n                aa = aa[-1:]\n            else:\n                aa = aa[-2:]\n            label = \"_\".join(aa)  # rel_path.replace(os.path.sep, \"_\")\n            # label = rel_path.replace(os.path.sep, \"_\")\n\n        for f in files:\n            if not get_file_condition(\n                file_name=f, root=root, exclude_filter=exclude_filter, include_filter=include_filter\n            ):\n                continue\n\n            if f.startswith(\".\") or \"checkpoint\" in f:\n                continue\n            _, ext = os.path.splitext(f)\n            if ext.lower() in types:\n                filenames.append(os.path.join(root, f))\n                labels.append(label)\n\n    if label_map is not None:\n        labels, _ = group_labels(labels, label_map)\n\n    if class_to_idx is None:\n        # building class index\n        unique_labels = set(labels)\n        sorted_labels = sorted(unique_labels, key=natural_key)\n        class_to_idx = {str(c): idx for idx, c in enumerate(sorted_labels)}\n\n    images_and_targets = [(f, l) for f, l in zip(filenames, labels) if l in class_to_idx]\n\n    if sort:\n        images_and_targets = sorted(images_and_targets, key=lambda k: natural_key(k[0]))\n\n    return np.array(images_and_targets)[:, 0], np.array(images_and_targets)[:, 1], class_to_idx\n</code></pre>"},{"location":"reference/quadra/utils/classification.html#quadra.utils.classification.find_test_image","title":"<code>find_test_image(folder, types=None, exclude_filter=None, include_filter=None, include_none_class=True, test_split_file=None, label_map=None)</code>","text":"<p>Given a path extract images and labels with filters, labels are based on the parent folder name of the images Args:     folder: root directory containing the images     types: only choose images with the extensions specified, if None use default extensions     exclude_filter: list of string filter to be used to exclude images. If None no filter will be applied.     include_filter: list of string filter to be used to include images. If None no filter will be applied.     include_none_class: if set to True convert all 'None' labels to None, otherwise ignore the image     test_split_file: if defined use the split defined inside the file Returns:     Two lists, one containing the images path and the other one containing the labels. Labels can be None.</p> Source code in <code>quadra/utils/classification.py</code> <pre><code>def find_test_image(\n    folder: str,\n    types: list[str] | None = None,\n    exclude_filter: list[str] | None = None,\n    include_filter: list[str] | None = None,\n    include_none_class: bool = True,\n    test_split_file: str | None = None,\n    label_map=None,\n) -&gt; tuple[list[str], list[str | None]]:\n\"\"\"Given a path extract images and labels with filters, labels are based on the parent folder name of the images\n    Args:\n        folder: root directory containing the images\n        types: only choose images with the extensions specified, if None use default extensions\n        exclude_filter: list of string filter to be used to exclude images. If None no filter will be applied.\n        include_filter: list of string filter to be used to include images. If None no filter will be applied.\n        include_none_class: if set to True convert all 'None' labels to None, otherwise ignore the image\n        test_split_file: if defined use the split defined inside the file\n    Returns:\n        Two lists, one containing the images path and the other one containing the labels. Labels can be None.\n    \"\"\"\n    if types is None:\n        types = [\".png\", \".jpg\", \".jpeg\", \".bmp\"]\n\n    labels = []\n    filenames = []\n\n    for root, _, files in os.walk(folder, topdown=False, followlinks=True):\n        rel_path = os.path.relpath(root, folder) if root != folder else \"\"\n        label: str | None = os.path.basename(rel_path)\n        for f in files:\n            if not get_file_condition(\n                file_name=f, root=root, exclude_filter=exclude_filter, include_filter=include_filter\n            ):\n                continue\n            if f.startswith(\".\") or \"checkpoint\" in f:\n                continue\n            _, ext = os.path.splitext(f)\n            if ext.lower() in types:\n                if label == \"None\":\n                    if include_none_class:\n                        label = None\n                    else:\n                        continue\n                filenames.append(os.path.join(root, f))\n                labels.append(label)\n\n    if test_split_file is not None:\n        if not os.path.isabs(test_split_file):\n            log.info(\n                \"test_split_file is not an absolute path. Trying to using folder argument %s as parent folder\", folder\n            )\n            test_split_file = os.path.join(folder, test_split_file)\n\n        if not os.path.exists(test_split_file):\n            raise FileNotFoundError(f\"test_split_file {test_split_file} does not exist\")\n\n        with open(test_split_file) as test_file:\n            test_split = test_file.read().splitlines()\n\n        file_samples = []\n        for row in test_split:\n            csv_values = row.split(\",\")\n            if len(csv_values) == 1:\n                # ensuring backward compatibility with old split file format\n                # old_format: sample, new_format: sample,class\n                sample_path = os.path.join(folder, csv_values[0])\n            else:\n                sample_path = os.path.join(folder, \",\".join(csv_values[:-1]))\n\n            file_samples.append(sample_path)\n\n        test_split = [os.path.join(folder, sample.strip()) for sample in file_samples]\n        labels = [t for s, t in zip(filenames, labels) if s in file_samples]\n        filenames = [s for s in filenames if s in file_samples]\n        log.info(\"Selected %d images using test_split_file for the test\", len(filenames))\n        if len(filenames) != len(file_samples):\n            log.warning(\n                \"test_split_file contains %d images but only %d images were found in the folder.\"\n                \"This may be due to duplicate lines in the test_split_file.\",\n                len(file_samples),\n                len(filenames),\n            )\n    else:\n        log.info(\"No test_split_file. Selected all %s images for the test\", folder)\n\n    if label_map is not None:\n        labels, _ = group_labels(labels, label_map)\n\n    return filenames, labels\n</code></pre>"},{"location":"reference/quadra/utils/classification.html#quadra.utils.classification.get_file_condition","title":"<code>get_file_condition(file_name, root, exclude_filter=None, include_filter=None)</code>","text":"<p>Check if a file should be included or excluded based on the filters provided.</p> <p>Parameters:</p> <ul> <li> file_name             (<code>str</code>)         \u2013          <p>Name of the file</p> </li> <li> root             (<code>str</code>)         \u2013          <p>Root directory of the file</p> </li> <li> exclude_filter             (<code>list[str] | None</code>, default:                 <code>None</code> )         \u2013          <p>List of string filter to be used to exclude images. If None no filter will be applied.</p> </li> <li> include_filter             (<code>list[str] | None</code>, default:                 <code>None</code> )         \u2013          <p>List of string filter to be used to include images. If None no filter will be applied.</p> </li> </ul> Source code in <code>quadra/utils/classification.py</code> <pre><code>def get_file_condition(\n    file_name: str, root: str, exclude_filter: list[str] | None = None, include_filter: list[str] | None = None\n):\n\"\"\"Check if a file should be included or excluded based on the filters provided.\n\n    Args:\n        file_name: Name of the file\n        root: Root directory of the file\n        exclude_filter: List of string filter to be used to exclude images. If None no filter will be applied.\n        include_filter: List of string filter to be used to include images. If None no filter will be applied.\n    \"\"\"\n    if exclude_filter is not None:\n        if any(fil in file_name for fil in exclude_filter):\n            return False\n\n        if any(fil in root for fil in exclude_filter):\n            return False\n\n    if include_filter is not None and (\n        not any(fil in file_name for fil in include_filter) and not any(fil in root for fil in include_filter)\n    ):\n        return False\n\n    return True\n</code></pre>"},{"location":"reference/quadra/utils/classification.html#quadra.utils.classification.get_results","title":"<code>get_results(test_labels, pred_labels, idx_to_labels=None, cl_rep_digits=3)</code>","text":"<p>Get prediction results from predicted and test labels.</p> <p>Parameters:</p> <ul> <li> test_labels         \u2013          <p>test labels</p> </li> <li> pred_labels         \u2013          <p>predicted labels</p> </li> <li> idx_to_labels         \u2013          <p>dictionary mapping indices to labels</p> </li> <li> cl_rep_digits         \u2013          <p>number of digits to use in the classification report. Default: 3</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str | dict</code>         \u2013          <p>A tuple that contains classification report as dictionary, <code>cm</code> is a pd.Dataframe representing</p> </li> <li> <code>DataFrame</code>         \u2013          <p>the Confusion Matrix, acc is the computed accuracy</p> </li> </ul> Source code in <code>quadra/utils/classification.py</code> <pre><code>def get_results(\n    test_labels: np.ndarray | list[int],\n    pred_labels: np.ndarray | list[int],\n    idx_to_labels: dict | None = None,\n    cl_rep_digits: int = 3,\n) -&gt; tuple[str | dict, pd.DataFrame, float]:\n\"\"\"Get prediction results from predicted and test labels.\n\n    Args:\n        test_labels : test labels\n        pred_labels : predicted labels\n        idx_to_labels : dictionary mapping indices to labels\n        cl_rep_digits : number of digits to use in the classification report. Default: 3\n\n    Returns:\n        A tuple that contains classification report as dictionary, `cm` is a pd.Dataframe representing\n        the Confusion Matrix, acc is the computed accuracy\n    \"\"\"\n    unique_labels = np.unique([test_labels, pred_labels])\n    cl_rep = classification_report(\n        y_true=test_labels,\n        y_pred=pred_labels,\n        labels=unique_labels,\n        digits=cl_rep_digits,\n        zero_division=0,\n    )\n\n    cm = confusion_matrix(y_true=test_labels, y_pred=pred_labels, labels=unique_labels)\n\n    acc = accuracy_score(y_true=test_labels, y_pred=pred_labels)\n\n    if idx_to_labels:\n        pd_cm = pd.DataFrame(\n            cm,\n            index=[f\"true:{idx_to_labels[x]}\" for x in unique_labels],\n            columns=[f\"pred:{idx_to_labels[x]}\" for x in unique_labels],\n        )\n    else:\n        pd_cm = pd.DataFrame(\n            cm,\n            index=[f\"true:{x}\" for x in unique_labels],\n            columns=[f\"pred:{x}\" for x in unique_labels],\n        )\n    return cl_rep, pd_cm, acc\n</code></pre>"},{"location":"reference/quadra/utils/classification.html#quadra.utils.classification.get_split","title":"<code>get_split(image_dir, exclude_filter=None, include_filter=None, test_size=0.3, random_state=42, class_to_idx=None, label_map=None, n_splits=1, include_none_class=False, limit_training_data=None, train_split_file=None)</code>","text":"<p>Given a folder, extract the absolute path of all the files with a valid extension and name and split them into train/test.</p> <p>Parameters:</p> <ul> <li> image_dir             (<code>str</code>)         \u2013          <p>Path to the folder containing the images</p> </li> <li> exclude_filter             (<code>list[str] | None</code>, default:                 <code>None</code> )         \u2013          <p>List of file name filter to be excluded: If None no filter will be applied</p> </li> <li> include_filter             (<code>list[str] | None</code>, default:                 <code>None</code> )         \u2013          <p>List of file name filter to be included: If None no filter will be applied</p> </li> <li> test_size             (<code>float</code>, default:                 <code>0.3</code> )         \u2013          <p>Percentage of data to be used for test</p> </li> <li> random_state             (<code>int</code>, default:                 <code>42</code> )         \u2013          <p>Random state to be used for reproducibility</p> </li> <li> class_to_idx             (<code>dict[str, int] | None</code>, default:                 <code>None</code> )         \u2013          <p>Dictionary of conversion btw folder name and index. Only file whose label is in dictionary key list will be considered. If None all files will be considered and a custom conversion is created.</p> </li> <li> label_map             (<code>dict | None</code>, default:                 <code>None</code> )         \u2013          <p>Dictionary of conversion btw folder name and label.</p> </li> <li> n_splits             (<code>int</code>, default:                 <code>1</code> )         \u2013          <p>Number of dataset subdivision (default 1 -&gt; train/test)</p> </li> <li> include_none_class             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>If set to True convert all 'None' labels to None</p> </li> <li> limit_training_data             (<code>int | None</code>, default:                 <code>None</code> )         \u2013          <p>If set to a value, limit the number of training samples to this value</p> </li> <li> train_split_file             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>If set to a path, use the file to split the dataset</p> </li> </ul> Source code in <code>quadra/utils/classification.py</code> <pre><code>def get_split(\n    image_dir: str,\n    exclude_filter: list[str] | None = None,\n    include_filter: list[str] | None = None,\n    test_size: float = 0.3,\n    random_state: int = 42,\n    class_to_idx: dict[str, int] | None = None,\n    label_map: dict | None = None,\n    n_splits: int = 1,\n    include_none_class: bool = False,\n    limit_training_data: int | None = None,\n    train_split_file: str | None = None,\n) -&gt; tuple[np.ndarray, np.ndarray, Generator[list, None, None], dict]:\n\"\"\"Given a folder, extract the absolute path of all the files with a valid extension and name\n    and split them into train/test.\n\n    Args:\n        image_dir: Path to the folder containing the images\n        exclude_filter: List of file name filter to be excluded: If None no filter will be applied\n        include_filter: List of file name filter to be included: If None no filter will be applied\n        test_size: Percentage of data to be used for test\n        random_state: Random state to be used for reproducibility\n        class_to_idx: Dictionary of conversion btw folder name and index.\n            Only file whose label is in dictionary key list will be considered.\n            If None all files will be considered and a custom conversion is created.\n        label_map: Dictionary of conversion btw folder name and label.\n        n_splits: Number of dataset subdivision (default 1 -&gt; train/test)\n        include_none_class: If set to True convert all 'None' labels to None\n        limit_training_data: If set to a value, limit the number of training samples to this value\n        train_split_file: If set to a path, use the file to split the dataset\n    \"\"\"\n    # TODO: Why is include_none_class not used?\n    # pylint: disable=unused-argument\n    assert os.path.isdir(image_dir), f\"Folder {image_dir} does not exist.\"\n    # Get samples and target\n    samples, targets, class_to_idx = find_images_and_targets(\n        folder=image_dir,\n        exclude_filter=exclude_filter,\n        include_filter=include_filter,\n        class_to_idx=class_to_idx,\n        label_map=label_map,\n        # include_none_class=include_none_class,\n    )\n\n    cl, counts = np.unique(targets, return_counts=True)\n\n    for num, _cl in zip(counts, cl):\n        if num == 1:\n            to_remove = np.where(np.array(targets) == _cl)[0][0]\n            samples = np.delete(np.array(samples), to_remove)\n            targets = np.delete(np.array(targets), to_remove)\n            class_to_idx.pop(_cl)\n\n    if train_split_file is not None:\n        with open(train_split_file) as f:\n            train_split = f.read().splitlines()\n\n        file_samples = []\n        for row in train_split:\n            csv_values = row.split(\",\")\n\n            if len(csv_values) == 1:\n                # ensuring backward compatibility with the old split file format\n                # old_format: sample, new_format: sample,class\n                sample_path = os.path.join(image_dir, csv_values[0])\n            else:\n                sample_path = os.path.join(image_dir, \",\".join(csv_values[:-1]))\n\n            file_samples.append(sample_path)\n\n        train_split = [os.path.join(image_dir, sample.strip()) for sample in file_samples]\n        targets = np.array([t for s, t in zip(samples, targets) if s in file_samples])\n        samples = np.array([s for s in samples if s in file_samples])\n\n    if limit_training_data is not None:\n        idx_to_keep = []\n        for cl in np.unique(targets):\n            cl_idx = np.where(np.array(targets) == cl)[0].tolist()\n            random.seed(random_state)\n            random.shuffle(cl_idx)\n            idx_to_keep.extend(cl_idx[:limit_training_data])\n\n        samples = np.asarray([samples[i] for i in idx_to_keep])\n        targets = np.asarray([targets[i] for i in idx_to_keep])\n\n    _, counts = np.unique(targets, return_counts=True)\n\n    if n_splits == 1:\n        split_technique = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n    else:\n        split_technique = StratifiedKFold(n_splits=n_splits, random_state=random_state, shuffle=True)\n\n    split = split_technique.split(samples, targets)\n\n    return np.array(samples), np.array(targets), split, class_to_idx\n</code></pre>"},{"location":"reference/quadra/utils/classification.html#quadra.utils.classification.group_labels","title":"<code>group_labels(labels, class_mapping)</code>","text":"<p>Group labels based on class_mapping.</p> <p>Raises:</p> <ul> <li> <code>ValueError</code>           \u2013          <p>if a label is not in class_mapping</p> </li> <li> <code>ValueError</code>           \u2013          <p>if a label is in class_mapping but has no corresponding value</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[list, dict]</code>         \u2013          <p>List of labels and a dictionary of labels and their corresponding group</p> </li> </ul> Example <pre><code>grouped_labels, class_to_idx = group_labels(labels, class_mapping={\"Good\": \"A\", \"Bad\": None})\nassert grouped_labels.count(\"Good\") == labels.count(\"A\")\nassert len(class_to_idx.keys()) == 2\n\ngrouped_labels, class_to_idx = group_labels(labels, class_mapping={\"Good\": \"A\", \"Defect\": \"B\", \"Bad\": None})\nassert grouped_labels.count(\"Bad\") == labels.count(\"C\") + labels.count(\"D\")\nassert len(class_to_idx.keys()) == 3\n\ngrouped_labels, class_to_idx = group_labels(labels, class_mapping={\"Good\": \"A\", \"Bad\": [\"B\", \"C\", \"D\"]})\nassert grouped_labels.count(\"Bad\") == labels.count(\"B\") + labels.count(\"C\") + labels.count(\"D\")\nassert len(class_to_idx.keys()) == 2\n</code></pre> Source code in <code>quadra/utils/classification.py</code> <pre><code>def group_labels(labels: Sequence[str | None], class_mapping: dict[str, str | None | list[str]]) -&gt; tuple[list, dict]:\n\"\"\"Group labels based on class_mapping.\n\n    Raises:\n        ValueError: if a label is not in class_mapping\n        ValueError: if a label is in class_mapping but has no corresponding value\n\n    Returns:\n       List of labels and a dictionary of labels and their corresponding group\n\n    Example:\n        ```python\n        grouped_labels, class_to_idx = group_labels(labels, class_mapping={\"Good\": \"A\", \"Bad\": None})\n        assert grouped_labels.count(\"Good\") == labels.count(\"A\")\n        assert len(class_to_idx.keys()) == 2\n\n        grouped_labels, class_to_idx = group_labels(labels, class_mapping={\"Good\": \"A\", \"Defect\": \"B\", \"Bad\": None})\n        assert grouped_labels.count(\"Bad\") == labels.count(\"C\") + labels.count(\"D\")\n        assert len(class_to_idx.keys()) == 3\n\n        grouped_labels, class_to_idx = group_labels(labels, class_mapping={\"Good\": \"A\", \"Bad\": [\"B\", \"C\", \"D\"]})\n        assert grouped_labels.count(\"Bad\") == labels.count(\"B\") + labels.count(\"C\") + labels.count(\"D\")\n        assert len(class_to_idx.keys()) == 2\n        ```\n    \"\"\"\n    grouped_labels = []\n    specified_targets = [k for k in class_mapping if class_mapping[k] is not None]\n    non_specified_targets = [k for k in class_mapping if class_mapping[k] is None]\n    if len(non_specified_targets) &gt; 1:\n        raise ValueError(f\"More than one non specified target: {non_specified_targets}\")\n    for label in labels:\n        found = False\n        for target in specified_targets:\n            if not found:\n                current_mapping = class_mapping[target]\n                if current_mapping is None:\n                    continue\n\n                if any(label in list(related_label) for related_label in current_mapping if related_label is not None):\n                    grouped_labels.append(target)\n                    found = True\n        if not found:\n            if len(non_specified_targets) &gt; 0:\n                grouped_labels.append(non_specified_targets[0])\n            else:\n                raise ValueError(f\"No target found for label: {label}\")\n    class_to_idx = {k: i for i, k in enumerate(class_mapping.keys())}\n    return grouped_labels, class_to_idx\n</code></pre>"},{"location":"reference/quadra/utils/classification.html#quadra.utils.classification.natural_key","title":"<code>natural_key(string_)</code>","text":"<p>See http://www.codinghorror.com/blog/archives/001018.html.</p> Source code in <code>quadra/utils/classification.py</code> <pre><code>def natural_key(string_):\n\"\"\"See http://www.codinghorror.com/blog/archives/001018.html.\"\"\"\n    return [int(s) if s.isdigit() else s for s in re.split(r\"(\\d+)\", string_.lower())]\n</code></pre>"},{"location":"reference/quadra/utils/classification.html#quadra.utils.classification.save_classification_result","title":"<code>save_classification_result(results, output_folder, test_dataloader, config, output, accuracy=None, confmat=None, grayscale_cams=None)</code>","text":"<p>Save csv results, confusion matrix and example images.</p> <p>Parameters:</p> <ul> <li> results             (<code>DataFrame</code>)         \u2013          <p>Dataframe containing the results</p> </li> <li> output_folder             (<code>str</code>)         \u2013          <p>Path to the output folder</p> </li> <li> confmat             (<code>DataFrame | None</code>, default:                 <code>None</code> )         \u2013          <p>Confusion matrix in a pandas dataframe, may be None if all test labels are unknown</p> </li> <li> accuracy             (<code>float | None</code>, default:                 <code>None</code> )         \u2013          <p>Accuracy of the model, is None if all test labels are unknown</p> </li> <li> test_dataloader             (<code>DataLoader</code>)         \u2013          <p>Dataloader used for testing</p> </li> <li> config             (<code>DictConfig</code>)         \u2013          <p>Configuration file</p> </li> <li> output             (<code>DictConfig</code>)         \u2013          <p>Output configuration</p> </li> <li> grayscale_cams             (<code>ndarray | None</code>, default:                 <code>None</code> )         \u2013          <p>List of grayscale grad_cam outputs ordered as the results</p> </li> </ul> Source code in <code>quadra/utils/classification.py</code> <pre><code>def save_classification_result(\n    results: pd.DataFrame,\n    output_folder: str,\n    test_dataloader: DataLoader,\n    config: DictConfig,\n    output: DictConfig,\n    accuracy: float | None = None,\n    confmat: pd.DataFrame | None = None,\n    grayscale_cams: np.ndarray | None = None,\n):\n\"\"\"Save csv results, confusion matrix and example images.\n\n    Args:\n        results: Dataframe containing the results\n        output_folder: Path to the output folder\n        confmat: Confusion matrix in a pandas dataframe, may be None if all test labels are unknown\n        accuracy: Accuracy of the model, is None if all test labels are unknown\n        test_dataloader: Dataloader used for testing\n        config: Configuration file\n        output: Output configuration\n        grayscale_cams: List of grayscale grad_cam outputs ordered as the results\n    \"\"\"\n    # Save csv\n    results.to_csv(os.path.join(output_folder, \"test_results.csv\"), index_label=\"index\")\n    if grayscale_cams is None:\n        log.info(\"Plotting only original examples, set gradcam = true in config file to also plot gradcam examples\")\n\n        save_gradcams = False\n    else:\n        log.info(\"Plotting original and gradcam examples\")\n        save_gradcams = True\n\n    if confmat is not None and accuracy is not None:\n        # Save confusion matrix\n        disp = ConfusionMatrixDisplay(\n            confusion_matrix=np.array(confmat),\n            display_labels=[x.replace(\"pred:\", \"\") for x in confmat.columns.to_list()],\n        )\n        disp.plot(include_values=True, cmap=plt.cm.Greens, ax=None, colorbar=False, xticks_rotation=90)\n        plt.title(f\"Confusion Matrix (Accuracy: {(accuracy * 100):.2f}%)\")\n        plt.savefig(\n            os.path.join(output_folder, \"test_confusion_matrix.png\"),\n            bbox_inches=\"tight\",\n            pad_inches=0,\n            dpi=300,\n        )\n        plt.close()\n\n    if output is not None and output.example:\n        log.info(\"Saving discordant/concordant examples in test folder\")\n        idx_to_class = test_dataloader.dataset.idx_to_class  # type: ignore[attr-defined]\n\n        # Get misclassified samples\n        images_folder = os.path.join(output_folder, \"example\")\n        if not os.path.isdir(images_folder):\n            os.makedirs(images_folder)\n        original_images_folder = os.path.join(images_folder, \"original\")\n        if not os.path.isdir(original_images_folder):\n            os.makedirs(original_images_folder)\n\n        gradcam_folder = os.path.join(images_folder, \"gradcam\")\n        if save_gradcams and not os.path.isdir(gradcam_folder):\n            os.makedirs(gradcam_folder)\n\n        for v in np.unique([results[\"real_label\"], results[\"pred_label\"]]):\n            if np.isnan(v) or v == -1:\n                continue\n\n            k = idx_to_class[v]\n            plot_classification_results(\n                test_dataloader.dataset,\n                unorm=UnNormalize(mean=config.transforms.mean, std=config.transforms.std),\n                pred_labels=results[\"pred_label\"].to_numpy(),\n                test_labels=results[\"real_label\"].to_numpy(),\n                grayscale_cams=grayscale_cams,\n                class_name=k,\n                original_folder=original_images_folder,\n                gradcam_folder=gradcam_folder,\n                idx_to_class=idx_to_class,\n                pred_class_to_plot=v,\n                what=\"con\",\n                rows=output.get(\"rows\", 3),\n                cols=output.get(\"cols\", 2),\n                figsize=output.get(\"figsize\", (20, 20)),\n                gradcam=save_gradcams,\n            )\n\n            plot_classification_results(\n                test_dataloader.dataset,\n                unorm=UnNormalize(mean=config.transforms.mean, std=config.transforms.std),\n                pred_labels=results[\"pred_label\"].to_numpy(),\n                test_labels=results[\"real_label\"].to_numpy(),\n                grayscale_cams=grayscale_cams,\n                class_name=k,\n                original_folder=original_images_folder,\n                gradcam_folder=gradcam_folder,\n                idx_to_class=idx_to_class,\n                pred_class_to_plot=v,\n                what=\"dis\",\n                rows=output.get(\"rows\", 3),\n                cols=output.get(\"cols\", 2),\n                figsize=output.get(\"figsize\", (20, 20)),\n                gradcam=save_gradcams,\n            )\n\n    else:\n        log.info(\"Not generating discordant/concordant examples. Check task.output.example in config file\")\n</code></pre>"},{"location":"reference/quadra/utils/deprecation.html","title":"deprecation","text":""},{"location":"reference/quadra/utils/deprecation.html#quadra.utils.deprecation.deprecated","title":"<code>deprecated(message)</code>","text":"<p>Decorator to mark a function as deprecated.</p> <p>Parameters:</p> <ul> <li> message             (<code>str</code>)         \u2013          <p>Message to be displayed when the function is called.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Callable</code>         \u2013          <p>Decoratored function.</p> </li> </ul> Source code in <code>quadra/utils/deprecation.py</code> <pre><code>def deprecated(message: str) -&gt; Callable:\n\"\"\"Decorator to mark a function as deprecated.\n\n    Args:\n        message: Message to be displayed when the function is called.\n\n    Returns:\n        Decoratored function.\n    \"\"\"\n\n    def deprecated_decorator(func_or_class: Callable) -&gt; Callable:\n\"\"\"Decorator to mark a function as deprecated.\"\"\"\n\n        @functools.wraps(func_or_class)\n        def wrapper(*args, **kwargs):\n\"\"\"Wrapper function to display a warning message.\"\"\"\n            warning_msg = f\"{func_or_class.__name__} is deprecated. {message}\"\n            logger.warning(warning_msg)\n            return func_or_class(*args, **kwargs)\n\n        return wrapper\n\n    return deprecated_decorator\n</code></pre>"},{"location":"reference/quadra/utils/evaluation.html","title":"evaluation","text":""},{"location":"reference/quadra/utils/evaluation.html#quadra.utils.evaluation.automatic_datamodule_batch_size","title":"<code>automatic_datamodule_batch_size(batch_size_attribute_name='batch_size')</code>","text":"<p>Automatically scale the datamodule batch size if the given function goes out of memory.</p> <p>Parameters:</p> <ul> <li> batch_size_attribute_name             (<code>str</code>, default:                 <code>'batch_size'</code> )         \u2013          <p>The name of the attribute to modify in the datamodule</p> </li> </ul> Source code in <code>quadra/utils/evaluation.py</code> <pre><code>def automatic_datamodule_batch_size(batch_size_attribute_name: str = \"batch_size\"):\n\"\"\"Automatically scale the datamodule batch size if the given function goes out of memory.\n\n    Args:\n        batch_size_attribute_name: The name of the attribute to modify in the datamodule\n    \"\"\"\n\n    def decorator(func: Callable):\n\"\"\"Decorator function.\"\"\"\n\n        @wraps(func)\n        def wrapper(self, *args, **kwargs):\n\"\"\"Wrapper function.\"\"\"\n            is_func_finished = False\n            starting_batch_size = None\n            automatic_batch_size_completed = False\n\n            if hasattr(self, \"automatic_batch_size_completed\"):\n                automatic_batch_size_completed = self.automatic_batch_size_completed\n\n            if hasattr(self, \"automatic_batch_size\"):\n                if not hasattr(self.automatic_batch_size, \"disable\") or not hasattr(\n                    self.automatic_batch_size, \"starting_batch_size\"\n                ):\n                    raise ValueError(\n                        \"The automatic_batch_size attribute should have the disable and starting_batch_size attributes\"\n                    )\n                starting_batch_size = (\n                    self.automatic_batch_size.starting_batch_size if not self.automatic_batch_size.disable else None\n                )\n\n            if starting_batch_size is not None and not automatic_batch_size_completed:\n                # If we already tried to reduce the batch size, we will start from the last batch size\n                log.info(\"Performing automatic batch size scaling from %d\", starting_batch_size)\n                setattr(self.datamodule, batch_size_attribute_name, starting_batch_size)\n\n            while not is_func_finished:\n                valid_exceptions = (RuntimeError,)\n\n                if ONNX_AVAILABLE:\n                    valid_exceptions += (RuntimeException,)\n\n                try:\n                    func(self, *args, **kwargs)\n                    is_func_finished = True\n                    self.automatic_batch_size_completed = True\n                    if torch.cuda.is_available():\n                        torch.cuda.empty_cache()\n                except valid_exceptions as e:\n                    current_batch_size = getattr(self.datamodule, batch_size_attribute_name)\n                    setattr(self.datamodule, batch_size_attribute_name, current_batch_size // 2)\n                    log.warning(\n                        \"The function %s went out of memory, trying to reduce the batch size to %d\",\n                        func.__name__,\n                        self.datamodule.batch_size,\n                    )\n\n                    if self.datamodule.batch_size == 0:\n                        raise RuntimeError(\n                            f\"Unable to run {func.__name__} with batch size 1, the program will exit\"\n                        ) from e\n\n                    if torch.cuda.is_available():\n                        torch.cuda.empty_cache()\n\n        return wrapper\n\n    return decorator\n</code></pre>"},{"location":"reference/quadra/utils/evaluation.html#quadra.utils.evaluation.calculate_mask_based_metrics","title":"<code>calculate_mask_based_metrics(images, th_masks, th_preds, threshold=0.5, show_orj_predictions=False, metric=score_dice, multilabel=False, n_classes=None)</code>","text":"<p>Calculate metrics based on masks and predictions.</p> <p>Parameters:</p> <ul> <li> images             (<code>ndarray</code>)         \u2013          <p>Images.</p> </li> <li> th_masks             (<code>Tensor</code>)         \u2013          <p>masks are tensors.</p> </li> <li> th_preds             (<code>Tensor</code>)         \u2013          <p>predictions are tensors.</p> </li> <li> threshold             (<code>float</code>, default:                 <code>0.5</code> )         \u2013          <p>Threshold to apply. Defaults to 0.5.</p> </li> <li> show_orj_predictions             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Flag to show original predictions. Defaults to False.</p> </li> <li> metric             (<code>Callable</code>, default:                 <code>score_dice</code> )         \u2013          <p>Metric to use comparison. Defaults to <code>score_dice</code>.</p> </li> <li> multilabel             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>True if segmentation is multiclass.</p> </li> <li> n_classes             (<code>int | None</code>, default:                 <code>None</code> )         \u2013          <p>Number of classes. If multilabel is False, this should be None.</p> </li> </ul> <p>Returns:</p> <ul> <li> dict (            <code>tuple[dict[str, float], dict[str, list[ndarray]], dict[str, list[ndarray]], dict[str, list[str | float]]]</code> )        \u2013          <p>Dictionary with metrics.</p> </li> </ul> Source code in <code>quadra/utils/evaluation.py</code> <pre><code>def calculate_mask_based_metrics(\n    images: np.ndarray,\n    th_masks: torch.Tensor,\n    th_preds: torch.Tensor,\n    threshold: float = 0.5,\n    show_orj_predictions: bool = False,\n    metric: Callable = score_dice,\n    multilabel: bool = False,\n    n_classes: int | None = None,\n) -&gt; tuple[\n    dict[str, float],\n    dict[str, list[np.ndarray]],\n    dict[str, list[np.ndarray]],\n    dict[str, list[str | float]],\n]:\n\"\"\"Calculate metrics based on masks and predictions.\n\n    Args:\n        images: Images.\n        th_masks: masks are tensors.\n        th_preds: predictions are tensors.\n        threshold: Threshold to apply. Defaults to 0.5.\n        show_orj_predictions: Flag to show original predictions. Defaults to False.\n        metric: Metric to use comparison. Defaults to `score_dice`.\n        multilabel: True if segmentation is multiclass.\n        n_classes: Number of classes. If multilabel is False, this should be None.\n\n    Returns:\n        dict: Dictionary with metrics.\n    \"\"\"\n    masks = th_masks.cpu().numpy()\n    preds = th_preds.squeeze(0).cpu().numpy()\n    th_thresh_preds = (th_preds &gt; threshold).float().cpu()\n    thresh_preds = th_thresh_preds.squeeze(0).numpy()\n    dice_scores = metric(th_thresh_preds, th_masks, reduction=None).numpy()\n    result = {}\n    if multilabel:\n        if n_classes is None:\n            raise ValueError(\"n_classes arg shouldn't be None when multilabel is True\")\n        preds_multilabel = (\n            torch.nn.functional.one_hot(th_preds.to(torch.int64), num_classes=n_classes).squeeze(1).permute(0, 3, 1, 2)\n        )\n        masks_multilabel = (\n            torch.nn.functional.one_hot(th_masks.to(torch.int64), num_classes=n_classes).squeeze(1).permute(0, 3, 1, 2)\n        ).to(preds_multilabel.device)\n        # get_stats multiclass, not considering background channel\n        tp, fp, fn, tn = smp.metrics.get_stats(\n            preds_multilabel[:, 1:, :, :].long(), masks_multilabel[:, 1:, :, :].long(), mode=\"multilabel\"\n        )\n    else:\n        tp, fp, fn, tn = smp.metrics.get_stats(th_thresh_preds.long(), th_masks.long(), mode=\"binary\")\n    per_image_iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro-imagewise\")\n    dataset_iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\")\n    result[\"F1_image\"] = round(float(smp.metrics.f1_score(tp, fp, fn, tn, reduction=\"micro-imagewise\").item()), 4)\n    result[\"F1_pixel\"] = round(float(smp.metrics.f1_score(tp, fp, fn, tn, reduction=\"micro\").item()), 4)\n    result[\"image_iou\"] = round(float(per_image_iou.item()), 4) if not per_image_iou.isnan() else np.nan\n    result[\"dataset_iou\"] = round(float(dataset_iou.item()), 4) if not dataset_iou.isnan() else np.nan\n    result[\"TP_pixel\"] = tp.sum().item()\n    result[\"FP_pixel\"] = fp.sum().item()\n    result[\"FN_pixel\"] = fn.sum().item()\n    result[\"TN_pixel\"] = tn.sum().item()\n    result[\"TP_image\"] = 0\n    result[\"FP_image\"] = 0\n    result[\"FN_image\"] = 0\n    result[\"TN_image\"] = 0\n    result[\"num_good_image\"] = 0\n    result[\"num_bad_image\"] = 0\n    bad_dice, good_dice = [], []\n    fg: dict[str, list[np.ndarray]] = {\"image\": [], \"mask\": [], \"thresh_pred\": []}\n    fb: dict[str, list[np.ndarray]] = {\"image\": [], \"mask\": [], \"thresh_pred\": []}\n    if show_orj_predictions:\n        fg[\"pred\"] = []\n        fb[\"pred\"] = []\n\n    area_graph: dict[str, list[str | float]] = {\n        \"Defect Area Percentage\": [],\n        \"Accuracy\": [],\n    }\n    for idx, (image, pred, mask, thresh_pred, dice_score) in enumerate(\n        zip(images, preds, masks, thresh_preds, dice_scores)\n    ):\n        if np.sum(mask) == 0:\n            good_dice.append(dice_score)\n        else:\n            bad_dice.append(dice_score)\n        if mask.sum() &gt; 0:\n            result[\"num_bad_image\"] += 1\n            if thresh_pred.sum() == 0:\n                result[\"FN_image\"] += 1\n                fg[\"image\"].append(image)\n                fg[\"mask\"].append(mask)\n                if show_orj_predictions:\n                    fg[\"pred\"].append(pred)\n                fg[\"thresh_pred\"].append(thresh_pred)\n            else:\n                result[\"TP_image\"] += 1\n            rp = regionprops(label(mask[0]))\n            for r in rp:\n                mask_partial = th_masks[idx, :, r.bbox[0] : r.bbox[2], r.bbox[1] : r.bbox[3]]\n                pred_partial = th_thresh_preds[idx, :, r.bbox[0] : r.bbox[2], r.bbox[1] : r.bbox[3]]\n                tp, fp, fn, tn = smp.metrics.get_stats(pred_partial.long(), mask_partial.long(), mode=\"binary\")\n                area = tp + fn\n                area_percentage = area.sum().item() * 100 / (image.shape[0] * image.shape[1])\n                defect_acc = smp.metrics.accuracy(tp, fp, fn, tn, reduction=\"micro\")\n                area_graph[\"Accuracy\"].append(defect_acc.item() * 100)\n                if area_percentage &lt;= 1:\n                    area_graph[\"Defect Area Percentage\"].append(\"Very Small &lt;1%\")\n                elif area_percentage &lt;= 10:\n                    area_graph[\"Defect Area Percentage\"].append(\"Small &lt;10%\")\n                elif area_percentage &lt;= 25:\n                    area_graph[\"Defect Area Percentage\"].append(\"Medium &lt;25%\")\n                else:\n                    area_graph[\"Defect Area Percentage\"].append(\"Large &gt;25%\")\n\n        if mask.sum() == 0:\n            result[\"num_good_image\"] += 1\n            if thresh_pred.sum() &gt; 0:\n                result[\"FP_image\"] += 1\n                fb[\"image\"].append(image)\n                fb[\"mask\"].append(mask)\n                if show_orj_predictions:\n                    fb[\"pred\"].append(pred)\n                fb[\"thresh_pred\"].append(thresh_pred)\n            else:\n                result[\"TN_image\"] += 1\n    result[\"bad_dice_score_mean\"] = np.mean(bad_dice) if len(bad_dice) &gt; 0 else \"null\"\n    result[\"bad_dice_score_std\"] = np.std(bad_dice) if len(bad_dice) &gt; 0 else \"null\"\n    result[\"good_dice_score_mean\"] = np.mean(good_dice) if len(good_dice) &gt; 0 else \"null\"\n    result[\"good_dice_score_std\"] = np.std(good_dice) if len(good_dice) &gt; 0 else \"null\"\n    return result, fg, fb, area_graph\n</code></pre>"},{"location":"reference/quadra/utils/evaluation.html#quadra.utils.evaluation.create_mask_report","title":"<code>create_mask_report(stage, output, mean, std, report_path, nb_samples=6, analysis=False, apply_sigmoid=True, show_all=False, threshold=0.5, metric=score_dice, show_orj_predictions=False)</code>","text":"<p>Create report for segmentation experiment Args:     stage: stage name. Train, validation or test     output: data produced by model     report_path: experiment path     mean: mean values     std: std values     nb_samples: number of samples     analysis: if True, analysis will be created     apply_sigmoid: if True, sigmoid will be applied to predictions     show_all: if True, all images will be shown     threshold: threshold for predictions     metric: metric function     show_orj_predictions: if True, original predictions will be shown.</p> <p>Returns:</p> <ul> <li> <code>list[str]</code>         \u2013          <p>list of paths to created images.</p> </li> </ul> Source code in <code>quadra/utils/evaluation.py</code> <pre><code>def create_mask_report(\n    stage: str,\n    output: dict[str, torch.Tensor],\n    mean: npt.ArrayLike,\n    std: npt.ArrayLike,\n    report_path: str,\n    nb_samples: int = 6,\n    analysis: bool = False,\n    apply_sigmoid: bool = True,\n    show_all: bool = False,\n    threshold: float = 0.5,\n    metric: Callable = score_dice,\n    show_orj_predictions: bool = False,\n) -&gt; list[str]:\n\"\"\"Create report for segmentation experiment\n    Args:\n        stage: stage name. Train, validation or test\n        output: data produced by model\n        report_path: experiment path\n        mean: mean values\n        std: std values\n        nb_samples: number of samples\n        analysis: if True, analysis will be created\n        apply_sigmoid: if True, sigmoid will be applied to predictions\n        show_all: if True, all images will be shown\n        threshold: threshold for predictions\n        metric: metric function\n        show_orj_predictions: if True, original predictions will be shown.\n\n    Returns:\n        list of paths to created images.\n    \"\"\"\n    if not os.path.exists(report_path):\n        os.makedirs(report_path)\n\n    th_images = output[\"image\"]\n    th_masks = output[\"mask\"]\n    th_preds = output[\"mask_pred\"]\n    th_labels = output[\"label\"]\n    n_classes = th_preds.shape[1]\n    # TODO: Apply sigmoid is a wrong name now\n    if apply_sigmoid:\n        if n_classes == 1:\n            th_preds = torch.nn.Sigmoid()(th_preds)\n            th_thresh_preds = (th_preds &gt; threshold).float()\n        else:\n            th_preds = torch.nn.Softmax(dim=1)(th_preds)\n            th_thresh_preds = torch.argmax(th_preds, dim=1).float().unsqueeze(1)\n            # Compute labels from the given masks since by default they are all 0\n            th_labels = th_masks.max(dim=2)[0].max(dim=2)[0].squeeze(dim=1)\n            show_orj_predictions = False\n\n    mean = np.asarray(mean)\n    std = np.asarray(std)\n    unnormalize = UnNormalize(mean, std)\n\n    images = np.array(\n        [(unnormalize(image).cpu().numpy().transpose((1, 2, 0)) * 255).astype(np.uint8) for image in th_images]\n    )\n    masks = th_masks.cpu().numpy()\n    preds = th_preds.squeeze(0).cpu().numpy()\n    thresh_preds = th_thresh_preds.squeeze(0).cpu().numpy()\n    dice_scores = metric(th_thresh_preds.cpu(), th_masks.cpu(), reduction=None).numpy()\n\n    labels = th_labels.cpu().numpy()\n    binary_labels = labels == 0\n\n    row_names = [\"Input\", \"Mask\", \"Pred\", f\"Pred&gt;{threshold}\"]\n    bounds = [(0, 255), (0.0, float(n_classes - 1)), (0.0, 1.0), (0.0, float(n_classes - 1))]\n    if not show_orj_predictions:\n        row_names.pop(2)\n        bounds.pop(2)\n\n    if not show_all:\n        sorted_idx = np.argsort(dice_scores)\n    else:\n        sorted_idx = np.arange(len(dice_scores))\n\n    binary_labels = binary_labels[sorted_idx]\n\n    non_zero_score_idx = sorted_idx[~binary_labels]\n    zero_score_idx = sorted_idx[binary_labels]\n    file_paths = []\n    for name, current_score_idx in zip([\"good\", \"bad\"], [zero_score_idx, non_zero_score_idx]):\n        if len(current_score_idx) == 0:\n            continue\n\n        nb_total_samples = len(current_score_idx)\n        nb_selected_samples = nb_total_samples if nb_samples &gt; nb_total_samples else nb_samples\n        fig_w = int(nb_selected_samples * 2)\n        fig_h = int(len(row_names) * 2)\n        if not show_all:\n            worst_idx = current_score_idx[:nb_selected_samples].tolist()\n            best_idx = current_score_idx[-nb_selected_samples:].tolist()\n            random_idx = np.random.choice(current_score_idx, nb_selected_samples, replace=False).tolist()\n\n            indexes = {\"best\": best_idx, \"worst\": worst_idx, \"random\": random_idx}\n        else:\n            indexes = {\"all\": current_score_idx[:nb_selected_samples].tolist()}\n        for k, v in indexes.items():\n            file_path = os.path.join(report_path, f\"{stage}_{name}_{k}_results.png\")\n            images_to_show = [images[v], masks[v], preds[v], thresh_preds[v]]\n            if not show_orj_predictions or n_classes &gt; 1:\n                images_to_show.pop(2)\n            create_grid_figure(\n                images_to_show,\n                nrows=len(row_names),\n                ncols=nb_selected_samples,\n                row_names=row_names,\n                file_path=file_path,\n                fig_size=(fig_w, fig_h),\n                bounds=bounds,\n            )\n            file_paths.append(file_path)\n    if analysis:\n        analysis_file_path = os.path.join(report_path, f\"{stage}_analysis.yaml\")\n        result, fg, fb, area_graph = calculate_mask_based_metrics(\n            images=images,\n            th_masks=th_masks,\n            th_preds=th_thresh_preds,\n            threshold=threshold,\n            show_orj_predictions=show_orj_predictions,\n            metric=metric,\n            multilabel=bool(n_classes &gt; 1),\n            n_classes=n_classes,\n        )\n\n        if len(fg[\"image\"]) &gt; 0:\n            if len(fg[\"image\"]) &gt; nb_samples:\n                for k, v in fg.items():\n                    fg[k] = v[:nb_samples]\n\n            fg_file_path = os.path.join(report_path, f\"{stage}_fn_results.png\")\n            fig_w = int(len(fg[\"image\"]) * 2)\n            create_grid_figure(\n                [fg for _, fg in fg.items()],\n                nrows=len(row_names),\n                ncols=len(fg[\"image\"]),\n                row_names=row_names,\n                file_path=fg_file_path,\n                fig_size=(fig_w, fig_h),\n                bounds=bounds,\n            )\n            file_paths.append(fg_file_path)\n\n        if len(fb[\"image\"]) &gt; 0:\n            if len(fb[\"image\"]) &gt; nb_samples:\n                for k, v in fb.items():\n                    fb[k] = v[:nb_samples]\n            fb_file_path = os.path.join(report_path, f\"{stage}_fp_results.png\")\n\n            fig_w = int(len(fb[\"image\"]) * 2)\n            create_grid_figure(\n                [fb for _, fb in fb.items()],\n                nrows=len(row_names),\n                ncols=len(fb[\"image\"]),\n                row_names=row_names,\n                file_path=fb_file_path,\n                fig_size=(fig_w, fig_h),\n                bounds=bounds,\n            )\n            file_paths.append(fb_file_path)\n        if len(area_graph[\"Defect Area Percentage\"]) &gt; 0:\n            fn_area_path = os.path.join(report_path, f\"{stage}_acc_area.png\")\n            fn_area_df = pd.DataFrame(area_graph)\n            ax = sns.boxplot(\n                x=\"Defect Area Percentage\",\n                y=\"Accuracy\",\n                data=fn_area_df,\n                order=[\"Very Small &lt;1%\", \"Small &lt;10%\", \"Medium &lt;25%\", \"Large &gt;25%\"],\n            )\n            ax.set_facecolor(\"white\")\n            fig = ax.get_figure()\n            fig.savefig(fn_area_path)\n            plt.close(fig)\n\n            file_paths.append(fn_area_path)\n        with open(analysis_file_path, \"w\") as file:\n            yaml.dump(literal_eval(str(result)), file, default_flow_style=False)\n        file_paths.append(analysis_file_path)\n\n    return file_paths\n</code></pre>"},{"location":"reference/quadra/utils/evaluation.html#quadra.utils.evaluation.dice","title":"<code>dice(input_tensor, target, smooth=1.0, eps=1e-08, reduction='mean')</code>","text":"<p>Dice loss computation function.</p> <p>Parameters:</p> <ul> <li> input_tensor             (<code>Tensor</code>)         \u2013          <p>input tensor coming from a model</p> </li> <li> target             (<code>Tensor</code>)         \u2013          <p>target tensor to compare with</p> </li> <li> smooth             (<code>float</code>, default:                 <code>1.0</code> )         \u2013          <p>smoothing factor</p> </li> <li> eps             (<code>float</code>, default:                 <code>1e-08</code> )         \u2013          <p>epsilon to avoid zero division</p> </li> <li> reduction             (<code>str | None</code>, default:                 <code>'mean'</code> )         \u2013          <p>reduction method, one of \"mean\", \"sum\", \"none\"</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>         \u2013          <p>The computed loss</p> </li> </ul> Source code in <code>quadra/utils/evaluation.py</code> <pre><code>def dice(\n    input_tensor: torch.Tensor,\n    target: torch.Tensor,\n    smooth: float = 1.0,\n    eps: float = 1e-8,\n    reduction: str | None = \"mean\",\n) -&gt; torch.Tensor:\n\"\"\"Dice loss computation function.\n\n    Args:\n        input_tensor:  input tensor coming from a model\n        target: target tensor to compare with\n        smooth: smoothing factor\n        eps: epsilon to avoid zero division\n        reduction: reduction method, one of \"mean\", \"sum\", \"none\"\n\n    Returns:\n        The computed loss\n    \"\"\"\n    bs = input_tensor.size(0)\n    iflat = input_tensor.contiguous().view(bs, -1)\n    tflat = target.contiguous().view(bs, -1)\n    intersection = (iflat * tflat).sum(-1)\n    loss = 1 - (2.0 * intersection + smooth) / (iflat.sum(-1) + tflat.sum(-1) + smooth + eps)\n\n    if reduction == \"mean\":\n        loss = loss.mean()\n    return loss\n</code></pre>"},{"location":"reference/quadra/utils/evaluation.html#quadra.utils.evaluation.score_dice","title":"<code>score_dice(y_pred, y_true, reduction=None)</code>","text":"<p>Calculate dice score.</p> Source code in <code>quadra/utils/evaluation.py</code> <pre><code>def score_dice(\n    y_pred,\n    y_true,\n    reduction=None,\n) -&gt; torch.Tensor:\n\"\"\"Calculate dice score.\"\"\"\n    return 1 - dice(y_pred, y_true, reduction=reduction)\n</code></pre>"},{"location":"reference/quadra/utils/evaluation.html#quadra.utils.evaluation.score_dice_smp","title":"<code>score_dice_smp(y_pred, y_true, mode='binary')</code>","text":"<p>Compute dice using smp function. Handle both binary and multiclass scenario.</p> <p>Parameters:</p> <ul> <li> y_pred             (<code>Tensor</code>)         \u2013          <p>1xCxHxW one channel for each class</p> </li> <li> y_true             (<code>Tensor</code>)         \u2013          <p>1x1xHxW true mask with value in [0, ..., n_classes]</p> </li> <li> mode             (<code>str</code>, default:                 <code>'binary'</code> )         \u2013          <p>\"binary\" or \"multiclass\"</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>         \u2013          <p>dice score</p> </li> </ul> Source code in <code>quadra/utils/evaluation.py</code> <pre><code>def score_dice_smp(y_pred: torch.Tensor, y_true: torch.Tensor, mode: str = \"binary\") -&gt; torch.Tensor:\n\"\"\"Compute dice using smp function. Handle both binary and multiclass scenario.\n\n    Args:\n        y_pred: 1xCxHxW one channel for each class\n        y_true: 1x1xHxW true mask with value in [0, ..., n_classes]\n        mode: \"binary\" or \"multiclass\"\n\n    Returns:\n        dice score\n    \"\"\"\n    if mode not in {BINARY_MODE, MULTICLASS_MODE}:\n        raise ValueError(f\"Mode {mode} not valid.\")\n\n    loss = DiceLoss(mode=mode, from_logits=False)\n\n    return 1 - loss(y_pred, y_true)\n</code></pre>"},{"location":"reference/quadra/utils/export.html","title":"export","text":""},{"location":"reference/quadra/utils/export.html#quadra.utils.export.export_model","title":"<code>export_model(config, model, export_folder, half_precision, input_shapes=None, idx_to_class=None, pytorch_model_type='model')</code>","text":"<p>Generate deployment models for the task.</p> <p>Parameters:</p> <ul> <li> config             (<code>DictConfig</code>)         \u2013          <p>Experiment config</p> </li> <li> model             (<code>Any</code>)         \u2013          <p>Model to be exported</p> </li> <li> export_folder             (<code>str</code>)         \u2013          <p>Path to save the exported model</p> </li> <li> half_precision             (<code>bool</code>)         \u2013          <p>Whether to use half precision for the exported model</p> </li> <li> input_shapes             (<code>list[Any] | None</code>, default:                 <code>None</code> )         \u2013          <p>Input shapes for the exported model</p> </li> <li> idx_to_class             (<code>dict[int, str] | None</code>, default:                 <code>None</code> )         \u2013          <p>Mapping from class index to class name</p> </li> <li> pytorch_model_type             (<code>Literal['backbone', 'model']</code>, default:                 <code>'model'</code> )         \u2013          <p>Type of the pytorch model config to be exported, if it's backbone on disk we will save the config.backbone config, otherwise we will save the config.model</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>         \u2013          <p>If the model is exported successfully, return a dictionary containing information about the exported model and</p> </li> <li> <code>dict[str, str]</code>         \u2013          <p>a second dictionary containing the paths to the exported models. Otherwise, return two empty dictionaries.</p> </li> </ul> Source code in <code>quadra/utils/export.py</code> <pre><code>def export_model(\n    config: DictConfig,\n    model: Any,\n    export_folder: str,\n    half_precision: bool,\n    input_shapes: list[Any] | None = None,\n    idx_to_class: dict[int, str] | None = None,\n    pytorch_model_type: Literal[\"backbone\", \"model\"] = \"model\",\n) -&gt; tuple[dict[str, Any], dict[str, str]]:\n\"\"\"Generate deployment models for the task.\n\n    Args:\n        config: Experiment config\n        model: Model to be exported\n        export_folder: Path to save the exported model\n        half_precision: Whether to use half precision for the exported model\n        input_shapes: Input shapes for the exported model\n        idx_to_class: Mapping from class index to class name\n        pytorch_model_type: Type of the pytorch model config to be exported, if it's backbone on disk we will save the\n            config.backbone config, otherwise we will save the config.model\n\n    Returns:\n        If the model is exported successfully, return a dictionary containing information about the exported model and\n        a second dictionary containing the paths to the exported models. Otherwise, return two empty dictionaries.\n    \"\"\"\n    if config.export is None or len(config.export.types) == 0:\n        log.info(\"No export type specified skipping export\")\n        return {}, {}\n\n    os.makedirs(export_folder, exist_ok=True)\n\n    if input_shapes is None:\n        # Try to get input shapes from config\n        # If this is also None we will try to retrieve it from the ModelSignatureWrapper, if it fails we can't export\n        input_shapes = config.export.input_shapes\n\n    export_paths = {}\n\n    for export_type in config.export.types:\n        if export_type == \"torchscript\":\n            out = export_torchscript_model(\n                model=model,\n                input_shapes=input_shapes,\n                output_path=export_folder,\n                half_precision=half_precision,\n            )\n\n            if out is None:\n                log.warning(\"Torchscript export failed, enable debug logging for more details\")\n                continue\n\n            export_path, input_shapes = out\n            export_paths[export_type] = export_path\n        elif export_type == \"pytorch\":\n            export_path = export_pytorch_model(\n                model=model,\n                output_path=export_folder,\n            )\n            export_paths[export_type] = export_path\n            with open(os.path.join(export_folder, \"model_config.yaml\"), \"w\") as f:\n                OmegaConf.save(getattr(config, pytorch_model_type), f, resolve=True)\n        elif export_type == \"onnx\":\n            if not hasattr(config.export, \"onnx\"):\n                log.warning(\"No onnx configuration found, skipping onnx export\")\n                continue\n\n            out = export_onnx_model(\n                model=model,\n                output_path=export_folder,\n                onnx_config=config.export.onnx,\n                input_shapes=input_shapes,\n                half_precision=half_precision,\n            )\n\n            if out is None:\n                log.warning(\"ONNX export failed, enable debug logging for more details\")\n                continue\n\n            export_path, input_shapes = out\n            export_paths[export_type] = export_path\n        else:\n            log.warning(\"Export type: %s not implemented\", export_type)\n\n    if len(export_paths) == 0:\n        log.warning(\"No export type was successful, no model will be available for deployment\")\n        return {}, export_paths\n\n    model_json = {\n        \"input_size\": input_shapes,\n        \"classes\": idx_to_class,\n        \"mean\": list(config.transforms.mean),\n        \"std\": list(config.transforms.std),\n    }\n\n    return model_json, export_paths\n</code></pre>"},{"location":"reference/quadra/utils/export.html#quadra.utils.export.export_onnx_model","title":"<code>export_onnx_model(model, output_path, onnx_config, input_shapes=None, half_precision=False, model_name='model.onnx')</code>","text":"<p>Export a PyTorch model with ONNX.</p> <p>Parameters:</p> <ul> <li> model             (<code>Module</code>)         \u2013          <p>PyTorch model to be exported</p> </li> <li> output_path             (<code>str</code>)         \u2013          <p>Path to save the model</p> </li> <li> input_shapes             (<code>list[Any] | None</code>, default:                 <code>None</code> )         \u2013          <p>Input shapes for tracing</p> </li> <li> onnx_config             (<code>DictConfig</code>)         \u2013          <p>ONNX export configuration</p> </li> <li> half_precision             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>If True, the model will be exported with half precision</p> </li> <li> model_name             (<code>str</code>, default:                 <code>'model.onnx'</code> )         \u2013          <p>Name of the exported model</p> </li> </ul> Source code in <code>quadra/utils/export.py</code> <pre><code>@torch.inference_mode()\ndef export_onnx_model(\n    model: nn.Module,\n    output_path: str,\n    onnx_config: DictConfig,\n    input_shapes: list[Any] | None = None,\n    half_precision: bool = False,\n    model_name: str = \"model.onnx\",\n) -&gt; tuple[str, Any] | None:\n\"\"\"Export a PyTorch model with ONNX.\n\n    Args:\n        model: PyTorch model to be exported\n        output_path: Path to save the model\n        input_shapes: Input shapes for tracing\n        onnx_config: ONNX export configuration\n        half_precision: If True, the model will be exported with half precision\n        model_name: Name of the exported model\n    \"\"\"\n    if not ONNX_AVAILABLE:\n        log.warning(\"ONNX is not installed, can not export model in this format.\")\n        log.warning(\"Please install ONNX capabilities for quadra with: poetry install -E onnx\")\n        return None\n\n    model.eval()\n    if half_precision:\n        model.to(\"cuda:0\")\n        model = model.half()\n    else:\n        model.cpu()\n\n    if hasattr(onnx_config, \"fixed_batch_size\") and onnx_config.fixed_batch_size is not None:\n        batch_size = onnx_config.fixed_batch_size\n    else:\n        batch_size = 1\n\n    model_inputs = extract_torch_model_inputs(\n        model=model, input_shapes=input_shapes, half_precision=half_precision, batch_size=batch_size\n    )\n    if model_inputs is None:\n        return None\n\n    if isinstance(model, ModelSignatureWrapper):\n        model = model.instance\n\n    inp, input_shapes = model_inputs\n\n    os.makedirs(output_path, exist_ok=True)\n\n    model_path = os.path.join(output_path, model_name)\n\n    input_names = onnx_config.input_names if hasattr(onnx_config, \"input_names\") else None\n\n    if input_names is None:\n        input_names = []\n        for i, _ in enumerate(inp):\n            input_names.append(f\"input_{i}\")\n\n    output = [model(*inp)]\n    output_names = onnx_config.output_names if hasattr(onnx_config, \"output_names\") else None\n\n    if output_names is None:\n        output_names = []\n        for i, _ in enumerate(output):\n            output_names.append(f\"output_{i}\")\n\n    dynamic_axes = onnx_config.dynamic_axes if hasattr(onnx_config, \"dynamic_axes\") else None\n\n    if hasattr(onnx_config, \"fixed_batch_size\") and onnx_config.fixed_batch_size is not None:\n        dynamic_axes = None\n    elif dynamic_axes is None:\n        dynamic_axes = {}\n        for i, _ in enumerate(input_names):\n            dynamic_axes[input_names[i]] = {0: \"batch_size\"}\n\n        for i, _ in enumerate(output_names):\n            dynamic_axes[output_names[i]] = {0: \"batch_size\"}\n\n    modified_onnx_config = cast(dict[str, Any], OmegaConf.to_container(onnx_config, resolve=True))\n\n    modified_onnx_config[\"input_names\"] = input_names\n    modified_onnx_config[\"output_names\"] = output_names\n    modified_onnx_config[\"dynamic_axes\"] = dynamic_axes\n\n    simplify = modified_onnx_config.pop(\"simplify\", False)\n    _ = modified_onnx_config.pop(\"fixed_batch_size\", None)\n\n    if len(inp) == 1:\n        inp = inp[0]\n\n    if isinstance(inp, list):\n        inp = tuple(inp)  # onnx doesn't like lists representing tuples of inputs\n\n    if isinstance(inp, dict):\n        raise ValueError(\"ONNX export does not support model with dict inputs\")\n\n    try:\n        torch.onnx.export(model=model, args=inp, f=model_path, **modified_onnx_config)\n\n        onnx_model = onnx.load(model_path)\n        # Check if ONNX model is valid\n        onnx.checker.check_model(onnx_model)\n    except Exception as e:\n        log.debug(\"ONNX export failed with error: %s\", e)\n        return None\n\n    log.info(\"ONNX model saved to %s\", os.path.join(os.getcwd(), model_path))\n\n    if half_precision:\n        is_export_ok = _safe_export_half_precision_onnx(\n            model=model,\n            export_model_path=model_path,\n            inp=inp,\n            onnx_config=onnx_config,\n            input_shapes=input_shapes,\n            input_names=input_names,\n        )\n\n        if not is_export_ok:\n            return None\n\n    if simplify:\n        log.info(\"Attempting to simplify ONNX model\")\n        onnx_model = onnx.load(model_path)\n\n        try:\n            simplified_model, check = onnx_simplify(onnx_model)\n        except Exception as e:\n            log.debug(\"ONNX simplification failed with error: %s\", e)\n            check = False\n\n        if not check:\n            log.warning(\"Something failed during model simplification, only original ONNX model will be exported\")\n        else:\n            model_filename, model_extension = os.path.splitext(model_name)\n            model_name = f\"{model_filename}_simplified{model_extension}\"\n            model_path = os.path.join(output_path, model_name)\n            onnx.save(simplified_model, model_path)\n            log.info(\"Simplified ONNX model saved to %s\", os.path.join(os.getcwd(), model_path))\n\n    return os.path.join(os.getcwd(), model_path), input_shapes\n</code></pre>"},{"location":"reference/quadra/utils/export.html#quadra.utils.export.export_pytorch_model","title":"<code>export_pytorch_model(model, output_path, model_name='model.pth')</code>","text":"<p>Export pytorch model's parameter dictionary using a deserialized state_dict.</p> <p>Parameters:</p> <ul> <li> model             (<code>Module</code>)         \u2013          <p>PyTorch model to be exported</p> </li> <li> output_path             (<code>str</code>)         \u2013          <p>Path to save the model</p> </li> <li> model_name             (<code>str</code>, default:                 <code>'model.pth'</code> )         \u2013          <p>Name of the exported model</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>         \u2013          <p>If the model is exported successfully, the path to the model is returned.</p> </li> </ul> Source code in <code>quadra/utils/export.py</code> <pre><code>def export_pytorch_model(model: nn.Module, output_path: str, model_name: str = \"model.pth\") -&gt; str:\n\"\"\"Export pytorch model's parameter dictionary using a deserialized state_dict.\n\n    Args:\n        model: PyTorch model to be exported\n        output_path: Path to save the model\n        model_name: Name of the exported model\n\n    Returns:\n        If the model is exported successfully, the path to the model is returned.\n\n    \"\"\"\n    if isinstance(model, ModelSignatureWrapper):\n        model = model.instance\n\n    os.makedirs(output_path, exist_ok=True)\n    model.eval()\n    model.cpu()\n    model_path = os.path.join(output_path, model_name)\n    torch.save(model.state_dict(), model_path)\n    log.info(\"Pytorch model saved to %s\", os.path.join(output_path, model_name))\n\n    return os.path.join(os.getcwd(), model_path)\n</code></pre>"},{"location":"reference/quadra/utils/export.html#quadra.utils.export.export_torchscript_model","title":"<code>export_torchscript_model(model, output_path, input_shapes=None, half_precision=False, model_name='model.pt')</code>","text":"<p>Export a PyTorch model with TorchScript.</p> <p>Parameters:</p> <ul> <li> model             (<code>Module</code>)         \u2013          <p>PyTorch model to be exported</p> </li> <li> input_shapes             (<code>list[Any] | None</code>, default:                 <code>None</code> )         \u2013          <p>Inputs shape for tracing</p> </li> <li> output_path             (<code>str</code>)         \u2013          <p>Path to save the model</p> </li> <li> half_precision             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>If True, the model will be exported with half precision</p> </li> <li> model_name             (<code>str</code>, default:                 <code>'model.pt'</code> )         \u2013          <p>Name of the exported model</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[str, Any] | None</code>         \u2013          <p>If the model is exported successfully, the path to the model and the input shape are returned.</p> </li> </ul> Source code in <code>quadra/utils/export.py</code> <pre><code>@torch.inference_mode()\ndef export_torchscript_model(\n    model: nn.Module,\n    output_path: str,\n    input_shapes: list[Any] | None = None,\n    half_precision: bool = False,\n    model_name: str = \"model.pt\",\n) -&gt; tuple[str, Any] | None:\n\"\"\"Export a PyTorch model with TorchScript.\n\n    Args:\n        model: PyTorch model to be exported\n        input_shapes: Inputs shape for tracing\n        output_path: Path to save the model\n        half_precision: If True, the model will be exported with half precision\n        model_name: Name of the exported model\n\n    Returns:\n        If the model is exported successfully, the path to the model and the input shape are returned.\n\n    \"\"\"\n    if isinstance(model, CflowLightning):\n        log.warning(\"Exporting cflow model with torchscript is not supported yet.\")\n        return None\n\n    model.eval()\n    if half_precision:\n        model.to(\"cuda:0\")\n        model = model.half()\n    else:\n        model.cpu()\n\n    model_inputs = extract_torch_model_inputs(model, input_shapes, half_precision)\n\n    if model_inputs is None:\n        return None\n\n    if isinstance(model, ModelSignatureWrapper):\n        model = model.instance\n\n    inp, input_shapes = model_inputs\n\n    try:\n        try:\n            model_jit = torch.jit.trace(model, inp)\n        except RuntimeError as e:\n            log.warning(\"Standard tracing failed with exception %s, attempting tracing with strict=False\", e)\n            model_jit = torch.jit.trace(model, inp, strict=False)\n\n        os.makedirs(output_path, exist_ok=True)\n\n        model_path = os.path.join(output_path, model_name)\n        model_jit.save(model_path)\n\n        log.info(\"Torchscript model saved to %s\", os.path.join(os.getcwd(), model_path))\n\n        return os.path.join(os.getcwd(), model_path), input_shapes\n    except Exception as e:\n        log.debug(\"Failed to export torchscript model with exception: %s\", e)\n        return None\n</code></pre>"},{"location":"reference/quadra/utils/export.html#quadra.utils.export.extract_torch_model_inputs","title":"<code>extract_torch_model_inputs(model, input_shapes=None, half_precision=False, batch_size=1)</code>","text":"<p>Extract the input shapes for the given model and generate a list of torch tensors with the given device and dtype.</p> <p>Parameters:</p> <ul> <li> model             (<code>Module | ModelSignatureWrapper</code>)         \u2013          <p>Module or ModelSignatureWrapper</p> </li> <li> input_shapes             (<code>list[Any] | None</code>, default:                 <code>None</code> )         \u2013          <p>Inputs shapes</p> </li> <li> half_precision             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>If True, the model will be exported with half precision</p> </li> <li> batch_size             (<code>int</code>, default:                 <code>1</code> )         \u2013          <p>Batch size for the input shapes</p> </li> </ul> Source code in <code>quadra/utils/export.py</code> <pre><code>def extract_torch_model_inputs(\n    model: nn.Module | ModelSignatureWrapper,\n    input_shapes: list[Any] | None = None,\n    half_precision: bool = False,\n    batch_size: int = 1,\n) -&gt; tuple[list[Any] | tuple[Any, ...] | torch.Tensor, list[Any]] | None:\n\"\"\"Extract the input shapes for the given model and generate a list of torch tensors with the\n    given device and dtype.\n\n    Args:\n        model: Module or ModelSignatureWrapper\n        input_shapes: Inputs shapes\n        half_precision: If True, the model will be exported with half precision\n        batch_size: Batch size for the input shapes\n    \"\"\"\n    if isinstance(model, ModelSignatureWrapper) and input_shapes is None:\n        input_shapes = model.input_shapes\n\n    if input_shapes is None:\n        log.warning(\n            \"Input shape is None, can not trace model! Please provide input_shapes in the task export configuration.\"\n        )\n        return None\n\n    if half_precision:\n        # TODO: This doesn't support bfloat16!!\n        inp = generate_torch_inputs(\n            input_shapes=input_shapes, device=\"cuda:0\", half_precision=True, dtype=torch.float16, batch_size=batch_size\n        )\n    else:\n        inp = generate_torch_inputs(\n            input_shapes=input_shapes, device=\"cpu\", half_precision=False, dtype=torch.float32, batch_size=batch_size\n        )\n\n    return inp, input_shapes\n</code></pre>"},{"location":"reference/quadra/utils/export.html#quadra.utils.export.generate_torch_inputs","title":"<code>generate_torch_inputs(input_shapes, device, half_precision=False, dtype=torch.float32, batch_size=1)</code>","text":"<p>Given a list of input shapes that can contain either lists, tuples or dicts, with tuples being the input shapes of the model, generate a list of torch tensors with the given device and dtype.</p> Source code in <code>quadra/utils/export.py</code> <pre><code>def generate_torch_inputs(\n    input_shapes: list[Any],\n    device: str | torch.device,\n    half_precision: bool = False,\n    dtype: torch.dtype = torch.float32,\n    batch_size: int = 1,\n) -&gt; list[Any] | tuple[Any, ...] | torch.Tensor:\n\"\"\"Given a list of input shapes that can contain either lists, tuples or dicts, with tuples being the input shapes\n    of the model, generate a list of torch tensors with the given device and dtype.\n    \"\"\"\n    inp = None\n\n    if isinstance(input_shapes, (ListConfig, DictConfig)):\n        input_shapes = OmegaConf.to_container(input_shapes)\n\n    if isinstance(input_shapes, list):\n        if any(isinstance(inp, (Sequence, dict)) for inp in input_shapes):\n            return [generate_torch_inputs(inp, device, half_precision, dtype) for inp in input_shapes]\n\n        # Base case\n        inp = torch.randn((batch_size, *input_shapes), dtype=dtype, device=device)\n\n    if isinstance(input_shapes, dict):\n        return {k: generate_torch_inputs(v, device, half_precision, dtype) for k, v in input_shapes.items()}\n\n    if isinstance(input_shapes, tuple):\n        if any(isinstance(inp, (Sequence, dict)) for inp in input_shapes):\n            # The tuple contains a list, tuple or dict\n            return tuple(generate_torch_inputs(inp, device, half_precision, dtype) for inp in input_shapes)\n\n        # Base case\n        inp = torch.randn((batch_size, *input_shapes), dtype=dtype, device=device)\n\n    if inp is None:\n        raise RuntimeError(\"Something went wrong during model export, unable to parse input shapes\")\n\n    if half_precision:\n        inp = inp.half()\n\n    return inp\n</code></pre>"},{"location":"reference/quadra/utils/export.html#quadra.utils.export.get_export_extension","title":"<code>get_export_extension(export_type)</code>","text":"<p>Get the extension of the exported model.</p> <p>Parameters:</p> <ul> <li> export_type             (<code>str</code>)         \u2013          <p>The type of the exported model.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>         \u2013          <p>The extension of the exported model.</p> </li> </ul> Source code in <code>quadra/utils/export.py</code> <pre><code>def get_export_extension(export_type: str) -&gt; str:\n\"\"\"Get the extension of the exported model.\n\n    Args:\n        export_type: The type of the exported model.\n\n    Returns:\n        The extension of the exported model.\n    \"\"\"\n    if export_type == \"onnx\":\n        extension = \"onnx\"\n    elif export_type == \"torchscript\":\n        extension = \"pt\"\n    elif export_type == \"pytorch\":\n        extension = \"pth\"\n    else:\n        raise ValueError(f\"Unsupported export type {export_type}\")\n\n    return extension\n</code></pre>"},{"location":"reference/quadra/utils/export.html#quadra.utils.export.import_deployment_model","title":"<code>import_deployment_model(model_path, inference_config, device, model_architecture=None)</code>","text":"<p>Try to import a model for deployment, currently only supports torchscript .pt files and state dictionaries .pth files.</p> <p>Parameters:</p> <ul> <li> model_path             (<code>str</code>)         \u2013          <p>Path to the model</p> </li> <li> inference_config             (<code>DictConfig</code>)         \u2013          <p>Inference configuration, should contain keys for the different deployment models</p> </li> <li> device             (<code>str</code>)         \u2013          <p>Device to load the model on</p> </li> <li> model_architecture             (<code>Module | None</code>, default:                 <code>None</code> )         \u2013          <p>Optional model architecture to use for loading a plain pytorch model</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>BaseEvaluationModel</code>         \u2013          <p>A tuple containing the model and the model type</p> </li> </ul> Source code in <code>quadra/utils/export.py</code> <pre><code>def import_deployment_model(\n    model_path: str,\n    inference_config: DictConfig,\n    device: str,\n    model_architecture: nn.Module | None = None,\n) -&gt; BaseEvaluationModel:\n\"\"\"Try to import a model for deployment, currently only supports torchscript .pt files and\n    state dictionaries .pth files.\n\n    Args:\n        model_path: Path to the model\n        inference_config: Inference configuration, should contain keys for the different deployment models\n        device: Device to load the model on\n        model_architecture: Optional model architecture to use for loading a plain pytorch model\n\n    Returns:\n        A tuple containing the model and the model type\n    \"\"\"\n    log.info(\"Importing trained model\")\n\n    file_extension = os.path.splitext(os.path.basename(model_path))[1]\n    deployment_model: BaseEvaluationModel | None = None\n\n    if file_extension == \".pt\":\n        deployment_model = TorchscriptEvaluationModel(config=inference_config.torchscript)\n    elif file_extension == \".pth\":\n        if model_architecture is None:\n            raise ValueError(\"model_architecture must be specified when loading a .pth file\")\n\n        deployment_model = TorchEvaluationModel(config=inference_config.pytorch, model_architecture=model_architecture)\n    elif file_extension == \".onnx\":\n        deployment_model = ONNXEvaluationModel(config=inference_config.onnx)\n\n    if deployment_model is not None:\n        deployment_model.load_from_disk(model_path=model_path, device=device)\n\n        log.info(\"Imported %s model\", deployment_model.__class__.__name__)\n\n        return deployment_model\n\n    raise ValueError(f\"Unable to load model with extension {file_extension}, valid extensions are: ['.pt', 'pth']\")\n</code></pre>"},{"location":"reference/quadra/utils/imaging.html","title":"imaging","text":""},{"location":"reference/quadra/utils/imaging.html#quadra.utils.imaging.crop_image","title":"<code>crop_image(image, roi)</code>","text":"<p>Crop an image given a roi in proper format.</p> <p>Parameters:</p> <ul> <li> image             (<code>ndarray</code>)         \u2013          <p>array of size HxW or HxWxC</p> </li> <li> roi             (<code>tuple[int, int, int, int]</code>)         \u2013          <p>(w_upper_left, h_upper_left, w_bottom_right, h_bottom_right)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>         \u2013          <p>Cropped image based on roi</p> </li> </ul> Source code in <code>quadra/utils/imaging.py</code> <pre><code>def crop_image(image: np.ndarray, roi: tuple[int, int, int, int]) -&gt; np.ndarray:\n\"\"\"Crop an image given a roi in proper format.\n\n    Args:\n        image: array of size HxW or HxWxC\n        roi: (w_upper_left, h_upper_left, w_bottom_right, h_bottom_right)\n\n    Returns:\n        Cropped image based on roi\n    \"\"\"\n    return image[roi[1] : roi[3], roi[0] : roi[2]]\n</code></pre>"},{"location":"reference/quadra/utils/imaging.html#quadra.utils.imaging.keep_aspect_ratio_resize","title":"<code>keep_aspect_ratio_resize(image, size=224, interpolation=1)</code>","text":"<p>Resize input image while keeping its aspect ratio.</p> Source code in <code>quadra/utils/imaging.py</code> <pre><code>def keep_aspect_ratio_resize(image: np.ndarray, size: int = 224, interpolation: int = 1) -&gt; np.ndarray:\n\"\"\"Resize input image while keeping its aspect ratio.\"\"\"\n    (h, w) = image.shape[:2]\n\n    if h &lt; w:\n        height = size\n        width = int(w * size / h)\n    else:\n        width = size\n        height = int(h * size / w)\n\n    resized = cv2.resize(image, (width, height), interpolation=interpolation)\n    return resized\n</code></pre>"},{"location":"reference/quadra/utils/logger.html","title":"logger","text":""},{"location":"reference/quadra/utils/logger.html#quadra.utils.logger.get_logger","title":"<code>get_logger(name=__name__)</code>","text":"<p>Initializes multi-GPU-friendly python logger.</p> Source code in <code>quadra/utils/logger.py</code> <pre><code>def get_logger(name=__name__) -&gt; logging.Logger:\n\"\"\"Initializes multi-GPU-friendly python logger.\"\"\"\n    logger = logging.getLogger(name)\n\n    # this ensures all logging levels get marked with the rank zero decorator\n    # otherwise logs would get multiplied for each GPU process in multi-GPU setup\n    for level in (\"debug\", \"info\", \"warning\", \"error\", \"exception\", \"fatal\", \"critical\"):\n        setattr(logger, level, rank_zero_only(getattr(logger, level)))\n\n    return logger\n</code></pre>"},{"location":"reference/quadra/utils/mlflow.html","title":"mlflow","text":""},{"location":"reference/quadra/utils/mlflow.html#quadra.utils.mlflow.get_mlflow_logger","title":"<code>get_mlflow_logger(trainer)</code>","text":"<p>Safely get Mlflow logger from Trainer loggers.</p> <p>Parameters:</p> <ul> <li> trainer             (<code>Trainer</code>)         \u2013          <p>Pytorch Lightning trainer.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>MLFlowLogger | None</code>         \u2013          <p>An mlflow logger if available, else None.</p> </li> </ul> Source code in <code>quadra/utils/mlflow.py</code> <pre><code>def get_mlflow_logger(trainer: Trainer) -&gt; MLFlowLogger | None:\n\"\"\"Safely get Mlflow logger from Trainer loggers.\n\n    Args:\n        trainer: Pytorch Lightning trainer.\n\n    Returns:\n        An mlflow logger if available, else None.\n    \"\"\"\n    if isinstance(trainer.logger, MLFlowLogger):\n        return trainer.logger\n\n    if isinstance(trainer.logger, list):\n        for logger in trainer.logger:\n            if isinstance(logger, MLFlowLogger):\n                return logger\n\n    return None\n</code></pre>"},{"location":"reference/quadra/utils/mlflow.html#quadra.utils.mlflow.infer_signature_input","title":"<code>infer_signature_input(input_tensor)</code>","text":"<p>Recursively infer the signature input format to pass to mlflow.models.infer_signature.</p> <p>Raises:</p> <ul> <li> <code>ValueError</code>           \u2013          <p>If the input type is not supported or when nested dicts or sequences are encountered.</p> </li> </ul> Source code in <code>quadra/utils/mlflow.py</code> <pre><code>def infer_signature_input(input_tensor: Any) -&gt; Any:\n\"\"\"Recursively infer the signature input format to pass to mlflow.models.infer_signature.\n\n    Raises:\n        ValueError: If the input type is not supported or when nested dicts or sequences are encountered.\n    \"\"\"\n    if isinstance(input_tensor, Sequence):\n        # Mlflow currently does not support sequence outputs, so we use a dict instead\n        signature = {}\n        for i, x in enumerate(input_tensor):\n            if isinstance(x, Sequence):\n                # Nested signature is currently not supported by mlflow\n                raise ValueError(\"Nested sequences are not supported\")\n                # TODO: Enable this once mlflow supports nested signatures\n                # signature[f\"output_{i}\"] = {f\"output_{j}\": infer_signature_torch(y) for j, y in enumerate(x)}\n            if isinstance(x, dict):\n                # Nested dicts are not supported\n                raise ValueError(\"Nested dicts are not supported\")\n\n            signature[f\"output_{i}\"] = infer_signature_input(x)\n    elif isinstance(input_tensor, torch.Tensor):\n        signature = input_tensor.cpu().numpy()\n    elif isinstance(input_tensor, dict):\n        signature = {}\n        for k, v in input_tensor.items():\n            if isinstance(v, dict):\n                # Nested dicts are not supported\n                raise ValueError(\"Nested dicts are not supported\")\n            if isinstance(v, Sequence):\n                # Nested signature is currently not supported by mlflow\n                raise ValueError(\"Nested sequences are not supported\")\n\n            signature[k] = infer_signature_input(v)\n    else:\n        raise ValueError(f\"Unable to infer signature for model output type {type(input_tensor)}\")\n\n    return signature\n</code></pre>"},{"location":"reference/quadra/utils/mlflow.html#quadra.utils.mlflow.infer_signature_model","title":"<code>infer_signature_model(model, data)</code>","text":"<p>Infer input and output signature for a PyTorch/Torchscript model.</p> Source code in <code>quadra/utils/mlflow.py</code> <pre><code>@torch.inference_mode()\ndef infer_signature_model(model: BaseEvaluationModel, data: list[Any]) -&gt; ModelSignature | None:\n\"\"\"Infer input and output signature for a PyTorch/Torchscript model.\"\"\"\n    model = model.eval()\n    model_output = model(*data)\n\n    try:\n        output_signature = infer_signature_input(model_output)\n\n        if len(data) == 1:\n            signature_input = infer_signature_input(data[0])\n        else:\n            signature_input = infer_signature_input(data)\n    except ValueError:\n        # TODO: Solve circular import as it is not possible to import get_logger right now\n        # log.warning(\"Unable to infer signature for model output type %s\", type(model_output))\n        return None\n\n    return infer_signature(signature_input, output_signature)\n</code></pre>"},{"location":"reference/quadra/utils/model_manager.html","title":"model_manager","text":""},{"location":"reference/quadra/utils/model_manager.html#quadra.utils.model_manager.AbstractModelManager","title":"<code>AbstractModelManager</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract class for model managers.</p>"},{"location":"reference/quadra/utils/model_manager.html#quadra.utils.model_manager.AbstractModelManager.delete_model","title":"<code>delete_model(model_name, version, description=None)</code>  <code>abstractmethod</code>","text":"<p>Delete a model with the given version.</p> Source code in <code>quadra/utils/model_manager.py</code> <pre><code>@abstractmethod\ndef delete_model(self, model_name: str, version: int, description: str | None = None) -&gt; None:\n\"\"\"Delete a model with the given version.\"\"\"\n</code></pre>"},{"location":"reference/quadra/utils/model_manager.html#quadra.utils.model_manager.AbstractModelManager.download_model","title":"<code>download_model(model_name, version, output_path)</code>  <code>abstractmethod</code>","text":"<p>Download the model with the given version to the given output path.</p> Source code in <code>quadra/utils/model_manager.py</code> <pre><code>@abstractmethod\ndef download_model(self, model_name: str, version: int, output_path: str) -&gt; None:\n\"\"\"Download the model with the given version to the given output path.\"\"\"\n</code></pre>"},{"location":"reference/quadra/utils/model_manager.html#quadra.utils.model_manager.AbstractModelManager.get_latest_version","title":"<code>get_latest_version(model_name)</code>  <code>abstractmethod</code>","text":"<p>Get the latest version of a model for all the possible stages or filtered by stage.</p> Source code in <code>quadra/utils/model_manager.py</code> <pre><code>@abstractmethod\ndef get_latest_version(self, model_name: str) -&gt; Any:\n\"\"\"Get the latest version of a model for all the possible stages or filtered by stage.\"\"\"\n</code></pre>"},{"location":"reference/quadra/utils/model_manager.html#quadra.utils.model_manager.AbstractModelManager.register_best_model","title":"<code>register_best_model(experiment_name, metric, model_name, description, tags=None, mode='max', model_path='deployment_model')</code>  <code>abstractmethod</code>","text":"<p>Register the best model from an experiment.</p> Source code in <code>quadra/utils/model_manager.py</code> <pre><code>@abstractmethod\ndef register_best_model(\n    self,\n    experiment_name: str,\n    metric: str,\n    model_name: str,\n    description: str,\n    tags: dict[str, Any] | None = None,\n    mode: Literal[\"max\", \"min\"] = \"max\",\n    model_path: str = \"deployment_model\",\n) -&gt; Any:\n\"\"\"Register the best model from an experiment.\"\"\"\n</code></pre>"},{"location":"reference/quadra/utils/model_manager.html#quadra.utils.model_manager.AbstractModelManager.register_model","title":"<code>register_model(model_location, model_name, description, tags=None)</code>  <code>abstractmethod</code>","text":"<p>Register a model in the model registry.</p> Source code in <code>quadra/utils/model_manager.py</code> <pre><code>@abstractmethod\ndef register_model(\n    self, model_location: str, model_name: str, description: str, tags: dict[str, Any] | None = None\n) -&gt; Any:\n\"\"\"Register a model in the model registry.\"\"\"\n</code></pre>"},{"location":"reference/quadra/utils/model_manager.html#quadra.utils.model_manager.AbstractModelManager.transition_model","title":"<code>transition_model(model_name, version, stage, description=None)</code>  <code>abstractmethod</code>","text":"<p>Transition the model with the given version to a new stage.</p> Source code in <code>quadra/utils/model_manager.py</code> <pre><code>@abstractmethod\ndef transition_model(self, model_name: str, version: int, stage: str, description: str | None = None) -&gt; Any:\n\"\"\"Transition the model with the given version to a new stage.\"\"\"\n</code></pre>"},{"location":"reference/quadra/utils/model_manager.html#quadra.utils.model_manager.MlflowModelManager","title":"<code>MlflowModelManager()</code>","text":"<p>             Bases: <code>AbstractModelManager</code></p> <p>Model manager for Mlflow.</p> Source code in <code>quadra/utils/model_manager.py</code> <pre><code>def __init__(self):\n    if not MLFLOW_AVAILABLE:\n        raise ImportError(\"Mlflow is not available, please install it with pip install mlflow\")\n\n    if os.getenv(\"MLFLOW_TRACKING_URI\") is None:\n        raise ValueError(\"MLFLOW_TRACKING_URI environment variable is not set\")\n\n    mlflow.set_tracking_uri(os.getenv(\"MLFLOW_TRACKING_URI\"))\n    self.client = MlflowClient()\n</code></pre>"},{"location":"reference/quadra/utils/model_manager.html#quadra.utils.model_manager.MlflowModelManager.delete_model","title":"<code>delete_model(model_name, version, description=None)</code>","text":"<p>Delete a model.</p> <p>Parameters:</p> <ul> <li> model_name             (<code>str</code>)         \u2013          <p>The name of the model</p> </li> <li> version             (<code>int</code>)         \u2013          <p>The version of the model</p> </li> <li> description             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>Why the model was deleted, this will be added to the model changelog</p> </li> </ul> Source code in <code>quadra/utils/model_manager.py</code> <pre><code>def delete_model(self, model_name: str, version: int, description: str | None = None) -&gt; None:\n\"\"\"Delete a model.\n\n    Args:\n        model_name: The name of the model\n        version: The version of the model\n        description: Why the model was deleted, this will be added to the model changelog\n    \"\"\"\n    model_stage = self._safe_get_stage(model_name, version)\n\n    if model_stage is None:\n        return\n\n    if (\n        input(\n            f\"Model named `{model_name}`, version {version} is in stage {model_stage}, \"\n            \"type the model name to continue deletion:\"\n        )\n        != model_name\n    ):\n        log.warning(\"Model name did not match, aborting deletion\")\n        return\n\n    log.info(\"Deleting model %s version %s\", model_name, version)\n    self.client.delete_model_version(model_name, version)\n\n    registered_model_description = self.client.get_registered_model(model_name).description\n\n    new_model_description = \"## **Deletion:**\\n\"\n    new_model_description += f\"### Version {version} from stage: {model_stage}\\n\"\n    new_model_description += self._get_author_and_date()\n    new_model_description += self._generate_description(description)\n\n    self.client.update_registered_model(model_name, registered_model_description + new_model_description)\n</code></pre>"},{"location":"reference/quadra/utils/model_manager.html#quadra.utils.model_manager.MlflowModelManager.download_model","title":"<code>download_model(model_name, version, output_path)</code>","text":"<p>Download the model with the given version to the given output path.</p> <p>Parameters:</p> <ul> <li> model_name             (<code>str</code>)         \u2013          <p>The name of the model</p> </li> <li> version             (<code>int</code>)         \u2013          <p>The version of the model</p> </li> <li> output_path             (<code>str</code>)         \u2013          <p>The path to save the model to</p> </li> </ul> Source code in <code>quadra/utils/model_manager.py</code> <pre><code>def download_model(self, model_name: str, version: int, output_path: str) -&gt; None:\n\"\"\"Download the model with the given version to the given output path.\n\n    Args:\n        model_name: The name of the model\n        version: The version of the model\n        output_path: The path to save the model to\n    \"\"\"\n    artifact_uri = self.client.get_model_version_download_uri(model_name, version)\n    log.info(\"Downloading model %s version %s from %s to %s\", model_name, version, artifact_uri, output_path)\n    if not os.path.exists(output_path):\n        log.info(\"Creating output path %s\", output_path)\n        os.makedirs(output_path)\n    mlflow.artifacts.download_artifacts(artifact_uri=artifact_uri, dst_path=output_path)\n</code></pre>"},{"location":"reference/quadra/utils/model_manager.html#quadra.utils.model_manager.MlflowModelManager.get_latest_version","title":"<code>get_latest_version(model_name)</code>","text":"<p>Get the latest version of a model.</p> <p>Parameters:</p> <ul> <li> model_name             (<code>str</code>)         \u2013          <p>The name of the model</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ModelVersion</code>         \u2013          <p>The model version</p> </li> </ul> Source code in <code>quadra/utils/model_manager.py</code> <pre><code>def get_latest_version(self, model_name: str) -&gt; ModelVersion:\n\"\"\"Get the latest version of a model.\n\n    Args:\n        model_name: The name of the model\n\n    Returns:\n        The model version\n    \"\"\"\n    latest_version = max(int(x.version) for x in self.client.get_latest_versions(model_name))\n    model_version = self.client.get_model_version(model_name, latest_version)\n\n    return model_version\n</code></pre>"},{"location":"reference/quadra/utils/model_manager.html#quadra.utils.model_manager.MlflowModelManager.register_best_model","title":"<code>register_best_model(experiment_name, metric, model_name, description=None, tags=None, mode='max', model_path='deployment_model')</code>","text":"<p>Register the best model from an experiment.</p> <p>Parameters:</p> <ul> <li> experiment_name             (<code>str</code>)         \u2013          <p>The name of the experiment</p> </li> <li> metric             (<code>str</code>)         \u2013          <p>The metric to use to determine the best model</p> </li> <li> model_name             (<code>str</code>)         \u2013          <p>The name of the model after it is registered</p> </li> <li> description             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>A description of the model, this will be added to the model changelog</p> </li> <li> tags             (<code>dict[str, Any] | None</code>, default:                 <code>None</code> )         \u2013          <p>A dictionary of tags to add to the model</p> </li> <li> mode             (<code>Literal['max', 'min']</code>, default:                 <code>'max'</code> )         \u2013          <p>The mode to use to determine the best model, either \"max\" or \"min\"</p> </li> <li> model_path             (<code>str</code>, default:                 <code>'deployment_model'</code> )         \u2013          <p>The path to the model within the experiment run</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ModelVersion | None</code>         \u2013          <p>The registered model version if successful, otherwise None</p> </li> </ul> Source code in <code>quadra/utils/model_manager.py</code> <pre><code>def register_best_model(\n    self,\n    experiment_name: str,\n    metric: str,\n    model_name: str,\n    description: str | None = None,\n    tags: dict[str, Any] | None = None,\n    mode: Literal[\"max\", \"min\"] = \"max\",\n    model_path: str = \"deployment_model\",\n) -&gt; ModelVersion | None:\n\"\"\"Register the best model from an experiment.\n\n    Args:\n        experiment_name: The name of the experiment\n        metric: The metric to use to determine the best model\n        model_name: The name of the model after it is registered\n        description: A description of the model, this will be added to the model changelog\n        tags: A dictionary of tags to add to the model\n        mode: The mode to use to determine the best model, either \"max\" or \"min\"\n        model_path: The path to the model within the experiment run\n\n    Returns:\n        The registered model version if successful, otherwise None\n    \"\"\"\n    if mode not in [\"max\", \"min\"]:\n        raise ValueError(f\"Mode must be either 'max' or 'min', got {mode}\")\n\n    experiment_id = self.client.get_experiment_by_name(experiment_name).experiment_id\n    runs = self.client.search_runs(experiment_ids=[experiment_id])\n\n    if len(runs) == 0:\n        log.error(\"No runs found for experiment %s\", experiment_name)\n        return None\n\n    best_run: Run | None = None\n\n    # We can only make comparisons if the model is on the top folder, otherwise just check if the folder exists\n    # TODO: Is there a better way to do this?\n    base_model_path = model_path.split(\"/\")[0]\n\n    for run in runs:\n        run_artifacts = [x.path for x in self.client.list_artifacts(run.info.run_id) if x.path == base_model_path]\n\n        if len(run_artifacts) == 0:\n            # If we don't find the given model path, skip this run\n            continue\n\n        if best_run is None:\n            # If we find a run with the model it must also have the metric\n            if run.data.metrics.get(metric) is not None:\n                best_run = run\n            continue\n\n        if mode == \"max\":\n            if run.data.metrics[metric] &gt; best_run.data.metrics[metric]:\n                best_run = run\n        elif run.data.metrics[metric] &lt; best_run.data.metrics[metric]:\n            best_run = run\n\n    if best_run is None:\n        log.error(\"No runs found for experiment %s with the given metric\", experiment_name)\n        return None\n\n    best_model_uri = f\"runs:/{best_run.info.run_id}/{model_path}\"\n\n    model_version = self.register_model(\n        model_location=best_model_uri, model_name=model_name, tags=tags, description=description\n    )\n\n    return model_version\n</code></pre>"},{"location":"reference/quadra/utils/model_manager.html#quadra.utils.model_manager.MlflowModelManager.register_model","title":"<code>register_model(model_location, model_name, description=None, tags=None)</code>","text":"<p>Register a model in the model registry.</p> <p>Parameters:</p> <ul> <li> model_location             (<code>str</code>)         \u2013          <p>The model uri</p> </li> <li> model_name             (<code>str</code>)         \u2013          <p>The name of the model after it is registered</p> </li> <li> description             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>A description of the model, this will be added to the model changelog</p> </li> <li> tags             (<code>dict[str, Any] | None</code>, default:                 <code>None</code> )         \u2013          <p>A dictionary of tags to add to the model</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ModelVersion</code>         \u2013          <p>The model version</p> </li> </ul> Source code in <code>quadra/utils/model_manager.py</code> <pre><code>def register_model(\n    self, model_location: str, model_name: str, description: str | None = None, tags: dict[str, Any] | None = None\n) -&gt; ModelVersion:\n\"\"\"Register a model in the model registry.\n\n    Args:\n        model_location: The model uri\n        model_name: The name of the model after it is registered\n        description: A description of the model, this will be added to the model changelog\n        tags: A dictionary of tags to add to the model\n\n    Returns:\n        The model version\n    \"\"\"\n    model_version = mlflow.register_model(model_uri=model_location, name=model_name, tags=tags)\n    log.info(\"Registered model %s with version %s\", model_name, model_version.version)\n    registered_model_description = self.client.get_registered_model(model_name).description\n\n    if model_version.version == \"1\":\n        header = \"# MODEL CHANGELOG\\n\"\n    else:\n        header = \"\"\n\n    new_model_description = VERSION_MD_TEMPLATE.format(model_version.version)\n    new_model_description += self._get_author_and_date()\n    new_model_description += self._generate_description(description)\n\n    self.client.update_registered_model(model_name, header + registered_model_description + new_model_description)\n\n    self.client.update_model_version(\n        model_name, model_version.version, \"# MODEL CHANGELOG\\n\" + new_model_description\n    )\n\n    return model_version\n</code></pre>"},{"location":"reference/quadra/utils/model_manager.html#quadra.utils.model_manager.MlflowModelManager.transition_model","title":"<code>transition_model(model_name, version, stage, description=None)</code>","text":"<p>Transition a model to a new stage.</p> <p>Parameters:</p> <ul> <li> model_name             (<code>str</code>)         \u2013          <p>The name of the model</p> </li> <li> version             (<code>int</code>)         \u2013          <p>The version of the model</p> </li> <li> stage             (<code>str</code>)         \u2013          <p>The stage of the model</p> </li> <li> description             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>A description of the transition, this will be added to the model changelog</p> </li> </ul> Source code in <code>quadra/utils/model_manager.py</code> <pre><code>def transition_model(\n    self, model_name: str, version: int, stage: str, description: str | None = None\n) -&gt; ModelVersion | None:\n\"\"\"Transition a model to a new stage.\n\n    Args:\n        model_name: The name of the model\n        version: The version of the model\n        stage: The stage of the model\n        description: A description of the transition, this will be added to the model changelog\n    \"\"\"\n    previous_stage = self._safe_get_stage(model_name, version)\n\n    if previous_stage is None:\n        return None\n\n    if previous_stage.lower() == stage.lower():\n        log.warning(\"Model %s version %s is already in stage %s\", model_name, version, stage)\n        return self.client.get_model_version(model_name, version)\n\n    log.info(\"Transitioning model %s version %s from %s to %s\", model_name, version, previous_stage, stage)\n    model_version = self.client.transition_model_version_stage(name=model_name, version=version, stage=stage)\n    new_stage = model_version.current_stage\n    registered_model_description = self.client.get_registered_model(model_name).description\n    single_model_description = self.client.get_model_version(model_name, version).description\n\n    new_model_description = \"## **Transition:**\\n\"\n    new_model_description += f\"### Version {model_version.version} from {previous_stage} to {new_stage}\\n\"\n    new_model_description += self._get_author_and_date()\n    new_model_description += self._generate_description(description)\n\n    self.client.update_registered_model(model_name, registered_model_description + new_model_description)\n    self.client.update_model_version(\n        model_name, model_version.version, single_model_description + new_model_description\n    )\n\n    return model_version\n</code></pre>"},{"location":"reference/quadra/utils/models.html","title":"models","text":""},{"location":"reference/quadra/utils/models.html#quadra.utils.models.AttentionExtractor","title":"<code>AttentionExtractor(model, attention_layer_name='attn_drop')</code>","text":"<p>             Bases: <code>Module</code></p> <p>General attention extractor.</p> <p>Parameters:</p> <ul> <li> model             (<code>Module</code>)         \u2013          <p>Backbone model which contains the attention layer. attention_layer_name: Attention layer for extracting attention maps. Defaults to \"attn_drop\".</p> </li> <li> attention_layer_name             (<code>str</code>, default:                 <code>'attn_drop'</code> )         \u2013          <p>Attention layer for extracting attention maps.</p> </li> </ul> Source code in <code>quadra/utils/models.py</code> <pre><code>def __init__(self, model: torch.nn.Module, attention_layer_name: str = \"attn_drop\"):\n    super().__init__()\n    self.model = model\n    modules = [module for module_name, module in self.model.named_modules() if attention_layer_name in module_name]\n    if modules:\n        modules[-1].register_forward_hook(self.get_attention)\n    self.attentions = torch.zeros((1, 0))\n</code></pre>"},{"location":"reference/quadra/utils/models.html#quadra.utils.models.AttentionExtractor.clear","title":"<code>clear()</code>","text":"<p>Clear the grabbed attentions.</p> Source code in <code>quadra/utils/models.py</code> <pre><code>def clear(self):\n\"\"\"Clear the grabbed attentions.\"\"\"\n    self.attentions = torch.zeros((1, 0))\n</code></pre>"},{"location":"reference/quadra/utils/models.html#quadra.utils.models.AttentionExtractor.get_attention","title":"<code>get_attention(module, input_tensor, output)</code>","text":"<p>Method to be registered to grab attentions.</p> Source code in <code>quadra/utils/models.py</code> <pre><code>def get_attention(self, module: nn.Module, input_tensor: torch.Tensor, output: torch.Tensor):  # pylint: disable=unused-argument\n\"\"\"Method to be registered to grab attentions.\"\"\"\n    self.attentions = output.detach().clone().cpu()\n</code></pre>"},{"location":"reference/quadra/utils/models.html#quadra.utils.models.AttentionExtractor.process_attention_maps","title":"<code>process_attention_maps(attentions, img_width, img_height)</code>  <code>staticmethod</code>","text":"<p>Preprocess attentions maps to be visualized.</p> <p>Parameters:</p> <ul> <li> attentions             (<code>Tensor</code>)         \u2013          <p>grabbed attentions</p> </li> <li> img_width             (<code>int</code>)         \u2013          <p>image width</p> </li> <li> img_height             (<code>int</code>)         \u2013          <p>image height</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>         \u2013          <p>torch.Tensor: preprocessed attentions, with the shape equal to the one of the image from</p> </li> <li> <code>Tensor</code>         \u2013          <p>which attentions has been computed</p> </li> </ul> Source code in <code>quadra/utils/models.py</code> <pre><code>@staticmethod\ndef process_attention_maps(attentions: torch.Tensor, img_width: int, img_height: int) -&gt; torch.Tensor:\n\"\"\"Preprocess attentions maps to be visualized.\n\n    Args:\n        attentions: grabbed attentions\n        img_width: image width\n        img_height: image height\n\n    Returns:\n        torch.Tensor: preprocessed attentions, with the shape equal to the one of the image from\n        which attentions has been computed\n    \"\"\"\n    if len(attentions.shape) == 4:\n        # vit\n        # batch, heads, N, N (class atention layer)\n        attentions = attentions[:, :, 0, 1:]  # batch, heads, height-1\n\n    else:\n        # xcit\n        # batch, heads, N\n        attentions = attentions[:, :, 1:]  # batch, heads, dim-1\n    nh = attentions.shape[1]\n    patch_size = int(math.sqrt(img_width * img_height / attentions.shape[-1]))\n    w_featmap = img_width // patch_size\n    h_featmap = img_height // patch_size\n\n    # we keep only the output patch attention we dont want cls\n    attentions = attentions.reshape(attentions.shape[0], nh, w_featmap, h_featmap)\n    attentions = F.interpolate(attentions, scale_factor=patch_size, mode=\"nearest\")\n    return attentions\n</code></pre>"},{"location":"reference/quadra/utils/models.html#quadra.utils.models.L2Norm","title":"<code>L2Norm</code>","text":"<p>             Bases: <code>Module</code></p> <p>Compute L2 Norm.</p>"},{"location":"reference/quadra/utils/models.html#quadra.utils.models.LSABlock","title":"<code>LSABlock(dim, num_heads, mlp_ratio=4.0, qkv_bias=False, drop=0.0, attn_drop=0.0, drop_path=0.0, act_layer=torch.nn.GELU, norm_layer=torch.nn.LayerNorm, mask_diagonal=True, learnable_temperature=True)</code>","text":"<p>             Bases: <code>Module</code></p> <p>Local Self Attention Block from https://arxiv.org/abs/2112.13492.</p> <p>Parameters:</p> <ul> <li> dim             (<code>int</code>)         \u2013          <p>embedding dimension</p> </li> <li> num_heads             (<code>int</code>)         \u2013          <p>number of attention heads</p> </li> <li> mlp_ratio             (<code>float</code>, default:                 <code>4.0</code> )         \u2013          <p>ratio of mlp hidden dim to embedding dim</p> </li> <li> qkv_bias             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>enable bias for qkv if True</p> </li> <li> drop             (<code>float</code>, default:                 <code>0.0</code> )         \u2013          <p>dropout rate</p> </li> <li> attn_drop             (<code>float</code>, default:                 <code>0.0</code> )         \u2013          <p>attention dropout rate</p> </li> <li> drop_path             (<code>float</code>, default:                 <code>0.0</code> )         \u2013          <p>stochastic depth rate</p> </li> <li> act_layer             (<code>type[Module]</code>, default:                 <code>GELU</code> )         \u2013          <p>activation layer</p> </li> <li> norm_layer             (<code>type[LayerNorm]</code>, default:                 <code>LayerNorm</code> )         \u2013          <p>: normalization layer</p> </li> <li> mask_diagonal             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>whether to mask Q^T x K diagonal with -infinity so not to count self relationship between tokens. Defaults to True</p> </li> <li> learnable_temperature             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>whether to use a learnable temperature as specified in https://arxiv.org/abs/2112.13492. Defaults to True.</p> </li> </ul> Source code in <code>quadra/utils/models.py</code> <pre><code>def __init__(\n    self,\n    dim: int,\n    num_heads: int,\n    mlp_ratio: float = 4.0,\n    qkv_bias: bool = False,\n    drop: float = 0.0,\n    attn_drop: float = 0.0,\n    drop_path: float = 0.0,\n    act_layer: type[nn.Module] = torch.nn.GELU,\n    norm_layer: type[torch.nn.LayerNorm] = torch.nn.LayerNorm,\n    mask_diagonal: bool = True,\n    learnable_temperature: bool = True,\n):\n    super().__init__()\n    self.norm1 = norm_layer(dim)\n    self.attn = LocalSelfAttention(\n        dim,\n        num_heads=num_heads,\n        qkv_bias=qkv_bias,\n        attn_drop=attn_drop,\n        proj_drop=drop,\n        mask_diagonal=mask_diagonal,\n        learnable_temperature=learnable_temperature,\n    )\n    # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n    self.drop_path = DropPath(drop_path) if drop_path &gt; 0.0 else torch.nn.Identity()\n    self.norm2 = norm_layer(dim)\n    mlp_hidden_dim = int(dim * mlp_ratio)\n    self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n</code></pre>"},{"location":"reference/quadra/utils/models.html#quadra.utils.models.LocalSelfAttention","title":"<code>LocalSelfAttention(dim, num_heads=8, qkv_bias=False, attn_drop=0.0, proj_drop=0.0, mask_diagonal=True, learnable_temperature=True)</code>","text":"<p>             Bases: <code>Module</code></p> <p>Local Self Attention from https://arxiv.org/abs/2112.13492.</p> <p>Parameters:</p> <ul> <li> dim             (<code>int</code>)         \u2013          <p>embedding dimension.</p> </li> <li> num_heads             (<code>int</code>, default:                 <code>8</code> )         \u2013          <p>number of attention heads.</p> </li> <li> qkv_bias             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>enable bias for qkv if True.</p> </li> <li> attn_drop             (<code>float</code>, default:                 <code>0.0</code> )         \u2013          <p>attention dropout rate.</p> </li> <li> proj_drop             (<code>float</code>, default:                 <code>0.0</code> )         \u2013          <p>projection dropout rate.</p> </li> <li> mask_diagonal             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>whether to mask Q^T x K diagonal with -infinity so not to count self relationship between tokens. Defaults to True.</p> </li> <li> learnable_temperature             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>whether to use a learnable temperature as specified in https://arxiv.org/abs/2112.13492. Defaults to True.</p> </li> </ul> Source code in <code>quadra/utils/models.py</code> <pre><code>def __init__(\n    self,\n    dim: int,\n    num_heads: int = 8,\n    qkv_bias: bool = False,\n    attn_drop: float = 0.0,\n    proj_drop: float = 0.0,\n    mask_diagonal: bool = True,\n    learnable_temperature: bool = True,\n):\n    super().__init__()\n    self.num_heads = num_heads\n    head_dim = dim // num_heads\n    self.mask_diagonal = mask_diagonal\n    if learnable_temperature:\n        self.register_parameter(\"scale\", torch.nn.Parameter(torch.tensor(head_dim**-0.5, requires_grad=True)))\n    else:\n        self.scale = head_dim**-0.5\n\n    self.qkv = torch.nn.Linear(dim, dim * 3, bias=qkv_bias)\n    self.attn_drop = torch.nn.Dropout(attn_drop)\n    self.proj = torch.nn.Linear(dim, dim)\n    self.proj_drop = torch.nn.Dropout(proj_drop)\n</code></pre>"},{"location":"reference/quadra/utils/models.html#quadra.utils.models.LocalSelfAttention.forward","title":"<code>forward(x)</code>","text":"<p>Computes the local self attention.</p> <p>Parameters:</p> <ul> <li> x             (<code>Tensor</code>)         \u2013          <p>input tensor</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>         \u2013          <p>Output of the local self attention.</p> </li> </ul> Source code in <code>quadra/utils/models.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"Computes the local self attention.\n\n    Args:\n        x: input tensor\n\n    Returns:\n        Output of the local self attention.\n    \"\"\"\n    B, N, C = x.shape\n    qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n    q, k, v = qkv.unbind(0)  # make torchscript happy (cannot use tensor as tuple)\n\n    attn = (q @ k.transpose(-2, -1)) * self.scale\n    if self.mask_diagonal:\n        attn[torch.eye(N, device=attn.device, dtype=torch.bool).repeat(B, self.num_heads, 1, 1)] = -float(\"inf\")\n    attn = attn.softmax(dim=-1)\n    attn = self.attn_drop(attn)\n\n    x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n    x = self.proj(x)\n    x = self.proj_drop(x)\n    return x\n</code></pre>"},{"location":"reference/quadra/utils/models.html#quadra.utils.models.PositionalEncoding1D","title":"<code>PositionalEncoding1D(d_model, temperature=10000.0, dropout=0.0, max_len=5000)</code>","text":"<p>             Bases: <code>Module</code></p> <p>Standard sine-cosine positional encoding from https://arxiv.org/abs/2010.11929.</p> <p>Parameters:</p> <ul> <li> d_model             (<code>int</code>)         \u2013          <p>Embedding dimension</p> </li> <li> temperature             (<code>float</code>, default:                 <code>10000.0</code> )         \u2013          <p>Temperature for the positional encoding. Defaults to 10000.0.</p> </li> <li> dropout             (<code>float</code>, default:                 <code>0.0</code> )         \u2013          <p>Dropout rate. Defaults to 0.0.</p> </li> <li> max_len             (<code>int</code>, default:                 <code>5000</code> )         \u2013          <p>Maximum length of the sequence. Defaults to 5000.</p> </li> </ul> Source code in <code>quadra/utils/models.py</code> <pre><code>def __init__(self, d_model: int, temperature: float = 10000.0, dropout: float = 0.0, max_len: int = 5000):\n    super().__init__()\n    self.dropout: torch.nn.Dropout | torch.nn.Identity\n    if dropout &gt; 0:\n        self.dropout = torch.nn.Dropout(p=dropout)\n    else:\n        self.dropout = torch.nn.Identity()\n\n    position = torch.arange(max_len).unsqueeze(1)\n    div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(temperature) / d_model))\n    self.pe = torch.zeros(max_len, 1, d_model)\n    self.pe[:, 0, 0::2] = torch.sin(position * div_term)\n    self.pe[:, 0, 1::2] = torch.cos(position * div_term)\n    self.pe = self.pe.permute(1, 0, 2)\n    self.pe = torch.nn.Parameter(self.pe)\n    self.pe.requires_grad = False\n</code></pre>"},{"location":"reference/quadra/utils/models.html#quadra.utils.models.PositionalEncoding1D.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the positional encoding.</p> <p>Parameters:</p> <ul> <li> x             (<code>Tensor</code>)         \u2013          <p>torch tensor [batch_size, seq_len, embedding_dim].</p> </li> </ul> Source code in <code>quadra/utils/models.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n\"\"\"Forward pass of the positional encoding.\n\n    Args:\n        x: torch tensor [batch_size, seq_len, embedding_dim].\n    \"\"\"\n    x = x + self.pe[:, : x.size(1), :]\n    return self.dropout(x)\n</code></pre>"},{"location":"reference/quadra/utils/models.html#quadra.utils.models.clip_gradients","title":"<code>clip_gradients(model, clip)</code>","text":"<p>Parameters:</p> <ul> <li> model             (<code>Module</code>)         \u2013          <p>The model</p> </li> <li> clip             (<code>float</code>)         \u2013          <p>The clip value.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[float]</code>         \u2013          <p>The norms of the gradients</p> </li> </ul> Source code in <code>quadra/utils/models.py</code> <pre><code>def clip_gradients(model: nn.Module, clip: float) -&gt; list[float]:\n\"\"\"Args:\n        model: The model\n        clip: The clip value.\n\n    Returns:\n        The norms of the gradients\n    \"\"\"\n    norms = []\n    for _, p in model.named_parameters():\n        if p.grad is not None:\n            param_norm = p.grad.data.norm(2)\n            norms.append(param_norm.item())\n            clip_coef = clip / (param_norm + 1e-6)\n            if clip_coef &lt; 1:\n                p.grad.data.mul_(clip_coef)\n    return norms\n</code></pre>"},{"location":"reference/quadra/utils/models.html#quadra.utils.models.create_net_hat","title":"<code>create_net_hat(dims, act_fun=torch.nn.ReLU, dropout_p=0)</code>","text":"<p>Create a sequence of linear layers with activation functions and dropout.</p> <p>Parameters:</p> <ul> <li> dims             (<code>list[int]</code>)         \u2013          <p>Dimension of hidden layers and output</p> </li> <li> act_fun             (<code>Callable</code>, default:                 <code>ReLU</code> )         \u2013          <p>activation function to use between layers, default ReLU</p> </li> <li> dropout_p             (<code>float</code>, default:                 <code>0</code> )         \u2013          <p>Dropout probability. Defaults to 0.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Sequential</code>         \u2013          <p>Sequence of linear layers of dimension specified by the input, each linear layer is followed by an activation function and optionally a dropout layer with the input probability</p> </li> </ul> Source code in <code>quadra/utils/models.py</code> <pre><code>def create_net_hat(dims: list[int], act_fun: Callable = torch.nn.ReLU, dropout_p: float = 0) -&gt; torch.nn.Sequential:\n\"\"\"Create a sequence of linear layers with activation functions and dropout.\n\n    Args:\n        dims: Dimension of hidden layers and output\n        act_fun: activation function to use between layers, default ReLU\n        dropout_p: Dropout probability. Defaults to 0.\n\n    Returns:\n        Sequence of linear layers of dimension specified by the input, each linear layer is followed\n            by an activation function and optionally a dropout layer with the input probability\n    \"\"\"\n    components: list[nn.Module] = []\n    for i, _ in enumerate(dims[:-2]):\n        if dropout_p &gt; 0:\n            components.append(torch.nn.Dropout(dropout_p))\n        components.append(net_hat(dims[i], dims[i + 1]))\n        components.append(act_fun())\n    components.append(net_hat(dims[-2], dims[-1]))\n    components.append(L2Norm())\n    return torch.nn.Sequential(*components)\n</code></pre>"},{"location":"reference/quadra/utils/models.html#quadra.utils.models.get_feature","title":"<code>get_feature(feature_extractor, dl, iteration_over_training=1, gradcam=False, classifier=None, input_shape=None, limit_batches=None)</code>","text":"<p>Given a dataloader and a PyTorch model, extract features with the model and return features and labels.</p> <p>Parameters:</p> <ul> <li> dl             (<code>DataLoader</code>)         \u2013          <p>PyTorch dataloader</p> </li> <li> feature_extractor             (<code>Module | BaseEvaluationModel</code>)         \u2013          <p>Pretrained PyTorch backbone</p> </li> <li> iteration_over_training             (<code>int</code>, default:                 <code>1</code> )         \u2013          <p>Extract feature iteration_over_training times for each image (best if used with augmentation)</p> </li> <li> gradcam             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to compute gradcams. Notice that it will slow the function</p> </li> <li> classifier             (<code>ClassifierMixin | None</code>, default:                 <code>None</code> )         \u2013          <p>Scikit-learn classifier</p> </li> <li> input_shape             (<code>tuple[int, int, int] | None</code>, default:                 <code>None</code> )         \u2013          <p>[H,W,C], backbone input shape, needed by classifier's pytorch wrapper</p> </li> <li> limit_batches             (<code>int | None</code>, default:                 <code>None</code> )         \u2013          <p>Limit the number of batches to be processed</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[ndarray, ndarray, ndarray | None]</code>         \u2013          <p>Tuple containing: features: Model features labels: input_labels grayscale_cams: Gradcam output maps, None if gradcam arg is False</p> </li> </ul> Source code in <code>quadra/utils/models.py</code> <pre><code>def get_feature(\n    feature_extractor: torch.nn.Module | BaseEvaluationModel,\n    dl: torch.utils.data.DataLoader,\n    iteration_over_training: int = 1,\n    gradcam: bool = False,\n    classifier: ClassifierMixin | None = None,\n    input_shape: tuple[int, int, int] | None = None,\n    limit_batches: int | None = None,\n) -&gt; tuple[np.ndarray, np.ndarray, np.ndarray | None]:\n\"\"\"Given a dataloader and a PyTorch model, extract features with the model and return features and labels.\n\n    Args:\n        dl: PyTorch dataloader\n        feature_extractor: Pretrained PyTorch backbone\n        iteration_over_training: Extract feature iteration_over_training times for each image\n            (best if used with augmentation)\n        gradcam: Whether to compute gradcams. Notice that it will slow the function\n        classifier: Scikit-learn classifier\n        input_shape: [H,W,C], backbone input shape, needed by classifier's pytorch wrapper\n        limit_batches: Limit the number of batches to be processed\n\n    Returns:\n        Tuple containing:\n            features: Model features\n            labels: input_labels\n            grayscale_cams: Gradcam output maps, None if gradcam arg is False\n    \"\"\"\n    if isinstance(feature_extractor, (TorchEvaluationModel, TorchscriptEvaluationModel)):\n        # If we are working with torch based evaluation models we need to extract the model\n        feature_extractor = feature_extractor.model\n    elif isinstance(feature_extractor, ONNXEvaluationModel):\n        gradcam = False\n\n    feature_extractor.eval()\n\n    # Setup gradcam\n    if gradcam:\n        if not hasattr(feature_extractor, \"features_extractor\"):\n            gradcam = False\n        elif isinstance(feature_extractor.features_extractor, timm.models.resnet.ResNet):\n            target_layers = [feature_extractor.features_extractor.layer4[-1]]\n            cam = GradCAM(\n                model=feature_extractor,\n                target_layers=target_layers,\n            )\n            for p in feature_extractor.features_extractor.layer4[-1].parameters():\n                p.requires_grad = True\n        elif is_vision_transformer(feature_extractor.features_extractor):\n            grad_rollout = VitAttentionGradRollout(\n                feature_extractor.features_extractor,\n                classifier=classifier,\n                example_input=None if input_shape is None else torch.randn(1, *input_shape),\n            )\n        else:\n            gradcam = False\n\n        if not gradcam:\n            log.warning(\"Gradcam not implemented for this backbone, it will not be computed\")\n\n    # Extract features from data\n\n    for iteration in range(iteration_over_training):\n        for i, b in enumerate(tqdm.tqdm(dl)):\n            x1, y1 = b\n\n            if hasattr(feature_extractor, \"parameters\"):\n                # Move input to the correct device and dtype\n                parameter = next(feature_extractor.parameters())\n                x1 = x1.to(parameter.device).to(parameter.dtype)\n            elif isinstance(feature_extractor, BaseEvaluationModel):\n                x1 = x1.to(feature_extractor.device).to(feature_extractor.model_dtype)\n\n            if gradcam:\n                y_hat = cast(\n                    Union[list[torch.Tensor], tuple[torch.Tensor], torch.Tensor], feature_extractor(x1).detach()\n                )\n                # mypy can't detect that gradcam is true only if we have a features_extractor\n                if is_vision_transformer(feature_extractor.features_extractor):  # type: ignore[union-attr]\n                    grayscale_cam_low_res = grad_rollout(\n                        input_tensor=x1, targets_list=y1\n                    )  # TODO: We are using labels (y1) but it would be better to use preds\n                    orig_shape = grayscale_cam_low_res.shape\n                    new_shape = (orig_shape[0], x1.shape[2], x1.shape[3])\n                    zoom_factors = tuple(np.array(new_shape) / np.array(orig_shape))\n                    grayscale_cam = ndimage.zoom(grayscale_cam_low_res, zoom_factors, order=1)\n                else:\n                    grayscale_cam = cam(input_tensor=x1, targets=None)\n                feature_extractor.zero_grad(set_to_none=True)  # type: ignore[union-attr]\n            else:\n                with torch.no_grad():\n                    y_hat = cast(Union[list[torch.Tensor], tuple[torch.Tensor], torch.Tensor], feature_extractor(x1))\n                grayscale_cams = None\n\n            if isinstance(y_hat, (list, tuple)):\n                y_hat = y_hat[0].cpu()\n            else:\n                y_hat = y_hat.cpu()\n\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n\n            if i == 0 and iteration == 0:\n                features = torch.cat([y_hat], dim=0)\n                labels = np.concatenate([y1])\n                if gradcam:\n                    grayscale_cams = grayscale_cam\n            else:\n                features = torch.cat([features, y_hat], dim=0)\n                labels = np.concatenate([labels, y1], axis=0)\n                if gradcam:\n                    grayscale_cams = np.concatenate([grayscale_cams, grayscale_cam], axis=0)\n\n            if limit_batches is not None and (i + 1) &gt;= limit_batches:\n                break\n\n    return features.detach().numpy(), labels, grayscale_cams\n</code></pre>"},{"location":"reference/quadra/utils/models.html#quadra.utils.models.init_weights","title":"<code>init_weights(m)</code>","text":"<p>Basic weight initialization.</p> Source code in <code>quadra/utils/models.py</code> <pre><code>def init_weights(m):\n\"\"\"Basic weight initialization.\"\"\"\n    classname = m.__class__.__name__\n    if classname.find(\"Conv2d\") != -1 or classname.find(\"ConvTranspose2d\") != -1:\n        nn.init.kaiming_uniform_(m.weight)\n        nn.init.zeros_(m.bias)\n    elif classname.find(\"BatchNorm\") != -1:\n        nn.init.normal_(m.weight, 1.0, 0.02)\n        nn.init.zeros_(m.bias)\n    elif classname.find(\"Linear\") != -1:\n        nn.init.xavier_normal_(m.weight)\n        m.bias.data.fill_(0)\n</code></pre>"},{"location":"reference/quadra/utils/models.html#quadra.utils.models.is_vision_transformer","title":"<code>is_vision_transformer(model)</code>","text":"<p>Verify if pytorch module is a Vision Transformer. This check is primarily needed for gradcam computation in classification tasks.</p> <p>Parameters:</p> <ul> <li> model             (<code>Module</code>)         \u2013          <p>Model</p> </li> </ul> Source code in <code>quadra/utils/models.py</code> <pre><code>def is_vision_transformer(model: torch.nn.Module) -&gt; bool:\n\"\"\"Verify if pytorch module is a Vision Transformer.\n    This check is primarily needed for gradcam computation in classification tasks.\n\n    Args:\n        model: Model\n    \"\"\"\n    return type(model).__name__ == \"VisionTransformer\"\n</code></pre>"},{"location":"reference/quadra/utils/models.html#quadra.utils.models.net_hat","title":"<code>net_hat(input_size, output_size)</code>","text":"<p>Create a linear layer with input and output neurons.</p> <p>Parameters:</p> <ul> <li> input_size             (<code>int</code>)         \u2013          <p>Number of input neurons</p> </li> <li> output_size             (<code>int</code>)         \u2013          <p>Number of output neurons.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Sequential</code>         \u2013          <p>A sequential containing a single Linear layer taking input neurons and producing output neurons</p> </li> </ul> Source code in <code>quadra/utils/models.py</code> <pre><code>def net_hat(input_size: int, output_size: int) -&gt; torch.nn.Sequential:\n\"\"\"Create a linear layer with input and output neurons.\n\n    Args:\n        input_size: Number of input neurons\n        output_size: Number of output neurons.\n\n    Returns:\n        A sequential containing a single Linear layer taking input neurons and producing output neurons\n\n    \"\"\"\n    return torch.nn.Sequential(torch.nn.Linear(input_size, output_size))\n</code></pre>"},{"location":"reference/quadra/utils/models.html#quadra.utils.models.trunc_normal_","title":"<code>trunc_normal_(tensor, mean=0.0, std=1.0, a=-2.0, b=2.0)</code>","text":"<p>Call <code>_no_grad_trunc_normal_</code> with <code>torch.no_grad()</code>.</p> <p>Parameters:</p> <ul> <li> tensor             (<code>Tensor</code>)         \u2013          <p>an n-dimensional <code>torch.Tensor</code></p> </li> <li> mean             (<code>float</code>, default:                 <code>0.0</code> )         \u2013          <p>the mean of the normal distribution</p> </li> <li> std             (<code>float</code>, default:                 <code>1.0</code> )         \u2013          <p>the standard deviation of the normal distribution</p> </li> <li> a             (<code>float</code>, default:                 <code>-2.0</code> )         \u2013          <p>the minimum cutoff</p> </li> <li> b             (<code>float</code>, default:                 <code>2.0</code> )         \u2013          <p>the maximum cutoff</p> </li> </ul> Source code in <code>quadra/utils/models.py</code> <pre><code>def trunc_normal_(tensor: torch.Tensor, mean: float = 0.0, std: float = 1.0, a: float = -2.0, b: float = 2.0):\n\"\"\"Call `_no_grad_trunc_normal_` with `torch.no_grad()`.\n\n    Args:\n        tensor: an n-dimensional `torch.Tensor`\n        mean: the mean of the normal distribution\n        std: the standard deviation of the normal distribution\n        a: the minimum cutoff\n        b: the maximum cutoff\n    \"\"\"\n    return _no_grad_trunc_normal_(tensor, mean, std, a, b)\n</code></pre>"},{"location":"reference/quadra/utils/resolver.html","title":"resolver","text":""},{"location":"reference/quadra/utils/resolver.html#quadra.utils.resolver.as_tuple","title":"<code>as_tuple(*args)</code>","text":"<p>Resolves a list of arguments to a tuple.</p> Source code in <code>quadra/utils/resolver.py</code> <pre><code>def as_tuple(*args: Any) -&gt; tuple[Any, ...]:\n\"\"\"Resolves a list of arguments to a tuple.\"\"\"\n    return tuple(args)\n</code></pre>"},{"location":"reference/quadra/utils/resolver.html#quadra.utils.resolver.multirun_subdir_beautify","title":"<code>multirun_subdir_beautify(subdir)</code>","text":"<p>Change the subdir name to be more readable and usable, this function will replace / with | to avoid creating undesired subdirectories and remove the left part of the equals sign to avoid having too long names.</p> <p>Parameters:</p> <ul> <li> subdir             (<code>str</code>)         \u2013          <p>The subdir name.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>         \u2013          <p>The beautified subdir name.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; multirun_subdir_beautify(\"experiment=pippo/anomaly/padim,trainer.batch_size=32\")\n\"pippo|anomaly|padim,32\"\n</code></pre> Source code in <code>quadra/utils/resolver.py</code> <pre><code>def multirun_subdir_beautify(subdir: str) -&gt; str:\n\"\"\"Change the subdir name to be more readable and usable, this function will replace / with | to avoid creating\n    undesired subdirectories and remove the left part of the equals sign to avoid having too long names.\n\n    Args:\n        subdir: The subdir name.\n\n    Returns:\n        The beautified subdir name.\n\n    Examples:\n        &gt;&gt;&gt; multirun_subdir_beautify(\"experiment=pippo/anomaly/padim,trainer.batch_size=32\")\n        \"pippo|anomaly|padim,32\"\n    \"\"\"\n    hydra_cfg = HydraConfig.get()\n    if hydra_cfg.mode is None or hydra_cfg.mode.name == \"RUN\":\n        return subdir\n    # Remove slashes to avoid creating multiple subdirs\n    # TODO: if right side of the equals sign has `,` this will not work.\n    subdir_list = subdir.replace(\"/\", \"|\").split(\",\")\n    subdir = \",\".join([x.split(\"=\")[1].replace(\" \", \"\") for x in subdir_list])\n\n    return subdir\n</code></pre>"},{"location":"reference/quadra/utils/resolver.html#quadra.utils.resolver.register_resolvers","title":"<code>register_resolvers()</code>","text":"<p>Register custom resolver.</p> Source code in <code>quadra/utils/resolver.py</code> <pre><code>def register_resolvers() -&gt; None:\n\"\"\"Register custom resolver.\"\"\"\n    OmegaConf.register_new_resolver(\"multirun_subdir_beautify\", multirun_subdir_beautify)\n    OmegaConf.register_new_resolver(\"as_tuple\", as_tuple)\n</code></pre>"},{"location":"reference/quadra/utils/segmentation.html","title":"segmentation","text":""},{"location":"reference/quadra/utils/segmentation.html#quadra.utils.segmentation.smooth_mask","title":"<code>smooth_mask(mask)</code>","text":"<p>Smooths for segmentation.</p> <p>Parameters:</p> <ul> <li> mask             (<code>ndarray</code>)         \u2013          <p>Input mask</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>         \u2013          <p>Smoothed mask</p> </li> </ul> Source code in <code>quadra/utils/segmentation.py</code> <pre><code>def smooth_mask(mask: np.ndarray) -&gt; np.ndarray:\n\"\"\"Smooths for segmentation.\n\n    Args:\n        mask: Input mask\n\n    Returns:\n        Smoothed mask\n    \"\"\"\n    labeled_mask = skimage.measure.label(mask)\n    labels = np.arange(0, np.max(labeled_mask) + 1)\n    output_mask = np.zeros_like(mask).astype(np.float32)\n    for l in labels:\n        component_mask = labeled_mask == l\n        _, distance = medial_axis(component_mask, return_distance=True)\n        component_mask_norm = distance ** (1 / 2.2)\n        component_mask_norm = (component_mask_norm - np.min(component_mask_norm)) / (\n            np.max(component_mask_norm) - np.min(component_mask_norm)\n        )\n        output_mask += component_mask_norm\n    output_mask = output_mask * mask\n    return output_mask\n</code></pre>"},{"location":"reference/quadra/utils/utils.html","title":"utils","text":"<p>Common utility functions. Some of them are mostly based on https://github.com/ashleve/lightning-hydra-template.</p>"},{"location":"reference/quadra/utils/utils.html#quadra.utils.utils.AllGatherSyncFunction","title":"<code>AllGatherSyncFunction</code>","text":"<p>             Bases: <code>Function</code></p> <p>Function to gather gradients from multiple GPUs.</p>"},{"location":"reference/quadra/utils/utils.html#quadra.utils.utils.HydraEncoder","title":"<code>HydraEncoder</code>","text":"<p>             Bases: <code>JSONEncoder</code></p> <p>Custom JSON encoder to handle OmegaConf objects.</p>"},{"location":"reference/quadra/utils/utils.html#quadra.utils.utils.HydraEncoder.default","title":"<code>default(o)</code>","text":"<p>Convert OmegaConf objects to base python objects.</p> Source code in <code>quadra/utils/utils.py</code> <pre><code>def default(self, o):\n\"\"\"Convert OmegaConf objects to base python objects.\"\"\"\n    if o is not None and OmegaConf.is_config(o):\n        return OmegaConf.to_container(o)\n    return json.JSONEncoder.default(self, o)\n</code></pre>"},{"location":"reference/quadra/utils/utils.html#quadra.utils.utils.NumpyEncoder","title":"<code>NumpyEncoder</code>","text":"<p>             Bases: <code>JSONEncoder</code></p> <p>Custom JSON encoder to handle numpy objects.</p>"},{"location":"reference/quadra/utils/utils.html#quadra.utils.utils.NumpyEncoder.default","title":"<code>default(o)</code>","text":"<p>Custom JSON encoder to handle numpy objects.</p> Source code in <code>quadra/utils/utils.py</code> <pre><code>def default(self, o):\n\"\"\"Custom JSON encoder to handle numpy objects.\"\"\"\n    if o is not None:\n        if isinstance(o, np.ndarray):\n            if o.size == 1:\n                return o.item()\n            return o.tolist()\n        if isinstance(o, np.number):\n            return o.item()\n    return json.JSONEncoder.default(self, o)\n</code></pre>"},{"location":"reference/quadra/utils/utils.html#quadra.utils.utils.concat_all_gather","title":"<code>concat_all_gather(tensor)</code>","text":"<p>Performs all_gather operation on the provided tensors. *** Warning ***: torch.distributed.all_gather has no gradient.</p> Source code in <code>quadra/utils/utils.py</code> <pre><code>@torch.no_grad()\ndef concat_all_gather(tensor):\n\"\"\"Performs all_gather operation on the provided tensors.\n    *** Warning ***: torch.distributed.all_gather has no gradient.\n    \"\"\"\n    tensors_gather = [torch.ones_like(tensor) for _ in range(torch.distributed.get_world_size())]\n    torch.distributed.all_gather(tensors_gather, tensor, async_op=False)\n\n    output = torch.cat(tensors_gather, dim=0)\n    return output\n</code></pre>"},{"location":"reference/quadra/utils/utils.html#quadra.utils.utils.extras","title":"<code>extras(config)</code>","text":"<p>A couple of optional utilities, controlled by main config file: - disabling warnings - forcing debug friendly configuration - verifying experiment name is set when running in experiment mode. Modifies DictConfig in place.</p> <p>Parameters:</p> <ul> <li> config             (<code>DictConfig</code>)         \u2013          <p>Configuration composed by Hydra.</p> </li> </ul> Source code in <code>quadra/utils/utils.py</code> <pre><code>def extras(config: DictConfig) -&gt; None:\n\"\"\"A couple of optional utilities, controlled by main config file:\n    - disabling warnings\n    - forcing debug friendly configuration\n    - verifying experiment name is set when running in experiment mode.\n    Modifies DictConfig in place.\n\n    Args:\n        config: Configuration composed by Hydra.\n    \"\"\"\n    logging.basicConfig()\n    logging.getLogger().setLevel(config.core.log_level.upper())\n\n    log = get_logger(__name__)\n    config.core.command += \" \".join(sys.argv)\n    config.core.experiment_path = os.getcwd()\n\n    # disable python warnings if &lt;config.ignore_warnings=True&gt;\n    if config.get(\"ignore_warnings\"):\n        log.info(\"Disabling python warnings! &lt;config.ignore_warnings=True&gt;\")\n        warnings.filterwarnings(\"ignore\")\n\n    # force debugger friendly configuration if &lt;config.trainer.fast_dev_run=True&gt;\n    # debuggers don't like GPUs and multiprocessing\n    if config.get(\"trainer\") and config.trainer.get(\"fast_dev_run\"):\n        log.info(\"Forcing debugger friendly configuration! &lt;config.trainer.fast_dev_run=True&gt;\")\n        if config.trainer.get(\"gpus\"):\n            config.trainer.devices = 1\n            config.trainer.accelerator = \"cpu\"\n            config.trainer.gpus = None\n        if config.datamodule.get(\"pin_memory\"):\n            config.datamodule.pin_memory = False\n        if config.datamodule.get(\"num_workers\"):\n            config.datamodule.num_workers = 0\n</code></pre>"},{"location":"reference/quadra/utils/utils.html#quadra.utils.utils.finish","title":"<code>finish(config, module, datamodule, trainer, callbacks, logger, export_folder)</code>","text":"<p>Upload config files to MLFlow server.</p> <p>Parameters:</p> <ul> <li> config             (<code>DictConfig</code>)         \u2013          <p>Configuration composed by Hydra.</p> </li> <li> module             (<code>LightningModule</code>)         \u2013          <p>LightningModule.</p> </li> <li> datamodule             (<code>LightningDataModule</code>)         \u2013          <p>LightningDataModule.</p> </li> <li> trainer             (<code>Trainer</code>)         \u2013          <p>LightningTrainer.</p> </li> <li> callbacks             (<code>list[Callback]</code>)         \u2013          <p>List of LightningCallbacks.</p> </li> <li> logger             (<code>list[Logger]</code>)         \u2013          <p>List of LightningLoggers.</p> </li> <li> export_folder             (<code>str</code>)         \u2013          <p>Folder where the deployment models are exported.</p> </li> </ul> Source code in <code>quadra/utils/utils.py</code> <pre><code>def finish(\n    config: DictConfig,\n    module: pl.LightningModule,\n    datamodule: pl.LightningDataModule,\n    trainer: pl.Trainer,\n    callbacks: list[pl.Callback],\n    logger: list[pl.loggers.Logger],\n    export_folder: str,\n) -&gt; None:\n\"\"\"Upload config files to MLFlow server.\n\n    Args:\n        config: Configuration composed by Hydra.\n        module: LightningModule.\n        datamodule: LightningDataModule.\n        trainer: LightningTrainer.\n        callbacks: List of LightningCallbacks.\n        logger: List of LightningLoggers.\n        export_folder: Folder where the deployment models are exported.\n    \"\"\"\n    # pylint: disable=unused-argument\n    if len(logger) &gt; 0 and config.core.get(\"upload_artifacts\"):\n        mlflow_logger = get_mlflow_logger(trainer=trainer)\n        tensorboard_logger = get_tensorboard_logger(trainer=trainer)\n        file_names = [\"config.yaml\", \"config_resolved.yaml\", \"config_tree.txt\", \"data/dataset.csv\"]\n        if \"16\" in str(trainer.precision):\n            index = _parse_gpu_ids(config.trainer.devices, include_cuda=True)[0]\n            device = \"cuda:\" + str(index)\n            half_precision = True\n        else:\n            device = \"cpu\"\n            half_precision = False\n\n        if mlflow_logger is not None:\n            config_paths = []\n\n            for f in file_names:\n                if os.path.isfile(os.path.join(os.getcwd(), f)):\n                    config_paths.append(os.path.join(os.getcwd(), f))\n\n            for path in config_paths:\n                mlflow_logger.experiment.log_artifact(\n                    run_id=mlflow_logger.run_id, local_path=path, artifact_path=\"metadata\"\n                )\n\n            deployed_models = glob.glob(os.path.join(export_folder, \"*\"))\n            model_json: dict[str, Any] | None = None\n\n            if os.path.exists(os.path.join(export_folder, \"model.json\")):\n                with open(os.path.join(export_folder, \"model.json\")) as json_file:\n                    model_json = json.load(json_file)\n\n            if model_json is not None:\n                input_size = model_json[\"input_size\"]\n                # Not a huge fan of this check\n                if not isinstance(input_size[0], list):\n                    # Input size is not a list of lists\n                    input_size = [input_size]\n                inputs = cast(\n                    list[Any],\n                    quadra_export.generate_torch_inputs(input_size, device=device, half_precision=half_precision),\n                )\n                types_to_upload = config.core.get(\"upload_models\")\n                for model_path in deployed_models:\n                    model_type = model_type_from_path(model_path)\n                    if model_type is None:\n                        logging.warning(\"%s model type not supported\", model_path)\n                        continue\n                    if model_type is not None and model_type in types_to_upload:\n                        if model_type == \"pytorch\":\n                            logging.warning(\"Pytorch format still not supported for mlflow upload\")\n                            continue\n\n                        model = quadra_export.import_deployment_model(\n                            model_path,\n                            device=device,\n                            inference_config=config.inference,\n                        )\n\n                        if model_type in [\"torchscript\", \"pytorch\"]:\n                            signature = infer_signature_model(model.model, inputs)\n                            with mlflow.start_run(run_id=mlflow_logger.run_id) as _:\n                                mlflow.pytorch.log_model(\n                                    model.model,\n                                    artifact_path=model_path,\n                                    signature=signature,\n                                )\n                        elif model_type in [\"onnx\", \"simplified_onnx\"] and ONNX_AVAILABLE:\n                            signature = infer_signature_model(model, inputs)\n                            with mlflow.start_run(run_id=mlflow_logger.run_id) as _:\n                                if model.model_path is None:\n                                    logging.warning(\n                                        \"Cannot log onnx model on mlflow, \\\n                                        BaseEvaluationModel 'model_path' attribute is None\"\n                                    )\n                                else:\n                                    model_proto = onnx.load(model.model_path)\n                                    mlflow.onnx.log_model(\n                                        model_proto,\n                                        artifact_path=model_path,\n                                        signature=signature,\n                                    )\n\n        if tensorboard_logger is not None:\n            config_paths = []\n            for f in file_names:\n                if os.path.isfile(os.path.join(os.getcwd(), f)):\n                    config_paths.append(os.path.join(os.getcwd(), f))\n\n            for path in config_paths:\n                upload_file_tensorboard(file_path=path, tensorboard_logger=tensorboard_logger)\n\n            tensorboard_logger.experiment.flush()\n</code></pre>"},{"location":"reference/quadra/utils/utils.html#quadra.utils.utils.flatten_list","title":"<code>flatten_list(input_list)</code>","text":"<p>Return an iterator over the flattened list.</p> <p>Parameters:</p> <ul> <li> input_list             (<code>Iterable[Any]</code>)         \u2013          <p>the list to be flattened</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>Any</code>         \u2013          <p>The iterator over the flattend list</p> </li> </ul> Source code in <code>quadra/utils/utils.py</code> <pre><code>def flatten_list(input_list: Iterable[Any]) -&gt; Iterator[Any]:\n\"\"\"Return an iterator over the flattened list.\n\n    Args:\n        input_list: the list to be flattened\n\n    Yields:\n        The iterator over the flattend list\n    \"\"\"\n    for v in input_list:\n        if isinstance(v, Iterable) and not isinstance(v, (str, bytes)):\n            yield from flatten_list(v)\n        else:\n            yield v\n</code></pre>"},{"location":"reference/quadra/utils/utils.html#quadra.utils.utils.get_device","title":"<code>get_device(cuda=True)</code>","text":"<p>Returns the device to use for training.</p> <p>Parameters:</p> <ul> <li> cuda             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>whether to use cuda or not</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>         \u2013          <p>The device to use</p> </li> </ul> Source code in <code>quadra/utils/utils.py</code> <pre><code>def get_device(cuda: bool = True) -&gt; str:\n\"\"\"Returns the device to use for training.\n\n    Args:\n        cuda: whether to use cuda or not\n\n    Returns:\n        The device to use\n    \"\"\"\n    if torch.cuda.is_available() and cuda:\n        return \"cuda:0\"\n\n    return \"cpu\"\n</code></pre>"},{"location":"reference/quadra/utils/utils.html#quadra.utils.utils.get_logger","title":"<code>get_logger(name=__name__)</code>","text":"<p>Initializes multi-GPU-friendly python logger.</p> Source code in <code>quadra/utils/utils.py</code> <pre><code>def get_logger(name=__name__) -&gt; logging.Logger:\n\"\"\"Initializes multi-GPU-friendly python logger.\"\"\"\n    logger = logging.getLogger(name)\n\n    # this ensures all logging levels get marked with the rank zero decorator\n    # otherwise logs would get multiplied for each GPU process in multi-GPU setup\n    for level in (\"debug\", \"info\", \"warning\", \"error\", \"exception\", \"fatal\", \"critical\"):\n        setattr(logger, level, rank_zero_only(getattr(logger, level)))\n\n    return logger\n</code></pre>"},{"location":"reference/quadra/utils/utils.html#quadra.utils.utils.get_tensorboard_logger","title":"<code>get_tensorboard_logger(trainer)</code>","text":"<p>Safely get tensorboard logger from Lightning Trainer loggers.</p> <p>Parameters:</p> <ul> <li> trainer             (<code>Trainer</code>)         \u2013          <p>Pytorch Lightning Trainer.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>TensorBoardLogger | None</code>         \u2013          <p>An mlflow logger if available, else None.</p> </li> </ul> Source code in <code>quadra/utils/utils.py</code> <pre><code>def get_tensorboard_logger(trainer: pl.Trainer) -&gt; TensorBoardLogger | None:\n\"\"\"Safely get tensorboard logger from Lightning Trainer loggers.\n\n    Args:\n        trainer: Pytorch Lightning Trainer.\n\n    Returns:\n        An mlflow logger if available, else None.\n    \"\"\"\n    if isinstance(trainer.logger, TensorBoardLogger):\n        return trainer.logger\n\n    if isinstance(trainer.logger, list):\n        for logger in trainer.logger:\n            if isinstance(logger, TensorBoardLogger):\n                return logger\n\n    return None\n</code></pre>"},{"location":"reference/quadra/utils/utils.html#quadra.utils.utils.load_envs","title":"<code>load_envs(env_file=None)</code>","text":"<p>Load all the environment variables defined in the <code>env_file</code>. This is equivalent to <code>. env_file</code> in bash.</p> <p>It is possible to define all the system specific variables in the <code>env_file</code>.</p> <p>Parameters:</p> <ul> <li> env_file             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>the file that defines the environment variables to use. If None          it searches for a <code>.env</code> file in the project.</p> </li> </ul> Source code in <code>quadra/utils/utils.py</code> <pre><code>def load_envs(env_file: str | None = None) -&gt; None:\n\"\"\"Load all the environment variables defined in the `env_file`.\n    This is equivalent to `. env_file` in bash.\n\n    It is possible to define all the system specific variables in the `env_file`.\n\n    Args:\n        env_file: the file that defines the environment variables to use. If None\n                     it searches for a `.env` file in the project.\n    \"\"\"\n    dotenv.load_dotenv(dotenv_path=env_file, override=True)\n</code></pre>"},{"location":"reference/quadra/utils/utils.html#quadra.utils.utils.log_hyperparameters","title":"<code>log_hyperparameters(config, model, trainer)</code>","text":"<p>This method controls which parameters from Hydra config are saved by Lightning loggers.</p> Additionaly saves <ul> <li>number of trainable model parameters</li> </ul> Source code in <code>quadra/utils/utils.py</code> <pre><code>@rank_zero_only\ndef log_hyperparameters(\n    config: DictConfig,\n    model: pl.LightningModule,\n    trainer: pl.Trainer,\n) -&gt; None:\n\"\"\"This method controls which parameters from Hydra config are saved by Lightning loggers.\n\n    Additionaly saves:\n        - number of trainable model parameters\n    \"\"\"\n    log = get_logger(__name__)\n\n    if not HydraConfig.initialized() or trainer.logger is None:\n        return\n\n    log.info(\"Logging hyperparameters!\")\n    hydra_cfg = HydraConfig.get()\n    hydra_choices = OmegaConf.to_container(hydra_cfg.runtime.choices)\n    if isinstance(hydra_choices, dict):\n        # For multirun override the choices that are not automatically updated\n        for item in hydra_cfg.overrides.task:\n            if \".\" in item:\n                continue\n\n            override, value = item.split(\"=\")\n            hydra_choices[override] = value\n\n        hparams = {}\n        hydra_choices_final = {}\n        for k, v in hydra_choices.items():\n            if isinstance(k, str):\n                k_replaced = k.replace(\"@\", \"_at_\")\n                hydra_choices_final[k_replaced] = v\n                if v is not None and isinstance(v, str) and \"@\" in v:\n                    hydra_choices_final[k_replaced] = v.replace(\"@\", \"_at_\")\n\n        hparams.update(hydra_choices_final)\n    else:\n        logging.warning(\"Hydra choices is not a dictionary, skip adding them to the logger\")\n    # save number of model parameters\n    hparams[\"model/params_total\"] = sum(p.numel() for p in model.parameters())\n    hparams[\"model/params_trainable\"] = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    hparams[\"model/params_not_trainable\"] = sum(p.numel() for p in model.parameters() if not p.requires_grad)\n    hparams[\"experiment_path\"] = config.core.experiment_path\n    hparams[\"command\"] = config.core.command\n    hparams[\"library/version\"] = str(quadra.__version__)\n\n    with open(os.devnull, \"w\") as fnull:\n        if subprocess.call([\"git\", \"-C\", get_original_cwd(), \"status\"], stderr=subprocess.STDOUT, stdout=fnull) == 0:\n            try:\n                hparams[\"git/commit\"] = subprocess.check_output([\"git\", \"rev-parse\", \"HEAD\"]).decode(\"ascii\").strip()\n                hparams[\"git/branch\"] = (\n                    subprocess.check_output([\"git\", \"rev-parse\", \"--abbrev-ref\", \"HEAD\"]).decode(\"ascii\").strip()\n                )\n                hparams[\"git/remote\"] = (\n                    subprocess.check_output([\"git\", \"remote\", \"get-url\", \"origin\"]).decode(\"ascii\").strip()\n                )\n            except subprocess.CalledProcessError:\n                log.warning(\n                    \"Could not get git commit, branch or remote information, the repository might not have any commits \"\n                    \" yet or it might have been initialized wrongly.\"\n                )\n        else:\n            log.warning(\"Could not find git repository, skipping git commit and branch info\")\n\n    # send hparams to all loggers\n    trainer.logger.log_hyperparams(hparams)\n</code></pre>"},{"location":"reference/quadra/utils/utils.html#quadra.utils.utils.model_type_from_path","title":"<code>model_type_from_path(model_path)</code>","text":"<p>Determine the type of the machine learning model based on its file extension.</p> <p>Parameters: - model_path (str): The file path of the machine learning model.</p> <ul> <li>str: The type of the model, which can be one of the following:</li> <li>\"torchscript\" if the model has a '.pt' extension (TorchScript).</li> <li>\"pytorch\" if the model has a '.pth' extension (PyTorch).</li> <li>\"simplified_onnx\" if the model file ends with 'simplified.onnx' (Simplified ONNX).</li> <li>\"onnx\" if the model has a '.onnx' extension (ONNX).</li> <li>\"json\" id the model has a '.json' extension (JSON).</li> <li>None if model extension is not supported.</li> </ul> <p>Example: <pre><code>model_path = \"path/to/your/model.onnx\"\nmodel_type = model_type_from_path(model_path)\nprint(f\"The model type is: {model_type}\")\n</code></pre></p> Source code in <code>quadra/utils/utils.py</code> <pre><code>def model_type_from_path(model_path: str) -&gt; str | None:\n\"\"\"Determine the type of the machine learning model based on its file extension.\n\n    Parameters:\n    - model_path (str): The file path of the machine learning model.\n\n    Returns:\n    - str: The type of the model, which can be one of the following:\n      - \"torchscript\" if the model has a '.pt' extension (TorchScript).\n      - \"pytorch\" if the model has a '.pth' extension (PyTorch).\n      - \"simplified_onnx\" if the model file ends with 'simplified.onnx' (Simplified ONNX).\n      - \"onnx\" if the model has a '.onnx' extension (ONNX).\n      - \"json\" id the model has a '.json' extension (JSON).\n      - None if model extension is not supported.\n\n    Example:\n    ```python\n    model_path = \"path/to/your/model.onnx\"\n    model_type = model_type_from_path(model_path)\n    print(f\"The model type is: {model_type}\")\n    ```\n    \"\"\"\n    if model_path.endswith(\".pt\"):\n        return \"torchscript\"\n    if model_path.endswith(\".pth\"):\n        return \"pytorch\"\n    if model_path.endswith(\"simplified.onnx\"):\n        return \"simplified_onnx\"\n    if model_path.endswith(\".onnx\"):\n        return \"onnx\"\n    if model_path.endswith(\".json\"):\n        return \"json\"\n    return None\n</code></pre>"},{"location":"reference/quadra/utils/utils.html#quadra.utils.utils.nested_set","title":"<code>nested_set(dic, keys, value)</code>","text":"<p>Assign the value of a dictionary using nested keys.</p> Source code in <code>quadra/utils/utils.py</code> <pre><code>def nested_set(dic: dict, keys: list[str], value: str) -&gt; None:\n\"\"\"Assign the value of a dictionary using nested keys.\"\"\"\n    for key in keys[:-1]:\n        dic = dic.setdefault(key, {})\n\n    dic[keys[-1]] = value\n</code></pre>"},{"location":"reference/quadra/utils/utils.html#quadra.utils.utils.print_config","title":"<code>print_config(config, fields=('task', 'trainer', 'model', 'datamodule', 'callbacks', 'logger', 'core', 'backbone', 'transforms', 'optimizer', 'scheduler'), resolve=True)</code>","text":"<p>Prints content of DictConfig using Rich library and its tree structure.</p> <p>Parameters:</p> <ul> <li> config             (<code>DictConfig</code>)         \u2013          <p>Configuration composed by Hydra.</p> </li> <li> fields             (<code>Sequence[str]</code>, default:                 <code>('task', 'trainer', 'model', 'datamodule', 'callbacks', 'logger', 'core', 'backbone', 'transforms', 'optimizer', 'scheduler')</code> )         \u2013          <p>Determines which main fields from config will be printed and in what order.</p> </li> <li> resolve             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Whether to resolve reference fields of DictConfig.</p> </li> </ul> Source code in <code>quadra/utils/utils.py</code> <pre><code>@rank_zero_only\ndef print_config(\n    config: DictConfig,\n    fields: Sequence[str] = (\n        \"task\",\n        \"trainer\",\n        \"model\",\n        \"datamodule\",\n        \"callbacks\",\n        \"logger\",\n        \"core\",\n        \"backbone\",\n        \"transforms\",\n        \"optimizer\",\n        \"scheduler\",\n    ),\n    resolve: bool = True,\n) -&gt; None:\n\"\"\"Prints content of DictConfig using Rich library and its tree structure.\n\n    Args:\n        config: Configuration composed by Hydra.\n        fields: Determines which main fields from config will\n            be printed and in what order.\n        resolve: Whether to resolve reference fields of DictConfig.\n    \"\"\"\n    style = \"dim\"\n    tree = rich.tree.Tree(\"CONFIG\", style=style, guide_style=style)\n\n    for field in fields:\n        branch = tree.add(field, style=style, guide_style=style)\n\n        config_section = config.get(field)\n        branch_content = str(config_section)\n        if isinstance(config_section, DictConfig):\n            branch_content = OmegaConf.to_yaml(config_section, resolve=resolve)\n\n        branch.add(rich.syntax.Syntax(branch_content, \"yaml\"))\n\n    rich.print(tree)\n\n    with open(\"config_tree.txt\", \"w\") as fp:\n        rich.print(tree, file=fp)\n</code></pre>"},{"location":"reference/quadra/utils/utils.html#quadra.utils.utils.setup_opencv","title":"<code>setup_opencv()</code>","text":"<p>Setup OpenCV to use only one thread and not use OpenCL.</p> Source code in <code>quadra/utils/utils.py</code> <pre><code>def setup_opencv() -&gt; None:\n\"\"\"Setup OpenCV to use only one thread and not use OpenCL.\"\"\"\n    cv2.setNumThreads(1)\n    cv2.ocl.setUseOpenCL(False)\n</code></pre>"},{"location":"reference/quadra/utils/utils.html#quadra.utils.utils.upload_file_tensorboard","title":"<code>upload_file_tensorboard(file_path, tensorboard_logger)</code>","text":"<p>Upload a file to tensorboard handling different extensions.</p> <p>Parameters:</p> <ul> <li> file_path             (<code>str</code>)         \u2013          <p>Path to the file to upload.</p> </li> <li> tensorboard_logger             (<code>TensorBoardLogger</code>)         \u2013          <p>Tensorboard logger instance.</p> </li> </ul> Source code in <code>quadra/utils/utils.py</code> <pre><code>def upload_file_tensorboard(file_path: str, tensorboard_logger: TensorBoardLogger) -&gt; None:\n\"\"\"Upload a file to tensorboard handling different extensions.\n\n    Args:\n        file_path: Path to the file to upload.\n        tensorboard_logger: Tensorboard logger instance.\n    \"\"\"\n    tag = os.path.basename(file_path)\n    ext = os.path.splitext(file_path)[1].lower()\n\n    if ext == \".json\":\n        with open(file_path) as f:\n            json_content = json.load(f)\n\n            json_content = f\"```json\\n{json.dumps(json_content, indent=4)}\\n```\"\n            tensorboard_logger.experiment.add_text(tag=tag, text_string=json_content, global_step=0)\n    elif ext in [\".yaml\", \".yml\"]:\n        with open(file_path) as f:\n            yaml_content = f.read()\n            yaml_content = f\"```yaml\\n{yaml_content}\\n```\"\n            tensorboard_logger.experiment.add_text(tag=tag, text_string=yaml_content, global_step=0)\n    else:\n        with open(file_path, encoding=\"utf-8\") as f:\n            tensorboard_logger.experiment.add_text(tag=tag, text_string=f.read().replace(\"\\n\", \"  \\n\"), global_step=0)\n\n    tensorboard_logger.experiment.flush()\n</code></pre>"},{"location":"reference/quadra/utils/validator.html","title":"validator","text":""},{"location":"reference/quadra/utils/validator.html#quadra.utils.validator.check_all_arguments","title":"<code>check_all_arguments(callable_variable, configuration_arguments, argument_names)</code>","text":"<p>Checks if all arguments passed from configuration are valid for the target class or function.</p> <p>Parameters:</p> <ul> <li> callable_variable         \u2013          <p>Full module path to the target class or function.</p> </li> <li> configuration_arguments         \u2013          <p>All arguments passed from configuration.</p> </li> <li> argument_names             (<code>list[str]</code>)         \u2013          <p>All arguments from the target class or function.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>           \u2013          <p>If the argument is not valid for the target class or function.</p> </li> </ul> Source code in <code>quadra/utils/validator.py</code> <pre><code>def check_all_arguments(callable_variable: str, configuration_arguments: list[str], argument_names: list[str]) -&gt; None:\n\"\"\"Checks if all arguments passed from configuration are valid for the target class or function.\n\n    Args:\n        callable_variable : Full module path to the target class or function.\n        configuration_arguments : All arguments passed from configuration.\n        argument_names: All arguments from the target class or function.\n\n    Raises:\n        ValueError: If the argument is not valid for the target class or function.\n    \"\"\"\n    for argument in configuration_arguments:\n        if argument not in argument_names:\n            error_string = (\n                f\"`{argument}` is not a valid argument passed \" f\"from configuration to `{callable_variable}`.\"\n            )\n            closest_match = difflib.get_close_matches(argument, argument_names, n=1, cutoff=0.5)\n            if len(closest_match) &gt; 0:\n                error_string += f\" Did you mean `{closest_match[0]}`?\"\n            raise ValueError(error_string)\n</code></pre>"},{"location":"reference/quadra/utils/validator.html#quadra.utils.validator.get_callable_arguments","title":"<code>get_callable_arguments(full_module_path)</code>","text":"<p>Gets all arguments from module path.</p> <p>Parameters:</p> <ul> <li> full_module_path             (<code>str</code>)         \u2013          <p>Full module path to the target class or function.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>           \u2013          <p>If the target is not a class or a function.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[list[str], bool]</code>         \u2013          <p>All arguments from the target class or function.</p> </li> </ul> Source code in <code>quadra/utils/validator.py</code> <pre><code>def get_callable_arguments(full_module_path: str) -&gt; tuple[list[str], bool]:\n\"\"\"Gets all arguments from module path.\n\n    Args:\n        full_module_path:  Full module path to the target class or function.\n\n    Raises:\n        ValueError: If the target is not a class or a function.\n\n    Returns:\n        All arguments from the target class or function.\n    \"\"\"\n    module_path, callable_name = full_module_path.rsplit(\".\", 1)\n    module = importlib.import_module(module_path)\n    callable_ = getattr(module, callable_name)\n    # check if it is a class\n    accepts_kwargs = False\n    if inspect.isclass(callable_):\n        arg_names = []\n        for cls in callable_.__mro__:\n            if cls is object:\n                break\n            # We don' access the instance but mypy complains\n            init_argspec = inspect.getfullargspec(cls.__init__)  # type: ignore\n            cls_arg_names = init_argspec.args[1:]\n            cls_kwonlyargs = init_argspec.kwonlyargs\n            arg_names.extend(cls_arg_names)\n            arg_names.extend(cls_kwonlyargs)\n            # if the target class or function accepts kwargs, we cannot check arguments\n            accepts_kwargs = init_argspec.varkw is not None or accepts_kwargs\n        arg_names = list(set(arg_names))\n    elif inspect.isfunction(callable_):\n        init_argspec = inspect.getfullargspec(callable_)\n        arg_names = []\n        arg_names.extend(init_argspec.args)\n        arg_names.extend(init_argspec.kwonlyargs)\n        accepts_kwargs = init_argspec.varkw is not None or accepts_kwargs\n    else:\n        raise ValueError(\"The target must be a class or a function.\")\n\n    return arg_names, accepts_kwargs\n</code></pre>"},{"location":"reference/quadra/utils/validator.html#quadra.utils.validator.validate_config","title":"<code>validate_config(_cfg, package_name='quadra')</code>","text":"<p>Recursively traverse OmegaConf object and check if arguments are valid for the target class or function. If not, raise a ValueError with a suggestion for the closest match of the argument name.</p> <p>Parameters:</p> <ul> <li> _cfg             (<code>DictConfig | ListConfig</code>)         \u2013          <p>OmegaConf object</p> </li> <li> package_name             (<code>str</code>, default:                 <code>'quadra'</code> )         \u2013          <p>package name to check for instantiation.</p> </li> </ul> Source code in <code>quadra/utils/validator.py</code> <pre><code>def validate_config(_cfg: DictConfig | ListConfig, package_name: str = \"quadra\") -&gt; None:\n\"\"\"Recursively traverse OmegaConf object and check if arguments are valid for the target class or function.\n    If not, raise a ValueError with a suggestion for the closest match of the argument name.\n\n    Args:\n        _cfg: OmegaConf object\n        package_name: package name to check for instantiation.\n    \"\"\"\n    # The below lines of code for looping over a DictConfig/ListConfig are\n    # borrowed from OmegaConf PR #719.\n    itr: Iterable[Any]\n    if isinstance(_cfg, ListConfig):\n        itr = range(len(_cfg))\n    else:\n        itr = _cfg\n    for key in itr:\n        if OmegaConf.is_missing(_cfg, key):\n            continue\n        if isinstance(key, str) and any(x in key for x in EXCLUDE_KEYS):\n            continue\n        if OmegaConf.is_config(_cfg[key]):\n            validate_config(_cfg[key])\n        elif isinstance(_cfg[key], str):\n            if key == \"_target_\":\n                callable_variable = str(_cfg[key])\n                if callable_variable.startswith(package_name):\n                    configuration_arguments = [str(x) for x in _cfg if x not in OMEGACONF_FIELDS]\n                    argument_names, accepts_kwargs = get_callable_arguments(callable_variable)\n                    if not accepts_kwargs:\n                        check_all_arguments(callable_variable, configuration_arguments, argument_names)\n        else:\n            logger.debug(\"Skipping %s from config. It is not supported.\", key)\n</code></pre>"},{"location":"reference/quadra/utils/visualization.html","title":"visualization","text":""},{"location":"reference/quadra/utils/visualization.html#quadra.utils.visualization.UnNormalize","title":"<code>UnNormalize(mean, std)</code>","text":"<p>Unnormalize a tensor image with mean and standard deviation.</p> Source code in <code>quadra/utils/visualization.py</code> <pre><code>def __init__(self, mean, std):\n    self.mean = mean\n    self.std = std\n</code></pre>"},{"location":"reference/quadra/utils/visualization.html#quadra.utils.visualization.UnNormalize.__call__","title":"<code>__call__(tensor, make_copy=True)</code>","text":"<p>Call function to unnormalize a tensor image with mean and standard deviation.</p> <p>Parameters:</p> <ul> <li> tensor             (<code>Tensor</code>)         \u2013          <p>Tensor image of size (C, H, W) to be normalized.</p> </li> <li> make_copy             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>whether to apply normalization to a copied tensor</p> </li> </ul> Source code in <code>quadra/utils/visualization.py</code> <pre><code>def __call__(self, tensor: torch.Tensor, make_copy=True) -&gt; torch.Tensor:\n\"\"\"Call function to unnormalize a tensor image with mean and standard deviation.\n\n    Args:\n        tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n        make_copy (bool): whether to apply normalization to a copied tensor\n    Returns:\n        Tensor: Normalized image.\n    \"\"\"\n    if make_copy:\n        new_t = tensor.detach().clone()\n    else:\n        new_t = tensor\n    for t, m, s in zip(new_t, self.mean, self.std):\n        t.mul_(s).add_(m)\n        # The normalize code -&gt; t.sub_(m).div_(s)\n    return new_t\n</code></pre>"},{"location":"reference/quadra/utils/visualization.html#quadra.utils.visualization.create_grid_figure","title":"<code>create_grid_figure(images, nrows, ncols, file_path, bounds, row_names=None, fig_size=(12, 8))</code>","text":"<p>Create a grid figure with images.</p> <p>Parameters:</p> <ul> <li> images             (<code>Iterable[Iterable[ndarray]]</code>)         \u2013          <p>List of images to plot.</p> </li> <li> nrows             (<code>int</code>)         \u2013          <p>Number of rows in the grid.</p> </li> <li> ncols             (<code>int</code>)         \u2013          <p>Number of columns in the grid.</p> </li> <li> file_path             (<code>str</code>)         \u2013          <p>Path to save the figure.</p> </li> <li> row_names             (<code>Iterable[str] | None</code>, default:                 <code>None</code> )         \u2013          <p>Row names. Defaults to None.</p> </li> <li> fig_size             (<code>tuple[int, int]</code>, default:                 <code>(12, 8)</code> )         \u2013          <p>Figure size. Defaults to (12, 8).</p> </li> <li> bounds             (<code>list[tuple[float, float]]</code>)         \u2013          <p>Bounds for the images. Defaults to None.</p> </li> </ul> Source code in <code>quadra/utils/visualization.py</code> <pre><code>def create_grid_figure(\n    images: Iterable[Iterable[np.ndarray]],\n    nrows: int,\n    ncols: int,\n    file_path: str,\n    bounds: list[tuple[float, float]],\n    row_names: Iterable[str] | None = None,\n    fig_size: tuple[int, int] = (12, 8),\n):\n\"\"\"Create a grid figure with images.\n\n    Args:\n        images: List of images to plot.\n        nrows: Number of rows in the grid.\n        ncols: Number of columns in the grid.\n        file_path: Path to save the figure.\n        row_names: Row names. Defaults to None.\n        fig_size: Figure size. Defaults to (12, 8).\n        bounds: Bounds for the images. Defaults to None.\n    \"\"\"\n    default_plt_backend = plt.get_backend()\n    plt.switch_backend(\"Agg\")\n    _, ax = plt.subplots(nrows=nrows, ncols=ncols, figsize=fig_size, squeeze=False)\n    for i, row in enumerate(images):\n        for j, image in enumerate(row):\n            image_to_plot = image[0] if len(image.shape) == 3 and image.shape[0] == 1 else image\n            ax[i][j].imshow(image_to_plot, vmin=bounds[i][0], vmax=bounds[i][1])\n            ax[i][j].get_xaxis().set_ticks([])\n            ax[i][j].get_yaxis().set_ticks([])\n    if row_names is not None:\n        for ax, name in zip(ax[:, 0], row_names):  # noqa: B020\n            ax.set_ylabel(name, rotation=90)\n\n    plt.tight_layout()\n    plt.savefig(file_path, bbox_inches=\"tight\", dpi=300, facecolor=\"white\", transparent=False)\n    plt.close()\n    plt.switch_backend(default_plt_backend)\n</code></pre>"},{"location":"reference/quadra/utils/visualization.html#quadra.utils.visualization.create_visualization_dataset","title":"<code>create_visualization_dataset(dataset)</code>","text":"<p>Create a visualization dataset by updating transforms.</p> Source code in <code>quadra/utils/visualization.py</code> <pre><code>def create_visualization_dataset(dataset: torch.utils.data.Dataset):\n\"\"\"Create a visualization dataset by updating transforms.\"\"\"\n\n    def convert_transforms(transforms: Any):\n\"\"\"Handle different types of transforms.\"\"\"\n        if isinstance(transforms, albumentations.BaseCompose):\n            transforms.transforms = convert_transforms(transforms.transforms)\n        if isinstance(transforms, (list, ListConfig, TransformsSeqType)):\n            transforms = [convert_transforms(t) for t in transforms]\n        if isinstance(transforms, (dict, DictConfig)):\n            for tname, t in transforms.items():\n                transforms[tname] = convert_transforms(t)\n        if isinstance(transforms, (Normalize, ToTensorV2)):\n            return NoOp(p=1)\n        return transforms\n\n    new_dataset = copy.deepcopy(dataset)\n    # TODO: Create dataset class that has a transform attribut, we can then use isinstance\n    if isinstance(dataset, torch.utils.data.Dataset):\n        transform = copy.deepcopy(dataset.transform)  # type: ignore[attr-defined]\n        if transform is not None:\n            new_transforms = convert_transforms(transform)\n            new_dataset.transform = new_transforms  # type: ignore[attr-defined]\n        else:\n            raise ValueError(f\"The dataset transform {type(transform)} is not supported\")\n    else:\n        raise ValueError(f\"The dataset type {dataset} is not supported\")\n    return new_dataset\n</code></pre>"},{"location":"reference/quadra/utils/visualization.html#quadra.utils.visualization.plot_classification_results","title":"<code>plot_classification_results(test_dataset, pred_labels, test_labels, class_name, original_folder, gradcam_folder=None, grayscale_cams=None, unorm=None, idx_to_class=None, what=None, real_class_to_plot=None, pred_class_to_plot=None, rows=1, cols=4, figsize=(20, 20), gradcam=False)</code>","text":"<p>Plot and save images extracted from classification. If gradcam is True, same images with a gradcam heatmap (layered on original image) will also be saved.</p> <p>Parameters:</p> <ul> <li> test_dataset             (<code>Dataset</code>)         \u2013          <p>Test dataset</p> </li> <li> pred_labels             (<code>ndarray</code>)         \u2013          <p>Predicted labels</p> </li> <li> test_labels             (<code>ndarray</code>)         \u2013          <p>Test labels</p> </li> <li> class_name             (<code>str</code>)         \u2013          <p>Name of the examples' class</p> </li> <li> original_folder             (<code>str</code>)         \u2013          <p>Folder where original examples will be saved</p> </li> <li> gradcam_folder             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>Folder in which gradcam examples will be saved</p> </li> <li> grayscale_cams             (<code>ndarray | None</code>, default:                 <code>None</code> )         \u2013          <p>Grayscale gradcams (ordered as pred_labels and test_labels)</p> </li> <li> unorm             (<code>Callable[[Tensor], Tensor] | None</code>, default:                 <code>None</code> )         \u2013          <p>Albumentations function to unormalize image</p> </li> <li> idx_to_class             (<code>dict | None</code>, default:                 <code>None</code> )         \u2013          <p>Dictionary of class conversion</p> </li> <li> what             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>Can be \"dis\" or \"conc\", used if real_class_to_plot or pred_class_to_plot are None</p> </li> <li> real_class_to_plot             (<code>int | None</code>, default:                 <code>None</code> )         \u2013          <p>Real class to plot.</p> </li> <li> pred_class_to_plot             (<code>int | None</code>, default:                 <code>None</code> )         \u2013          <p>Pred class to plot.</p> </li> <li> rows             (<code>int | None</code>, default:                 <code>1</code> )         \u2013          <p>How many rows in the plot there will be.</p> </li> <li> cols             (<code>int</code>, default:                 <code>4</code> )         \u2013          <p>How many cols in the plot there will be.</p> </li> <li> figsize             (<code>tuple[int, int]</code>, default:                 <code>(20, 20)</code> )         \u2013          <p>The figure size.</p> </li> <li> gradcam             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Whether to save also the gradcam version of the examples</p> </li> </ul> Source code in <code>quadra/utils/visualization.py</code> <pre><code>def plot_classification_results(\n    test_dataset: torch.utils.data.Dataset,\n    pred_labels: np.ndarray,\n    test_labels: np.ndarray,\n    class_name: str,\n    original_folder: str,\n    gradcam_folder: str | None = None,\n    grayscale_cams: np.ndarray | None = None,\n    unorm: Callable[[torch.Tensor], torch.Tensor] | None = None,\n    idx_to_class: dict | None = None,\n    what: str | None = None,\n    real_class_to_plot: int | None = None,\n    pred_class_to_plot: int | None = None,\n    rows: int | None = 1,\n    cols: int = 4,\n    figsize: tuple[int, int] = (20, 20),\n    gradcam: bool = False,\n) -&gt; None:\n\"\"\"Plot and save images extracted from classification. If gradcam is True, same images\n    with a gradcam heatmap (layered on original image) will also be saved.\n\n    Args:\n        test_dataset: Test dataset\n        pred_labels: Predicted labels\n        test_labels: Test labels\n        class_name: Name of the examples' class\n        original_folder: Folder where original examples will be saved\n        gradcam_folder: Folder in which gradcam examples will be saved\n        grayscale_cams: Grayscale gradcams (ordered as pred_labels and test_labels)\n        unorm: Albumentations function to unormalize image\n        idx_to_class: Dictionary of class conversion\n        what: Can be \"dis\" or \"conc\", used if real_class_to_plot or pred_class_to_plot are None\n        real_class_to_plot: Real class to plot.\n        pred_class_to_plot: Pred class to plot.\n        rows: How many rows in the plot there will be.\n        cols: How many cols in the plot there will be.\n        figsize: The figure size.\n        gradcam: Whether to save also the gradcam version of the examples\n\n    \"\"\"\n    to_plot = True\n    if gradcam:\n        if grayscale_cams is None:\n            raise ValueError(\"gradcam is True but grayscale_cams is None\")\n        if gradcam_folder is None:\n            raise ValueError(\"gradcam is True but gradcam_folder is None\")\n\n    if real_class_to_plot is not None:\n        sample_idx = np.where(test_labels == real_class_to_plot)[0]\n        if gradcam and grayscale_cams is not None:\n            grayscale_cams = grayscale_cams[test_labels == real_class_to_plot]\n        pred_labels = pred_labels[test_labels == real_class_to_plot]\n        test_labels = test_labels[test_labels == real_class_to_plot]\n\n    if pred_class_to_plot is not None:\n        sample_idx = np.where(pred_labels == pred_class_to_plot)[0]\n        if gradcam and grayscale_cams is not None:\n            grayscale_cams = grayscale_cams[pred_labels == pred_class_to_plot]\n        test_labels = test_labels[pred_labels == pred_class_to_plot]\n        pred_labels = pred_labels[pred_labels == pred_class_to_plot]\n\n    if pred_class_to_plot is None and real_class_to_plot is None:\n        raise ValueError(\"'real_class_to_plot' and 'pred_class_to_plot' must not be both None\")\n\n    if what is not None:\n        if what == \"dis\":\n            cordant = pred_labels != test_labels\n        elif what == \"con\":\n            cordant = pred_labels == test_labels\n        else:\n            raise AssertionError(f\"{what} not a valid plot type. Must be con or dis\")\n\n        sample_idx = np.array(sample_idx)[cordant]\n        pred_labels = np.array(pred_labels)[cordant]\n        test_labels = np.array(test_labels)[cordant]\n        if gradcam:\n            grayscale_cams = np.array(grayscale_cams)[cordant]\n\n    # randomize\n    idx_random = random.sample(range(len(sample_idx)), len(sample_idx))\n\n    sample_idx = sample_idx[idx_random]\n    pred_labels = pred_labels[idx_random]\n    test_labels = test_labels[idx_random]\n    if gradcam and grayscale_cams is not None:\n        grayscale_cams = grayscale_cams[idx_random]\n\n    cordant_chunks = list(_chunks(sample_idx, cols))\n\n    if len(sample_idx) == 0:\n        to_plot = False\n        print(\"Nothing to plot\")\n    else:\n        if rows is None or rows == 0:\n            total_rows = len(cordant_chunks)\n        else:\n            total_rows = len(cordant_chunks[:rows])\n        if gradcam:\n            modality_list = [\"original\", \"gradcam\"]\n        else:\n            modality_list = [\"original\"]\n        for modality in modality_list:\n            fig = plt.figure(figsize=figsize)\n            grid = ImageGrid(\n                fig,\n                111,  # similar to subplot(111)\n                nrows_ncols=(total_rows, cols),\n                axes_pad=(0.2, 0.5),\n            )\n            for i, ax in enumerate(grid):\n                if idx_to_class is not None:\n                    try:\n                        pred_label = idx_to_class[pred_labels[i]]\n                    except Exception:\n                        pred_label = pred_labels[i]\n                    try:\n                        test_label = idx_to_class[test_labels[i]]\n                    except Exception:\n                        test_label = test_labels[i]\n\n                ax.axis(\"off\")\n                ax.set_title(f\"True: {str(test_label)}\\nPred {str(pred_label)}\")\n                image, _ = test_dataset[sample_idx[i]]\n\n                if unorm is not None:\n                    image = np.array(unorm(image))\n                if modality == \"gradcam\" and grayscale_cams is not None:\n                    grayscale_cam = grayscale_cams[i]\n                    rgb_cam = show_cam_on_image(\n                        np.transpose(image, (1, 2, 0)), grayscale_cam, use_rgb=True, image_weight=0.7\n                    )\n\n                    ax.imshow(rgb_cam, cmap=\"gray\")\n                    if i == len(pred_labels) - 1:\n                        break\n                else:\n                    if isinstance(image, torch.Tensor):\n                        image = image.cpu().numpy()\n\n                    if image.max() &lt;= 1:\n                        image = image * 255\n                    image = image.astype(int)\n\n                    if len(image.shape) == 3:\n                        if image.shape[0] == 1:\n                            image = image[0]\n                        elif image.shape[0] == 3:\n                            image = image.transpose((1, 2, 0))\n                    ax.imshow(image, cmap=\"gray\")\n                    if i == len(pred_labels) - 1:\n                        break\n\n            for item in grid:\n                item.axis(\"off\")\n\n            if to_plot:\n                save_folder: str = \"\"\n                if modality == \"gradcam\" and gradcam_folder is not None:\n                    save_folder = gradcam_folder\n                elif modality == \"original\":\n                    save_folder = original_folder\n                else:\n                    log.warning(\"modality %s has no corresponding folder\", modality)\n                    return\n\n                plt.savefig(\n                    os.path.join(save_folder, f\"{what}cordant_{class_name}_\" + modality + \".png\"),\n                    bbox_inches=\"tight\",\n                    pad_inches=0,\n                )\n                plt.close()\n</code></pre>"},{"location":"reference/quadra/utils/visualization.html#quadra.utils.visualization.plot_multiclass_prediction","title":"<code>plot_multiclass_prediction(image, prediction_image, ground_truth_image, class_to_idx, plot_original=True, ignore_class=0, image_height=10, save_path=None, color_map='tab20')</code>","text":"<p>Function used to plot the image predicted.</p> <p>Parameters:</p> <ul> <li> image             (<code>ndarray</code>)         \u2013          <p>The image to plot</p> </li> <li> prediction_image             (<code>ndarray</code>)         \u2013          <p>The prediction image</p> </li> <li> ground_truth_image             (<code>ndarray</code>)         \u2013          <p>The ground truth image</p> </li> <li> class_to_idx             (<code>dict[str, int]</code>)         \u2013          <p>The class to idx mapping</p> </li> <li> plot_original             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Whether to plot the original image</p> </li> <li> ignore_class             (<code>int | None</code>, default:                 <code>0</code> )         \u2013          <p>The class to ignore</p> </li> <li> image_height             (<code>int</code>, default:                 <code>10</code> )         \u2013          <p>The height of the output figure</p> </li> <li> save_path             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>The path to save the figure</p> </li> <li> color_map             (<code>str</code>, default:                 <code>'tab20'</code> )         \u2013          <p>The color map to use. Defaults to \"tab20\".</p> </li> </ul> Source code in <code>quadra/utils/visualization.py</code> <pre><code>def plot_multiclass_prediction(\n    image: np.ndarray,\n    prediction_image: np.ndarray,\n    ground_truth_image: np.ndarray,\n    class_to_idx: dict[str, int],\n    plot_original: bool = True,\n    ignore_class: int | None = 0,\n    image_height: int = 10,\n    save_path: str | None = None,\n    color_map: str = \"tab20\",\n) -&gt; None:\n\"\"\"Function used to plot the image predicted.\n\n    Args:\n        image: The image to plot\n        prediction_image: The prediction image\n        ground_truth_image: The ground truth image\n        class_to_idx: The class to idx mapping\n        plot_original: Whether to plot the original image\n        ignore_class: The class to ignore\n        image_height: The height of the output figure\n        save_path: The path to save the figure\n        color_map: The color map to use. Defaults to \"tab20\".\n    \"\"\"\n    image = image[0 : prediction_image.shape[0], 0 : prediction_image.shape[1], :]\n    class_idxs = list(class_to_idx.values())\n    cm = get_cmap(color_map)\n    cmap = {str(c): tuple(int(i * 255) for i in cm(c / len(class_idxs))[:-1]) for c in class_idxs}\n    output_images = []\n    titles = []\n    if plot_original:\n        output_images.append(image)\n        titles.append(\"Original Image\")\n\n    ground_truth_mask = reconstruct_multiclass_mask(ground_truth_image, image.shape, cmap, ignore_class=ignore_class)\n    output_images.append(ground_truth_mask)\n    titles.append(\"Ground Truth Mask\")\n\n    prediction_mask = reconstruct_multiclass_mask(\n        prediction_image,\n        image.shape,\n        cmap,\n        ignore_class=ignore_class,\n    )\n    output_images.append(prediction_mask)\n    titles.append(\"Prediction Mask\")\n    if ignore_class is not None:\n        prediction_mask = reconstruct_multiclass_mask(\n            prediction_image, image.shape, cmap, ignore_class=ignore_class, ground_truth_mask=ground_truth_image\n        )\n        prediction_title = f\"Prediction Mask \\n (Ignoring Ground Truth Class: {ignore_class})\"\n        output_images.append(prediction_mask)\n        titles.append(prediction_title)\n\n    _, axs = plt.subplots(\n        ncols=len(output_images),\n        nrows=1,\n        figsize=(len(output_images) * image_height, image_height),\n        squeeze=False,\n        facecolor=\"white\",\n    )\n    for i, output_image in output_images:\n        axs[0, i].imshow(show_mask_on_image(image, output_image))\n        axs[0, i].set_title(titles[i])\n        axs[0, i].axis(\"off\")\n    custom_lines = [Line2D([0], [0], color=tuple(i / 255.0 for i in cmap[str(c)]), lw=4) for c in class_idxs]\n    custom_labels = list(class_to_idx.keys())\n    axs[0, -1].legend(custom_lines, custom_labels, loc=\"center left\", bbox_to_anchor=(1.01, 0.81), borderaxespad=0)\n    if save_path is not None:\n        plt.savefig(save_path, bbox_inches=\"tight\")\n        plt.close()\n</code></pre>"},{"location":"reference/quadra/utils/visualization.html#quadra.utils.visualization.reconstruct_multiclass_mask","title":"<code>reconstruct_multiclass_mask(mask, image_shape, color_map, ignore_class=None, ground_truth_mask=None)</code>","text":"<p>Reconstruct a multiclass mask from a single channel mask.</p> <p>Parameters:</p> <ul> <li> mask             (<code>ndarray</code>)         \u2013          <p>A single channel mask.</p> </li> <li> image_shape             (<code>Tuple[int, ...]</code>)         \u2013          <p>The shape of the image.</p> </li> <li> color_map             (<code>ListedColormap</code>)         \u2013          <p>The color map to use.</p> </li> <li> ignore_class             (<code>Optional[int]</code>, default:                 <code>None</code> )         \u2013          <p>The class to ignore. Defaults to None.</p> </li> <li> ground_truth_mask             (<code>Optional[ndarray]</code>, default:                 <code>None</code> )         \u2013          <p>The ground truth mask. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> mask (            <code>ndarray</code> )        \u2013          <p>np.ndarray</p> </li> </ul> Source code in <code>quadra/utils/visualization.py</code> <pre><code>def reconstruct_multiclass_mask(\n    mask: np.ndarray,\n    image_shape: tuple[int, ...],\n    color_map: ListedColormap,\n    ignore_class: int | None = None,\n    ground_truth_mask: np.ndarray | None = None,\n) -&gt; np.ndarray:\n\"\"\"Reconstruct a multiclass mask from a single channel mask.\n\n    Args:\n        mask (np.ndarray): A single channel mask.\n        image_shape (Tuple[int, ...]): The shape of the image.\n        color_map (ListedColormap): The color map to use.\n        ignore_class (Optional[int], optional): The class to ignore. Defaults to None.\n        ground_truth_mask (Optional[np.ndarray], optional): The ground truth mask. Defaults to None.\n\n    Returns:\n        mask: np.ndarray\n    \"\"\"\n    output_mask = np.zeros(image_shape)\n    for c in np.unique(mask):\n        if ignore_class is not None and c == ignore_class:\n            continue\n\n        output_mask[mask == c] = color_map[str(c)]\n\n    if ignore_class is not None and ground_truth_mask is not None:\n        output_mask[ground_truth_mask == ignore_class] = [0, 0, 0]\n\n    return output_mask\n</code></pre>"},{"location":"reference/quadra/utils/visualization.html#quadra.utils.visualization.show_mask_on_image","title":"<code>show_mask_on_image(image, mask)</code>","text":"<p>Show a mask on an image.</p> <p>Parameters:</p> <ul> <li> image             (<code>ndarray</code>)         \u2013          <p>The image.</p> </li> <li> mask             (<code>ndarray</code>)         \u2013          <p>The mask.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>         \u2013          <p>np.ndarray: The image with the mask.</p> </li> </ul> Source code in <code>quadra/utils/visualization.py</code> <pre><code>def show_mask_on_image(image: np.ndarray, mask: np.ndarray) -&gt; np.ndarray:\n\"\"\"Show a mask on an image.\n\n    Args:\n        image (np.ndarray): The image.\n        mask (np.ndarray): The mask.\n\n    Returns:\n        np.ndarray: The image with the mask.\n    \"\"\"\n    image = image.astype(np.float32) / 255\n    mask = mask.astype(np.float32) / 255\n    out = mask + image\n    out = out / np.max(out)\n    return (255 * out).astype(np.uint8)\n</code></pre>"},{"location":"reference/quadra/utils/vit_explainability.html","title":"vit_explainability","text":""},{"location":"reference/quadra/utils/vit_explainability.html#quadra.utils.vit_explainability.LinearModelPytorchWrapper","title":"<code>LinearModelPytorchWrapper(backbone, linear_classifier, example_input, device)</code>","text":"<p>             Bases: <code>Module</code></p> <p>Pytorch wrapper for scikit-learn linear models.</p> <p>Parameters:</p> <ul> <li> backbone             (<code>Module</code>)         \u2013          <p>Backbone</p> </li> <li> linear_classifier             (<code>LinearClassifierMixin</code>)         \u2013          <p>ScikitLearn linear classifier model</p> </li> <li> example_input             (<code>Tensor</code>)         \u2013          <p>Input example needed to obtain output shape</p> </li> <li> device             (<code>device</code>)         \u2013          <p>The device to use. Defaults to \"cpu\"</p> </li> </ul> Source code in <code>quadra/utils/vit_explainability.py</code> <pre><code>def __init__(\n    self,\n    backbone: torch.nn.Module,\n    linear_classifier: LinearClassifierMixin,\n    example_input: torch.Tensor,\n    device: torch.device,\n):\n    super().__init__()\n    self.device = device\n    self.backbone = backbone.to(device)\n    if not isinstance(linear_classifier, LinearClassifierMixin):\n        raise TypeError(\"Classifier is not of type LinearClassifierMixin.\")\n    self.num_classes = len(linear_classifier.classes_)\n    self.linear_classifier = linear_classifier\n    with torch.no_grad():\n        output = self.backbone(example_input.to(device))\n        num_filters = output.shape[-1]\n\n    self.classifier = torch.nn.Linear(num_filters, self.num_classes).to(device)\n    self.classifier.weight.data = torch.from_numpy(linear_classifier.coef_).float()\n    self.classifier.bias.data = torch.from_numpy(linear_classifier.intercept_).float()\n</code></pre>"},{"location":"reference/quadra/utils/vit_explainability.html#quadra.utils.vit_explainability.VitAttentionGradRollout","title":"<code>VitAttentionGradRollout(model, attention_layer_names=None, discard_ratio=0.9, classifier=None, example_input=None)</code>","text":"<p>Attention gradient rollout class. Constructor registers hooks to the model's specified layers. Only 4 layers by default given the high load on gpu. Best gradcams obtained using all blocks.</p> <p>Parameters:</p> <ul> <li> model             (<code>Module</code>)         \u2013          <p>Pytorch model</p> </li> <li> attention_layer_names             (<code>list[str] | None</code>, default:                 <code>None</code> )         \u2013          <p>On which layers to register the hooks</p> </li> <li> discard_ratio             (<code>float</code>, default:                 <code>0.9</code> )         \u2013          <p>Percentage of elements to discard</p> </li> <li> classifier             (<code>LinearClassifierMixin | None</code>, default:                 <code>None</code> )         \u2013          <p>Scikit-learn classifier. Leave it to None if model already has a classifier on top.</p> </li> </ul> Source code in <code>quadra/utils/vit_explainability.py</code> <pre><code>def __init__(  # pylint: disable=W0102\n    self,\n    model: torch.nn.Module,\n    attention_layer_names: list[str] | None = None,\n    discard_ratio: float = 0.9,\n    classifier: LinearClassifierMixin | None = None,\n    example_input: torch.Tensor | None = None,\n):\n    if attention_layer_names is None:\n        attention_layer_names = [\n            \"blocks.6.attn.attn_drop\",\n            \"blocks.7.attn.attn_drop\",\n            \"blocks.10.attn.attn_drop\",\n            \"blocks.11.attn.attn_drop\",\n        ]\n\n    if classifier is not None:\n        if example_input is None:\n            raise ValueError(\n                \"Must provide an input example to LinearModelPytorchWrapper when classifier is not None\"\n            )\n        self.model = LinearModelPytorchWrapper(\n            backbone=model,\n            linear_classifier=classifier,\n            example_input=example_input,\n            device=next(model.parameters()).device,\n        )\n    else:\n        self.model = model  # type: ignore[assignment]\n\n    self.discard_ratio = discard_ratio\n    self.f_hook_handles: list[torch.utils.hooks.RemovableHandle] = []\n    self.b_hook_handles: list[torch.utils.hooks.RemovableHandle] = []\n    for name, module in self.model.named_modules():\n        for layer_name in attention_layer_names:\n            if layer_name in name:\n                self.f_hook_handles.append(module.register_forward_hook(self.get_attention))\n                self.b_hook_handles.append(module.register_backward_hook(self.get_attention_gradient))\n    self.attentions: list[torch.Tensor] = []\n    self.attention_gradients: list[torch.Tensor] = []\n    # Activate gradients\n    blocks_list = [x.split(\"blocks\")[1].split(\".attn\")[0] for x in attention_layer_names]\n    for name, module in model.named_modules():\n        for p in module.parameters():\n            if \"blocks\" in name and any(x in name for x in blocks_list):\n                p.requires_grad = True\n</code></pre>"},{"location":"reference/quadra/utils/vit_explainability.html#quadra.utils.vit_explainability.VitAttentionGradRollout.__call__","title":"<code>__call__(input_tensor, targets_list)</code>","text":"<p>Called when the class instance is used as a function.</p> <p>Parameters:</p> <ul> <li> input_tensor             (<code>Tensor</code>)         \u2013          <p>Model's input tensor</p> </li> <li> targets_list             (<code>list[int]</code>)         \u2013          <p>List of targets. If None, argmax is used</p> </li> </ul> <p>Returns:</p> <ul> <li> out (            <code>ndarray</code> )        \u2013          <p>Batch of output masks</p> </li> </ul> Source code in <code>quadra/utils/vit_explainability.py</code> <pre><code>def __call__(self, input_tensor: torch.Tensor, targets_list: list[int]) -&gt; np.ndarray:\n\"\"\"Called when the class instance is used as a function.\n\n    Args:\n        input_tensor: Model's input tensor\n        targets_list: List of targets. If None, argmax is used\n\n    Returns:\n        out: Batch of output masks\n    \"\"\"\n    self.attentions.clear()\n    self.attention_gradients.clear()\n\n    self.model.zero_grad(set_to_none=True)\n    self.model.to(input_tensor.device)\n    output = self.model(input_tensor).cpu()\n\n    class_mask = torch.zeros(output.shape)\n    if targets_list is None:\n        targets_list = output.argmax(dim=1)\n    class_mask[torch.arange(output.shape[0]), targets_list] = 1\n    loss = (output * class_mask).sum()\n    loss.backward()\n    out = grad_rollout(\n        self.attentions,\n        self.attention_gradients,\n        self.discard_ratio,\n        aspect_ratio=(input_tensor.shape[-1] / input_tensor.shape[-2]),\n    )\n\n    return out\n</code></pre>"},{"location":"reference/quadra/utils/vit_explainability.html#quadra.utils.vit_explainability.VitAttentionGradRollout.get_attention","title":"<code>get_attention(module, inpt, out)</code>","text":"<p>Hook to return attention.</p> <p>Parameters:</p> <ul> <li> module             (<code>Module</code>)         \u2013          <p>Torch module</p> </li> <li> inpt             (<code>Tensor</code>)         \u2013          <p>Input tensor</p> </li> <li> out             (<code>Tensor</code>)         \u2013          <p>Output tensor, in this case the attention</p> </li> </ul> Source code in <code>quadra/utils/vit_explainability.py</code> <pre><code>def get_attention(\n    self,\n    module: torch.nn.Module,\n    inpt: torch.Tensor,\n    out: torch.Tensor,\n) -&gt; None:\n\"\"\"Hook to return attention.\n\n    Args:\n        module: Torch module\n        inpt: Input tensor\n        out: Output tensor, in this case the attention\n    \"\"\"\n    self.attentions.append(out.detach().clone().cpu())\n</code></pre>"},{"location":"reference/quadra/utils/vit_explainability.html#quadra.utils.vit_explainability.VitAttentionGradRollout.get_attention_gradient","title":"<code>get_attention_gradient(module, grad_input, grad_output)</code>","text":"<p>Hook to return attention.</p> <p>Parameters:</p> <ul> <li> module             (<code>Module</code>)         \u2013          <p>Torch module</p> </li> <li> grad_input             (<code>Tensor</code>)         \u2013          <p>Gradients' input tensor</p> </li> <li> grad_output             (<code>Tensor</code>)         \u2013          <p>Gradients' output tensor, in this case the attention</p> </li> </ul> Source code in <code>quadra/utils/vit_explainability.py</code> <pre><code>def get_attention_gradient(\n    self,\n    module: torch.nn.Module,\n    grad_input: torch.Tensor,\n    grad_output: torch.Tensor,\n) -&gt; None:\n\"\"\"Hook to return attention.\n\n    Args:\n        module: Torch module\n        grad_input: Gradients' input tensor\n        grad_output: Gradients' output tensor, in this case the attention\n    \"\"\"\n    self.attention_gradients.append(grad_input[0].detach().clone().cpu())\n</code></pre>"},{"location":"reference/quadra/utils/vit_explainability.html#quadra.utils.vit_explainability.VitAttentionRollout","title":"<code>VitAttentionRollout(model, attention_layer_names=None, head_fusion='mean', discard_ratio=0.9)</code>","text":"<p>Attention gradient rollout class. Constructor registers hooks to the model's specified layers. Only 4 layers by default given the high load on gpu. Best gradcams obtained using all blocks.</p> <p>Parameters:</p> <ul> <li> model             (<code>Module</code>)         \u2013          <p>Model</p> </li> <li> attention_layer_names             (<code>list[str] | None</code>, default:                 <code>None</code> )         \u2013          <p>On which layers to register the hook</p> </li> <li> head_fusion             (<code>str</code>, default:                 <code>'mean'</code> )         \u2013          <p>Strategy of fusion for attention heads</p> </li> <li> discard_ratio             (<code>float</code>, default:                 <code>0.9</code> )         \u2013          <p>Percentage of elements to discard</p> </li> </ul> Source code in <code>quadra/utils/vit_explainability.py</code> <pre><code>def __init__(\n    self,\n    model: torch.nn.Module,\n    attention_layer_names: list[str] | None = None,\n    head_fusion: str = \"mean\",\n    discard_ratio: float = 0.9,\n):\n    if attention_layer_names is None:\n        attention_layer_names = [\n            \"blocks.6.attn.attn_drop\",\n            \"blocks.7.attn.attn_drop\",\n            \"blocks.10.attn.attn_drop\",\n            \"blocks.11.attn.attn_drop\",\n        ]\n    self.model = model\n    self.head_fusion = head_fusion\n    self.discard_ratio = discard_ratio\n    self.f_hook_handles: list[torch.utils.hooks.RemovableHandle] = []\n    for name, module in self.model.named_modules():\n        for layer_name in attention_layer_names:\n            if layer_name in name:\n                self.f_hook_handles.append(module.register_forward_hook(self.get_attention))\n    self.attentions: list[torch.Tensor] = []\n</code></pre>"},{"location":"reference/quadra/utils/vit_explainability.html#quadra.utils.vit_explainability.VitAttentionRollout.__call__","title":"<code>__call__(input_tensor)</code>","text":"<p>Called when the class instance is used as a function.</p> <p>Parameters:</p> <ul> <li> input_tensor             (<code>Tensor</code>)         \u2013          <p>Input tensor</p> </li> </ul> <p>Returns:</p> <ul> <li> out (            <code>ndarray</code> )        \u2013          <p>Batch of output masks</p> </li> </ul> Source code in <code>quadra/utils/vit_explainability.py</code> <pre><code>def __call__(self, input_tensor: torch.Tensor) -&gt; np.ndarray:\n\"\"\"Called when the class instance is used as a function.\n\n    Args:\n        input_tensor: Input tensor\n\n    Returns:\n        out: Batch of output masks\n    \"\"\"\n    self.attentions.clear()\n    with torch.no_grad():\n        self.model(input_tensor)\n    out = rollout(\n        self.attentions,\n        self.discard_ratio,\n        self.head_fusion,\n        aspect_ratio=(input_tensor.shape[-1] / input_tensor.shape[-2]),\n    )\n\n    return out\n</code></pre>"},{"location":"reference/quadra/utils/vit_explainability.html#quadra.utils.vit_explainability.VitAttentionRollout.get_attention","title":"<code>get_attention(module, inpt, out)</code>","text":"<p>Hook to return attention.</p> <p>Parameters:</p> <ul> <li> module             (<code>Module</code>)         \u2013          <p>Torch module</p> </li> <li> inpt             (<code>Tensor</code>)         \u2013          <p>Input tensor</p> </li> <li> out             (<code>Tensor</code>)         \u2013          <p>Output tensor, in this case the attention</p> </li> </ul> Source code in <code>quadra/utils/vit_explainability.py</code> <pre><code>def get_attention(\n    self,\n    module: torch.nn.Module,\n    inpt: torch.Tensor,\n    out: torch.Tensor,\n) -&gt; None:\n\"\"\"Hook to return attention.\n\n    Args:\n        module: Torch module\n        inpt: Input tensor\n        out: Output tensor, in this case the attention\n    \"\"\"\n    self.attentions.append(out.detach().clone().cpu())\n</code></pre>"},{"location":"reference/quadra/utils/vit_explainability.html#quadra.utils.vit_explainability.grad_rollout","title":"<code>grad_rollout(attentions, gradients, discard_ratio=0.9, aspect_ratio=1.0)</code>","text":"<p>Apply gradient rollout on Attention matrices.</p> <p>Parameters:</p> <ul> <li> attentions             (<code>list[Tensor]</code>)         \u2013          <p>Attention matrices</p> </li> <li> gradients             (<code>list[Tensor]</code>)         \u2013          <p>Target class gradient matrices</p> </li> <li> discard_ratio             (<code>float</code>, default:                 <code>0.9</code> )         \u2013          <p>Percentage of elements to discard</p> </li> <li> aspect_ratio             (<code>float</code>, default:                 <code>1.0</code> )         \u2013          <p>Model inputs' width divided by height</p> </li> </ul> <p>Returns:</p> <ul> <li> mask (            <code>ndarray</code> )        \u2013          <p>Output mask, still needs a resize</p> </li> </ul> Source code in <code>quadra/utils/vit_explainability.py</code> <pre><code>def grad_rollout(\n    attentions: list[torch.Tensor], gradients: list[torch.Tensor], discard_ratio: float = 0.9, aspect_ratio: float = 1.0\n) -&gt; np.ndarray:\n\"\"\"Apply gradient rollout on Attention matrices.\n\n    Args:\n        attentions: Attention matrices\n        gradients: Target class gradient matrices\n        discard_ratio: Percentage of elements to discard\n        aspect_ratio: Model inputs' width divided by height\n\n    Returns:\n        mask: Output mask, still needs a resize\n    \"\"\"\n    result = torch.eye(attentions[0].size(-1))\n    with torch.no_grad():\n        for attention, grad in zip(attentions, gradients):\n            weights = grad\n            attention_heads_fused = torch.mean((attention * weights), dim=1)\n            attention_heads_fused[attention_heads_fused &lt; 0] = 0\n            # Drop the lowest attentions, but\n            # don't drop the class token\n            flat = attention_heads_fused.view(attention_heads_fused.size(0), -1)\n            _, indices = flat.topk(int(flat.size(-1) * discard_ratio), -1, False)\n            flat.scatter_(-1, indices, 0)\n            I = torch.eye(attention_heads_fused.size(-1))\n            a = (attention_heads_fused + 1.0 * I) / 2\n            a = a / a.sum(dim=-1).unsqueeze(1)\n            result = torch.matmul(a, result)\n    # Look at the total attention between the class token,\n    # and the image patches\n    mask = result[:, 0, 1:]\n    batch_size = mask.size(0)\n    # TODO: Non squared input-size handling can be improved. Not easy though\n    height = math.floor((mask.size(-1) / aspect_ratio) ** 0.5)\n    total_size = mask.size(-1)\n    width = math.floor(total_size / height)\n    if mask.size(-1) &gt; (height * width):\n        to_remove = mask.size(-1) - (height * width)\n        mask = mask[:, :-to_remove]\n    mask = mask.reshape(batch_size, height, width).numpy()\n    mask = mask / mask.max(axis=(1, 2), keepdims=True)\n\n    return mask\n</code></pre>"},{"location":"reference/quadra/utils/vit_explainability.html#quadra.utils.vit_explainability.rollout","title":"<code>rollout(attentions, discard_ratio=0.9, head_fusion='mean', aspect_ratio=1.0)</code>","text":"<p>Apply rollout on Attention matrices.</p> <p>Parameters:</p> <ul> <li> attentions             (<code>list[Tensor]</code>)         \u2013          <p>List of Attention matrices coming from different blocks</p> </li> <li> discard_ratio             (<code>float</code>, default:                 <code>0.9</code> )         \u2013          <p>Percentage of elements to discard</p> </li> <li> head_fusion             (<code>str</code>, default:                 <code>'mean'</code> )         \u2013          <p>Strategy of fusion of attention heads</p> </li> <li> aspect_ratio             (<code>float</code>, default:                 <code>1.0</code> )         \u2013          <p>Model inputs' width divided by height</p> </li> </ul> <p>Returns:</p> <ul> <li> mask (            <code>ndarray</code> )        \u2013          <p>Output mask, still needs a resize</p> </li> </ul> Source code in <code>quadra/utils/vit_explainability.py</code> <pre><code>def rollout(\n    attentions: list[torch.Tensor], discard_ratio: float = 0.9, head_fusion: str = \"mean\", aspect_ratio: float = 1.0\n) -&gt; np.ndarray:\n\"\"\"Apply rollout on Attention matrices.\n\n    Args:\n        attentions: List of Attention matrices coming from different blocks\n        discard_ratio: Percentage of elements to discard\n        head_fusion: Strategy of fusion of attention heads\n        aspect_ratio: Model inputs' width divided by height\n\n    Returns:\n        mask: Output mask, still needs a resize\n    \"\"\"\n    result = torch.eye(attentions[0].size(-1))\n    with torch.no_grad():\n        for attention in attentions:\n            if head_fusion == \"mean\":\n                attention_heads_fused = attention.mean(dim=1)\n            elif head_fusion == \"max\":\n                attention_heads_fused = attention.max(dim=1)[0]\n            elif head_fusion == \"min\":\n                attention_heads_fused = attention.min(dim=1)[0]\n            else:\n                raise ValueError(\"Attention head fusion type Not supported\")\n            # Drop the lowest attentions, but\n            # don't drop the class token\n            flat = attention_heads_fused.view(attention_heads_fused.size(0), -1)\n            _, indices = flat.topk(int(flat.size(-1) * discard_ratio), -1, False)\n            flat.scatter_(-1, indices, 0)\n            identity_matrix = torch.eye(attention_heads_fused.size(-1))\n            a = (attention_heads_fused + 1.0 * identity_matrix) / 2\n            a = a / a.sum(dim=-1).unsqueeze(1)\n            result = torch.matmul(a, result)\n    # Look at the total attention between the class token and the image patches\n    mask = result[:, 0, 1:]\n    batch_size = mask.size(0)\n    # TODO: Non squared input-size handling can be improved. Not easy though\n    height = math.floor((mask.size(-1) / aspect_ratio) ** 0.5)\n    total_size = mask.size(-1)\n    width = math.floor(total_size / height)\n    if mask.size(-1) &gt; (height * width):\n        to_remove = mask.size(-1) - (height * width)\n        mask = mask[:, :-to_remove]\n    mask = mask.reshape(batch_size, height, width).numpy()\n    mask = mask / mask.max(axis=(1, 2), keepdims=True)\n\n    return mask\n</code></pre>"},{"location":"reference/quadra/utils/patch/index.html","title":"patch","text":""},{"location":"reference/quadra/utils/patch/index.html#quadra.utils.patch.RleEncoder","title":"<code>RleEncoder</code>","text":"<p>             Bases: <code>JSONEncoder</code></p> <p>Custom encoder to convert numpy arrays to RLE.</p>"},{"location":"reference/quadra/utils/patch/index.html#quadra.utils.patch.RleEncoder.default","title":"<code>default(o)</code>","text":"<p>Customize standard encoder behaviour to convert numpy arrays to RLE.</p> Source code in <code>quadra/utils/patch/model.py</code> <pre><code>def default(self, o: Any):\n\"\"\"Customize standard encoder behaviour to convert numpy arrays to RLE.\"\"\"\n    if isinstance(o, np.ndarray):\n        return mask2rle(o)\n    return json.JSONEncoder.default(self, o)\n</code></pre>"},{"location":"reference/quadra/utils/patch/index.html#quadra.utils.patch.compute_patch_metrics","title":"<code>compute_patch_metrics(test_img_info, test_results, overlap, idx_to_class, patch_num_h=None, patch_num_w=None, patch_w=None, patch_h=None, return_polygon=False, patch_reconstruction_method='priority', annotated_good=None)</code>","text":"<p>Compute the metrics of a patch dataset.</p> <p>Parameters:</p> <ul> <li> test_img_info             (<code>list[PatchDatasetFileFormat]</code>)         \u2013          <p>List of observation paths and mask paths</p> </li> <li> test_results             (<code>DataFrame</code>)         \u2013          <p>Pandas dataframe containing the results of an SklearnClassificationTrainer utility</p> </li> <li> patch_num_h             (<code>int | None</code>, default:                 <code>None</code> )         \u2013          <p>Number of vertical patches (required if patch_w and patch_h are None)</p> </li> <li> patch_num_w             (<code>int | None</code>, default:                 <code>None</code> )         \u2013          <p>Number of horizontal patches (required if patch_w and patch_h are None)</p> </li> <li> patch_h             (<code>int | None</code>, default:                 <code>None</code> )         \u2013          <p>Patch height (required if patch_num_h and patch_num_w are None)</p> </li> <li> patch_w             (<code>int | None</code>, default:                 <code>None</code> )         \u2013          <p>Patch width (required if patch_num_h and patch_num_w are None)</p> </li> <li> overlap             (<code>float</code>)         \u2013          <p>Percentage of overlap between the patches</p> </li> <li> idx_to_class             (<code>dict</code>)         \u2013          <p>Dict mapping an index to the corresponding class name</p> </li> <li> return_polygon             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>if set to true convert the reconstructed mask into polygons, otherwise return the mask</p> </li> <li> patch_reconstruction_method             (<code>str</code>, default:                 <code>'priority'</code> )         \u2013          <p>How to compute the label of overlapping patches, can either be: priority: Assign the top priority label (i.e the one with greater index) to overlapping regions major_voting: Assign the most present label among the patches label overlapping a pixel</p> </li> <li> annotated_good             (<code>list[int] | None</code>, default:                 <code>None</code> )         \u2013          <p>List of indices of annotations to be treated as good.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[int, int, int, list[dict]]</code>         \u2013          <p>Tuple containing: false_region_bad: Number of false bad regions detected in the dataset false_region_good: Number of missed defects true_region_bad: Number of correctly identified defects reconstructions: If polygon is true this is a List of dict containing     {         \"file_path\": image_path,         \"mask_path\": mask_path,         \"file_name\": observation_name,         \"prediction\": [{             \"label\": predicted_label,             \"points\": List of dict coordinates \"x\" and \"y\" representing the points of a polygon that             surrounds an image area covered by patches of label = predicted_label         }]     } else its a list of dict containing     {         \"file_path\": image_path,         \"mask_path\": mask_path,         \"file_name\": observation_name,         \"prediction\": numpy array containing the reconstructed mask     }</p> </li> </ul> Source code in <code>quadra/utils/patch/metrics.py</code> <pre><code>def compute_patch_metrics(\n    test_img_info: list[PatchDatasetFileFormat],\n    test_results: pd.DataFrame,\n    overlap: float,\n    idx_to_class: dict,\n    patch_num_h: int | None = None,\n    patch_num_w: int | None = None,\n    patch_w: int | None = None,\n    patch_h: int | None = None,\n    return_polygon: bool = False,\n    patch_reconstruction_method: str = \"priority\",\n    annotated_good: list[int] | None = None,\n) -&gt; tuple[int, int, int, list[dict]]:\n\"\"\"Compute the metrics of a patch dataset.\n\n    Args:\n        test_img_info: List of observation paths and mask paths\n        test_results: Pandas dataframe containing the results of an SklearnClassificationTrainer utility\n        patch_num_h: Number of vertical patches (required if patch_w and patch_h are None)\n        patch_num_w: Number of horizontal patches (required if patch_w and patch_h are None)\n        patch_h: Patch height (required if patch_num_h and patch_num_w are None)\n        patch_w: Patch width (required if patch_num_h and patch_num_w are None)\n        overlap: Percentage of overlap between the patches\n        idx_to_class: Dict mapping an index to the corresponding class name\n        return_polygon: if set to true convert the reconstructed mask into polygons, otherwise return the mask\n        patch_reconstruction_method: How to compute the label of overlapping patches, can either be:\n            priority: Assign the top priority label (i.e the one with greater index) to overlapping regions\n            major_voting: Assign the most present label among the patches label overlapping a pixel\n        annotated_good: List of indices of annotations to be treated as good.\n\n    Returns:\n        Tuple containing:\n            false_region_bad: Number of false bad regions detected in the dataset\n            false_region_good: Number of missed defects\n            true_region_bad: Number of correctly identified defects\n            reconstructions: If polygon is true this is a List of dict containing\n                {\n                    \"file_path\": image_path,\n                    \"mask_path\": mask_path,\n                    \"file_name\": observation_name,\n                    \"prediction\": [{\n                        \"label\": predicted_label,\n                        \"points\": List of dict coordinates \"x\" and \"y\" representing the points of a polygon that\n                        surrounds an image area covered by patches of label = predicted_label\n                    }]\n                }\n            else its a list of dict containing\n                {\n                    \"file_path\": image_path,\n                    \"mask_path\": mask_path,\n                    \"file_name\": observation_name,\n                    \"prediction\": numpy array containing the reconstructed mask\n                }\n    \"\"\"\n    assert patch_reconstruction_method in [\n        \"priority\",\n        \"major_voting\",\n    ], \"Patch reconstruction method not recognized, valid values are priority, major_voting\"\n\n    if (patch_h is not None and patch_w is not None) and (patch_num_h is not None and patch_num_w is not None):\n        raise ValueError(\"Either number of patches or patch size is required for reconstruction\")\n\n    assert (patch_h is not None and patch_w is not None) or (\n        patch_num_h is not None and patch_num_w is not None\n    ), \"Either number of patches or patch size is required for reconstruction\"\n\n    if patch_h is not None and patch_w is not None and patch_num_h is not None and patch_num_w is not None:\n        warnings.warn(\n            \"Both number of patches and patch dimension are specified, using number of patches by default\",\n            UserWarning,\n            stacklevel=2,\n        )\n\n    log.info(\"Computing patch metrics!\")\n\n    false_region_bad = 0\n    false_region_good = 0\n    true_region_bad = 0\n    reconstructions = []\n    test_results[\"filename\"] = test_results[\"sample\"].apply(\n        lambda x: \"_\".join(os.path.basename(x).replace(\"#DISCARD#\", \"\").split(\"_\")[0:-1])\n    )\n\n    for info in tqdm(test_img_info):\n        img_path = info.image_path\n        mask_path = info.mask_path\n\n        img_json_entry = {\n            \"image_path\": img_path,\n            \"mask_path\": mask_path,\n            \"file_name\": os.path.basename(img_path),\n            \"prediction\": None,\n        }\n\n        test_img = cv2.imread(img_path)\n\n        img_name = os.path.basename(img_path)\n\n        h = test_img.shape[0]\n        w = test_img.shape[1]\n\n        gt_img = None\n\n        if mask_path is not None and os.path.exists(mask_path):\n            gt_img = cv2.imread(mask_path, 0)\n            if test_img.shape[0:2] != gt_img.shape:\n                # Ensure that the mask has the same size as the image by padding it with zeros\n                log.warning(\"Found mask with different size than the image, padding it with zeros!\")\n                gt_img = np.pad(\n                    gt_img, ((0, test_img.shape[0] - gt_img.shape[0]), (0, test_img.shape[1] - gt_img.shape[1]))\n                )\n        if patch_num_h is not None and patch_num_w is not None:\n            patch_size, step = compute_patch_info(h, w, patch_num_h, patch_num_w, overlap)\n        elif patch_h is not None and patch_w is not None:\n            [patch_num_h, patch_num_w], step = compute_patch_info_from_patch_dim(h, w, patch_h, patch_w, overlap)\n            patch_size = (patch_h, patch_w)\n        else:\n            raise ValueError(\n                \"Either number of patches or patch size is required for reconstruction, this should not happen\"\n                \" at this stage\"\n            )\n\n        img_patches = get_sorted_patches_by_image(test_results, img_name)\n        pred = img_patches[\"pred_label\"].to_numpy().reshape(patch_num_h, patch_num_w)\n\n        # Treat annotated good predictions as background, this is an optimistic assumption that assumes that the\n        # remaining background is good, but it is not always true so maybe on non annotated areas we are missing\n        # defects and it would be necessary to handle this in a different way.\n        if annotated_good is not None:\n            pred[np.isin(pred, annotated_good)] = 0\n        if patch_num_h is not None and patch_num_w is not None:\n            output_mask, predicted_defect = reconstruct_patch(\n                input_img_shape=test_img.shape,\n                patch_size=patch_size,\n                pred=pred,\n                patch_num_h=patch_num_h,\n                patch_num_w=patch_num_w,\n                idx_to_class=idx_to_class,\n                step=step,\n                return_polygon=return_polygon,\n                method=patch_reconstruction_method,\n            )\n        else:\n            raise ValueError(\"`patch_num_h` and `patch_num_w` cannot be None at this point\")\n\n        if return_polygon:\n            img_json_entry[\"prediction\"] = predicted_defect\n        else:\n            img_json_entry[\"prediction\"] = output_mask\n\n        reconstructions.append(img_json_entry)\n        if gt_img is not None:\n            if annotated_good is not None:\n                gt_img[np.isin(gt_img, annotated_good)] = 0\n\n            gt_img_binary = (gt_img &gt; 0).astype(bool)  # type: ignore[operator]\n            regions_pred = label(output_mask).astype(np.uint8)\n\n            for k in range(1, regions_pred.max() + 1):\n                region = (regions_pred == k).astype(bool)\n                # If there's no overlap with the gt\n                if np.sum(np.bitwise_and(region, gt_img_binary)) == 0:\n                    false_region_bad += 1\n\n            output_mask = (output_mask &gt; 0).astype(np.uint8)\n            gt_img = label(gt_img)\n\n            for i in range(1, gt_img.max() + 1):  # type: ignore[union-attr]\n                region = (gt_img == i).astype(bool)  # type: ignore[union-attr]\n                if np.sum(np.bitwise_and(region, output_mask)) == 0:\n                    false_region_good += 1\n                else:\n                    true_region_bad += 1\n\n    return false_region_bad, false_region_good, true_region_bad, reconstructions\n</code></pre>"},{"location":"reference/quadra/utils/patch/index.html#quadra.utils.patch.generate_patch_dataset","title":"<code>generate_patch_dataset(data_dictionary, class_to_idx, val_size=0.3, test_size=0.0, seed=42, patch_number=None, patch_size=None, overlap=0.0, output_folder='extraction_data', save_original_images_and_masks=True, area_threshold=0.45, area_defect_threshold=0.2, mask_extension='_mask', mask_output_folder=None, save_mask=False, clear_output_folder=False, mask_preprocessing=None, train_filename='dataset.txt', repeat_good_images=1, balance_defects=True, annotated_good=None, num_workers=1)</code>","text":"<p>Giving a data_dictionary as:</p> <p>{     'base_name': '163931_1_5.jpg',     'path': 'extraction_data/1/163931_1_5.jpg',     'mask': 'extraction_data/1/163931_1_5_mask.jpg' } This function will generate patches datasets based on the defined split number, one for training, one for validation and one for testing respectively under output_folder/train, output_folder/val and output_folder/test, the training dataset will contain h5 files and a txt file resulting from a call to the generate_classification_patch_train_dataset, while the test dataset will contain patches saved on disk divided in subfolders per class, patch extraction is done in a sliding window fashion. Original images and masks (preprocessed if mask_preprocessing is present) will also be saved under output_folder/original/images and output_folder/original/masks. If patch number is specified the patch size will be calculated accordingly, if the image is not divisible by the patch number two possible behaviours can occur:     - if the patch reconstruction is smaller than the original image a new patch will be generated containing the     pixels from the edge of the image (E.g the new patch will contain the last patch_size pixels of the original     image)     - if the patch reconstruction is bigger than the original image the last patch will contain the pixels from the     edge of the image same as above, but without adding a new patch to the count.</p> <p>Parameters:</p> <ul> <li> data_dictionary             (<code>list[dict]</code>)         \u2013          <p>Dictionary as above</p> </li> <li> val_size             (<code>float</code>, default:                 <code>0.3</code> )         \u2013          <p>percentage of the dictionary entries to be used for validation</p> </li> <li> test_size             (<code>float</code>, default:                 <code>0.0</code> )         \u2013          <p>percentage of the dictionary entries to be used for testing</p> </li> <li> seed             (<code>int</code>, default:                 <code>42</code> )         \u2013          <p>seed for rng based operations</p> </li> <li> clear_output_folder             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>flag used to delete all the data in subfolder</p> </li> <li> class_to_idx             (<code>dict</code>)         \u2013          <p>Dictionary {\"defect\": value in mask.. }</p> </li> <li> output_folder             (<code>str</code>, default:                 <code>'extraction_data'</code> )         \u2013          <p>root_folder where to extract the data</p> </li> <li> save_original_images_and_masks             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>If True, images and masks will be copied inside output_folder/original/</p> </li> <li> area_threshold             (<code>float</code>, default:                 <code>0.45</code> )         \u2013          <p>Minimum percentage of defected patch area present in the mask to classify the patch as defect</p> </li> <li> area_defect_threshold             (<code>float</code>, default:                 <code>0.2</code> )         \u2013          <p>Minimum percentage of single defect present in the patch to classify the patch as defect</p> </li> <li> mask_extension             (<code>str</code>, default:                 <code>'_mask'</code> )         \u2013          <p>Extension used to assign image to mask</p> </li> <li> mask_output_folder             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>Optional folder in which to save the masks</p> </li> <li> save_mask             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Flag to save the mask</p> </li> <li> patch_number             (<code>tuple[int, int] | None</code>, default:                 <code>None</code> )         \u2013          <p>Optional number of patches for each side, required if patch_size is None</p> </li> <li> patch_size             (<code>tuple[int, int] | None</code>, default:                 <code>None</code> )         \u2013          <p>Optional dimension of the patch, required if patch_number is None</p> </li> <li> overlap             (<code>float</code>, default:                 <code>0.0</code> )         \u2013          <p>Overlap of the patches [0, 1]</p> </li> <li> mask_preprocessing             (<code>Callable | None</code>, default:                 <code>None</code> )         \u2013          <p>Optional function applied to masks, this can be useful for example to convert an image in range [0-255] to the required [0-1]</p> </li> <li> train_filename             (<code>str</code>, default:                 <code>'dataset.txt'</code> )         \u2013          <p>Name of the file containing mapping between h5 files and labels for training</p> </li> <li> repeat_good_images             (<code>int</code>, default:                 <code>1</code> )         \u2013          <p>Number of repetition for images with emtpy or None mask</p> </li> <li> balance_defects             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>If true add one good entry for each defect extracted</p> </li> <li> annotated_good             (<code>list[str] | None</code>, default:                 <code>None</code> )         \u2013          <p>List of labels that are annotated but considered as good</p> </li> <li> num_workers             (<code>int</code>, default:                 <code>1</code> )         \u2013          <p>Number of workers used for the h5 creation</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict | None</code>         \u2013          <p>None if data_dictionary is empty, otherwise return a dictionary containing informations about the dataset</p> </li> </ul> Source code in <code>quadra/utils/patch/dataset.py</code> <pre><code>def generate_patch_dataset(\n    data_dictionary: list[dict],\n    class_to_idx: dict,\n    val_size: float = 0.3,\n    test_size: float = 0.0,\n    seed: int = 42,\n    patch_number: tuple[int, int] | None = None,\n    patch_size: tuple[int, int] | None = None,\n    overlap: float = 0.0,\n    output_folder: str = \"extraction_data\",\n    save_original_images_and_masks: bool = True,\n    area_threshold: float = 0.45,\n    area_defect_threshold: float = 0.2,\n    mask_extension: str = \"_mask\",\n    mask_output_folder: str | None = None,\n    save_mask: bool = False,\n    clear_output_folder: bool = False,\n    mask_preprocessing: Callable | None = None,\n    train_filename: str = \"dataset.txt\",\n    repeat_good_images: int = 1,\n    balance_defects: bool = True,\n    annotated_good: list[str] | None = None,\n    num_workers: int = 1,\n) -&gt; dict | None:\n\"\"\"Giving a data_dictionary as:\n    &gt;&gt;&gt; {\n    &gt;&gt;&gt;     'base_name': '163931_1_5.jpg',\n    &gt;&gt;&gt;     'path': 'extraction_data/1/163931_1_5.jpg',\n    &gt;&gt;&gt;     'mask': 'extraction_data/1/163931_1_5_mask.jpg'\n    &gt;&gt;&gt;}\n    This function will generate patches datasets based on the defined split number, one for training, one for validation\n    and one for testing respectively under output_folder/train, output_folder/val and output_folder/test, the training\n    dataset will contain h5 files and a txt file resulting from a call to the\n    generate_classification_patch_train_dataset, while the test dataset will contain patches saved on disk divided\n    in subfolders per class, patch extraction is done in a sliding window fashion.\n    Original images and masks (preprocessed if mask_preprocessing is present) will also be saved under\n    output_folder/original/images and output_folder/original/masks.\n    If patch number is specified the patch size will be calculated accordingly, if the image is not divisible by the\n    patch number two possible behaviours can occur:\n        - if the patch reconstruction is smaller than the original image a new patch will be generated containing the\n        pixels from the edge of the image (E.g the new patch will contain the last patch_size pixels of the original\n        image)\n        - if the patch reconstruction is bigger than the original image the last patch will contain the pixels from the\n        edge of the image same as above, but without adding a new patch to the count.\n\n    Args:\n        data_dictionary: Dictionary as above\n        val_size: percentage of the dictionary entries to be used for validation\n        test_size: percentage of the dictionary entries to be used for testing\n        seed: seed for rng based operations\n        clear_output_folder: flag used to delete all the data in subfolder\n        class_to_idx: Dictionary {\"defect\": value in mask.. }\n        output_folder: root_folder where to extract the data\n        save_original_images_and_masks: If True, images and masks will be copied inside output_folder/original/\n        area_threshold: Minimum percentage of defected patch area present in the mask to classify the patch as defect\n        area_defect_threshold: Minimum percentage of single defect present in the patch to classify the patch as defect\n        mask_extension: Extension used to assign image to mask\n        mask_output_folder: Optional folder in which to save the masks\n        save_mask: Flag to save the mask\n        patch_number: Optional number of patches for each side, required if patch_size is None\n        patch_size: Optional dimension of the patch, required if patch_number is None\n        overlap: Overlap of the patches [0, 1]\n        mask_preprocessing: Optional function applied to masks, this can be useful for example to convert an image in\n            range [0-255] to the required [0-1]\n        train_filename: Name of the file containing mapping between h5 files and labels for training\n        repeat_good_images: Number of repetition for images with emtpy or None mask\n        balance_defects: If true add one good entry for each defect extracted\n        annotated_good: List of labels that are annotated but considered as good\n        num_workers: Number of workers used for the h5 creation\n\n    Returns:\n        None if data_dictionary is empty, otherwise return a dictionary containing informations about the dataset\n\n    \"\"\"\n    if len(data_dictionary) == 0:\n        warnings.warn(\"Input data dictionary is empty!\", UserWarning, stacklevel=2)\n        return None\n\n    if val_size &lt; 0 or test_size &lt; 0 or (val_size + test_size) &gt; 1:\n        raise ValueError(\"Validation and Test size must be greater or equal than zero and sum up to maximum 1\")\n    if clear_output_folder and os.path.exists(output_folder):\n        shutil.rmtree(output_folder)\n    os.makedirs(output_folder, exist_ok=True)\n    os.makedirs(os.path.join(output_folder, \"original\"), exist_ok=True)\n    if save_original_images_and_masks:\n        log.info(\"Moving original images and masks to dataset folder...\")\n        os.makedirs(os.path.join(output_folder, \"original\", \"images\"), exist_ok=True)\n        os.makedirs(os.path.join(output_folder, \"original\", \"masks\"), exist_ok=True)\n\n        for i, item in enumerate(data_dictionary):\n            img_new_path = os.path.join(\"original\", \"images\", item[\"base_name\"])\n            shutil.copy(item[\"path\"], os.path.join(output_folder, img_new_path))\n            data_dictionary[i][\"path\"] = img_new_path\n\n            if item[\"mask\"] is not None:\n                mask = cv2.imread(item[\"mask\"])\n                if mask_preprocessing is not None:\n                    mask = mask_preprocessing(mask).astype(np.uint8)\n                mask_new_path = os.path.join(\"original\", \"masks\", os.path.splitext(item[\"base_name\"])[0] + \".png\")\n                cv2.imwrite(os.path.join(output_folder, mask_new_path), mask)\n                data_dictionary[i][\"mask\"] = mask_new_path\n\n    shuffled_indices = np.random.default_rng(seed).permutation(len(data_dictionary))\n    data_dictionary = [data_dictionary[i] for i in shuffled_indices]\n    log.info(\"Performing multilabel stratification...\")\n    train_data_dictionary, val_data_dictionary, test_data_dictionary = multilabel_stratification(\n        output_folder=output_folder,\n        data_dictionary=data_dictionary,\n        num_classes=len(class_to_idx.values()),\n        val_size=val_size,\n        test_size=test_size,\n    )\n\n    log.info(\"Train set size: %d\", len(train_data_dictionary))\n    log.info(\"Validation set size: %d\", len(val_data_dictionary))\n    log.info(\"Test set size: %d\", len(test_data_dictionary))\n\n    idx_to_class = {v: k for (k, v) in class_to_idx.items()}\n\n    os.makedirs(output_folder, exist_ok=True)\n\n    dataset_info = {\n        \"patch_size\": patch_size,\n        \"patch_number\": patch_number,\n        \"overlap\": overlap,\n        \"annotated_good\": annotated_good,\n        \"train_files\": [{\"image_path\": x[\"path\"], \"mask_path\": x[\"mask\"]} for x in train_data_dictionary],\n        \"val_files\": [{\"image_path\": x[\"path\"], \"mask_path\": x[\"mask\"]} for x in val_data_dictionary],\n        \"test_files\": [{\"image_path\": x[\"path\"], \"mask_path\": x[\"mask\"]} for x in test_data_dictionary],\n    }\n\n    with open(os.path.join(output_folder, \"info.json\"), \"w\") as f:\n        json.dump(dataset_info, f)\n\n    if len(train_data_dictionary) &gt; 0:\n        log.info(\"Generating train set\")\n        generate_patch_sampling_dataset(\n            data_dictionary=train_data_dictionary,\n            patch_number=patch_number,\n            patch_size=patch_size,\n            overlap=overlap,\n            idx_to_class=idx_to_class,\n            balance_defects=balance_defects,\n            repeat_good_images=repeat_good_images,\n            output_folder=output_folder,\n            subfolder_name=\"train\",\n            train_filename=train_filename,\n            annotated_good=annotated_good if annotated_good is None else [class_to_idx[x] for x in annotated_good],\n            num_workers=num_workers,\n        )\n\n    for phase, split_dict in zip([\"val\", \"test\"], [val_data_dictionary, test_data_dictionary]):\n        if len(split_dict) &gt; 0:\n            log.info(\"Generating %s set\", phase)\n            generate_patch_sliding_window_dataset(\n                data_dictionary=split_dict,\n                patch_number=patch_number,\n                patch_size=patch_size,\n                overlap=overlap,\n                output_folder=output_folder,\n                subfolder_name=phase,\n                area_threshold=area_threshold,\n                area_defect_threshold=area_defect_threshold,\n                mask_extension=mask_extension,\n                mask_output_folder=mask_output_folder,\n                save_mask=save_mask,\n                class_to_idx=class_to_idx,\n            )\n\n    log.info(\"All done! Datasets saved to %s\", output_folder)\n\n    return dataset_info\n</code></pre>"},{"location":"reference/quadra/utils/patch/index.html#quadra.utils.patch.get_image_mask_association","title":"<code>get_image_mask_association(data_folder, mask_folder=None, mask_extension='', warning_on_missing_mask=True)</code>","text":"<p>Function used to match images and mask from a folder or sub-folders.</p> <p>Parameters:</p> <ul> <li> data_folder             (<code>str</code>)         \u2013          <p>root data folder containing images or images and masks</p> </li> <li> mask_folder             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>Optional root directory used to search only the masks</p> </li> <li> mask_extension             (<code>str</code>, default:                 <code>''</code> )         \u2013          <p>extension used to identify the mask file, it's mandatory if mask_folder is not specified warning_on_missing_mask: if set to True a warning will be raised if a mask is missing, disable if you know that many images do not have a mask.</p> </li> <li> warning_on_missing_mask             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>if set to True a warning will be raised if a mask is missing, disable if you know</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[dict]</code>         \u2013          <p>List of dict like:</p> </li> <li> <code>list[dict]</code>         \u2013          <p>[</p> </li> <li> <code>list[dict]</code>         \u2013          <p>{ 'base_name': '161927.tiff', 'path': 'test_dataset_patch/images/161927.tiff', 'mask': 'test_dataset_patch/masks/161927_mask.tiff'</p> </li> <li> <code>list[dict]</code>         \u2013          <p>}, ...</p> </li> <li> <code>list[dict]</code>         \u2013          <p>]</p> </li> </ul> Source code in <code>quadra/utils/patch/dataset.py</code> <pre><code>def get_image_mask_association(\n    data_folder: str,\n    mask_folder: str | None = None,\n    mask_extension: str = \"\",\n    warning_on_missing_mask: bool = True,\n) -&gt; list[dict]:\n\"\"\"Function used to match images and mask from a folder or sub-folders.\n\n    Args:\n        data_folder: root data folder containing images or images and masks\n        mask_folder: Optional root directory used to search only the masks\n        mask_extension: extension used to identify the mask file, it's mandatory if mask_folder is not specified\n            warning_on_missing_mask: if set to True a warning will be raised if a mask is missing, disable if you know\n            that many images do not have a mask.\n        warning_on_missing_mask: if set to True a warning will be raised if a mask is missing, disable if you know\n\n    Returns:\n        List of dict like:\n        [\n        {\n            'base_name': '161927.tiff',\n            'path': 'test_dataset_patch/images/161927.tiff',\n            'mask': 'test_dataset_patch/masks/161927_mask.tiff'\n        }, ...\n        ]\n    \"\"\"\n    # get all the images from the data folder\n    data_images = glob.glob(os.path.join(data_folder, \"**\", \"*\"), recursive=True)\n\n    basenames = [os.path.splitext(os.path.basename(image))[0] for image in data_images]\n\n    if len(set(basenames)) != len(basenames):\n        raise ValueError(\"Found multiple images with the same name and different extension, this is not supported.\")\n\n    log.info(\"Found: %d images in %s\", len(data_images), data_folder)\n    # divide images and mask if in the same folder\n    # if mask folder is specified search mask in that folder\n    if mask_folder:\n        masks_images = []\n        for basename in basenames:\n            mask_path = os.path.join(mask_folder, f\"{basename}{mask_extension}.*\")\n            mask_path_list = glob.glob(mask_path)\n\n            if len(mask_path_list) == 1:\n                masks_images.append(mask_path_list[0])\n            elif warning_on_missing_mask:\n                log.warning(\"Mask for %s not found\", basename)\n    else:\n        if mask_extension == \"\":\n            raise ValueError(\"If no mask folder is provided, mask extension is mandatory it cannot be empty.\")\n\n        masks_images = [image for image in data_images if mask_extension in image]\n        data_images = [image for image in data_images if mask_extension not in image]\n\n    # build support dictionary\n    unique_images = [{\"base_name\": os.path.basename(image), \"path\": image, \"mask\": None} for image in data_images]\n\n    images_stem = [os.path.splitext(str(image[\"base_name\"]))[0] + mask_extension for image in unique_images]\n    masks_stem = [os.path.splitext(os.path.basename(mask))[0] for mask in masks_images]\n\n    # search corrispondency between file or folders\n    for i, image_stem in enumerate(images_stem):\n        if image_stem in masks_stem:\n            unique_images[i][\"mask\"] = masks_images[masks_stem.index(image_stem)]\n\n    log.info(\"Unique images with mask: %d\", len([uni for uni in unique_images if uni.get(\"mask\") is not None]))\n    log.info(\"Unique images with no mask: %d\", len([uni for uni in unique_images if uni.get(\"mask\") is None]))\n\n    return unique_images\n</code></pre>"},{"location":"reference/quadra/utils/patch/index.html#quadra.utils.patch.plot_patch_reconstruction","title":"<code>plot_patch_reconstruction(reconstruction, idx_to_class, class_to_idx, ignore_classes=None, is_polygon=True)</code>","text":"<p>Helper function for plotting the patch reconstruction.</p> <p>Parameters:</p> <ul> <li> reconstruction             (<code>dict</code>)         \u2013          <p>Dict following this structure {     \"file_path\": str,     \"mask_path\": str,     \"prediction\": {         \"label\": str,         \"points\": [{\"x\": int, \"y\": int}]     } } if is_polygon else {     \"file_path\": str,     \"mask_path\": str,     \"prediction\": np.ndarray }</p> </li> <li> idx_to_class             (<code>dict[int, str]</code>)         \u2013          <p>Dictionary mapping indices to label names</p> </li> <li> class_to_idx             (<code>dict[str, int]</code>)         \u2013          <p>Dictionary mapping class names to indices</p> </li> <li> ignore_classes             (<code>list[int] | None</code>, default:                 <code>None</code> )         \u2013          <p>Eventually the classes to not plot</p> </li> <li> is_polygon             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Boolean indicating if the prediction is a polygon or a mask.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Figure</code>         \u2013          <p>Matplotlib plot showing predicted patch regions and eventually gt</p> </li> </ul> Source code in <code>quadra/utils/patch/visualization.py</code> <pre><code>def plot_patch_reconstruction(\n    reconstruction: dict,\n    idx_to_class: dict[int, str],\n    class_to_idx: dict[str, int],\n    ignore_classes: list[int] | None = None,\n    is_polygon: bool = True,\n) -&gt; Figure:\n\"\"\"Helper function for plotting the patch reconstruction.\n\n    Args:\n        reconstruction: Dict following this structure\n            {\n                \"file_path\": str,\n                \"mask_path\": str,\n                \"prediction\": {\n                    \"label\": str,\n                    \"points\": [{\"x\": int, \"y\": int}]\n                }\n            } if is_polygon else\n            {\n                \"file_path\": str,\n                \"mask_path\": str,\n                \"prediction\": np.ndarray\n            }\n        idx_to_class: Dictionary mapping indices to label names\n        class_to_idx: Dictionary mapping class names to indices\n        ignore_classes: Eventually the classes to not plot\n        is_polygon: Boolean indicating if the prediction is a polygon or a mask.\n\n    Returns:\n        Matplotlib plot showing predicted patch regions and eventually gt\n\n    \"\"\"\n    cmap_name = \"tab10\"\n\n    # 10 classes + good\n    if len(idx_to_class.values()) &gt; 11:\n        cmap_name = \"tab20\"\n\n    cmap = get_cmap(cmap_name)\n    test_img = cv2.imread(reconstruction[\"image_path\"])\n    test_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)\n    gt_img = None\n\n    if reconstruction[\"mask_path\"] is not None and os.path.isfile(reconstruction[\"mask_path\"]):\n        gt_img = cv2.imread(reconstruction[\"mask_path\"], 0)\n\n    out = np.zeros((test_img.shape[0], test_img.shape[1]), dtype=np.uint8)\n\n    if is_polygon:\n        for _, region in enumerate(reconstruction[\"prediction\"]):\n            points = [[item[\"x\"], item[\"y\"]] for item in region[\"points\"]]\n            c_label = region[\"label\"]\n\n            out = cv2.drawContours(\n                out,\n                np.array([points], np.int32),\n                -1,\n                class_to_idx[c_label],\n                thickness=cv2.FILLED,\n            )  # type: ignore[call-overload]\n    else:\n        out = reconstruction[\"prediction\"]\n\n    fig = plot_patch_results(\n        image=test_img,\n        prediction_image=out,\n        ground_truth_image=gt_img,\n        plot_original=True,\n        ignore_classes=ignore_classes,\n        save_path=None,\n        class_to_idx=class_to_idx,\n        cmap=cmap,\n    )\n\n    return fig\n</code></pre>"},{"location":"reference/quadra/utils/patch/index.html#quadra.utils.patch.plot_patch_results","title":"<code>plot_patch_results(image, prediction_image, ground_truth_image, class_to_idx, plot_original=True, ignore_classes=None, image_height=10, save_path=None, cmap=None)</code>","text":"<p>Function used to plot the image predicted.</p> <p>Parameters:</p> <ul> <li> prediction_image             (<code>ndarray</code>)         \u2013          <p>The prediction image</p> </li> <li> image             (<code>ndarray</code>)         \u2013          <p>The original image to plot</p> </li> <li> ground_truth_image             (<code>ndarray | None</code>)         \u2013          <p>The ground truth image</p> </li> <li> class_to_idx             (<code>dict[str, int]</code>)         \u2013          <p>Dictionary mapping class names to indices</p> </li> <li> plot_original             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Boolean to plot the original image</p> </li> <li> ignore_classes             (<code>list[int] | None</code>, default:                 <code>None</code> )         \u2013          <p>The classes to ignore, default is 0</p> </li> <li> image_height             (<code>int</code>, default:                 <code>10</code> )         \u2013          <p>The height of the output figure</p> </li> <li> save_path             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>The path to save the figure</p> </li> <li> cmap             (<code>Colormap | None</code>, default:                 <code>None</code> )         \u2013          <p>The colormap to use. If None, tab20 is used</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Figure</code>         \u2013          <p>The matplotlib figure</p> </li> </ul> Source code in <code>quadra/utils/patch/visualization.py</code> <pre><code>def plot_patch_results(\n    image: np.ndarray,\n    prediction_image: np.ndarray,\n    ground_truth_image: np.ndarray | None,\n    class_to_idx: dict[str, int],\n    plot_original: bool = True,\n    ignore_classes: list[int] | None = None,\n    image_height: int = 10,\n    save_path: str | None = None,\n    cmap: Colormap | None = None,\n) -&gt; Figure:\n\"\"\"Function used to plot the image predicted.\n\n    Args:\n        prediction_image: The prediction image\n        image: The original image to plot\n        ground_truth_image: The ground truth image\n        class_to_idx: Dictionary mapping class names to indices\n        plot_original: Boolean to plot the original image\n        ignore_classes: The classes to ignore, default is 0\n        image_height: The height of the output figure\n        save_path: The path to save the figure\n        cmap: The colormap to use. If None, tab20 is used\n\n    Returns:\n        The matplotlib figure\n    \"\"\"\n    if ignore_classes is None:\n        ignore_classes = [0]\n\n    if cmap is None:\n        cmap = get_cmap(\"tab20\")\n\n    image = image[0 : prediction_image.shape[0], 0 : prediction_image.shape[1], :]\n    idx_to_class = {v: k for k, v in class_to_idx.items()}\n\n    if ignore_classes is not None:\n        class_to_idx = {k: v for k, v in class_to_idx.items() if v not in ignore_classes}\n\n    class_idxs = list(class_to_idx.values())\n\n    cmap = {str(c): tuple(int(i * 255) for i in cmap(c / len(class_idxs))[:-1]) for c in class_idxs}\n    output_images = []\n    titles = []\n\n    if plot_original:\n        output_images.append(image)\n        titles.append(\"Original Image\")\n\n    if ground_truth_image is not None:\n        ground_truth_image = ground_truth_image[0 : prediction_image.shape[0], 0 : prediction_image.shape[1]]\n        ground_truth_mask = create_rgb_mask(ground_truth_image, cmap, ignore_classes=ignore_classes)\n        output_images.append(ground_truth_mask)\n        titles.append(\"Ground Truth Mask\")\n\n    prediction_mask = create_rgb_mask(\n        prediction_image,\n        cmap,\n        ignore_classes=ignore_classes,\n    )\n\n    output_images.append(prediction_mask)\n    titles.append(\"Prediction Mask\")\n    if ignore_classes is not None and ground_truth_image is not None:\n        prediction_mask = create_rgb_mask(\n            prediction_image, cmap, ignore_classes=ignore_classes, ground_truth_mask=ground_truth_image\n        )\n\n        ignored_classes_str = [idx_to_class[c] for c in ignore_classes]\n        prediction_title = f\"Prediction Mask \\n (Ignoring Ground Truth Class: {ignored_classes_str})\"\n        output_images.append(prediction_mask)\n        titles.append(prediction_title)\n\n    fig, axs = plt.subplots(\n        ncols=len(output_images),\n        nrows=1,\n        figsize=(len(output_images) * image_height, image_height),\n        squeeze=False,\n        facecolor=\"white\",\n    )\n\n    for i, output_image in enumerate(output_images):\n        axs[0, i].imshow(show_mask_on_image(image, output_image))\n        axs[0, i].set_title(titles[i])\n        axs[0, i].axis(\"off\")\n\n    custom_lines = [Line2D([0], [0], color=tuple(i / 255.0 for i in cmap[str(c)]), lw=4) for c in class_idxs]\n    custom_labels = list(class_to_idx.keys())\n    axs[0, -1].legend(custom_lines, custom_labels, loc=\"center left\", bbox_to_anchor=(1.01, 0.81), borderaxespad=0)\n    if save_path is not None:\n        plt.savefig(save_path, bbox_inches=\"tight\")\n        plt.close()\n\n    return fig\n</code></pre>"},{"location":"reference/quadra/utils/patch/index.html#quadra.utils.patch.reconstruct_patch","title":"<code>reconstruct_patch(input_img_shape, patch_size, pred, patch_num_h, patch_num_w, idx_to_class, step, return_polygon=True, method='priority')</code>","text":"<p>Reconstructs the prediction image from the patches.</p> <p>Parameters:</p> <ul> <li> input_img_shape             (<code>tuple[int, ...]</code>)         \u2013          <p>The size of the reconstructed image</p> </li> <li> patch_size             (<code>tuple[int, int]</code>)         \u2013          <p>Array defining the patch size</p> </li> <li> pred             (<code>ndarray</code>)         \u2013          <p>Numpy array containing reconstructed prediction (patch_num_h x patch_num_w)</p> </li> <li> patch_num_h             (<code>int</code>)         \u2013          <p>Number of vertical patches</p> </li> <li> patch_num_w             (<code>int</code>)         \u2013          <p>Number of horizontal patches</p> </li> <li> idx_to_class             (<code>dict</code>)         \u2013          <p>Dictionary mapping indices to labels</p> </li> <li> step             (<code>tuple[int, int]</code>)         \u2013          <p>Array defining the step size to be used for reconstruction</p> </li> <li> return_polygon             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>If true compute predicted polygons. Defaults to True.</p> </li> <li> method             (<code>str</code>, default:                 <code>'priority'</code> )         \u2013          <p>Reconstruction method to be used. Currently supported: \"priority\" and \"major_voting\"</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[ndarray, list[dict]]</code>         \u2013          <p>(reconstructed_prediction_image, predictions) where predictions is an array of objects [{     \"label\": Predicted_label,     \"points\": List of dict coordinates \"x\" and \"y\" representing the points of a polygon that         surrounds an image area covered by patches of label = predicted_label }]</p> </li> </ul> Source code in <code>quadra/utils/patch/metrics.py</code> <pre><code>def reconstruct_patch(\n    input_img_shape: tuple[int, ...],\n    patch_size: tuple[int, int],\n    pred: np.ndarray,\n    patch_num_h: int,\n    patch_num_w: int,\n    idx_to_class: dict,\n    step: tuple[int, int],\n    return_polygon: bool = True,\n    method: str = \"priority\",\n) -&gt; tuple[np.ndarray, list[dict]]:\n\"\"\"Reconstructs the prediction image from the patches.\n\n    Args:\n        input_img_shape: The size of the reconstructed image\n        patch_size: Array defining the patch size\n        pred: Numpy array containing reconstructed prediction (patch_num_h x patch_num_w)\n        patch_num_h: Number of vertical patches\n        patch_num_w: Number of horizontal patches\n        idx_to_class: Dictionary mapping indices to labels\n        step: Array defining the step size to be used for reconstruction\n        return_polygon: If true compute predicted polygons. Defaults to True.\n        method: Reconstruction method to be used. Currently supported: \"priority\" and \"major_voting\"\n\n    Returns:\n        (reconstructed_prediction_image, predictions) where predictions is an array of objects\n            [{\n                \"label\": Predicted_label,\n                \"points\": List of dict coordinates \"x\" and \"y\" representing the points of a polygon that\n                    surrounds an image area covered by patches of label = predicted_label\n            }]\n    \"\"\"\n    if method == \"priority\":\n        return _reconstruct_patch_priority(\n            input_img_shape,\n            patch_size,\n            pred,\n            patch_num_h,\n            patch_num_w,\n            idx_to_class,\n            step,\n            return_polygon,\n        )\n    if method == \"major_voting\":\n        return _reconstruct_patch_major_voting(\n            input_img_shape,\n            patch_size,\n            pred,\n            patch_num_h,\n            patch_num_w,\n            idx_to_class,\n            step,\n            return_polygon,\n        )\n\n    raise ValueError(f\"Invalid reconstruction method {method}\")\n</code></pre>"},{"location":"reference/quadra/utils/patch/index.html#quadra.utils.patch.save_classification_result","title":"<code>save_classification_result(results, output_folder, confusion_matrix, accuracy, test_dataloader, reconstructions, config, output, ignore_classes=None)</code>","text":"<p>Save classification results.</p> <p>Parameters:</p> <ul> <li> results             (<code>DataFrame</code>)         \u2013          <p>Dataframe containing the classification results</p> </li> <li> output_folder             (<code>str</code>)         \u2013          <p>Folder where to save the results</p> </li> <li> confusion_matrix             (<code>DataFrame | None</code>)         \u2013          <p>Confusion matrix</p> </li> <li> accuracy             (<code>float</code>)         \u2013          <p>Accuracy of the model</p> </li> <li> test_dataloader             (<code>DataLoader</code>)         \u2013          <p>Dataloader used for testing</p> </li> <li> reconstructions             (<code>list[dict]</code>)         \u2013          <p>List of dictionaries containing polygons or masks</p> </li> <li> config             (<code>DictConfig</code>)         \u2013          <p>Experiment configuration</p> </li> <li> output             (<code>DictConfig</code>)         \u2013          <p>Output configuration</p> </li> <li> ignore_classes             (<code>list[int] | None</code>, default:                 <code>None</code> )         \u2013          <p>Eventual classes to ignore during reconstruction plot. Defaults to None.</p> </li> </ul> Source code in <code>quadra/utils/patch/model.py</code> <pre><code>def save_classification_result(\n    results: pd.DataFrame,\n    output_folder: str,\n    confusion_matrix: pd.DataFrame | None,\n    accuracy: float,\n    test_dataloader: DataLoader,\n    reconstructions: list[dict],\n    config: DictConfig,\n    output: DictConfig,\n    ignore_classes: list[int] | None = None,\n):\n\"\"\"Save classification results.\n\n    Args:\n        results: Dataframe containing the classification results\n        output_folder: Folder where to save the results\n        confusion_matrix: Confusion matrix\n        accuracy: Accuracy of the model\n        test_dataloader: Dataloader used for testing\n        reconstructions: List of dictionaries containing polygons or masks\n        config: Experiment configuration\n        output: Output configuration\n        ignore_classes: Eventual classes to ignore during reconstruction plot. Defaults to None.\n    \"\"\"\n    # Save csv\n    results.to_csv(os.path.join(output_folder, \"test_results.csv\"), index_label=\"index\")\n\n    if confusion_matrix is not None:\n        # Save confusion matrix\n        disp = ConfusionMatrixDisplay(\n            confusion_matrix=np.array(confusion_matrix),\n            display_labels=[x.replace(\"pred:\", \"\") for x in confusion_matrix.columns.to_list()],\n        )\n        disp.plot(include_values=True, cmap=plt.cm.Greens, ax=None, colorbar=False, xticks_rotation=90)\n        plt.title(f\"Confusion Matrix (Accuracy: {(accuracy * 100):.2f}%)\")\n        plt.savefig(\n            os.path.join(output_folder, \"test_confusion_matrix.png\"),\n            bbox_inches=\"tight\",\n            pad_inches=0,\n            dpi=300,\n        )\n        plt.close()\n\n    if output.example:\n        if not hasattr(test_dataloader.dataset, \"idx_to_class\"):\n            raise ValueError(\"The provided dataset does not have an attribute 'idx_to_class\")\n\n        idx_to_class = test_dataloader.dataset.idx_to_class\n\n        # Get misclassified samples\n        example_folder = os.path.join(output_folder, \"example\")\n        if not os.path.isdir(example_folder):\n            os.makedirs(example_folder)\n\n        # Skip if no no ground truth is available\n        if not all(results[\"real_label\"] == -1):\n            for v in np.unique([results[\"real_label\"], results[\"pred_label\"]]):\n                if v == -1:\n                    continue\n\n                k = idx_to_class[v]\n\n                if ignore_classes is not None and v in ignore_classes:\n                    continue\n\n                plot_classification_results(\n                    test_dataloader.dataset,\n                    unorm=UnNormalize(mean=config.transforms.mean, std=config.transforms.std),\n                    pred_labels=results[\"pred_label\"].to_numpy(),\n                    test_labels=results[\"real_label\"].to_numpy(),\n                    class_name=k,\n                    original_folder=example_folder,\n                    idx_to_class=idx_to_class,\n                    pred_class_to_plot=v,\n                    what=\"con\",\n                    rows=output.get(\"rows\", 3),\n                    cols=output.get(\"cols\", 2),\n                    figsize=output.get(\"figsize\", (20, 20)),\n                )\n\n                plot_classification_results(\n                    test_dataloader.dataset,\n                    unorm=UnNormalize(mean=config.transforms.mean, std=config.transforms.std),\n                    pred_labels=results[\"pred_label\"].to_numpy(),\n                    test_labels=results[\"real_label\"].to_numpy(),\n                    class_name=k,\n                    original_folder=example_folder,\n                    idx_to_class=idx_to_class,\n                    pred_class_to_plot=v,\n                    what=\"dis\",\n                    rows=output.get(\"rows\", 3),\n                    cols=output.get(\"cols\", 2),\n                    figsize=output.get(\"figsize\", (20, 20)),\n                )\n\n        for counter, reconstruction in enumerate(reconstructions):\n            is_polygon = True\n            if isinstance(reconstruction[\"prediction\"], np.ndarray):\n                is_polygon = False\n\n            if is_polygon:\n                if len(reconstruction[\"prediction\"]) == 0:\n                    continue\n            elif reconstruction[\"prediction\"].sum() == 0:\n                continue\n\n            if counter &gt; 5:\n                break\n\n            to_plot = plot_patch_reconstruction(\n                reconstruction,\n                idx_to_class,\n                class_to_idx=test_dataloader.dataset.class_to_idx,  # type: ignore[attr-defined]\n                ignore_classes=ignore_classes,\n                is_polygon=is_polygon,\n            )\n\n            if to_plot:\n                output_name = f\"reconstruction_{os.path.splitext(os.path.basename(reconstruction['file_name']))[0]}.png\"\n                plt.savefig(os.path.join(example_folder, output_name), bbox_inches=\"tight\", pad_inches=0)\n\n            plt.close()\n</code></pre>"},{"location":"reference/quadra/utils/patch/dataset.html","title":"dataset","text":""},{"location":"reference/quadra/utils/patch/dataset.html#quadra.utils.patch.dataset.InvalidNumWorkersNumberException","title":"<code>InvalidNumWorkersNumberException</code>","text":"<p>             Bases: <code>Exception</code></p> <p>Exception raised when an invalid number of workers is passed to a function.</p>"},{"location":"reference/quadra/utils/patch/dataset.html#quadra.utils.patch.dataset.InvalidParameterCombinationException","title":"<code>InvalidParameterCombinationException</code>","text":"<p>             Bases: <code>Exception</code></p> <p>Exception raised when an invalid combination of parameters is passed to a function.</p>"},{"location":"reference/quadra/utils/patch/dataset.html#quadra.utils.patch.dataset.PatchDatasetFileFormat","title":"<code>PatchDatasetFileFormat</code>  <code>dataclass</code>","text":"<p>Model representing the content of the patch dataset split_files field in the info.json file.</p>"},{"location":"reference/quadra/utils/patch/dataset.html#quadra.utils.patch.dataset.PatchDatasetInfo","title":"<code>PatchDatasetInfo</code>  <code>dataclass</code>","text":"<p>Model representing the content of the patch dataset info.json file.</p>"},{"location":"reference/quadra/utils/patch/dataset.html#quadra.utils.patch.dataset.__save_patch_dataset","title":"<code>__save_patch_dataset(image_patches, labelled_patches=None, mask_patches=None, labelled_mask=None, output_folder='extraction_data', image_name='example', area_threshold=0.45, area_defect_threshold=0.2, mask_extension='_mask', save_mask=False, mask_output_folder=None, class_to_idx=None)</code>","text":"<p>Given a view_as_window computed patches, masks and labelled mask, save all the images in subdirectory divided by name and position in the grid, ambiguous patches i.e. the one that contains defects but with not enough to go above defined thresholds are marked as #DISCARD# and should be discarded in training. Patches of images without ground truth are saved inside the None folder.</p> <p>Parameters:</p> <ul> <li> image_patches             (<code>ndarray</code>)         \u2013          <p>[n, m, patch_w, patch_h, channel] numpy array of the image patches</p> </li> <li> mask_patches             (<code>ndarray | None</code>, default:                 <code>None</code> )         \u2013          <p>[n, m, patch_w, patch_h] numpy array of mask patches</p> </li> <li> labelled_patches             (<code>ndarray | None</code>, default:                 <code>None</code> )         \u2013          <p>[n, m, patch_w, patch_h] numpy array of labelled mask patch</p> </li> <li> labelled_mask             (<code>ndarray | None</code>, default:                 <code>None</code> )         \u2013          <p>numpy array in which each defect in the image is labelled using connected components</p> </li> <li> class_to_idx             (<code>dict | None</code>, default:                 <code>None</code> )         \u2013          <p>Dictionary with the mapping {\"class\" -&gt; class in mask}, it must cover all indices contained in the masks</p> </li> <li> save_mask             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>flag to save or ignore mask</p> </li> <li> output_folder             (<code>str</code>, default:                 <code>'extraction_data'</code> )         \u2013          <p>folder where to save data</p> </li> <li> mask_extension             (<code>str</code>, default:                 <code>'_mask'</code> )         \u2013          <p>postfix of the saved mask based on the image name</p> </li> <li> mask_output_folder             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>Optional folder in which to save the masks</p> </li> <li> image_name             (<code>str</code>, default:                 <code>'example'</code> )         \u2013          <p>name to use in order to save the data</p> </li> <li> area_threshold             (<code>float</code>, default:                 <code>0.45</code> )         \u2013          <p>minimum percentage of defected patch area present in the mask to classify the patch as defect</p> </li> <li> area_defect_threshold             (<code>float</code>, default:                 <code>0.2</code> )         \u2013          <p>minimum percentage of single defect present in the patch to classify the patch as defect</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>None</code>         \u2013          <p>None</p> </li> </ul> Source code in <code>quadra/utils/patch/dataset.py</code> <pre><code>def __save_patch_dataset(\n    image_patches: np.ndarray,\n    labelled_patches: np.ndarray | None = None,\n    mask_patches: np.ndarray | None = None,\n    labelled_mask: np.ndarray | None = None,\n    output_folder: str = \"extraction_data\",\n    image_name: str = \"example\",\n    area_threshold: float = 0.45,\n    area_defect_threshold: float = 0.2,\n    mask_extension: str = \"_mask\",\n    save_mask: bool = False,\n    mask_output_folder: str | None = None,\n    class_to_idx: dict | None = None,\n) -&gt; None:\n\"\"\"Given a view_as_window computed patches, masks and labelled mask, save all the images in subdirectory\n    divided by name and position in the grid, ambiguous patches i.e. the one that contains defects but with not enough\n    to go above defined thresholds are marked as #DISCARD# and should be discarded in training.\n    Patches of images without ground truth are saved inside the None folder.\n\n    Args:\n        image_patches: [n, m, patch_w, patch_h, channel] numpy array of the image patches\n        mask_patches: [n, m, patch_w, patch_h] numpy array of mask patches\n        labelled_patches: [n, m, patch_w, patch_h] numpy array of labelled mask patch\n        labelled_mask: numpy array in which each defect in the image is labelled using connected components\n        class_to_idx: Dictionary with the mapping {\"class\" -&gt; class in mask}, it must cover all indices\n            contained in the masks\n        save_mask: flag to save or ignore mask\n        output_folder: folder where to save data\n        mask_extension: postfix of the saved mask based on the image name\n        mask_output_folder: Optional folder in which to save the masks\n        image_name: name to use in order to save the data\n        area_threshold: minimum percentage of defected patch area present in the mask to classify the patch as defect\n        area_defect_threshold: minimum percentage of single defect present in the patch to classify the patch as defect\n\n    Returns:\n        None\n    \"\"\"\n    if class_to_idx is not None:\n        log.debug(\"Classes from dict: %s\", class_to_idx)\n        index_to_class = {v: k for k, v in class_to_idx.items()}\n        log.debug(\"Inverse class: %s\", index_to_class)\n        reference_classes = index_to_class\n\n        if mask_patches is not None:\n            classes_in_mask = set(np.unique(mask_patches))\n            missing_classes = set(classes_in_mask).difference(class_to_idx.values())\n\n            assert len(missing_classes) == 0, f\"Found index in mask that has no corresponding class {missing_classes}\"\n    elif mask_patches is not None:\n        reference_classes = {k: str(v) for k, v in enumerate(list(np.unique(mask_patches)))}\n    else:\n        raise ValueError(\"If no `class_to_idx` is provided, `mask_patches` must be provided\")\n\n    log.debug(\"Classes from mask: %s\", reference_classes)\n    class_to_idx = {v: k for k, v in reference_classes.items()}\n    log.debug(\"Final reference classes: %s\", reference_classes)\n\n    # create subdirectory for the saving data\n    for cl in reference_classes.values():\n        os.makedirs(os.path.join(output_folder, str(cl)), exist_ok=True)\n\n        if mask_output_folder is not None:\n            os.makedirs(os.path.join(output_folder, mask_output_folder, str(cl)), exist_ok=True)\n\n    if mask_output_folder is None:\n        mask_output_folder = output_folder\n    else:\n        mask_output_folder = os.path.join(output_folder, mask_output_folder)\n\n    log.debug(\"Mask out: %s\", mask_output_folder)\n\n    if mask_patches is None:\n        os.makedirs(os.path.join(output_folder, str(None)), exist_ok=True)\n    # for [i, j] in patches location\n    for row_index in range(image_patches.shape[0]):\n        for col_index in range(image_patches.shape[1]):\n            # default class it's the one in index 0\n            output_class = reference_classes.get(0)\n            image = image_patches[row_index, col_index]\n\n            discard_in_training = True\n            if mask_patches is not None and labelled_patches is not None:\n                discard_in_training = False\n                max_defected_area = 0\n                mask = mask_patches[row_index, col_index]\n                patch_area_th = mask.shape[0] * mask.shape[1] * area_threshold\n\n                if mask.sum() &gt; 0:\n                    discard_in_training = True\n                    for k, v in class_to_idx.items():\n                        if v == 0:\n                            continue\n\n                        mask_patch = mask == int(v)\n                        defected_area = mask_patch.sum()\n\n                        if defected_area &gt; 0:\n                            # If enough defected area is inside the patch\n                            if defected_area &gt; patch_area_th:\n                                if defected_area &gt; max_defected_area:\n                                    output_class = k\n                                    max_defected_area = defected_area\n                                    discard_in_training = False\n                            else:\n                                all_defects_in_patch = mask_patch * labelled_patches[row_index, col_index]\n\n                                # For each different defect inside the area check\n                                # if enough part of it is contained in the patch\n                                for defect_id in np.unique(all_defects_in_patch):\n                                    if defect_id == 0:\n                                        continue\n\n                                    defect_area_in_patch = (all_defects_in_patch == defect_id).sum()\n                                    defect_area_th = (labelled_mask == defect_id).sum() * area_defect_threshold\n\n                                    if defect_area_in_patch &gt; defect_area_th:\n                                        output_class = k\n                                        if defect_area_in_patch &gt; max_defected_area:\n                                            max_defected_area = defect_area_in_patch\n                                            discard_in_training = False\n                        else:\n                            discard_in_training = False\n\n                if save_mask:\n                    mask_name = f\"{image_name}_{row_index * image_patches.shape[1] + col_index}{mask_extension}.png\"\n\n                    if discard_in_training:\n                        mask_name = \"#DISCARD#\" + mask_name\n                    cv2.imwrite(\n                        os.path.join(\n                            mask_output_folder,\n                            output_class,  # type: ignore[arg-type]\n                            mask_name,\n                        ),\n                        mask.astype(np.uint8),\n                    )\n            else:\n                output_class = \"None\"\n\n            patch_name = f\"{image_name}_{row_index * image_patches.shape[1] + col_index}.png\"\n            if discard_in_training:\n                patch_name = \"#DISCARD#\" + patch_name\n\n            cv2.imwrite(\n                os.path.join(\n                    output_folder,\n                    output_class,  # type: ignore[arg-type]\n                    patch_name,\n                ),\n                image,\n            )\n</code></pre>"},{"location":"reference/quadra/utils/patch/dataset.html#quadra.utils.patch.dataset.compute_patch_info","title":"<code>compute_patch_info(img_h, img_w, patch_num_h, patch_num_w, overlap=0.0)</code>","text":"<p>Compute the patch size and step size given the number of patches and the overlap.</p> <p>Parameters:</p> <ul> <li> img_h             (<code>int</code>)         \u2013          <p>height of the image</p> </li> <li> img_w             (<code>int</code>)         \u2013          <p>width of the image</p> </li> <li> patch_num_h             (<code>int</code>)         \u2013          <p>number of vertical patches</p> </li> <li> patch_num_w             (<code>int</code>)         \u2013          <p>number of horizontal patches</p> </li> <li> overlap             (<code>float</code>, default:                 <code>0.0</code> )         \u2013          <p>percentage of overlap between patches.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[tuple[int, int], tuple[int, int]]</code>         \u2013          <p>Tuple containing: patch_size: [size_y, size_x] Dimension of the patch step_size: [step_y, step_x]  Step size</p> </li> </ul> Source code in <code>quadra/utils/patch/dataset.py</code> <pre><code>def compute_patch_info(\n    img_h: int,\n    img_w: int,\n    patch_num_h: int,\n    patch_num_w: int,\n    overlap: float = 0.0,\n) -&gt; tuple[tuple[int, int], tuple[int, int]]:\n\"\"\"Compute the patch size and step size given the number of patches and the overlap.\n\n    Args:\n        img_h: height of the image\n        img_w: width of the image\n        patch_num_h: number of vertical patches\n        patch_num_w: number of horizontal patches\n        overlap: percentage of overlap between patches.\n\n    Returns:\n        Tuple containing:\n            patch_size: [size_y, size_x] Dimension of the patch\n            step_size: [step_y, step_x]  Step size\n    \"\"\"\n    patch_size_h = np.ceil(img_h / (1 + (patch_num_h - 1) - (patch_num_h - 1) * overlap)).astype(int)\n    step_h = patch_size_h - np.ceil(overlap * patch_size_h).astype(int)\n\n    patch_size_w = np.ceil(img_w / (1 + (patch_num_w - 1) - (patch_num_w - 1) * overlap)).astype(int)\n    step_w = patch_size_w - np.ceil(overlap * patch_size_w).astype(int)\n\n    # We want a combination of patch size and step that if the image is not divisible by the number of patches\n    # will try to fit the maximum number of patches in the image + ONLY 1 extra patch that will be taken from the end\n    # of the image.\n\n    counter = 0\n    original_patch_size_h = patch_size_h\n    original_patch_size_w = patch_size_w\n    original_step_h = step_h\n    original_step_w = step_w\n\n    while (patch_num_h - 1) * step_h + patch_size_h &lt; img_h or (patch_num_h - 2) * step_h + patch_size_h &gt; img_h:\n        counter += 1\n        if (patch_num_h - 1) * (step_h + 1) + patch_size_h &lt; img_h:\n            step_h += 1\n        else:\n            patch_size_h += 1\n\n        if counter == 100:\n            # We probably entered an infinite loop, restart with smaller step size\n            step_h = original_step_h - 1\n            patch_size_h = original_patch_size_h\n            counter = 0\n\n    counter = 0\n    while (patch_num_w - 1) * step_w + patch_size_w &lt; img_w or (patch_num_w - 2) * step_w + patch_size_w &gt; img_w:\n        counter += 1\n        if (patch_num_w - 1) * (step_w + 1) + patch_size_w &lt; img_w:\n            step_w += 1\n        else:\n            patch_size_w += 1\n\n        if counter == 100:\n            # We probably entered an infinite loop, restart with smaller step size\n            step_w = original_step_w - 1\n            patch_size_w = original_patch_size_w\n            counter = 0\n\n    return (patch_size_h, patch_size_w), (step_h, step_w)\n</code></pre>"},{"location":"reference/quadra/utils/patch/dataset.html#quadra.utils.patch.dataset.compute_patch_info_from_patch_dim","title":"<code>compute_patch_info_from_patch_dim(img_h, img_w, patch_height, patch_width, overlap=0.0)</code>","text":"<p>Compute patch info given the patch dimension Args:     img_h: height of the image     img_w: width of the image     patch_height: patch height     patch_width: patch width     overlap: overlap percentage [0, 1].</p> <p>Returns:</p> <ul> <li> <code>tuple[tuple[int, int], tuple[int, int]]</code>         \u2013          <p>Tuple of number of patches, step</p> </li> </ul> Source code in <code>quadra/utils/patch/dataset.py</code> <pre><code>def compute_patch_info_from_patch_dim(\n    img_h: int,\n    img_w: int,\n    patch_height: int,\n    patch_width: int,\n    overlap: float = 0.0,\n) -&gt; tuple[tuple[int, int], tuple[int, int]]:\n\"\"\"Compute patch info given the patch dimension\n    Args:\n        img_h: height of the image\n        img_w: width of the image\n        patch_height: patch height\n        patch_width: patch width\n        overlap: overlap percentage [0, 1].\n\n    Returns:\n        Tuple of number of patches, step\n\n    \"\"\"\n    assert 1 &gt;= overlap &gt;= 0, f\"Invalid overlap. Must be between [0, 1], received {overlap}\"\n    step_h = patch_height - int(overlap * patch_height)\n    step_w = patch_width - int(overlap * patch_width)\n\n    patch_num_h = np.ceil(((img_h - patch_height) / step_h) + 1).astype(int)\n    patch_num_w = np.ceil(((img_w - patch_width) / step_w) + 1).astype(int)\n\n    # Handle the case where the last patch does not cover the full image, I need to do this rather than np.ceil\n    # because I don't want to add a new patch if the last one exceeds already the image!\n    if ((patch_num_h - 1) * step_h) + patch_height &lt; img_h:\n        patch_num_h += 1\n    if ((patch_num_w - 1) * step_w) + patch_width &lt; img_w:\n        patch_num_w += 1\n\n    return (patch_num_h, patch_num_w), (step_h, step_w)\n</code></pre>"},{"location":"reference/quadra/utils/patch/dataset.html#quadra.utils.patch.dataset.compute_safe_patch_range","title":"<code>compute_safe_patch_range(sampled_point, patch_size, image_size)</code>","text":"<p>Computes the safe patch size for the given image size.</p> <p>Parameters:</p> <ul> <li> sampled_point             (<code>int</code>)         \u2013          <p>the sampled point</p> </li> <li> patch_size             (<code>int</code>)         \u2013          <p>the size of the patch</p> </li> <li> image_size             (<code>int</code>)         \u2013          <p>the size of the image.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>int</code>         \u2013          <p>Tuple containing the safe patch range [left, right] such that</p> </li> <li> <code>int</code>         \u2013          <p>[sampled_point - left : sampled_point + right] will be within the image size.</p> </li> </ul> Source code in <code>quadra/utils/patch/dataset.py</code> <pre><code>def compute_safe_patch_range(sampled_point: int, patch_size: int, image_size: int) -&gt; tuple[int, int]:\n\"\"\"Computes the safe patch size for the given image size.\n\n    Args:\n        sampled_point: the sampled point\n        patch_size: the size of the patch\n        image_size: the size of the image.\n\n    Returns:\n        Tuple containing the safe patch range [left, right] such that\n        [sampled_point - left : sampled_point + right] will be within the image size.\n    \"\"\"\n    left = patch_size // 2\n    right = patch_size // 2\n\n    if sampled_point + right &gt; image_size:\n        right = image_size - sampled_point\n        left = patch_size - right\n\n    if sampled_point - left &lt; 0:\n        left = sampled_point\n        right = patch_size - left\n\n    return left, right\n</code></pre>"},{"location":"reference/quadra/utils/patch/dataset.html#quadra.utils.patch.dataset.create_h5","title":"<code>create_h5(data_dictionary, idx_to_class, overlap, repeat_good_images, balance_defects, output_folder, labelled_masks_path, sampling_dataset_folder, annotated_good=None, patch_size=None, patch_number=None)</code>","text":"<p>Create h5 files for each image in the dataset.</p> <p>Parameters:</p> <ul> <li> data_dictionary             (<code>list[dict[Any, Any]]</code>)         \u2013          <p>Dictionary containing image and mask mapping</p> </li> <li> idx_to_class             (<code>dict</code>)         \u2013          <p>Dict mapping an index to the corresponding class name</p> </li> <li> overlap             (<code>float</code>)         \u2013          <p>Percentage of overlap between patches</p> </li> <li> repeat_good_images             (<code>int</code>)         \u2013          <p>Number of repetition for images with emtpy or None mask</p> </li> <li> balance_defects             (<code>bool</code>)         \u2013          <p>If true add one good entry for each defect extracted</p> </li> <li> output_folder             (<code>str</code>)         \u2013          <p>root folder</p> </li> <li> overlap             (<code>float</code>)         \u2013          <p>Percentage of overlap between patches</p> </li> <li> annotated_good             (<code>list[int] | None</code>, default:                 <code>None</code> )         \u2013          <p>List of class indices that are considered good other than the background</p> </li> <li> labelled_masks_path             (<code>str</code>)         \u2013          <p>paths of labelled masks</p> </li> <li> sampling_dataset_folder             (<code>str</code>)         \u2013          <p>folder of the dataset</p> </li> <li> patch_size             (<code>tuple[int, int] | None</code>, default:                 <code>None</code> )         \u2013          <p>Dimension of the patch, required if patch_number is None</p> </li> <li> patch_number             (<code>tuple[int, int] | None</code>, default:                 <code>None</code> )         \u2013          <p>Number of patches for each side, required if patch_size is None.</p> </li> </ul> <p>Returns:</p> <ul> <li> output_list (            <code>list[str]</code> )        \u2013          <p>List of h5 files' names</p> </li> </ul> Source code in <code>quadra/utils/patch/dataset.py</code> <pre><code>def create_h5(\n    data_dictionary: list[dict[Any, Any]],\n    idx_to_class: dict,\n    overlap: float,\n    repeat_good_images: int,\n    balance_defects: bool,\n    output_folder: str,\n    labelled_masks_path: str,\n    sampling_dataset_folder: str,\n    annotated_good: list[int] | None = None,\n    patch_size: tuple[int, int] | None = None,\n    patch_number: tuple[int, int] | None = None,\n) -&gt; list[str]:\n\"\"\"Create h5 files for each image in the dataset.\n\n    Args:\n        data_dictionary: Dictionary containing image and mask mapping\n        idx_to_class: Dict mapping an index to the corresponding class name\n        overlap: Percentage of overlap between patches\n        repeat_good_images: Number of repetition for images with emtpy or None mask\n        balance_defects: If true add one good entry for each defect extracted\n        output_folder: root folder\n        overlap: Percentage of overlap between patches\n        annotated_good: List of class indices that are considered good other than the background\n        labelled_masks_path: paths of labelled masks\n        sampling_dataset_folder: folder of the dataset\n        patch_size: Dimension of the patch, required if patch_number is None\n        patch_number: Number of patches for each side, required if patch_size is None.\n\n    Returns:\n        output_list: List of h5 files' names\n\n    \"\"\"\n    if patch_number is None and patch_size is None:\n        raise InvalidParameterCombinationException(\"One between patch number or patch size must be specified!\")\n\n    output_list = []\n    for item in tqdm(data_dictionary):\n        log.debug(\"Processing %s\", item[\"base_name\"])\n        # this works even if item[\"path\"] is already an absolute path\n        img = cv2.imread(os.path.join(output_folder, item[\"path\"]))\n\n        h = img.shape[0]\n        w = img.shape[1]\n\n        if item[\"mask\"] is None:\n            mask = np.zeros([h, w])\n        else:\n            # this works even if item[\"mask\"] is already an absolute path\n            mask = cv2.imread(os.path.join(output_folder, item[\"mask\"]), 0)  # type: ignore[assignment]\n\n        if patch_size is not None:\n            patch_height = patch_size[1]\n            patch_width = patch_size[0]\n        else:\n            # Mypy complains because patch_number is Optional, but we already checked that it is not None.\n            [patch_height, patch_width], _ = compute_patch_info(\n                h,\n                w,\n                patch_number[0],  # type: ignore[index]\n                patch_number[1],  # type: ignore[index]\n                overlap,\n            )\n\n        h5_file_name_good = os.path.join(sampling_dataset_folder, f\"{os.path.splitext(item['base_name'])[0]}_good.h5\")\n\n        disable_good = False\n\n        with h5py.File(h5_file_name_good, \"w\") as f:\n            f.create_dataset(\"img_path\", data=item[\"path\"])\n            f.create_dataset(\"patch_size\", data=np.array([patch_height, patch_width]))\n\n            target = idx_to_class[0]\n\n            if mask.sum() == 0:\n                f.create_dataset(\"triangles\", data=np.array([], dtype=np.uint8), dtype=np.uint8)\n                f.create_dataset(\"triangles_weights\", data=np.array([], dtype=np.uint8), dtype=np.uint8)\n\n                for _ in range(repeat_good_images):\n                    output_list.append(f\"{os.path.basename(h5_file_name_good)},{target}\\n\")\n\n                continue\n\n            binary_mask = (mask &gt; 0).astype(np.uint8)\n\n            # Dilate the defects and take the background\n            binary_mask = np.logical_not(cv2.dilate(binary_mask, np.ones([patch_height, patch_width]))).astype(np.uint8)\n\n            temp_binary_mask = deepcopy(binary_mask)\n            # Remove the edges of the image as they are unsafe for sampling without padding\n            temp_binary_mask[0 : patch_height // 2, :] = 0\n            temp_binary_mask[:, 0 : patch_width // 2] = 0\n            temp_binary_mask[-patch_height // 2 :, :] = 0\n            temp_binary_mask[:, -patch_width // 2 :] = 0\n\n            if temp_binary_mask.sum() != 0:\n                # If the mask without the edges is not empty use it, otherwise use the original mask as it is not\n                # possible to sample a patch that will not exceed the edges, this must be taken care by the patch\n                # sampler used during training\n                binary_mask = temp_binary_mask\n\n            # In the case of hx1 or 1xw number of patches we must make sure that the sampling row or the sampling\n            # column is empty, if it isn't remove it from the possible sampling area\n            if patch_height == img.shape[0]:\n                must_clear_indices = np.where(binary_mask.sum(axis=0) != img.shape[0])[0]\n                binary_mask[:, must_clear_indices] = 0\n\n            if patch_width == img.shape[1]:\n                must_clear_indices = np.where(binary_mask.sum(axis=1) != img.shape[1])[0]\n                binary_mask[must_clear_indices, :] = 0\n\n            # If there's no space for sampling good patches skip it\n            if binary_mask.sum() == 0:\n                disable_good = True\n            else:\n                triangles, weights = triangulate_region(binary_mask)\n                if triangles is None:\n                    disable_good = True\n                else:\n                    log.debug(\n                        \"Saving %s triangles for %s with label %s\",\n                        triangles.shape[0],\n                        os.path.basename(h5_file_name_good),\n                        target,\n                    )\n\n                    f.create_dataset(\"mask_path\", data=item[\"mask\"])\n                    # Points from extracted triangles should be sufficiently far from all the defects allowing to sample\n                    # good patches almost all the time\n                    f.create_dataset(\"triangles\", data=triangles, dtype=np.int32)\n                    f.create_dataset(\"triangles_weights\", data=weights, dtype=np.float64)\n\n                    # Avoid saving the good h5 file here because otherwise I'll have one more good compared to the\n                    # number of defects\n                    if not balance_defects:\n                        output_list.append(f\"{os.path.basename(h5_file_name_good)},{target}\\n\")\n\n        if disable_good:\n            os.remove(h5_file_name_good)\n\n        labelled_mask = label(mask)\n        cv2.imwrite(os.path.join(labelled_masks_path, f\"{os.path.splitext(item['base_name'])[0]}.png\"), labelled_mask)\n\n        real_defects_mask = None\n\n        if annotated_good is not None:\n            # Remove true defected area from the good labeled mask\n            # If we want this to be even more restrictive we could also include the background as we don't know for sure\n            # it will not contain any defects\n            real_defects_mask = (~np.isin(mask, [0] + annotated_good)).astype(np.uint8)\n            real_defects_mask = cv2.dilate(real_defects_mask, np.ones([patch_height, patch_width])).astype(bool)\n\n        for i in np.unique(labelled_mask):\n            if i == 0:\n                continue\n\n            current_mask = (labelled_mask == i).astype(np.uint8)\n            target_idx = (mask * current_mask).max()\n\n            # When we have good annotations we want to avoid sampling patches containing true defects, to do so we\n            # reduce the extraction area based on the area covered by the other defects\n            if annotated_good is not None and real_defects_mask is not None and target_idx in annotated_good:\n                # a - b = a &amp; ~b\n                # pylint: disable=invalid-unary-operand-type\n                current_mask = np.bitwise_and(current_mask.astype(bool), ~real_defects_mask).astype(np.uint8)\n            else:\n                # When dealing with small defects the number of points that will be sampled will be limited and patches\n                # will mostly be centered around the defect, to overcome this issue enlarge defect bounding box by 50%\n                # of the difference between the patch_size and the defect bb size, we don't do this on good labels to\n                # avoid invalidating the reduction applied before.\n                props = regionprops(current_mask)[0]\n                bbox_size = [props.bbox[2] - props.bbox[0], props.bbox[3] - props.bbox[1]]\n                diff_bbox = np.array([max(0, patch_height - bbox_size[0]), max(0, patch_width - bbox_size[1])])\n\n                if diff_bbox[0] != 0:\n                    current_mask = cv2.dilate(current_mask, np.ones([diff_bbox[0] // 2, 1]))\n                if diff_bbox[1] != 0:\n                    current_mask = cv2.dilate(current_mask, np.ones([1, diff_bbox[1] // 2]))\n\n            if current_mask.sum() == 0:\n                # If it's not possible to sample a labelled good patch basically\n                continue\n\n            temp_current_mask = deepcopy(current_mask)\n            # Remove the edges of the image as they are unsafe for sampling without padding\n            temp_current_mask[0 : patch_height // 2, :] = 0\n            temp_current_mask[:, 0 : patch_width // 2] = 0\n            temp_current_mask[-patch_height // 2 :, :] = 0\n            temp_current_mask[:, -patch_width // 2 :] = 0\n\n            if temp_current_mask.sum() != 0:\n                # If the mask without the edges is not empty use it, otherwise use the original mask as it is not\n                # possible to sample a patch that will not exceed the edges, this must be taken care by the patch\n                # sampler used during training\n                current_mask = temp_current_mask\n\n            triangles, weights = triangulate_region(current_mask)\n\n            if triangles is not None:\n                h5_file_name = os.path.join(sampling_dataset_folder, f\"{os.path.splitext(item['base_name'])[0]}_{i}.h5\")\n\n                target = idx_to_class[target_idx]\n\n                log.debug(\n                    \"Saving %s triangles for %s with label %s\",\n                    triangles.shape[0],\n                    os.path.basename(h5_file_name),\n                    target,\n                )\n\n                with h5py.File(h5_file_name, \"w\") as f:\n                    f.create_dataset(\"img_path\", data=item[\"path\"])\n                    f.create_dataset(\"mask_path\", data=item[\"mask\"])\n                    f.create_dataset(\"patch_size\", data=np.array([patch_height, patch_width]))\n                    f.create_dataset(\"triangles\", data=triangles, dtype=np.int32)\n                    f.create_dataset(\"triangles_weights\", data=weights, dtype=np.float64)\n                    f.create_dataset(\"labelled_index\", data=i, dtype=np.int32)\n\n                if annotated_good is not None and target_idx in annotated_good:\n                    # I treat annotate good images exactly the same as I would treat background\n                    for _ in range(repeat_good_images):\n                        output_list.append(f\"{os.path.basename(h5_file_name)},{target}\\n\")\n                else:\n                    output_list.append(f\"{os.path.basename(h5_file_name)},{target}\\n\")\n\n                if balance_defects:\n                    if not disable_good:\n                        output_list.append(f\"{os.path.basename(h5_file_name_good)},{idx_to_class[0]}\\n\")\n                    else:\n                        log.debug(\n                            \"Unable to add a good defect for %s, since there's no way to sample good patches\",\n                            h5_file_name,\n                        )\n    return output_list\n</code></pre>"},{"location":"reference/quadra/utils/patch/dataset.html#quadra.utils.patch.dataset.extract_patches","title":"<code>extract_patches(image, patch_number, patch_size, step, overlap)</code>","text":"<p>From an image extract N x M Patch[h, w] if the image is not perfectly divided by the number of patches of given dimension the last patch will contain a replica of the original image taken in range [-img_h:, :] or [:, -img_w:].</p> <p>Parameters:</p> <ul> <li> image             (<code>ndarray</code>)         \u2013          <p>Numpy array of the image</p> </li> <li> patch_number             (<code>tuple[int, ...]</code>)         \u2013          <p>number of patches to be extracted</p> </li> <li> patch_size             (<code>tuple[int, ...]</code>)         \u2013          <p>dimension of the patch</p> </li> <li> step             (<code>tuple[int, ...]</code>)         \u2013          <p>step of the patch extraction</p> </li> <li> overlap             (<code>float</code>)         \u2013          <p>horizontal and vertical patch overlapping in range [0, 1]</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>         \u2013          <p>Patches [N, M, 1, image_w, image_h, image_c]</p> </li> </ul> Source code in <code>quadra/utils/patch/dataset.py</code> <pre><code>def extract_patches(\n    image: np.ndarray,\n    patch_number: tuple[int, ...],\n    patch_size: tuple[int, ...],\n    step: tuple[int, ...],\n    overlap: float,\n) -&gt; np.ndarray:\n\"\"\"From an image extract N x M Patch[h, w] if the image is not perfectly divided by the number of patches of given\n    dimension the last patch will contain a replica of the original image taken in range [-img_h:, :] or [:, -img_w:].\n\n    Args:\n        image: Numpy array of the image\n        patch_number: number of patches to be extracted\n        patch_size: dimension of the patch\n        step: step of the patch extraction\n        overlap: horizontal and vertical patch overlapping in range [0, 1]\n\n    Returns:\n        Patches [N, M, 1, image_w, image_h, image_c]\n\n    \"\"\"\n    assert 1.0 &gt;= overlap &gt;= 0.0, f\"Overlap must be between 0 and 1. Received {overlap}\"\n    (patch_num_h, patch_num_w) = patch_number\n    (patch_height, patch_width) = patch_size\n\n    pad_h = (patch_num_h - 1) * step[0] + patch_size[0] - image.shape[0]\n    pad_w = (patch_num_w - 1) * step[1] + patch_size[1] - image.shape[1]\n    # if the image has 3 channel change dimension\n    if len(image.shape) == 3:\n        patch_size = (patch_size[0], patch_size[1], image.shape[2])\n        step = (step[0], step[1], image.shape[2])\n\n    # If this is not true there's some strange case I didn't take into account\n    if pad_h &lt; 0 or pad_w &lt; 0:\n        raise ValueError(\"Something went wrong with the patch extraction, expected positive padding values\")\n\n    if pad_h &gt; 0 or pad_w &gt; 0:\n        # We work with copies as view_as_windows returns a view of the original image\n        crop_img = deepcopy(image)\n\n        if pad_h:\n            crop_img = crop_img[0 : (patch_num_h - 2) * step[0] + patch_height, :]\n\n        if pad_w:\n            crop_img = crop_img[:, 0 : (patch_num_w - 2) * step[1] + patch_width]\n\n        # Extract safe patches inside the image\n        patches = view_as_windows(crop_img, patch_size, step=step)\n    else:\n        patches = view_as_windows(image, patch_size, step=step)\n\n    extra_patches_h = None\n    extra_patches_w = None\n\n    if pad_h &gt; 0:\n        # Append extra patches taken from the edge of the image\n        extra_patches_h = view_as_windows(image[-patch_height:, :], patch_size, step=step)\n\n    if pad_w &gt; 0:\n        extra_patches_w = view_as_windows(image[:, -patch_width:], patch_size, step=step)\n\n        if extra_patches_h is not None:\n            # Add an extra column and set is content to the bottom right patch area of the original image if both\n            # dimension requires extra patches\n            if extra_patches_h.ndim == 6:\n                # RGB\n                extra_patches_h = np.concatenate(\n                    [\n                        extra_patches_h,\n                        (np.zeros([1, 1, 1, patch_size[0], patch_size[1], extra_patches_h.shape[-1]], dtype=np.uint8)),\n                    ],\n                    axis=1,\n                )\n            else:\n                extra_patches_h = np.concatenate(\n                    [extra_patches_h, (np.zeros([1, 1, patch_size[0], patch_size[1]], dtype=np.uint8))], axis=1\n                )\n\n            if extra_patches_h is None:\n                # Required by mypy as it cannot infer that extra_patch_h cannot be None\n                raise ValueError(\"Extra patch h cannot be None!\")\n\n            extra_patches_h[:, -1, :] = image[-patch_height:, -patch_width:]\n\n    if patches.ndim == 6:\n        # With RGB images there's an extra dimension, axis 2 is important don't use plain squeeze or it breaks if\n        # the number of patches is set to 1!\n        patches = patches.squeeze(axis=2)\n\n    if extra_patches_w is not None:\n        if extra_patches_w.ndim == 6:\n            # RGB\n            patches = np.concatenate([patches, extra_patches_w.squeeze(2)], axis=1)\n        else:\n            patches = np.concatenate([patches, extra_patches_w], axis=1)\n\n    if extra_patches_h is not None:\n        if extra_patches_h.ndim == 6:\n            # RGB\n            patches = np.concatenate([patches, extra_patches_h.squeeze(2)], axis=0)\n        else:\n            patches = np.concatenate([patches, extra_patches_h], axis=0)\n\n    # If this is not true there's some strange case I didn't take into account\n    assert (\n        patches.shape[0] == patch_num_h and patches.shape[1] == patch_num_w\n    ), f\"Patch shape {patches.shape} does not match the expected shape {patch_number}\"\n\n    return patches\n</code></pre>"},{"location":"reference/quadra/utils/patch/dataset.html#quadra.utils.patch.dataset.from_rgb_to_idx","title":"<code>from_rgb_to_idx(img, class_to_color, class_to_idx)</code>","text":"<p>Parameters:</p> <ul> <li> img             (<code>ndarray</code>)         \u2013          <p>Rgb mask in which each different color is associated with a class</p> </li> <li> class_to_color             (<code>dict</code>)         \u2013          <p>Dict \"key\": [R, G, B]</p> </li> <li> class_to_idx             (<code>dict</code>)         \u2013          <p>Dict \"key\": class_idx.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray</code>         \u2013          <p>Grayscale mask in which each class is associated with a specific index</p> </li> </ul> Source code in <code>quadra/utils/patch/dataset.py</code> <pre><code>def from_rgb_to_idx(img: np.ndarray, class_to_color: dict, class_to_idx: dict) -&gt; np.ndarray:\n\"\"\"Args:\n        img: Rgb mask in which each different color is associated with a class\n        class_to_color: Dict \"key\": [R, G, B]\n        class_to_idx: Dict \"key\": class_idx.\n\n    Returns:\n        Grayscale mask in which each class is associated with a specific index\n    \"\"\"\n    img = img.astype(int)\n    # Use negative values to avoid strange behaviour in the remote eventuality\n    # of someone using a color like [1, 255, 255]\n    for classe, color in class_to_color.items():\n        img[np.all(img == color, axis=-1).astype(bool), 0] = -class_to_idx[classe]\n\n    img = np.abs(img[:, :, 0])\n\n    return img.astype(np.uint8)\n</code></pre>"},{"location":"reference/quadra/utils/patch/dataset.html#quadra.utils.patch.dataset.generate_patch_dataset","title":"<code>generate_patch_dataset(data_dictionary, class_to_idx, val_size=0.3, test_size=0.0, seed=42, patch_number=None, patch_size=None, overlap=0.0, output_folder='extraction_data', save_original_images_and_masks=True, area_threshold=0.45, area_defect_threshold=0.2, mask_extension='_mask', mask_output_folder=None, save_mask=False, clear_output_folder=False, mask_preprocessing=None, train_filename='dataset.txt', repeat_good_images=1, balance_defects=True, annotated_good=None, num_workers=1)</code>","text":"<p>Giving a data_dictionary as:</p> <p>{     'base_name': '163931_1_5.jpg',     'path': 'extraction_data/1/163931_1_5.jpg',     'mask': 'extraction_data/1/163931_1_5_mask.jpg' } This function will generate patches datasets based on the defined split number, one for training, one for validation and one for testing respectively under output_folder/train, output_folder/val and output_folder/test, the training dataset will contain h5 files and a txt file resulting from a call to the generate_classification_patch_train_dataset, while the test dataset will contain patches saved on disk divided in subfolders per class, patch extraction is done in a sliding window fashion. Original images and masks (preprocessed if mask_preprocessing is present) will also be saved under output_folder/original/images and output_folder/original/masks. If patch number is specified the patch size will be calculated accordingly, if the image is not divisible by the patch number two possible behaviours can occur:     - if the patch reconstruction is smaller than the original image a new patch will be generated containing the     pixels from the edge of the image (E.g the new patch will contain the last patch_size pixels of the original     image)     - if the patch reconstruction is bigger than the original image the last patch will contain the pixels from the     edge of the image same as above, but without adding a new patch to the count.</p> <p>Parameters:</p> <ul> <li> data_dictionary             (<code>list[dict]</code>)         \u2013          <p>Dictionary as above</p> </li> <li> val_size             (<code>float</code>, default:                 <code>0.3</code> )         \u2013          <p>percentage of the dictionary entries to be used for validation</p> </li> <li> test_size             (<code>float</code>, default:                 <code>0.0</code> )         \u2013          <p>percentage of the dictionary entries to be used for testing</p> </li> <li> seed             (<code>int</code>, default:                 <code>42</code> )         \u2013          <p>seed for rng based operations</p> </li> <li> clear_output_folder             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>flag used to delete all the data in subfolder</p> </li> <li> class_to_idx             (<code>dict</code>)         \u2013          <p>Dictionary {\"defect\": value in mask.. }</p> </li> <li> output_folder             (<code>str</code>, default:                 <code>'extraction_data'</code> )         \u2013          <p>root_folder where to extract the data</p> </li> <li> save_original_images_and_masks             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>If True, images and masks will be copied inside output_folder/original/</p> </li> <li> area_threshold             (<code>float</code>, default:                 <code>0.45</code> )         \u2013          <p>Minimum percentage of defected patch area present in the mask to classify the patch as defect</p> </li> <li> area_defect_threshold             (<code>float</code>, default:                 <code>0.2</code> )         \u2013          <p>Minimum percentage of single defect present in the patch to classify the patch as defect</p> </li> <li> mask_extension             (<code>str</code>, default:                 <code>'_mask'</code> )         \u2013          <p>Extension used to assign image to mask</p> </li> <li> mask_output_folder             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>Optional folder in which to save the masks</p> </li> <li> save_mask             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>Flag to save the mask</p> </li> <li> patch_number             (<code>tuple[int, int] | None</code>, default:                 <code>None</code> )         \u2013          <p>Optional number of patches for each side, required if patch_size is None</p> </li> <li> patch_size             (<code>tuple[int, int] | None</code>, default:                 <code>None</code> )         \u2013          <p>Optional dimension of the patch, required if patch_number is None</p> </li> <li> overlap             (<code>float</code>, default:                 <code>0.0</code> )         \u2013          <p>Overlap of the patches [0, 1]</p> </li> <li> mask_preprocessing             (<code>Callable | None</code>, default:                 <code>None</code> )         \u2013          <p>Optional function applied to masks, this can be useful for example to convert an image in range [0-255] to the required [0-1]</p> </li> <li> train_filename             (<code>str</code>, default:                 <code>'dataset.txt'</code> )         \u2013          <p>Name of the file containing mapping between h5 files and labels for training</p> </li> <li> repeat_good_images             (<code>int</code>, default:                 <code>1</code> )         \u2013          <p>Number of repetition for images with emtpy or None mask</p> </li> <li> balance_defects             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>If true add one good entry for each defect extracted</p> </li> <li> annotated_good             (<code>list[str] | None</code>, default:                 <code>None</code> )         \u2013          <p>List of labels that are annotated but considered as good</p> </li> <li> num_workers             (<code>int</code>, default:                 <code>1</code> )         \u2013          <p>Number of workers used for the h5 creation</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict | None</code>         \u2013          <p>None if data_dictionary is empty, otherwise return a dictionary containing informations about the dataset</p> </li> </ul> Source code in <code>quadra/utils/patch/dataset.py</code> <pre><code>def generate_patch_dataset(\n    data_dictionary: list[dict],\n    class_to_idx: dict,\n    val_size: float = 0.3,\n    test_size: float = 0.0,\n    seed: int = 42,\n    patch_number: tuple[int, int] | None = None,\n    patch_size: tuple[int, int] | None = None,\n    overlap: float = 0.0,\n    output_folder: str = \"extraction_data\",\n    save_original_images_and_masks: bool = True,\n    area_threshold: float = 0.45,\n    area_defect_threshold: float = 0.2,\n    mask_extension: str = \"_mask\",\n    mask_output_folder: str | None = None,\n    save_mask: bool = False,\n    clear_output_folder: bool = False,\n    mask_preprocessing: Callable | None = None,\n    train_filename: str = \"dataset.txt\",\n    repeat_good_images: int = 1,\n    balance_defects: bool = True,\n    annotated_good: list[str] | None = None,\n    num_workers: int = 1,\n) -&gt; dict | None:\n\"\"\"Giving a data_dictionary as:\n    &gt;&gt;&gt; {\n    &gt;&gt;&gt;     'base_name': '163931_1_5.jpg',\n    &gt;&gt;&gt;     'path': 'extraction_data/1/163931_1_5.jpg',\n    &gt;&gt;&gt;     'mask': 'extraction_data/1/163931_1_5_mask.jpg'\n    &gt;&gt;&gt;}\n    This function will generate patches datasets based on the defined split number, one for training, one for validation\n    and one for testing respectively under output_folder/train, output_folder/val and output_folder/test, the training\n    dataset will contain h5 files and a txt file resulting from a call to the\n    generate_classification_patch_train_dataset, while the test dataset will contain patches saved on disk divided\n    in subfolders per class, patch extraction is done in a sliding window fashion.\n    Original images and masks (preprocessed if mask_preprocessing is present) will also be saved under\n    output_folder/original/images and output_folder/original/masks.\n    If patch number is specified the patch size will be calculated accordingly, if the image is not divisible by the\n    patch number two possible behaviours can occur:\n        - if the patch reconstruction is smaller than the original image a new patch will be generated containing the\n        pixels from the edge of the image (E.g the new patch will contain the last patch_size pixels of the original\n        image)\n        - if the patch reconstruction is bigger than the original image the last patch will contain the pixels from the\n        edge of the image same as above, but without adding a new patch to the count.\n\n    Args:\n        data_dictionary: Dictionary as above\n        val_size: percentage of the dictionary entries to be used for validation\n        test_size: percentage of the dictionary entries to be used for testing\n        seed: seed for rng based operations\n        clear_output_folder: flag used to delete all the data in subfolder\n        class_to_idx: Dictionary {\"defect\": value in mask.. }\n        output_folder: root_folder where to extract the data\n        save_original_images_and_masks: If True, images and masks will be copied inside output_folder/original/\n        area_threshold: Minimum percentage of defected patch area present in the mask to classify the patch as defect\n        area_defect_threshold: Minimum percentage of single defect present in the patch to classify the patch as defect\n        mask_extension: Extension used to assign image to mask\n        mask_output_folder: Optional folder in which to save the masks\n        save_mask: Flag to save the mask\n        patch_number: Optional number of patches for each side, required if patch_size is None\n        patch_size: Optional dimension of the patch, required if patch_number is None\n        overlap: Overlap of the patches [0, 1]\n        mask_preprocessing: Optional function applied to masks, this can be useful for example to convert an image in\n            range [0-255] to the required [0-1]\n        train_filename: Name of the file containing mapping between h5 files and labels for training\n        repeat_good_images: Number of repetition for images with emtpy or None mask\n        balance_defects: If true add one good entry for each defect extracted\n        annotated_good: List of labels that are annotated but considered as good\n        num_workers: Number of workers used for the h5 creation\n\n    Returns:\n        None if data_dictionary is empty, otherwise return a dictionary containing informations about the dataset\n\n    \"\"\"\n    if len(data_dictionary) == 0:\n        warnings.warn(\"Input data dictionary is empty!\", UserWarning, stacklevel=2)\n        return None\n\n    if val_size &lt; 0 or test_size &lt; 0 or (val_size + test_size) &gt; 1:\n        raise ValueError(\"Validation and Test size must be greater or equal than zero and sum up to maximum 1\")\n    if clear_output_folder and os.path.exists(output_folder):\n        shutil.rmtree(output_folder)\n    os.makedirs(output_folder, exist_ok=True)\n    os.makedirs(os.path.join(output_folder, \"original\"), exist_ok=True)\n    if save_original_images_and_masks:\n        log.info(\"Moving original images and masks to dataset folder...\")\n        os.makedirs(os.path.join(output_folder, \"original\", \"images\"), exist_ok=True)\n        os.makedirs(os.path.join(output_folder, \"original\", \"masks\"), exist_ok=True)\n\n        for i, item in enumerate(data_dictionary):\n            img_new_path = os.path.join(\"original\", \"images\", item[\"base_name\"])\n            shutil.copy(item[\"path\"], os.path.join(output_folder, img_new_path))\n            data_dictionary[i][\"path\"] = img_new_path\n\n            if item[\"mask\"] is not None:\n                mask = cv2.imread(item[\"mask\"])\n                if mask_preprocessing is not None:\n                    mask = mask_preprocessing(mask).astype(np.uint8)\n                mask_new_path = os.path.join(\"original\", \"masks\", os.path.splitext(item[\"base_name\"])[0] + \".png\")\n                cv2.imwrite(os.path.join(output_folder, mask_new_path), mask)\n                data_dictionary[i][\"mask\"] = mask_new_path\n\n    shuffled_indices = np.random.default_rng(seed).permutation(len(data_dictionary))\n    data_dictionary = [data_dictionary[i] for i in shuffled_indices]\n    log.info(\"Performing multilabel stratification...\")\n    train_data_dictionary, val_data_dictionary, test_data_dictionary = multilabel_stratification(\n        output_folder=output_folder,\n        data_dictionary=data_dictionary,\n        num_classes=len(class_to_idx.values()),\n        val_size=val_size,\n        test_size=test_size,\n    )\n\n    log.info(\"Train set size: %d\", len(train_data_dictionary))\n    log.info(\"Validation set size: %d\", len(val_data_dictionary))\n    log.info(\"Test set size: %d\", len(test_data_dictionary))\n\n    idx_to_class = {v: k for (k, v) in class_to_idx.items()}\n\n    os.makedirs(output_folder, exist_ok=True)\n\n    dataset_info = {\n        \"patch_size\": patch_size,\n        \"patch_number\": patch_number,\n        \"overlap\": overlap,\n        \"annotated_good\": annotated_good,\n        \"train_files\": [{\"image_path\": x[\"path\"], \"mask_path\": x[\"mask\"]} for x in train_data_dictionary],\n        \"val_files\": [{\"image_path\": x[\"path\"], \"mask_path\": x[\"mask\"]} for x in val_data_dictionary],\n        \"test_files\": [{\"image_path\": x[\"path\"], \"mask_path\": x[\"mask\"]} for x in test_data_dictionary],\n    }\n\n    with open(os.path.join(output_folder, \"info.json\"), \"w\") as f:\n        json.dump(dataset_info, f)\n\n    if len(train_data_dictionary) &gt; 0:\n        log.info(\"Generating train set\")\n        generate_patch_sampling_dataset(\n            data_dictionary=train_data_dictionary,\n            patch_number=patch_number,\n            patch_size=patch_size,\n            overlap=overlap,\n            idx_to_class=idx_to_class,\n            balance_defects=balance_defects,\n            repeat_good_images=repeat_good_images,\n            output_folder=output_folder,\n            subfolder_name=\"train\",\n            train_filename=train_filename,\n            annotated_good=annotated_good if annotated_good is None else [class_to_idx[x] for x in annotated_good],\n            num_workers=num_workers,\n        )\n\n    for phase, split_dict in zip([\"val\", \"test\"], [val_data_dictionary, test_data_dictionary]):\n        if len(split_dict) &gt; 0:\n            log.info(\"Generating %s set\", phase)\n            generate_patch_sliding_window_dataset(\n                data_dictionary=split_dict,\n                patch_number=patch_number,\n                patch_size=patch_size,\n                overlap=overlap,\n                output_folder=output_folder,\n                subfolder_name=phase,\n                area_threshold=area_threshold,\n                area_defect_threshold=area_defect_threshold,\n                mask_extension=mask_extension,\n                mask_output_folder=mask_output_folder,\n                save_mask=save_mask,\n                class_to_idx=class_to_idx,\n            )\n\n    log.info(\"All done! Datasets saved to %s\", output_folder)\n\n    return dataset_info\n</code></pre>"},{"location":"reference/quadra/utils/patch/dataset.html#quadra.utils.patch.dataset.generate_patch_sampling_dataset","title":"<code>generate_patch_sampling_dataset(data_dictionary, output_folder, idx_to_class, overlap, repeat_good_images=1, balance_defects=True, patch_number=None, patch_size=None, subfolder_name='train', train_filename='dataset.txt', annotated_good=None, num_workers=1)</code>","text":"<p>Generate a dataset of patches.</p> <p>Parameters:</p> <ul> <li> data_dictionary             (<code>list[dict[Any, Any]]</code>)         \u2013          <p>Dictionary containing image and mask mapping</p> </li> <li> output_folder             (<code>str</code>)         \u2013          <p>root folder</p> </li> <li> idx_to_class             (<code>dict</code>)         \u2013          <p>Dict mapping an index to the corresponding class name</p> </li> <li> repeat_good_images             (<code>int</code>, default:                 <code>1</code> )         \u2013          <p>Number of repetition for images with emtpy or None mask</p> </li> <li> balance_defects             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>If true add one good entry for each defect extracted</p> </li> <li> patch_number             (<code>tuple[int, int] | None</code>, default:                 <code>None</code> )         \u2013          <p>Optional number of patches for each side, required if patch_size is None</p> </li> <li> patch_size             (<code>tuple[int, int] | None</code>, default:                 <code>None</code> )         \u2013          <p>Optional dimension of the patch, required if patch_number is None</p> </li> <li> overlap             (<code>float</code>)         \u2013          <p>Percentage of overlap between patches</p> </li> <li> subfolder_name             (<code>str</code>, default:                 <code>'train'</code> )         \u2013          <p>name of the subfolder where to store h5 files for defected images and dataset txt</p> </li> <li> train_filename             (<code>str</code>, default:                 <code>'dataset.txt'</code> )         \u2013          <p>Name of the file in which to store the mappings between h5 files and labels</p> </li> <li> annotated_good             (<code>list[int] | None</code>, default:                 <code>None</code> )         \u2013          <p>List of class indices that are considered good other than the background</p> </li> <li> num_workers             (<code>int</code>, default:                 <code>1</code> )         \u2013          <p>Number of processes used to create h5 files.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>None</code>         \u2013          <p>Create a txt file containing tuples path,label where path is a pointer to the generated h5 file and label is the corresponding label</p> <p>Each generated h5 file contains five fields:     img_path: Pointer to the location of the original image     mask_path: Optional pointer to the mask file, is missing if the mask is completely empty or is     not present     patch_size: dimension of the patches on the interested image     triangles: List of triangles that covers the defect     triangles_weights: Which weight should be given to each triangle for sampling</p> </li> </ul> Source code in <code>quadra/utils/patch/dataset.py</code> <pre><code>def generate_patch_sampling_dataset(\n    data_dictionary: list[dict[Any, Any]],\n    output_folder: str,\n    idx_to_class: dict,\n    overlap: float,\n    repeat_good_images: int = 1,\n    balance_defects: bool = True,\n    patch_number: tuple[int, int] | None = None,\n    patch_size: tuple[int, int] | None = None,\n    subfolder_name: str = \"train\",\n    train_filename: str = \"dataset.txt\",\n    annotated_good: list[int] | None = None,\n    num_workers: int = 1,\n) -&gt; None:\n\"\"\"Generate a dataset of patches.\n\n    Args:\n        data_dictionary: Dictionary containing image and mask mapping\n        output_folder: root folder\n        idx_to_class: Dict mapping an index to the corresponding class name\n        repeat_good_images: Number of repetition for images with emtpy or None mask\n        balance_defects: If true add one good entry for each defect extracted\n        patch_number: Optional number of patches for each side, required if patch_size is None\n        patch_size: Optional dimension of the patch, required if patch_number is None\n        overlap: Percentage of overlap between patches\n        subfolder_name: name of the subfolder where to store h5 files for defected images and dataset txt\n        train_filename: Name of the file in which to store the mappings between h5 files and labels\n        annotated_good: List of class indices that are considered good other than the background\n        num_workers: Number of processes used to create h5 files.\n\n    Returns:\n        Create a txt file containing tuples path,label where path is a pointer to the generated h5 file and label is the\n            corresponding label\n\n            Each generated h5 file contains five fields:\n                img_path: Pointer to the location of the original image\n                mask_path: Optional pointer to the mask file, is missing if the mask is completely empty or is\n                not present\n                patch_size: dimension of the patches on the interested image\n                triangles: List of triangles that covers the defect\n                triangles_weights: Which weight should be given to each triangle for sampling\n\n    \"\"\"\n    if patch_number is None and patch_size is None:\n        raise InvalidParameterCombinationException(\"One between patch number or patch size must be specified!\")\n\n    sampling_dataset_folder = os.path.join(output_folder, subfolder_name)\n\n    os.makedirs(sampling_dataset_folder, exist_ok=True)\n    labelled_masks_path = os.path.join(output_folder, \"original\", \"labelled_masks\")\n    os.makedirs(labelled_masks_path, exist_ok=True)\n\n    with open(os.path.join(sampling_dataset_folder, train_filename), \"w\") as output_file:\n        if num_workers &lt; 1:\n            raise InvalidNumWorkersNumberException(\"Workers must be &gt;= 1\")\n\n        if num_workers &gt; 1:\n            log.info(\"Executing generate_patch_sampling_dataset w/ more than 1 worker!\")\n\n            split_data_dictionary = np.array_split(np.asarray(data_dictionary), num_workers)\n\n            with Pool(num_workers) as pool:\n                res_list = pool.map(\n                    partial(\n                        create_h5,\n                        patch_size=patch_size,\n                        patch_number=patch_number,\n                        idx_to_class=idx_to_class,\n                        overlap=overlap,\n                        repeat_good_images=repeat_good_images,\n                        balance_defects=balance_defects,\n                        annotated_good=annotated_good,\n                        output_folder=output_folder,\n                        labelled_masks_path=labelled_masks_path,\n                        sampling_dataset_folder=sampling_dataset_folder,\n                    ),\n                    split_data_dictionary,\n                )\n\n            res = list(itertools.chain(*res_list))\n        else:\n            res = create_h5(\n                data_dictionary=data_dictionary,\n                patch_size=patch_size,\n                patch_number=patch_number,\n                idx_to_class=idx_to_class,\n                overlap=overlap,\n                repeat_good_images=repeat_good_images,\n                balance_defects=balance_defects,\n                annotated_good=annotated_good,\n                output_folder=output_folder,\n                labelled_masks_path=labelled_masks_path,\n                sampling_dataset_folder=sampling_dataset_folder,\n            )\n\n        for line in res:\n            output_file.write(line)\n</code></pre>"},{"location":"reference/quadra/utils/patch/dataset.html#quadra.utils.patch.dataset.generate_patch_sliding_window_dataset","title":"<code>generate_patch_sliding_window_dataset(data_dictionary, subfolder_name, patch_number=None, patch_size=None, overlap=0.0, output_folder='extraction_data', area_threshold=0.45, area_defect_threshold=0.2, mask_extension='_mask', mask_output_folder=None, save_mask=False, class_to_idx=None)</code>","text":"<p>Giving a data_dictionary as:</p> <p>{     'base_name': '163931_1_5.jpg',     'path': 'extraction_data/1/163931_1_5.jpg',     'mask': 'extraction_data/1/163931_1_5_mask.jpg' } This function will extract the patches and save the file and the mask in subdirectory Args:     data_dictionary: Dictionary as above     subfolder_name: Name of the subfolder where to save the extracted patches (output_folder/subfolder_name)     class_to_idx: Dictionary {\"defect\": value in mask.. }     output_folder: root_folder where to extract the data     area_threshold: minimum percentage of defected patch area present in the mask to classify the patch as defect     area_defect_threshold: minimum percentage of single defect present in the patch to classify the patch as defect     mask_extension: extension used to assign image to mask     mask_output_folder: Optional folder in which to save the masks     save_mask: flag to save the mask     patch_number: Optional number of patches for each side, required if patch_size is None     patch_size: Optional dimension of the patch, required if patch_number is None     overlap: overlap of the patches [0, 1].</p> <p>Returns:</p> <ul> <li> <code>None</code>         \u2013          <p>None.</p> </li> </ul> Source code in <code>quadra/utils/patch/dataset.py</code> <pre><code>def generate_patch_sliding_window_dataset(\n    data_dictionary: list[dict],\n    subfolder_name: str,\n    patch_number: tuple[int, int] | None = None,\n    patch_size: tuple[int, int] | None = None,\n    overlap: float = 0.0,\n    output_folder: str = \"extraction_data\",\n    area_threshold: float = 0.45,\n    area_defect_threshold: float = 0.2,\n    mask_extension: str = \"_mask\",\n    mask_output_folder: str | None = None,\n    save_mask: bool = False,\n    class_to_idx: dict | None = None,\n) -&gt; None:\n\"\"\"Giving a data_dictionary as:\n    &gt;&gt;&gt; {\n    &gt;&gt;&gt;     'base_name': '163931_1_5.jpg',\n    &gt;&gt;&gt;     'path': 'extraction_data/1/163931_1_5.jpg',\n    &gt;&gt;&gt;     'mask': 'extraction_data/1/163931_1_5_mask.jpg'\n    &gt;&gt;&gt;}\n    This function will extract the patches and save the file and the mask in subdirectory\n    Args:\n        data_dictionary: Dictionary as above\n        subfolder_name: Name of the subfolder where to save the extracted patches (output_folder/subfolder_name)\n        class_to_idx: Dictionary {\"defect\": value in mask.. }\n        output_folder: root_folder where to extract the data\n        area_threshold: minimum percentage of defected patch area present in the mask to classify the patch as defect\n        area_defect_threshold: minimum percentage of single defect present in the patch to classify the patch as defect\n        mask_extension: extension used to assign image to mask\n        mask_output_folder: Optional folder in which to save the masks\n        save_mask: flag to save the mask\n        patch_number: Optional number of patches for each side, required if patch_size is None\n        patch_size: Optional dimension of the patch, required if patch_number is None\n        overlap: overlap of the patches [0, 1].\n\n    Returns:\n        None.\n\n    \"\"\"\n    if save_mask and len(mask_extension) == 0 and mask_output_folder is None:\n        raise InvalidParameterCombinationException(\n            \"If mask output folder is not set you must specify a mask extension in order to save masks!\"\n        )\n\n    if patch_number is None and patch_size is None:\n        raise InvalidParameterCombinationException(\"One between patch number or patch size must be specified!\")\n\n    for data in tqdm(data_dictionary):\n        base_id = data.get(\"base_name\")\n        base_path = data.get(\"path\")\n        base_mask = data.get(\"mask\")\n\n        assert base_id is not None, \"Cannot find base id in data_dictionary\"\n        assert base_path is not None, \"Cannot find image in data_dictionary\"\n\n        image = cv2.imread(os.path.join(output_folder, base_path))\n        h = image.shape[0]\n        w = image.shape[1]\n\n        log.debug(\"Processing %s with shape %s\", base_id, image.shape)\n        mask = mask_patches = None\n        labelled_mask = labelled_patches = None\n\n        if base_mask is not None:\n            mask = cv2.imread(os.path.join(output_folder, base_mask), 0)\n            labelled_mask = label(mask)\n\n        if patch_size is not None:\n            [patch_height, patch_width] = patch_size\n            [patch_num_h, patch_num_w], step = compute_patch_info_from_patch_dim(\n                h, w, patch_height, patch_width, overlap\n            )\n        elif patch_number is not None:\n            [patch_height, patch_width], step = compute_patch_info(h, w, patch_number[0], patch_number[1], overlap)\n            [patch_num_h, patch_num_w] = patch_number\n        else:\n            # mypy does not recognize that this is unreachable\n            raise InvalidParameterCombinationException(\"One between patch number or patch size must be specified!\")\n\n        log.debug(\n            \"Extracting %s patches with size %s, step %s\", [patch_num_h, patch_num_w], [patch_height, patch_width], step\n        )\n        image_patches = extract_patches(image, (patch_num_h, patch_num_w), (patch_height, patch_width), step, overlap)\n\n        if mask is not None:\n            if labelled_mask is None:\n                raise ValueError(\"Labelled mask cannot be None!\")\n            mask_patches = extract_patches(mask, (patch_num_h, patch_num_w), (patch_height, patch_width), step, overlap)\n            labelled_patches = extract_patches(\n                labelled_mask, (patch_num_h, patch_num_w), (patch_height, patch_width), step, overlap\n            )\n            assert image_patches.shape[:-1] == mask_patches.shape, \"Image patches and mask patches mismatch!\"\n\n        log.debug(\"Image patches shape: %s\", image_patches.shape)\n        __save_patch_dataset(\n            image_patches=image_patches,\n            mask_patches=mask_patches,\n            labelled_patches=labelled_patches,\n            labelled_mask=labelled_mask,\n            image_name=os.path.splitext(base_id)[0],\n            output_folder=os.path.join(output_folder, subfolder_name),\n            area_threshold=area_threshold,\n            area_defect_threshold=area_defect_threshold,\n            mask_extension=mask_extension,\n            save_mask=save_mask,\n            mask_output_folder=mask_output_folder,\n            class_to_idx=class_to_idx,\n        )\n</code></pre>"},{"location":"reference/quadra/utils/patch/dataset.html#quadra.utils.patch.dataset.get_image_mask_association","title":"<code>get_image_mask_association(data_folder, mask_folder=None, mask_extension='', warning_on_missing_mask=True)</code>","text":"<p>Function used to match images and mask from a folder or sub-folders.</p> <p>Parameters:</p> <ul> <li> data_folder             (<code>str</code>)         \u2013          <p>root data folder containing images or images and masks</p> </li> <li> mask_folder             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>Optional root directory used to search only the masks</p> </li> <li> mask_extension             (<code>str</code>, default:                 <code>''</code> )         \u2013          <p>extension used to identify the mask file, it's mandatory if mask_folder is not specified warning_on_missing_mask: if set to True a warning will be raised if a mask is missing, disable if you know that many images do not have a mask.</p> </li> <li> warning_on_missing_mask             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>if set to True a warning will be raised if a mask is missing, disable if you know</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[dict]</code>         \u2013          <p>List of dict like:</p> </li> <li> <code>list[dict]</code>         \u2013          <p>[</p> </li> <li> <code>list[dict]</code>         \u2013          <p>{ 'base_name': '161927.tiff', 'path': 'test_dataset_patch/images/161927.tiff', 'mask': 'test_dataset_patch/masks/161927_mask.tiff'</p> </li> <li> <code>list[dict]</code>         \u2013          <p>}, ...</p> </li> <li> <code>list[dict]</code>         \u2013          <p>]</p> </li> </ul> Source code in <code>quadra/utils/patch/dataset.py</code> <pre><code>def get_image_mask_association(\n    data_folder: str,\n    mask_folder: str | None = None,\n    mask_extension: str = \"\",\n    warning_on_missing_mask: bool = True,\n) -&gt; list[dict]:\n\"\"\"Function used to match images and mask from a folder or sub-folders.\n\n    Args:\n        data_folder: root data folder containing images or images and masks\n        mask_folder: Optional root directory used to search only the masks\n        mask_extension: extension used to identify the mask file, it's mandatory if mask_folder is not specified\n            warning_on_missing_mask: if set to True a warning will be raised if a mask is missing, disable if you know\n            that many images do not have a mask.\n        warning_on_missing_mask: if set to True a warning will be raised if a mask is missing, disable if you know\n\n    Returns:\n        List of dict like:\n        [\n        {\n            'base_name': '161927.tiff',\n            'path': 'test_dataset_patch/images/161927.tiff',\n            'mask': 'test_dataset_patch/masks/161927_mask.tiff'\n        }, ...\n        ]\n    \"\"\"\n    # get all the images from the data folder\n    data_images = glob.glob(os.path.join(data_folder, \"**\", \"*\"), recursive=True)\n\n    basenames = [os.path.splitext(os.path.basename(image))[0] for image in data_images]\n\n    if len(set(basenames)) != len(basenames):\n        raise ValueError(\"Found multiple images with the same name and different extension, this is not supported.\")\n\n    log.info(\"Found: %d images in %s\", len(data_images), data_folder)\n    # divide images and mask if in the same folder\n    # if mask folder is specified search mask in that folder\n    if mask_folder:\n        masks_images = []\n        for basename in basenames:\n            mask_path = os.path.join(mask_folder, f\"{basename}{mask_extension}.*\")\n            mask_path_list = glob.glob(mask_path)\n\n            if len(mask_path_list) == 1:\n                masks_images.append(mask_path_list[0])\n            elif warning_on_missing_mask:\n                log.warning(\"Mask for %s not found\", basename)\n    else:\n        if mask_extension == \"\":\n            raise ValueError(\"If no mask folder is provided, mask extension is mandatory it cannot be empty.\")\n\n        masks_images = [image for image in data_images if mask_extension in image]\n        data_images = [image for image in data_images if mask_extension not in image]\n\n    # build support dictionary\n    unique_images = [{\"base_name\": os.path.basename(image), \"path\": image, \"mask\": None} for image in data_images]\n\n    images_stem = [os.path.splitext(str(image[\"base_name\"]))[0] + mask_extension for image in unique_images]\n    masks_stem = [os.path.splitext(os.path.basename(mask))[0] for mask in masks_images]\n\n    # search corrispondency between file or folders\n    for i, image_stem in enumerate(images_stem):\n        if image_stem in masks_stem:\n            unique_images[i][\"mask\"] = masks_images[masks_stem.index(image_stem)]\n\n    log.info(\"Unique images with mask: %d\", len([uni for uni in unique_images if uni.get(\"mask\") is not None]))\n    log.info(\"Unique images with no mask: %d\", len([uni for uni in unique_images if uni.get(\"mask\") is None]))\n\n    return unique_images\n</code></pre>"},{"location":"reference/quadra/utils/patch/dataset.html#quadra.utils.patch.dataset.load_train_file","title":"<code>load_train_file(train_file_path, include_filter=None, exclude_filter=None, class_to_skip=None)</code>","text":"<p>Load a train file and return a list of samples and a list of targets. It is expected that train files will be in     the same location as the train_file_path.</p> <p>Parameters:</p> <ul> <li> train_file_path             (<code>str</code>)         \u2013          <p>Training file location</p> </li> <li> include_filter             (<code>list[str] | None</code>, default:                 <code>None</code> )         \u2013          <p>Include only samples that contain one of the element of this list</p> </li> <li> exclude_filter             (<code>list[str] | None</code>, default:                 <code>None</code> )         \u2013          <p>Exclude all samples that contain one of the element of this list</p> </li> <li> class_to_skip             (<code>list | None</code>, default:                 <code>None</code> )         \u2013          <p>if not None, exlude all the samples with labels present in this list.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[list[str], list[str]]</code>         \u2013          <p>List of samples and list of targets</p> </li> </ul> Source code in <code>quadra/utils/patch/dataset.py</code> <pre><code>def load_train_file(\n    train_file_path: str,\n    include_filter: list[str] | None = None,\n    exclude_filter: list[str] | None = None,\n    class_to_skip: list | None = None,\n) -&gt; tuple[list[str], list[str]]:\n\"\"\"Load a train file and return a list of samples and a list of targets. It is expected that train files will be in\n        the same location as the train_file_path.\n\n    Args:\n        train_file_path: Training file location\n        include_filter: Include only samples that contain one of the element of this list\n        exclude_filter: Exclude all samples that contain one of the element of this list\n        class_to_skip: if not None, exlude all the samples with labels present in this list.\n\n    Returns:\n        List of samples and list of targets\n\n    \"\"\"\n    samples = []\n    targets = []\n\n    with open(train_file_path) as f:\n        lines = f.read().splitlines()\n        for line in lines:\n            sample, target = line.split(\",\")\n            if class_to_skip is not None and target in class_to_skip:\n                continue\n            samples.append(sample)\n            targets.append(target)\n\n    include_filter = [] if include_filter is None else include_filter\n    exclude_filter = [] if exclude_filter is None else exclude_filter\n\n    valid_samples_indices = [\n        i\n        for (i, x) in enumerate(samples)\n        if (len(include_filter) == 0 or any(f in x for f in include_filter))\n        and (len(exclude_filter) == 0 or not any(f in x for f in exclude_filter))\n    ]\n\n    samples = [samples[i] for i in valid_samples_indices]\n    targets = [targets[i] for i in valid_samples_indices]\n\n    train_folder = os.path.dirname(train_file_path)\n    samples = [os.path.join(train_folder, x) for x in samples]\n\n    return samples, targets\n</code></pre>"},{"location":"reference/quadra/utils/patch/dataset.html#quadra.utils.patch.dataset.multilabel_stratification","title":"<code>multilabel_stratification(output_folder, data_dictionary, num_classes, val_size, test_size)</code>","text":"<p>Split data dictionary using multilabel based stratification, place every sample with None     mask inside the test set,for all the others read the labels contained in the masks     to create one-hot encoded labels.</p> <p>Parameters:</p> <ul> <li> output_folder             (<code>str</code>)         \u2013          <p>root folder of the dataset</p> </li> <li> data_dictionary             (<code>list[dict]</code>)         \u2013          <p>Data dictionary as described in generate patch dataset</p> </li> <li> num_classes             (<code>int</code>)         \u2013          <p>Number of classes contained in the dataset, required for one hot encoding</p> </li> <li> val_size             (<code>float</code>)         \u2013          <p>Percentage of data to be used for validation</p> </li> <li> test_size             (<code>float</code>)         \u2013          <p>Percentage of data to be used for test</p> </li> </ul> Source code in <code>quadra/utils/patch/dataset.py</code> <pre><code>def multilabel_stratification(\n    output_folder: str,\n    data_dictionary: list[dict],\n    num_classes: int,\n    val_size: float,\n    test_size: float,\n) -&gt; tuple[list[dict], list[dict], list[dict]]:\n\"\"\"Split data dictionary using multilabel based stratification, place every sample with None\n        mask inside the test set,for all the others read the labels contained in the masks\n        to create one-hot encoded labels.\n\n    Args:\n        output_folder: root folder of the dataset\n        data_dictionary: Data dictionary as described in generate patch dataset\n        num_classes: Number of classes contained in the dataset, required for one hot encoding\n        val_size: Percentage of data to be used for validation\n        test_size: Percentage of data to be used for test\n    Returns:\n        Three data dictionaries, one for training, one for validation and one for test\n\n    \"\"\"\n    if val_size + test_size == 0:\n        return data_dictionary, [], []\n    if val_size == 1:\n        return [], data_dictionary, []\n    if test_size == 1:\n        return [], [], data_dictionary\n\n    test_data_dictionary = list(filter(lambda q: q[\"mask\"] is None, data_dictionary))\n    log.info(\"Number of images with no mask inserted in test_data_dictionary: %d\", len(test_data_dictionary))\n    empty_test_size = len(test_data_dictionary) / len(data_dictionary)\n    data_dictionary = list(filter(lambda q: q[\"mask\"] is not None, data_dictionary))\n\n    if len(data_dictionary) == 0:\n        # All the item in the data dictionary have None mask, put everything in test\n        warnings.warn(\n            \"All the images have None mask and the test size is not equal to 1! Put everything in test\",\n            UserWarning,\n            stacklevel=2,\n        )\n        return [], [], test_data_dictionary\n\n    x = []\n    y = None\n    for item in data_dictionary:\n        one_hot = np.zeros([1, num_classes], dtype=np.int16)\n        if item[\"mask\"] is None:\n            continue\n        # this works even if item[\"mask\"] is already an absolute path\n        mask = cv2.imread(os.path.join(output_folder, item[\"mask\"]), 0)\n\n        labels = np.unique(mask)\n\n        one_hot[:, labels] = 1\n        x.append(item[\"base_name\"])\n        if y is None:\n            y = one_hot\n        else:\n            y = np.concatenate([y, one_hot])\n\n    x_test: list[Any] | np.ndarray\n\n    if empty_test_size &gt; test_size:\n        warnings.warn(\n            (\n                \"The percentage of images with None label is greater than the test_size, the newest test_size is\"\n                f\" {empty_test_size}!\"\n            ),\n            UserWarning,\n            stacklevel=2,\n        )\n        x_train, _, x_val, _ = iterative_train_test_split(np.expand_dims(np.array(x), 1), y, val_size)\n        x_test = [q[\"base_name\"] for q in test_data_dictionary]\n    else:\n        test_size -= empty_test_size\n        x_train, _, x_remaining, y_remaining = iterative_train_test_split(\n            np.expand_dims(np.array(x), 1), y, val_size + test_size\n        )\n\n        if x_remaining.shape[0] == 1:\n            if test_size == 0:\n                x_val = x_remaining\n                x_test = np.array([])\n            elif val_size == 0:\n                x_test = x_remaining\n                x_val = np.array([])\n            else:\n                log.warning(\"Not enough data to create the test split, only a validation set of size 1 will be created\")\n                x_val = x_remaining\n                x_test = np.array([])\n        else:\n            x_val, _, x_test, _ = iterative_train_test_split(\n                x_remaining, y_remaining, test_size / (val_size + test_size)\n            )\n        # Here x_test should be always a numpy array, but mypy does not recognize it\n        x_test = [q[0] for q in x_test.tolist()]  # type: ignore[union-attr]\n        x_test.extend([q[\"base_name\"] for q in test_data_dictionary])\n\n    train_data_dictionary = list(filter(lambda q: q[\"base_name\"] in x_train, data_dictionary))\n    val_data_dictionary = list(filter(lambda q: q[\"base_name\"] in x_val, data_dictionary))\n    test_data_dictionary = list(filter(lambda q: q[\"base_name\"] in x_test, data_dictionary + test_data_dictionary))\n\n    return train_data_dictionary, val_data_dictionary, test_data_dictionary\n</code></pre>"},{"location":"reference/quadra/utils/patch/dataset.html#quadra.utils.patch.dataset.triangle_area","title":"<code>triangle_area(triangle)</code>","text":"<p>Compute the area of a triangle defined by 3 points.</p> <p>Parameters:</p> <ul> <li> triangle             (<code>ndarray</code>)         \u2013          <p>Array of shape 3x2 containing the coordinates of a triangle.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float</code>         \u2013          <p>The area of the triangle</p> </li> </ul> Source code in <code>quadra/utils/patch/dataset.py</code> <pre><code>def triangle_area(triangle: np.ndarray) -&gt; float:\n\"\"\"Compute the area of a triangle defined by 3 points.\n\n    Args:\n        triangle: Array of shape 3x2 containing the coordinates of a triangle.\n\n    Returns:\n        The area of the triangle\n\n    \"\"\"\n    [y1, x1], [y2, x2], [y3, x3] = triangle\n    return abs(0.5 * (((x2 - x1) * (y3 - y1)) - ((x3 - x1) * (y2 - y1))))\n</code></pre>"},{"location":"reference/quadra/utils/patch/dataset.html#quadra.utils.patch.dataset.triangulate_region","title":"<code>triangulate_region(mask)</code>","text":"<p>Extract from a binary image containing a single roi (with or without holes) a list of triangles (and their normalized area) that completely subdivide an approximated polygon defined around mask contours, the output can be used to easily sample uniformly points that are almost guarantee to lie inside the roi.</p> <p>Parameters:</p> <ul> <li> mask             (<code>ndimage</code>)         \u2013          <p>Binary image defining a region of interest</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[ndarray | None, ndarray | None]</code>         \u2013          <p>Tuple containing: triangles: a numpy array containing a list of list of vertices (y, x) of the triangles defined over a     polygon that contains the entire region weights: areas of each triangle rescaled (area_i / sum(areas))</p> </li> </ul> Source code in <code>quadra/utils/patch/dataset.py</code> <pre><code>def triangulate_region(mask: ndimage) -&gt; tuple[np.ndarray | None, np.ndarray | None]:\n\"\"\"Extract from a binary image containing a single roi (with or without holes) a list of triangles\n    (and their normalized area) that completely subdivide an approximated polygon defined around mask contours,\n    the output can be used to easily sample uniformly points that are almost guarantee to lie inside the roi.\n\n    Args:\n        mask: Binary image defining a region of interest\n\n    Returns:\n        Tuple containing:\n            triangles: a numpy array containing a list of list of vertices (y, x) of the triangles defined over a\n                polygon that contains the entire region\n            weights: areas of each triangle rescaled (area_i / sum(areas))\n\n    \"\"\"\n    polygon_points, hier = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_TC89_L1)\n\n    if not np.all(hier[:, :, 3] == -1):  # there are holes\n        holes = ndimage.binary_fill_holes(mask).astype(np.uint8)\n        holes -= mask\n        holes = (holes &gt; 0).astype(np.uint8)\n        if holes.sum() &gt; 0:  # there are holes\n            for hole in regionprops(label(holes)):\n                y_hole_center = int(hole.centroid[0])\n                mask[y_hole_center] = 0\n\n        polygon_points, _ = cv2.findContours(mask, cv2.RETR_LIST, cv2.CHAIN_APPROX_TC89_L1)\n\n    final_approx = []\n\n    # Extract a simpler approximation of the contour\n    for cnt in polygon_points:\n        epsilon = 0.01 * cv2.arcLength(cnt, True)\n        approx = cv2.approxPolyDP(cnt, epsilon, True)\n        final_approx.append(approx)\n\n    triangles = None\n\n    for approx in final_approx:\n        contours_tripy = [x[0] for x in approx]\n        current_triangles = earclip(contours_tripy)\n\n        if len(current_triangles) == 0:\n            # This can only happen is a defect is like one pixel wide...\n            continue\n\n        current_triangles = np.array([list(x) for x in current_triangles])\n\n        triangles = current_triangles if triangles is None else np.concatenate([triangles, current_triangles])\n\n    if triangles is None:\n        return None, None\n\n    # Swap x and y to match cv2\n    triangles = triangles[..., ::-1]\n\n    weights = np.array([triangle_area(x) for x in triangles])\n    weights = weights / weights.sum()\n\n    return triangles, weights\n</code></pre>"},{"location":"reference/quadra/utils/patch/dataset.html#quadra.utils.patch.dataset.trisample","title":"<code>trisample(triangle)</code>","text":"<p>Sample a point uniformly in a triangle.</p> <p>Parameters:</p> <ul> <li> triangle             (<code>ndarray</code>)         \u2013          <p>Array of shape 3x2 containing the coordinates of a triangle.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[int, int]</code>         \u2013          <p>Sample point uniformly in the triangle</p> </li> </ul> Source code in <code>quadra/utils/patch/dataset.py</code> <pre><code>def trisample(triangle: np.ndarray) -&gt; tuple[int, int]:\n\"\"\"Sample a point uniformly in a triangle.\n\n    Args:\n        triangle: Array of shape 3x2 containing the coordinates of a triangle.\n\n    Returns:\n        Sample point uniformly in the triangle\n\n    \"\"\"\n    [y1, x1], [y2, x2], [y3, x3] = triangle\n\n    r1 = random.random()\n    r2 = random.random()\n\n    s1 = math.sqrt(r1)\n\n    x = x1 * (1.0 - s1) + x2 * (1.0 - r2) * s1 + x3 * r2 * s1\n    y = y1 * (1.0 - s1) + y2 * (1.0 - r2) * s1 + y3 * r2 * s1\n\n    return int(y), int(x)\n</code></pre>"},{"location":"reference/quadra/utils/patch/metrics.html","title":"metrics","text":""},{"location":"reference/quadra/utils/patch/metrics.html#quadra.utils.patch.metrics.compute_patch_metrics","title":"<code>compute_patch_metrics(test_img_info, test_results, overlap, idx_to_class, patch_num_h=None, patch_num_w=None, patch_w=None, patch_h=None, return_polygon=False, patch_reconstruction_method='priority', annotated_good=None)</code>","text":"<p>Compute the metrics of a patch dataset.</p> <p>Parameters:</p> <ul> <li> test_img_info             (<code>list[PatchDatasetFileFormat]</code>)         \u2013          <p>List of observation paths and mask paths</p> </li> <li> test_results             (<code>DataFrame</code>)         \u2013          <p>Pandas dataframe containing the results of an SklearnClassificationTrainer utility</p> </li> <li> patch_num_h             (<code>int | None</code>, default:                 <code>None</code> )         \u2013          <p>Number of vertical patches (required if patch_w and patch_h are None)</p> </li> <li> patch_num_w             (<code>int | None</code>, default:                 <code>None</code> )         \u2013          <p>Number of horizontal patches (required if patch_w and patch_h are None)</p> </li> <li> patch_h             (<code>int | None</code>, default:                 <code>None</code> )         \u2013          <p>Patch height (required if patch_num_h and patch_num_w are None)</p> </li> <li> patch_w             (<code>int | None</code>, default:                 <code>None</code> )         \u2013          <p>Patch width (required if patch_num_h and patch_num_w are None)</p> </li> <li> overlap             (<code>float</code>)         \u2013          <p>Percentage of overlap between the patches</p> </li> <li> idx_to_class             (<code>dict</code>)         \u2013          <p>Dict mapping an index to the corresponding class name</p> </li> <li> return_polygon             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>if set to true convert the reconstructed mask into polygons, otherwise return the mask</p> </li> <li> patch_reconstruction_method             (<code>str</code>, default:                 <code>'priority'</code> )         \u2013          <p>How to compute the label of overlapping patches, can either be: priority: Assign the top priority label (i.e the one with greater index) to overlapping regions major_voting: Assign the most present label among the patches label overlapping a pixel</p> </li> <li> annotated_good             (<code>list[int] | None</code>, default:                 <code>None</code> )         \u2013          <p>List of indices of annotations to be treated as good.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[int, int, int, list[dict]]</code>         \u2013          <p>Tuple containing: false_region_bad: Number of false bad regions detected in the dataset false_region_good: Number of missed defects true_region_bad: Number of correctly identified defects reconstructions: If polygon is true this is a List of dict containing     {         \"file_path\": image_path,         \"mask_path\": mask_path,         \"file_name\": observation_name,         \"prediction\": [{             \"label\": predicted_label,             \"points\": List of dict coordinates \"x\" and \"y\" representing the points of a polygon that             surrounds an image area covered by patches of label = predicted_label         }]     } else its a list of dict containing     {         \"file_path\": image_path,         \"mask_path\": mask_path,         \"file_name\": observation_name,         \"prediction\": numpy array containing the reconstructed mask     }</p> </li> </ul> Source code in <code>quadra/utils/patch/metrics.py</code> <pre><code>def compute_patch_metrics(\n    test_img_info: list[PatchDatasetFileFormat],\n    test_results: pd.DataFrame,\n    overlap: float,\n    idx_to_class: dict,\n    patch_num_h: int | None = None,\n    patch_num_w: int | None = None,\n    patch_w: int | None = None,\n    patch_h: int | None = None,\n    return_polygon: bool = False,\n    patch_reconstruction_method: str = \"priority\",\n    annotated_good: list[int] | None = None,\n) -&gt; tuple[int, int, int, list[dict]]:\n\"\"\"Compute the metrics of a patch dataset.\n\n    Args:\n        test_img_info: List of observation paths and mask paths\n        test_results: Pandas dataframe containing the results of an SklearnClassificationTrainer utility\n        patch_num_h: Number of vertical patches (required if patch_w and patch_h are None)\n        patch_num_w: Number of horizontal patches (required if patch_w and patch_h are None)\n        patch_h: Patch height (required if patch_num_h and patch_num_w are None)\n        patch_w: Patch width (required if patch_num_h and patch_num_w are None)\n        overlap: Percentage of overlap between the patches\n        idx_to_class: Dict mapping an index to the corresponding class name\n        return_polygon: if set to true convert the reconstructed mask into polygons, otherwise return the mask\n        patch_reconstruction_method: How to compute the label of overlapping patches, can either be:\n            priority: Assign the top priority label (i.e the one with greater index) to overlapping regions\n            major_voting: Assign the most present label among the patches label overlapping a pixel\n        annotated_good: List of indices of annotations to be treated as good.\n\n    Returns:\n        Tuple containing:\n            false_region_bad: Number of false bad regions detected in the dataset\n            false_region_good: Number of missed defects\n            true_region_bad: Number of correctly identified defects\n            reconstructions: If polygon is true this is a List of dict containing\n                {\n                    \"file_path\": image_path,\n                    \"mask_path\": mask_path,\n                    \"file_name\": observation_name,\n                    \"prediction\": [{\n                        \"label\": predicted_label,\n                        \"points\": List of dict coordinates \"x\" and \"y\" representing the points of a polygon that\n                        surrounds an image area covered by patches of label = predicted_label\n                    }]\n                }\n            else its a list of dict containing\n                {\n                    \"file_path\": image_path,\n                    \"mask_path\": mask_path,\n                    \"file_name\": observation_name,\n                    \"prediction\": numpy array containing the reconstructed mask\n                }\n    \"\"\"\n    assert patch_reconstruction_method in [\n        \"priority\",\n        \"major_voting\",\n    ], \"Patch reconstruction method not recognized, valid values are priority, major_voting\"\n\n    if (patch_h is not None and patch_w is not None) and (patch_num_h is not None and patch_num_w is not None):\n        raise ValueError(\"Either number of patches or patch size is required for reconstruction\")\n\n    assert (patch_h is not None and patch_w is not None) or (\n        patch_num_h is not None and patch_num_w is not None\n    ), \"Either number of patches or patch size is required for reconstruction\"\n\n    if patch_h is not None and patch_w is not None and patch_num_h is not None and patch_num_w is not None:\n        warnings.warn(\n            \"Both number of patches and patch dimension are specified, using number of patches by default\",\n            UserWarning,\n            stacklevel=2,\n        )\n\n    log.info(\"Computing patch metrics!\")\n\n    false_region_bad = 0\n    false_region_good = 0\n    true_region_bad = 0\n    reconstructions = []\n    test_results[\"filename\"] = test_results[\"sample\"].apply(\n        lambda x: \"_\".join(os.path.basename(x).replace(\"#DISCARD#\", \"\").split(\"_\")[0:-1])\n    )\n\n    for info in tqdm(test_img_info):\n        img_path = info.image_path\n        mask_path = info.mask_path\n\n        img_json_entry = {\n            \"image_path\": img_path,\n            \"mask_path\": mask_path,\n            \"file_name\": os.path.basename(img_path),\n            \"prediction\": None,\n        }\n\n        test_img = cv2.imread(img_path)\n\n        img_name = os.path.basename(img_path)\n\n        h = test_img.shape[0]\n        w = test_img.shape[1]\n\n        gt_img = None\n\n        if mask_path is not None and os.path.exists(mask_path):\n            gt_img = cv2.imread(mask_path, 0)\n            if test_img.shape[0:2] != gt_img.shape:\n                # Ensure that the mask has the same size as the image by padding it with zeros\n                log.warning(\"Found mask with different size than the image, padding it with zeros!\")\n                gt_img = np.pad(\n                    gt_img, ((0, test_img.shape[0] - gt_img.shape[0]), (0, test_img.shape[1] - gt_img.shape[1]))\n                )\n        if patch_num_h is not None and patch_num_w is not None:\n            patch_size, step = compute_patch_info(h, w, patch_num_h, patch_num_w, overlap)\n        elif patch_h is not None and patch_w is not None:\n            [patch_num_h, patch_num_w], step = compute_patch_info_from_patch_dim(h, w, patch_h, patch_w, overlap)\n            patch_size = (patch_h, patch_w)\n        else:\n            raise ValueError(\n                \"Either number of patches or patch size is required for reconstruction, this should not happen\"\n                \" at this stage\"\n            )\n\n        img_patches = get_sorted_patches_by_image(test_results, img_name)\n        pred = img_patches[\"pred_label\"].to_numpy().reshape(patch_num_h, patch_num_w)\n\n        # Treat annotated good predictions as background, this is an optimistic assumption that assumes that the\n        # remaining background is good, but it is not always true so maybe on non annotated areas we are missing\n        # defects and it would be necessary to handle this in a different way.\n        if annotated_good is not None:\n            pred[np.isin(pred, annotated_good)] = 0\n        if patch_num_h is not None and patch_num_w is not None:\n            output_mask, predicted_defect = reconstruct_patch(\n                input_img_shape=test_img.shape,\n                patch_size=patch_size,\n                pred=pred,\n                patch_num_h=patch_num_h,\n                patch_num_w=patch_num_w,\n                idx_to_class=idx_to_class,\n                step=step,\n                return_polygon=return_polygon,\n                method=patch_reconstruction_method,\n            )\n        else:\n            raise ValueError(\"`patch_num_h` and `patch_num_w` cannot be None at this point\")\n\n        if return_polygon:\n            img_json_entry[\"prediction\"] = predicted_defect\n        else:\n            img_json_entry[\"prediction\"] = output_mask\n\n        reconstructions.append(img_json_entry)\n        if gt_img is not None:\n            if annotated_good is not None:\n                gt_img[np.isin(gt_img, annotated_good)] = 0\n\n            gt_img_binary = (gt_img &gt; 0).astype(bool)  # type: ignore[operator]\n            regions_pred = label(output_mask).astype(np.uint8)\n\n            for k in range(1, regions_pred.max() + 1):\n                region = (regions_pred == k).astype(bool)\n                # If there's no overlap with the gt\n                if np.sum(np.bitwise_and(region, gt_img_binary)) == 0:\n                    false_region_bad += 1\n\n            output_mask = (output_mask &gt; 0).astype(np.uint8)\n            gt_img = label(gt_img)\n\n            for i in range(1, gt_img.max() + 1):  # type: ignore[union-attr]\n                region = (gt_img == i).astype(bool)  # type: ignore[union-attr]\n                if np.sum(np.bitwise_and(region, output_mask)) == 0:\n                    false_region_good += 1\n                else:\n                    true_region_bad += 1\n\n    return false_region_bad, false_region_good, true_region_bad, reconstructions\n</code></pre>"},{"location":"reference/quadra/utils/patch/metrics.html#quadra.utils.patch.metrics.from_mask_to_polygon","title":"<code>from_mask_to_polygon(mask_img)</code>","text":"<p>Convert a mask of pattern to a list of polygon vertices.</p> <p>Parameters:</p> <ul> <li> mask_img             (<code>ndarray</code>)         \u2013          <p>masked patch reconstruction image</p> </li> </ul> Source code in <code>quadra/utils/patch/metrics.py</code> <pre><code>def from_mask_to_polygon(mask_img: np.ndarray) -&gt; list:\n\"\"\"Convert a mask of pattern to a list of polygon vertices.\n\n    Args:\n        mask_img: masked patch reconstruction image\n    Returns:\n        a list of lists containing the coordinates of the polygons containing each region of the mask:\n        [\n            [\n                {\n                    \"x\": 1.1,\n                    \"y\": 2.2\n                },\n                {\n                    \"x\": 2.1,\n                    \"y\": 3.2\n                }\n            ], ...\n        ].\n    \"\"\"\n    points_dict = []\n    # find vertices of polygon: points -&gt; list of array of dim n_vertex, 1, 2(x,y)\n    polygon_points, hier = cv2.findContours(mask_img, cv2.RETR_TREE, cv2.CHAIN_APPROX_TC89_L1)\n\n    if not hier[:, :, 2:].all(-1).all():  # there are holes\n        holes = ndimage.binary_fill_holes(mask_img).astype(int)\n        holes -= mask_img\n        holes = (holes &gt; 0).astype(np.uint8)\n        if holes.sum() &gt; 0:  # there are holes\n            for hole in regionprops(label(holes)):\n                a, _, _, _d = hole.bbox\n                mask_img[a] = 0\n\n        polygon_points, hier = cv2.findContours(mask_img, cv2.RETR_LIST, cv2.CHAIN_APPROX_TC89_L1)\n\n    for pol in polygon_points:\n        #  pol: n_vertex, 1, 2\n        current_poly = []\n        for point in pol:\n            current_poly.append({\"x\": int(point[0, 0]), \"y\": int(point[0, 1])})\n        points_dict.append(current_poly)\n\n    return points_dict\n</code></pre>"},{"location":"reference/quadra/utils/patch/metrics.html#quadra.utils.patch.metrics.get_sorted_patches_by_image","title":"<code>get_sorted_patches_by_image(test_results, img_name)</code>","text":"<p>Gets the patches of a given image sorted by patch number.</p> <p>Parameters:</p> <ul> <li> test_results             (<code>DataFrame</code>)         \u2013          <p>Pandas dataframe containing test results like the one produced by SklearnClassificationTrainer</p> </li> <li> img_name             (<code>str</code>)         \u2013          <p>name of the image used to filter the results.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>         \u2013          <p>test results filtered by image name and sorted by patch number</p> </li> </ul> Source code in <code>quadra/utils/patch/metrics.py</code> <pre><code>def get_sorted_patches_by_image(test_results: pd.DataFrame, img_name: str) -&gt; pd.DataFrame:\n\"\"\"Gets the patches of a given image sorted by patch number.\n\n    Args:\n        test_results: Pandas dataframe containing test results like the one produced by SklearnClassificationTrainer\n        img_name: name of the image used to filter the results.\n\n    Returns:\n        test results filtered by image name and sorted by patch number\n    \"\"\"\n    img_patches = test_results[test_results[\"filename\"] == os.path.splitext(img_name)[0]]\n    patches_idx = np.array(\n        [int(os.path.basename(x).split(\"_\")[-1].replace(\".png\", \"\")) for x in img_patches[\"sample\"].tolist()]\n    )\n    patches_idx = np.argsort(patches_idx).tolist()\n    img_patches = img_patches.iloc[patches_idx]\n\n    return img_patches\n</code></pre>"},{"location":"reference/quadra/utils/patch/metrics.html#quadra.utils.patch.metrics.reconstruct_patch","title":"<code>reconstruct_patch(input_img_shape, patch_size, pred, patch_num_h, patch_num_w, idx_to_class, step, return_polygon=True, method='priority')</code>","text":"<p>Reconstructs the prediction image from the patches.</p> <p>Parameters:</p> <ul> <li> input_img_shape             (<code>tuple[int, ...]</code>)         \u2013          <p>The size of the reconstructed image</p> </li> <li> patch_size             (<code>tuple[int, int]</code>)         \u2013          <p>Array defining the patch size</p> </li> <li> pred             (<code>ndarray</code>)         \u2013          <p>Numpy array containing reconstructed prediction (patch_num_h x patch_num_w)</p> </li> <li> patch_num_h             (<code>int</code>)         \u2013          <p>Number of vertical patches</p> </li> <li> patch_num_w             (<code>int</code>)         \u2013          <p>Number of horizontal patches</p> </li> <li> idx_to_class             (<code>dict</code>)         \u2013          <p>Dictionary mapping indices to labels</p> </li> <li> step             (<code>tuple[int, int]</code>)         \u2013          <p>Array defining the step size to be used for reconstruction</p> </li> <li> return_polygon             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>If true compute predicted polygons. Defaults to True.</p> </li> <li> method             (<code>str</code>, default:                 <code>'priority'</code> )         \u2013          <p>Reconstruction method to be used. Currently supported: \"priority\" and \"major_voting\"</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[ndarray, list[dict]]</code>         \u2013          <p>(reconstructed_prediction_image, predictions) where predictions is an array of objects [{     \"label\": Predicted_label,     \"points\": List of dict coordinates \"x\" and \"y\" representing the points of a polygon that         surrounds an image area covered by patches of label = predicted_label }]</p> </li> </ul> Source code in <code>quadra/utils/patch/metrics.py</code> <pre><code>def reconstruct_patch(\n    input_img_shape: tuple[int, ...],\n    patch_size: tuple[int, int],\n    pred: np.ndarray,\n    patch_num_h: int,\n    patch_num_w: int,\n    idx_to_class: dict,\n    step: tuple[int, int],\n    return_polygon: bool = True,\n    method: str = \"priority\",\n) -&gt; tuple[np.ndarray, list[dict]]:\n\"\"\"Reconstructs the prediction image from the patches.\n\n    Args:\n        input_img_shape: The size of the reconstructed image\n        patch_size: Array defining the patch size\n        pred: Numpy array containing reconstructed prediction (patch_num_h x patch_num_w)\n        patch_num_h: Number of vertical patches\n        patch_num_w: Number of horizontal patches\n        idx_to_class: Dictionary mapping indices to labels\n        step: Array defining the step size to be used for reconstruction\n        return_polygon: If true compute predicted polygons. Defaults to True.\n        method: Reconstruction method to be used. Currently supported: \"priority\" and \"major_voting\"\n\n    Returns:\n        (reconstructed_prediction_image, predictions) where predictions is an array of objects\n            [{\n                \"label\": Predicted_label,\n                \"points\": List of dict coordinates \"x\" and \"y\" representing the points of a polygon that\n                    surrounds an image area covered by patches of label = predicted_label\n            }]\n    \"\"\"\n    if method == \"priority\":\n        return _reconstruct_patch_priority(\n            input_img_shape,\n            patch_size,\n            pred,\n            patch_num_h,\n            patch_num_w,\n            idx_to_class,\n            step,\n            return_polygon,\n        )\n    if method == \"major_voting\":\n        return _reconstruct_patch_major_voting(\n            input_img_shape,\n            patch_size,\n            pred,\n            patch_num_h,\n            patch_num_w,\n            idx_to_class,\n            step,\n            return_polygon,\n        )\n\n    raise ValueError(f\"Invalid reconstruction method {method}\")\n</code></pre>"},{"location":"reference/quadra/utils/patch/model.html","title":"model","text":""},{"location":"reference/quadra/utils/patch/model.html#quadra.utils.patch.model.RleEncoder","title":"<code>RleEncoder</code>","text":"<p>             Bases: <code>JSONEncoder</code></p> <p>Custom encoder to convert numpy arrays to RLE.</p>"},{"location":"reference/quadra/utils/patch/model.html#quadra.utils.patch.model.RleEncoder.default","title":"<code>default(o)</code>","text":"<p>Customize standard encoder behaviour to convert numpy arrays to RLE.</p> Source code in <code>quadra/utils/patch/model.py</code> <pre><code>def default(self, o: Any):\n\"\"\"Customize standard encoder behaviour to convert numpy arrays to RLE.\"\"\"\n    if isinstance(o, np.ndarray):\n        return mask2rle(o)\n    return json.JSONEncoder.default(self, o)\n</code></pre>"},{"location":"reference/quadra/utils/patch/model.html#quadra.utils.patch.model.save_classification_result","title":"<code>save_classification_result(results, output_folder, confusion_matrix, accuracy, test_dataloader, reconstructions, config, output, ignore_classes=None)</code>","text":"<p>Save classification results.</p> <p>Parameters:</p> <ul> <li> results             (<code>DataFrame</code>)         \u2013          <p>Dataframe containing the classification results</p> </li> <li> output_folder             (<code>str</code>)         \u2013          <p>Folder where to save the results</p> </li> <li> confusion_matrix             (<code>DataFrame | None</code>)         \u2013          <p>Confusion matrix</p> </li> <li> accuracy             (<code>float</code>)         \u2013          <p>Accuracy of the model</p> </li> <li> test_dataloader             (<code>DataLoader</code>)         \u2013          <p>Dataloader used for testing</p> </li> <li> reconstructions             (<code>list[dict]</code>)         \u2013          <p>List of dictionaries containing polygons or masks</p> </li> <li> config             (<code>DictConfig</code>)         \u2013          <p>Experiment configuration</p> </li> <li> output             (<code>DictConfig</code>)         \u2013          <p>Output configuration</p> </li> <li> ignore_classes             (<code>list[int] | None</code>, default:                 <code>None</code> )         \u2013          <p>Eventual classes to ignore during reconstruction plot. Defaults to None.</p> </li> </ul> Source code in <code>quadra/utils/patch/model.py</code> <pre><code>def save_classification_result(\n    results: pd.DataFrame,\n    output_folder: str,\n    confusion_matrix: pd.DataFrame | None,\n    accuracy: float,\n    test_dataloader: DataLoader,\n    reconstructions: list[dict],\n    config: DictConfig,\n    output: DictConfig,\n    ignore_classes: list[int] | None = None,\n):\n\"\"\"Save classification results.\n\n    Args:\n        results: Dataframe containing the classification results\n        output_folder: Folder where to save the results\n        confusion_matrix: Confusion matrix\n        accuracy: Accuracy of the model\n        test_dataloader: Dataloader used for testing\n        reconstructions: List of dictionaries containing polygons or masks\n        config: Experiment configuration\n        output: Output configuration\n        ignore_classes: Eventual classes to ignore during reconstruction plot. Defaults to None.\n    \"\"\"\n    # Save csv\n    results.to_csv(os.path.join(output_folder, \"test_results.csv\"), index_label=\"index\")\n\n    if confusion_matrix is not None:\n        # Save confusion matrix\n        disp = ConfusionMatrixDisplay(\n            confusion_matrix=np.array(confusion_matrix),\n            display_labels=[x.replace(\"pred:\", \"\") for x in confusion_matrix.columns.to_list()],\n        )\n        disp.plot(include_values=True, cmap=plt.cm.Greens, ax=None, colorbar=False, xticks_rotation=90)\n        plt.title(f\"Confusion Matrix (Accuracy: {(accuracy * 100):.2f}%)\")\n        plt.savefig(\n            os.path.join(output_folder, \"test_confusion_matrix.png\"),\n            bbox_inches=\"tight\",\n            pad_inches=0,\n            dpi=300,\n        )\n        plt.close()\n\n    if output.example:\n        if not hasattr(test_dataloader.dataset, \"idx_to_class\"):\n            raise ValueError(\"The provided dataset does not have an attribute 'idx_to_class\")\n\n        idx_to_class = test_dataloader.dataset.idx_to_class\n\n        # Get misclassified samples\n        example_folder = os.path.join(output_folder, \"example\")\n        if not os.path.isdir(example_folder):\n            os.makedirs(example_folder)\n\n        # Skip if no no ground truth is available\n        if not all(results[\"real_label\"] == -1):\n            for v in np.unique([results[\"real_label\"], results[\"pred_label\"]]):\n                if v == -1:\n                    continue\n\n                k = idx_to_class[v]\n\n                if ignore_classes is not None and v in ignore_classes:\n                    continue\n\n                plot_classification_results(\n                    test_dataloader.dataset,\n                    unorm=UnNormalize(mean=config.transforms.mean, std=config.transforms.std),\n                    pred_labels=results[\"pred_label\"].to_numpy(),\n                    test_labels=results[\"real_label\"].to_numpy(),\n                    class_name=k,\n                    original_folder=example_folder,\n                    idx_to_class=idx_to_class,\n                    pred_class_to_plot=v,\n                    what=\"con\",\n                    rows=output.get(\"rows\", 3),\n                    cols=output.get(\"cols\", 2),\n                    figsize=output.get(\"figsize\", (20, 20)),\n                )\n\n                plot_classification_results(\n                    test_dataloader.dataset,\n                    unorm=UnNormalize(mean=config.transforms.mean, std=config.transforms.std),\n                    pred_labels=results[\"pred_label\"].to_numpy(),\n                    test_labels=results[\"real_label\"].to_numpy(),\n                    class_name=k,\n                    original_folder=example_folder,\n                    idx_to_class=idx_to_class,\n                    pred_class_to_plot=v,\n                    what=\"dis\",\n                    rows=output.get(\"rows\", 3),\n                    cols=output.get(\"cols\", 2),\n                    figsize=output.get(\"figsize\", (20, 20)),\n                )\n\n        for counter, reconstruction in enumerate(reconstructions):\n            is_polygon = True\n            if isinstance(reconstruction[\"prediction\"], np.ndarray):\n                is_polygon = False\n\n            if is_polygon:\n                if len(reconstruction[\"prediction\"]) == 0:\n                    continue\n            elif reconstruction[\"prediction\"].sum() == 0:\n                continue\n\n            if counter &gt; 5:\n                break\n\n            to_plot = plot_patch_reconstruction(\n                reconstruction,\n                idx_to_class,\n                class_to_idx=test_dataloader.dataset.class_to_idx,  # type: ignore[attr-defined]\n                ignore_classes=ignore_classes,\n                is_polygon=is_polygon,\n            )\n\n            if to_plot:\n                output_name = f\"reconstruction_{os.path.splitext(os.path.basename(reconstruction['file_name']))[0]}.png\"\n                plt.savefig(os.path.join(example_folder, output_name), bbox_inches=\"tight\", pad_inches=0)\n\n            plt.close()\n</code></pre>"},{"location":"reference/quadra/utils/patch/visualization.html","title":"visualization","text":""},{"location":"reference/quadra/utils/patch/visualization.html#quadra.utils.patch.visualization.create_rgb_mask","title":"<code>create_rgb_mask(mask, color_map, ignore_classes=None, ground_truth_mask=None)</code>","text":"<p>Convert index mask to RGB mask.</p> Source code in <code>quadra/utils/patch/visualization.py</code> <pre><code>def create_rgb_mask(\n    mask: np.ndarray,\n    color_map: dict,\n    ignore_classes: list[int] | None = None,\n    ground_truth_mask: np.ndarray | None = None,\n):\n\"\"\"Convert index mask to RGB mask.\"\"\"\n    output_mask = np.zeros([mask.shape[0], mask.shape[1], 3])\n    for c in np.unique(mask):\n        if ignore_classes is not None and c in ignore_classes:\n            continue\n\n        output_mask[mask == c] = color_map[str(c)]\n    if ignore_classes is not None and ground_truth_mask is not None:\n        output_mask[np.isin(ground_truth_mask, ignore_classes)] = [0, 0, 0]\n\n    return output_mask\n</code></pre>"},{"location":"reference/quadra/utils/patch/visualization.html#quadra.utils.patch.visualization.plot_patch_reconstruction","title":"<code>plot_patch_reconstruction(reconstruction, idx_to_class, class_to_idx, ignore_classes=None, is_polygon=True)</code>","text":"<p>Helper function for plotting the patch reconstruction.</p> <p>Parameters:</p> <ul> <li> reconstruction             (<code>dict</code>)         \u2013          <p>Dict following this structure {     \"file_path\": str,     \"mask_path\": str,     \"prediction\": {         \"label\": str,         \"points\": [{\"x\": int, \"y\": int}]     } } if is_polygon else {     \"file_path\": str,     \"mask_path\": str,     \"prediction\": np.ndarray }</p> </li> <li> idx_to_class             (<code>dict[int, str]</code>)         \u2013          <p>Dictionary mapping indices to label names</p> </li> <li> class_to_idx             (<code>dict[str, int]</code>)         \u2013          <p>Dictionary mapping class names to indices</p> </li> <li> ignore_classes             (<code>list[int] | None</code>, default:                 <code>None</code> )         \u2013          <p>Eventually the classes to not plot</p> </li> <li> is_polygon             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Boolean indicating if the prediction is a polygon or a mask.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Figure</code>         \u2013          <p>Matplotlib plot showing predicted patch regions and eventually gt</p> </li> </ul> Source code in <code>quadra/utils/patch/visualization.py</code> <pre><code>def plot_patch_reconstruction(\n    reconstruction: dict,\n    idx_to_class: dict[int, str],\n    class_to_idx: dict[str, int],\n    ignore_classes: list[int] | None = None,\n    is_polygon: bool = True,\n) -&gt; Figure:\n\"\"\"Helper function for plotting the patch reconstruction.\n\n    Args:\n        reconstruction: Dict following this structure\n            {\n                \"file_path\": str,\n                \"mask_path\": str,\n                \"prediction\": {\n                    \"label\": str,\n                    \"points\": [{\"x\": int, \"y\": int}]\n                }\n            } if is_polygon else\n            {\n                \"file_path\": str,\n                \"mask_path\": str,\n                \"prediction\": np.ndarray\n            }\n        idx_to_class: Dictionary mapping indices to label names\n        class_to_idx: Dictionary mapping class names to indices\n        ignore_classes: Eventually the classes to not plot\n        is_polygon: Boolean indicating if the prediction is a polygon or a mask.\n\n    Returns:\n        Matplotlib plot showing predicted patch regions and eventually gt\n\n    \"\"\"\n    cmap_name = \"tab10\"\n\n    # 10 classes + good\n    if len(idx_to_class.values()) &gt; 11:\n        cmap_name = \"tab20\"\n\n    cmap = get_cmap(cmap_name)\n    test_img = cv2.imread(reconstruction[\"image_path\"])\n    test_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)\n    gt_img = None\n\n    if reconstruction[\"mask_path\"] is not None and os.path.isfile(reconstruction[\"mask_path\"]):\n        gt_img = cv2.imread(reconstruction[\"mask_path\"], 0)\n\n    out = np.zeros((test_img.shape[0], test_img.shape[1]), dtype=np.uint8)\n\n    if is_polygon:\n        for _, region in enumerate(reconstruction[\"prediction\"]):\n            points = [[item[\"x\"], item[\"y\"]] for item in region[\"points\"]]\n            c_label = region[\"label\"]\n\n            out = cv2.drawContours(\n                out,\n                np.array([points], np.int32),\n                -1,\n                class_to_idx[c_label],\n                thickness=cv2.FILLED,\n            )  # type: ignore[call-overload]\n    else:\n        out = reconstruction[\"prediction\"]\n\n    fig = plot_patch_results(\n        image=test_img,\n        prediction_image=out,\n        ground_truth_image=gt_img,\n        plot_original=True,\n        ignore_classes=ignore_classes,\n        save_path=None,\n        class_to_idx=class_to_idx,\n        cmap=cmap,\n    )\n\n    return fig\n</code></pre>"},{"location":"reference/quadra/utils/patch/visualization.html#quadra.utils.patch.visualization.plot_patch_results","title":"<code>plot_patch_results(image, prediction_image, ground_truth_image, class_to_idx, plot_original=True, ignore_classes=None, image_height=10, save_path=None, cmap=None)</code>","text":"<p>Function used to plot the image predicted.</p> <p>Parameters:</p> <ul> <li> prediction_image             (<code>ndarray</code>)         \u2013          <p>The prediction image</p> </li> <li> image             (<code>ndarray</code>)         \u2013          <p>The original image to plot</p> </li> <li> ground_truth_image             (<code>ndarray | None</code>)         \u2013          <p>The ground truth image</p> </li> <li> class_to_idx             (<code>dict[str, int]</code>)         \u2013          <p>Dictionary mapping class names to indices</p> </li> <li> plot_original             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>Boolean to plot the original image</p> </li> <li> ignore_classes             (<code>list[int] | None</code>, default:                 <code>None</code> )         \u2013          <p>The classes to ignore, default is 0</p> </li> <li> image_height             (<code>int</code>, default:                 <code>10</code> )         \u2013          <p>The height of the output figure</p> </li> <li> save_path             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>The path to save the figure</p> </li> <li> cmap             (<code>Colormap | None</code>, default:                 <code>None</code> )         \u2013          <p>The colormap to use. If None, tab20 is used</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Figure</code>         \u2013          <p>The matplotlib figure</p> </li> </ul> Source code in <code>quadra/utils/patch/visualization.py</code> <pre><code>def plot_patch_results(\n    image: np.ndarray,\n    prediction_image: np.ndarray,\n    ground_truth_image: np.ndarray | None,\n    class_to_idx: dict[str, int],\n    plot_original: bool = True,\n    ignore_classes: list[int] | None = None,\n    image_height: int = 10,\n    save_path: str | None = None,\n    cmap: Colormap | None = None,\n) -&gt; Figure:\n\"\"\"Function used to plot the image predicted.\n\n    Args:\n        prediction_image: The prediction image\n        image: The original image to plot\n        ground_truth_image: The ground truth image\n        class_to_idx: Dictionary mapping class names to indices\n        plot_original: Boolean to plot the original image\n        ignore_classes: The classes to ignore, default is 0\n        image_height: The height of the output figure\n        save_path: The path to save the figure\n        cmap: The colormap to use. If None, tab20 is used\n\n    Returns:\n        The matplotlib figure\n    \"\"\"\n    if ignore_classes is None:\n        ignore_classes = [0]\n\n    if cmap is None:\n        cmap = get_cmap(\"tab20\")\n\n    image = image[0 : prediction_image.shape[0], 0 : prediction_image.shape[1], :]\n    idx_to_class = {v: k for k, v in class_to_idx.items()}\n\n    if ignore_classes is not None:\n        class_to_idx = {k: v for k, v in class_to_idx.items() if v not in ignore_classes}\n\n    class_idxs = list(class_to_idx.values())\n\n    cmap = {str(c): tuple(int(i * 255) for i in cmap(c / len(class_idxs))[:-1]) for c in class_idxs}\n    output_images = []\n    titles = []\n\n    if plot_original:\n        output_images.append(image)\n        titles.append(\"Original Image\")\n\n    if ground_truth_image is not None:\n        ground_truth_image = ground_truth_image[0 : prediction_image.shape[0], 0 : prediction_image.shape[1]]\n        ground_truth_mask = create_rgb_mask(ground_truth_image, cmap, ignore_classes=ignore_classes)\n        output_images.append(ground_truth_mask)\n        titles.append(\"Ground Truth Mask\")\n\n    prediction_mask = create_rgb_mask(\n        prediction_image,\n        cmap,\n        ignore_classes=ignore_classes,\n    )\n\n    output_images.append(prediction_mask)\n    titles.append(\"Prediction Mask\")\n    if ignore_classes is not None and ground_truth_image is not None:\n        prediction_mask = create_rgb_mask(\n            prediction_image, cmap, ignore_classes=ignore_classes, ground_truth_mask=ground_truth_image\n        )\n\n        ignored_classes_str = [idx_to_class[c] for c in ignore_classes]\n        prediction_title = f\"Prediction Mask \\n (Ignoring Ground Truth Class: {ignored_classes_str})\"\n        output_images.append(prediction_mask)\n        titles.append(prediction_title)\n\n    fig, axs = plt.subplots(\n        ncols=len(output_images),\n        nrows=1,\n        figsize=(len(output_images) * image_height, image_height),\n        squeeze=False,\n        facecolor=\"white\",\n    )\n\n    for i, output_image in enumerate(output_images):\n        axs[0, i].imshow(show_mask_on_image(image, output_image))\n        axs[0, i].set_title(titles[i])\n        axs[0, i].axis(\"off\")\n\n    custom_lines = [Line2D([0], [0], color=tuple(i / 255.0 for i in cmap[str(c)]), lw=4) for c in class_idxs]\n    custom_labels = list(class_to_idx.keys())\n    axs[0, -1].legend(custom_lines, custom_labels, loc=\"center left\", bbox_to_anchor=(1.01, 0.81), borderaxespad=0)\n    if save_path is not None:\n        plt.savefig(save_path, bbox_inches=\"tight\")\n        plt.close()\n\n    return fig\n</code></pre>"},{"location":"reference/quadra/utils/patch/visualization.html#quadra.utils.patch.visualization.show_mask_on_image","title":"<code>show_mask_on_image(image, mask)</code>","text":"<p>Plot mask on top of the original image.</p> Source code in <code>quadra/utils/patch/visualization.py</code> <pre><code>def show_mask_on_image(image: np.ndarray, mask: np.ndarray):\n\"\"\"Plot mask on top of the original image.\"\"\"\n    image = image.astype(np.float32) / 255\n    mask = mask.astype(np.float32) / 255\n    out = mask + image.astype(np.float32)\n    out = out / np.max(out)\n    return np.uint8(255 * out)\n</code></pre>"},{"location":"reference/quadra/utils/tests/index.html","title":"tests","text":""},{"location":"reference/quadra/utils/tests/index.html#submodules","title":"Submodules","text":"<ul> <li>fixtures</li> </ul>"},{"location":"reference/quadra/utils/tests/index.html#python-files","title":"Python Files","text":"<ul> <li>helpers.py</li> <li>models.py </li> </ul>"},{"location":"reference/quadra/utils/tests/helpers.html","title":"helpers","text":""},{"location":"reference/quadra/utils/tests/helpers.html#quadra.utils.tests.helpers.check_deployment_model","title":"<code>check_deployment_model(export_type)</code>","text":"<p>Check that the runtime model is present and valid.</p> <p>Parameters:</p> <ul> <li> export_type             (<code>str</code>)         \u2013          <p>The type of the exported model.</p> </li> </ul> Source code in <code>quadra/utils/tests/helpers.py</code> <pre><code>def check_deployment_model(export_type: str):\n\"\"\"Check that the runtime model is present and valid.\n\n    Args:\n        export_type: The type of the exported model.\n    \"\"\"\n    extension = get_export_extension(export_type)\n\n    assert os.path.exists(f\"deployment_model/model.{extension}\")\n    assert os.path.exists(\"deployment_model/model.json\")\n</code></pre>"},{"location":"reference/quadra/utils/tests/helpers.html#quadra.utils.tests.helpers.execute_quadra_experiment","title":"<code>execute_quadra_experiment(overrides, experiment_path)</code>","text":"<p>Execute quadra experiment.</p> Source code in <code>quadra/utils/tests/helpers.py</code> <pre><code>def execute_quadra_experiment(overrides: list[str], experiment_path: Path) -&gt; None:\n\"\"\"Execute quadra experiment.\"\"\"\n    with initialize_config_module(config_module=\"quadra.configs\", version_base=\"1.3.0\"):\n        if not experiment_path.exists():\n            experiment_path.mkdir(parents=True)\n        os.chdir(experiment_path)\n        # cfg = compose(config_name=\"config\", overrides=overrides)\n        cfg = compose(config_name=\"config\", overrides=overrides, return_hydra_config=True)\n        # workaround without actual main function\n        # check https://github.com/facebookresearch/hydra/issues/2017 for more details\n        HydraConfig.instance().set_config(cfg)\n\n        main(cfg)\n</code></pre>"},{"location":"reference/quadra/utils/tests/helpers.html#quadra.utils.tests.helpers.get_quadra_test_device","title":"<code>get_quadra_test_device()</code>","text":"<p>Get the device to use for the tests. If the QUADRA_TEST_DEVICE environment variable is set, it is used.</p> Source code in <code>quadra/utils/tests/helpers.py</code> <pre><code>def get_quadra_test_device():\n\"\"\"Get the device to use for the tests. If the QUADRA_TEST_DEVICE environment variable is set, it is used.\"\"\"\n    return os.environ.get(\"QUADRA_TEST_DEVICE\", \"cpu\")\n</code></pre>"},{"location":"reference/quadra/utils/tests/helpers.html#quadra.utils.tests.helpers.setup_trainer_for_lightning","title":"<code>setup_trainer_for_lightning()</code>","text":"<p>Setup trainer for lightning depending on the device. If cuda is used, the device index is also set. If cpu is used, the trainer is set to lightning_cpu.</p> <p>Returns:</p> <ul> <li> <code>list[str]</code>         \u2013          <p>A list of overrides for the trainer.</p> </li> </ul> Source code in <code>quadra/utils/tests/helpers.py</code> <pre><code>def setup_trainer_for_lightning() -&gt; list[str]:\n\"\"\"Setup trainer for lightning depending on the device. If cuda is used, the device index is also set.\n    If cpu is used, the trainer is set to lightning_cpu.\n\n    Returns:\n        A list of overrides for the trainer.\n    \"\"\"\n    overrides = []\n    device = get_quadra_test_device()\n    torch_device = torch.device(device)\n    if torch_device.type == \"cuda\":\n        device_index = torch_device.index\n        overrides.append(\"trainer=lightning_gpu\")\n        overrides.append(f\"trainer.devices=[{device_index}]\")\n    else:\n        overrides.append(\"trainer=lightning_cpu\")\n\n    return overrides\n</code></pre>"},{"location":"reference/quadra/utils/tests/models.html","title":"models","text":""},{"location":"reference/quadra/utils/tests/models.html#quadra.utils.tests.models.DoubleInputModel","title":"<code>DoubleInputModel</code>","text":"<p>             Bases: <code>Module</code></p> <p>Model taking two inputs.</p>"},{"location":"reference/quadra/utils/tests/models.html#quadra.utils.tests.models.SingleInputModel","title":"<code>SingleInputModel</code>","text":"<p>             Bases: <code>Module</code></p> <p>Model taking a single input.</p>"},{"location":"reference/quadra/utils/tests/models.html#quadra.utils.tests.models.UnsupportedInputModel","title":"<code>UnsupportedInputModel</code>","text":"<p>             Bases: <code>Module</code></p> <p>Model taking an unsupported input.</p>"},{"location":"reference/quadra/utils/tests/fixtures/index.html","title":"fixtures","text":""},{"location":"reference/quadra/utils/tests/fixtures/index.html#submodules","title":"Submodules","text":"<ul> <li>models</li> <li>dataset </li> </ul>"},{"location":"reference/quadra/utils/tests/fixtures/dataset/index.html","title":"dataset","text":""},{"location":"reference/quadra/utils/tests/fixtures/dataset/index.html#quadra.utils.tests.fixtures.dataset.AnomalyDatasetArguments","title":"<code>AnomalyDatasetArguments</code>  <code>dataclass</code>","text":"<p>Anomaly dataset arguments.</p> <p>Parameters:</p> <ul> <li> train_samples             (<code>int</code>)         \u2013          <p>number of train samples</p> </li> <li> val_samples             (<code>tuple[int, int]</code>)         \u2013          <p>number of validation samples (good, bad)</p> </li> <li> test_samples             (<code>tuple[int, int]</code>)         \u2013          <p>number of test samples (good, bad)</p> </li> </ul>"},{"location":"reference/quadra/utils/tests/fixtures/dataset/index.html#quadra.utils.tests.fixtures.dataset.ClassificationDatasetArguments","title":"<code>ClassificationDatasetArguments</code>  <code>dataclass</code>","text":"<p>Classification dataset arguments.</p> <p>Parameters:</p> <ul> <li> samples             (<code>list[int]</code>)         \u2013          <p>number of samples per class</p> </li> <li> classes             (<code>list[str] | None</code>, default:                 <code>None</code> )         \u2013          <p>class names, if set it must be the same length as samples</p> </li> <li> val_size             (<code>float | None</code>, default:                 <code>None</code> )         \u2013          <p>validation set size</p> </li> <li> test_size             (<code>float | None</code>, default:                 <code>None</code> )         \u2013          <p>test set size</p> </li> </ul>"},{"location":"reference/quadra/utils/tests/fixtures/dataset/index.html#quadra.utils.tests.fixtures.dataset.ClassificationMultilabelDatasetArguments","title":"<code>ClassificationMultilabelDatasetArguments</code>  <code>dataclass</code>","text":"<p>Classification dataset arguments.</p> <p>Parameters:</p> <ul> <li> samples             (<code>list[int]</code>)         \u2013          <p>number of samples per class</p> </li> <li> classes             (<code>list[str] | None</code>, default:                 <code>None</code> )         \u2013          <p>class names, if set it must be the same length as samples</p> </li> <li> val_size             (<code>float | None</code>, default:                 <code>None</code> )         \u2013          <p>validation set size</p> </li> <li> test_size             (<code>float | None</code>, default:                 <code>None</code> )         \u2013          <p>test set size</p> </li> <li> percentage_other_classes             (<code>float | None</code>, default:                 <code>0.0</code> )         \u2013          <p>probability of adding other classes to the labels of each sample</p> </li> </ul>"},{"location":"reference/quadra/utils/tests/fixtures/dataset/index.html#quadra.utils.tests.fixtures.dataset.ClassificationPatchDatasetArguments","title":"<code>ClassificationPatchDatasetArguments</code>  <code>dataclass</code>","text":"<p>Classification patch dataset arguments.</p> <p>Parameters:</p> <ul> <li> samples             (<code>list[int]</code>)         \u2013          <p>number of samples per class</p> </li> <li> overlap             (<code>float</code>)         \u2013          <p>overlap between patches</p> </li> <li> patch_size             (<code>tuple[int, int] | None</code>, default:                 <code>None</code> )         \u2013          <p>patch size</p> </li> <li> patch_number             (<code>tuple[int, int] | None</code>, default:                 <code>None</code> )         \u2013          <p>number of patches</p> </li> <li> classes             (<code>list[str] | None</code>, default:                 <code>None</code> )         \u2013          <p>class names, if set it must be the same length as samples</p> </li> <li> val_size             (<code>float | None</code>, default:                 <code>0.0</code> )         \u2013          <p>validation set size</p> </li> <li> test_size             (<code>float | None</code>, default:                 <code>0.0</code> )         \u2013          <p>test set size</p> </li> <li> annotated_good             (<code>list[str] | None</code>, default:                 <code>None</code> )         \u2013          <p>list of class names that are considered as good annotations (E.g. [\"good\"])</p> </li> </ul>"},{"location":"reference/quadra/utils/tests/fixtures/dataset/index.html#quadra.utils.tests.fixtures.dataset.SegmentationDatasetArguments","title":"<code>SegmentationDatasetArguments</code>  <code>dataclass</code>","text":"<p>Segmentation dataset arguments.</p> <p>Parameters:</p> <ul> <li> train_samples             (<code>list[int]</code>)         \u2013          <p>List of samples per class in train set, element at index 0 are good samples</p> </li> <li> val_samples             (<code>list[int] | None</code>, default:                 <code>None</code> )         \u2013          <p>List of samples per class in validation set, same as above.</p> </li> <li> test_samples             (<code>list[int] | None</code>, default:                 <code>None</code> )         \u2013          <p>List of samples per class in test set, same as above.</p> </li> <li> classes             (<code>list[str] | None</code>, default:                 <code>None</code> )         \u2013          <p>Optional list of class names, must be equal to len(train_samples) - 1</p> </li> </ul>"},{"location":"reference/quadra/utils/tests/fixtures/dataset/index.html#quadra.utils.tests.fixtures.dataset.anomaly_dataset","title":"<code>anomaly_dataset(tmp_path, dataset_arguments)</code>","text":"<p>Fixture used to dinamically generate anomaly dataset. By default images are random grayscales with size 10x10.</p> <p>Parameters:</p> <ul> <li> tmp_path             (<code>Path</code>)         \u2013          <p>path to temporary directory</p> </li> <li> dataset_arguments             (<code>AnomalyDatasetArguments</code>)         \u2013          <p>dataset arguments</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[str, AnomalyDatasetArguments]</code>         \u2013          <p>path to anomaly dataset</p> </li> </ul> Source code in <code>quadra/utils/tests/fixtures/dataset/anomaly.py</code> <pre><code>@pytest.fixture\ndef anomaly_dataset(tmp_path: Path, dataset_arguments: AnomalyDatasetArguments) -&gt; tuple[str, AnomalyDatasetArguments]:\n\"\"\"Fixture used to dinamically generate anomaly dataset. By default images are random grayscales with size 10x10.\n\n    Args:\n        tmp_path: path to temporary directory\n        dataset_arguments: dataset arguments\n\n    Returns:\n        path to anomaly dataset\n    \"\"\"\n    yield _build_anomaly_dataset(tmp_path, dataset_arguments)\n    if tmp_path.exists():\n        shutil.rmtree(tmp_path)\n</code></pre>"},{"location":"reference/quadra/utils/tests/fixtures/dataset/index.html#quadra.utils.tests.fixtures.dataset.base_anomaly_dataset","title":"<code>base_anomaly_dataset(tmp_path, request)</code>","text":"Generate base anomaly dataset with the following parameters <ul> <li>train_samples: 10</li> <li>val_samples: (10, 10)</li> <li>test_samples: (10, 10).</li> </ul> <p>Parameters:</p> <ul> <li> tmp_path             (<code>Path</code>)         \u2013          <p>Path to temporary directory</p> </li> <li> request             (<code>Any</code>)         \u2013          <p>Pytest SubRequest object</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>tuple[str, AnomalyDatasetArguments]</code>         \u2013          <p>Path to anomaly dataset and dataset arguments</p> </li> </ul> Source code in <code>quadra/utils/tests/fixtures/dataset/anomaly.py</code> <pre><code>@pytest.fixture(\n    params=[AnomalyDatasetArguments(**{\"train_samples\": 10, \"val_samples\": (1, 1), \"test_samples\": (1, 1)})]\n)\ndef base_anomaly_dataset(tmp_path: Path, request: Any) -&gt; tuple[str, AnomalyDatasetArguments]:\n\"\"\"Generate base anomaly dataset with the following parameters:\n        - train_samples: 10\n        - val_samples: (10, 10)\n        - test_samples: (10, 10).\n\n    Args:\n        tmp_path: Path to temporary directory\n        request: Pytest SubRequest object\n\n    Yields:\n        Path to anomaly dataset and dataset arguments\n    \"\"\"\n    yield _build_anomaly_dataset(tmp_path, request.param)\n    if tmp_path.exists():\n        shutil.rmtree(tmp_path)\n</code></pre>"},{"location":"reference/quadra/utils/tests/fixtures/dataset/index.html#quadra.utils.tests.fixtures.dataset.base_binary_segmentation_dataset","title":"<code>base_binary_segmentation_dataset(tmp_path, request)</code>","text":"Generate a base binary segmentation dataset with the following structure <ul> <li>3 good and 2 bad samples in train set</li> <li>2 good and 2 bad samples in validation set</li> <li>11 good and 1 bad sample in test set</li> <li>2 classes: good and bad.</li> </ul> <p>Parameters:</p> <ul> <li> tmp_path             (<code>Path</code>)         \u2013          <p>path to temporary directory</p> </li> <li> request             (<code>Any</code>)         \u2013          <p>pytest request</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>tuple[str, SegmentationDatasetArguments, dict[str, int]]</code>         \u2013          <p>Tuple containing path to dataset, dataset arguments and class to index mapping</p> </li> </ul> Source code in <code>quadra/utils/tests/fixtures/dataset/segmentation.py</code> <pre><code>@pytest.fixture(\n    params=[\n        SegmentationDatasetArguments(\n            **{\"train_samples\": [3, 2], \"val_samples\": [2, 2], \"test_samples\": [1, 1], \"classes\": [\"bad\"]}\n        )\n    ]\n)\ndef base_binary_segmentation_dataset(\n    tmp_path: Path, request: Any\n) -&gt; tuple[str, SegmentationDatasetArguments, dict[str, int]]:\n\"\"\"Generate a base binary segmentation dataset with the following structure:\n        - 3 good and 2 bad samples in train set\n        - 2 good and 2 bad samples in validation set\n        - 11 good and 1 bad sample in test set\n        - 2 classes: good and bad.\n\n    Args:\n        tmp_path: path to temporary directory\n        request: pytest request\n\n    Yields:\n        Tuple containing path to dataset, dataset arguments and class to index mapping\n    \"\"\"\n    yield _build_segmentation_dataset(tmp_path, request.param)\n    if tmp_path.exists():\n        shutil.rmtree(tmp_path)\n</code></pre>"},{"location":"reference/quadra/utils/tests/fixtures/dataset/index.html#quadra.utils.tests.fixtures.dataset.base_classification_dataset","title":"<code>base_classification_dataset(tmp_path, request)</code>","text":"Generate base classification dataset with the following parameters <ul> <li>10 samples per class</li> <li>2 classes (class_1 and class_2) By default generated images are grayscale and 10x10 pixels.</li> </ul> <p>Parameters:</p> <ul> <li> tmp_path             (<code>Path</code>)         \u2013          <p>path to temporary directory</p> </li> <li> request             (<code>Any</code>)         \u2013          <p>pytest request</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>tuple[str, ClassificationDatasetArguments]</code>         \u2013          <p>Tuple containing path to created dataset and dataset arguments</p> </li> </ul> Source code in <code>quadra/utils/tests/fixtures/dataset/classification.py</code> <pre><code>@pytest.fixture(\n    params=[\n        ClassificationDatasetArguments(\n            **{\"samples\": [10, 10], \"classes\": [\"class_1\", \"class_2\"], \"val_size\": 0.1, \"test_size\": 0.1}\n        )\n    ]\n)\ndef base_classification_dataset(tmp_path: Path, request: Any) -&gt; tuple[str, ClassificationDatasetArguments]:\n\"\"\"Generate base classification dataset with the following parameters:\n        - 10 samples per class\n        - 2 classes (class_1 and class_2)\n        By default generated images are grayscale and 10x10 pixels.\n\n    Args:\n        tmp_path: path to temporary directory\n        request: pytest request\n\n    Yields:\n        Tuple containing path to created dataset and dataset arguments\n    \"\"\"\n    yield _build_classification_dataset(tmp_path, request.param)\n    if tmp_path.exists():\n        shutil.rmtree(tmp_path)\n</code></pre>"},{"location":"reference/quadra/utils/tests/fixtures/dataset/index.html#quadra.utils.tests.fixtures.dataset.base_multiclass_segmentation_dataset","title":"<code>base_multiclass_segmentation_dataset(tmp_path, request)</code>","text":"Generate a base binary segmentation dataset with the following structure <ul> <li>2 good, 2 defect_1 and 2 defect_2 samples in train set</li> <li>2 good, 2 defect_1 and 2 defect_2 samples in validation set</li> <li>1 good, 1 defect_1 and 1 defect_2 sample in test set</li> <li>3 classes: good, defect_1 and defect_2.</li> </ul> <p>Parameters:</p> <ul> <li> tmp_path             (<code>Path</code>)         \u2013          <p>path to temporary directory</p> </li> <li> request             (<code>Any</code>)         \u2013          <p>pytest request</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>tuple[str, SegmentationDatasetArguments, dict[str, int]]</code>         \u2013          <p>Tuple containing path to dataset, dataset arguments and class to index mapping</p> </li> </ul> Source code in <code>quadra/utils/tests/fixtures/dataset/segmentation.py</code> <pre><code>@pytest.fixture(\n    params=[\n        SegmentationDatasetArguments(\n            **{\n                \"train_samples\": [2, 2, 2],\n                \"val_samples\": [2, 2, 2],\n                \"test_samples\": [1, 1, 1],\n                \"classes\": [\"defect_1\", \"defect_2\"],\n            }\n        )\n    ]\n)\ndef base_multiclass_segmentation_dataset(\n    tmp_path: Path, request: Any\n) -&gt; tuple[str, SegmentationDatasetArguments, dict[str, int]]:\n\"\"\"Generate a base binary segmentation dataset with the following structure:\n        - 2 good, 2 defect_1 and 2 defect_2 samples in train set\n        - 2 good, 2 defect_1 and 2 defect_2 samples in validation set\n        - 1 good, 1 defect_1 and 1 defect_2 sample in test set\n        - 3 classes: good, defect_1 and defect_2.\n\n    Args:\n        tmp_path: path to temporary directory\n        request: pytest request\n\n    Yields:\n        Tuple containing path to dataset, dataset arguments and class to index mapping\n    \"\"\"\n    yield _build_segmentation_dataset(tmp_path, request.param)\n    if tmp_path.exists():\n        shutil.rmtree(tmp_path)\n</code></pre>"},{"location":"reference/quadra/utils/tests/fixtures/dataset/index.html#quadra.utils.tests.fixtures.dataset.base_multilabel_classification_dataset","title":"<code>base_multilabel_classification_dataset(tmp_path, request)</code>","text":"Fixture to generate base multilabel classification dataset with the following parameters <ul> <li>10 samples per class</li> <li>3 classes (class_1, class_2 and class_3)</li> <li>10% of samples in validation set</li> <li>10% of samples in test set</li> <li>30% of possibility to add each other class to the sample By default generated images are grayscale and 10x10 pixels.</li> </ul> <p>Parameters:</p> <ul> <li> tmp_path             (<code>Path</code>)         \u2013          <p>path to temporary directory</p> </li> <li> request             (<code>Any</code>)         \u2013          <p>pytest request</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>tuple[str, ClassificationMultilabelDatasetArguments]</code>         \u2013          <p>Tuple containing path to created dataset and dataset arguments</p> </li> </ul> Source code in <code>quadra/utils/tests/fixtures/dataset/classification.py</code> <pre><code>@pytest.fixture(\n    params=[\n        ClassificationMultilabelDatasetArguments(\n            **{\n                \"samples\": [10, 10, 10],\n                \"classes\": [\"class_1\", \"class_2\", \"class_3\"],\n                \"val_size\": 0.1,\n                \"test_size\": 0.1,\n                \"percentage_other_classes\": 0.3,\n            }\n        )\n    ]\n)\ndef base_multilabel_classification_dataset(\n    tmp_path: Path, request: Any\n) -&gt; tuple[str, ClassificationMultilabelDatasetArguments]:\n\"\"\"Fixture to generate base multilabel classification dataset with the following parameters:\n        - 10 samples per class\n        - 3 classes (class_1, class_2 and class_3)\n        - 10% of samples in validation set\n        - 10% of samples in test set\n        - 30% of possibility to add each other class to the sample\n        By default generated images are grayscale and 10x10 pixels.\n\n    Args:\n        tmp_path: path to temporary directory\n        request: pytest request\n\n    Yields:\n        Tuple containing path to created dataset and dataset arguments\n    \"\"\"\n    yield _build_multilabel_classification_dataset(tmp_path, request.param)\n    if tmp_path.exists():\n        shutil.rmtree(tmp_path)\n</code></pre>"},{"location":"reference/quadra/utils/tests/fixtures/dataset/index.html#quadra.utils.tests.fixtures.dataset.base_patch_classification_dataset","title":"<code>base_patch_classification_dataset(tmp_path, request)</code>","text":"Generate a classification patch dataset with the following parameters <ul> <li>3 classes named bg, a and b</li> <li>5, 5 and 5 samples for each class</li> <li>2 horizontal patches and 2 vertical patches</li> <li>0% overlap</li> <li>10% validation set</li> <li>10% test set.</li> </ul> <p>Parameters:</p> <ul> <li> tmp_path             (<code>Path</code>)         \u2013          <p>path to temporary directory</p> </li> <li> request             (<code>Any</code>)         \u2013          <p>pytest SubRequest object</p> </li> </ul> Source code in <code>quadra/utils/tests/fixtures/dataset/classification.py</code> <pre><code>@pytest.fixture(\n    params=[\n        ClassificationPatchDatasetArguments(\n            **{\n                \"samples\": [5, 5, 5],\n                \"classes\": [\"bg\", \"a\", \"b\"],\n                \"patch_number\": [2, 2],\n                \"overlap\": 0,\n                \"val_size\": 0.1,\n                \"test_size\": 0.1,\n            }\n        )\n    ]\n)\ndef base_patch_classification_dataset(\n    tmp_path: Path, request: Any\n) -&gt; tuple[str, ClassificationDatasetArguments, dict[str, int]]:\n\"\"\"Generate a classification patch dataset with the following parameters:\n        - 3 classes named bg, a and b\n        - 5, 5 and 5 samples for each class\n        - 2 horizontal patches and 2 vertical patches\n        - 0% overlap\n        - 10% validation set\n        - 10% test set.\n\n    Args:\n        tmp_path: path to temporary directory\n        request: pytest SubRequest object\n    \"\"\"\n    yield _build_classification_patch_dataset(tmp_path, request.param)\n    if tmp_path.exists():\n        shutil.rmtree(tmp_path)\n</code></pre>"},{"location":"reference/quadra/utils/tests/fixtures/dataset/index.html#quadra.utils.tests.fixtures.dataset.classification_dataset","title":"<code>classification_dataset(tmp_path, dataset_arguments)</code>","text":"<p>Generate classification dataset. If val_size or test_size are set, it will generate a train.txt, val.txt and     test.txt file in the dataset directory. By default generated images are 10x10 pixels.</p> <p>Parameters:</p> <ul> <li> tmp_path             (<code>Path</code>)         \u2013          <p>path to temporary directory</p> </li> <li> dataset_arguments             (<code>ClassificationDatasetArguments</code>)         \u2013          <p>dataset arguments</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>tuple[str, ClassificationDatasetArguments]</code>         \u2013          <p>Tuple containing path to created dataset and dataset arguments</p> </li> </ul> Source code in <code>quadra/utils/tests/fixtures/dataset/classification.py</code> <pre><code>@pytest.fixture\ndef classification_dataset(\n    tmp_path: Path, dataset_arguments: ClassificationDatasetArguments\n) -&gt; tuple[str, ClassificationDatasetArguments]:\n\"\"\"Generate classification dataset. If val_size or test_size are set, it will generate a train.txt, val.txt and\n        test.txt file in the dataset directory. By default generated images are 10x10 pixels.\n\n    Args:\n        tmp_path: path to temporary directory\n        dataset_arguments: dataset arguments\n\n    Yields:\n        Tuple containing path to created dataset and dataset arguments\n    \"\"\"\n    yield _build_classification_dataset(tmp_path, dataset_arguments)\n    if tmp_path.exists():\n        shutil.rmtree(tmp_path)\n</code></pre>"},{"location":"reference/quadra/utils/tests/fixtures/dataset/index.html#quadra.utils.tests.fixtures.dataset.classification_patch_dataset","title":"<code>classification_patch_dataset(tmp_path, dataset_arguments)</code>","text":"<p>Fixture to dinamically generate a classification patch dataset.</p> <pre><code>By default generated images are 224x224 pixels\nand associated masks contains a 50x50 pixels square with the corresponding image class, so at the current stage\nis not possible to have images with multiple annotations. The patch dataset will be generated using the standard\nparameters of generate_patch_dataset function.\n</code></pre> <p>Parameters:</p> <ul> <li> tmp_path             (<code>Path</code>)         \u2013          <p>path to temporary directory</p> </li> <li> dataset_arguments             (<code>ClassificationDatasetArguments</code>)         \u2013          <p>dataset arguments</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>tuple[str, ClassificationDatasetArguments, dict[str, int]]</code>         \u2013          <p>Tuple containing path to created dataset, dataset arguments and class to index mapping</p> </li> </ul> Source code in <code>quadra/utils/tests/fixtures/dataset/classification.py</code> <pre><code>@pytest.fixture\ndef classification_patch_dataset(\n    tmp_path: Path, dataset_arguments: ClassificationDatasetArguments\n) -&gt; tuple[str, ClassificationDatasetArguments, dict[str, int]]:\n\"\"\"Fixture to dinamically generate a classification patch dataset.\n\n        By default generated images are 224x224 pixels\n        and associated masks contains a 50x50 pixels square with the corresponding image class, so at the current stage\n        is not possible to have images with multiple annotations. The patch dataset will be generated using the standard\n        parameters of generate_patch_dataset function.\n\n    Args:\n        tmp_path: path to temporary directory\n        dataset_arguments: dataset arguments\n\n    Yields:\n        Tuple containing path to created dataset, dataset arguments and class to index mapping\n    \"\"\"\n    yield _build_classification_patch_dataset(tmp_path, dataset_arguments)\n    if tmp_path.exists():\n        shutil.rmtree(tmp_path)\n</code></pre>"},{"location":"reference/quadra/utils/tests/fixtures/dataset/index.html#quadra.utils.tests.fixtures.dataset.imagenette_dataset","title":"<code>imagenette_dataset(tmp_path)</code>","text":"<p>Generate a mock imagenette dataset to test efficient_ad model.</p> <p>Parameters:</p> <ul> <li> tmp_path             (<code>Path</code>)         \u2013          <p>Path to temporary directory</p> </li> <li> request         \u2013          <p>Pytest SubRequest object</p> </li> </ul> Source code in <code>quadra/utils/tests/fixtures/dataset/imagenette.py</code> <pre><code>@pytest.fixture\ndef imagenette_dataset(tmp_path: Path) -&gt; str:\n\"\"\"Generate a mock imagenette dataset to test efficient_ad model.\n\n    Args:\n        tmp_path: Path to temporary directory\n        request: Pytest SubRequest object\n    Yields:\n        Path to imagenette dataset folder\n    \"\"\"\n    yield _build_imagenette_dataset(tmp_path, classes=3, class_samples=3)\n\n    if tmp_path.exists():\n        shutil.rmtree(tmp_path)\n</code></pre>"},{"location":"reference/quadra/utils/tests/fixtures/dataset/index.html#quadra.utils.tests.fixtures.dataset.multilabel_classification_dataset","title":"<code>multilabel_classification_dataset(tmp_path, dataset_arguments)</code>","text":"<p>Fixture to dinamically generate a multilabel classification dataset.     Generates a samples.txt file in the dataset directory containing the path to the image and the corresponding     classes. If val_size or test_size are set, it will generate a train.txt, val.txt and test.txt file in the     dataset directory. By default generated images are 10x10 pixels.</p> <p>Parameters:</p> <ul> <li> tmp_path             (<code>Path</code>)         \u2013          <p>path to temporary directory</p> </li> <li> dataset_arguments             (<code>ClassificationMultilabelDatasetArguments</code>)         \u2013          <p>dataset arguments</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[str, ClassificationMultilabelDatasetArguments]</code>         \u2013          <p>Tuple containing path to created dataset and dataset arguments</p> </li> </ul> Source code in <code>quadra/utils/tests/fixtures/dataset/classification.py</code> <pre><code>@pytest.fixture\ndef multilabel_classification_dataset(\n    tmp_path: Path, dataset_arguments: ClassificationMultilabelDatasetArguments\n) -&gt; tuple[str, ClassificationMultilabelDatasetArguments]:\n\"\"\"Fixture to dinamically generate a multilabel classification dataset.\n        Generates a samples.txt file in the dataset directory containing the path to the image and the corresponding\n        classes. If val_size or test_size are set, it will generate a train.txt, val.txt and test.txt file in the\n        dataset directory. By default generated images are 10x10 pixels.\n\n    Args:\n        tmp_path: path to temporary directory\n        dataset_arguments: dataset arguments\n\n    Returns:\n        Tuple containing path to created dataset and dataset arguments\n    \"\"\"\n    yield _build_multilabel_classification_dataset(tmp_path, dataset_arguments)\n    if tmp_path.exists():\n        shutil.rmtree(tmp_path)\n</code></pre>"},{"location":"reference/quadra/utils/tests/fixtures/dataset/index.html#quadra.utils.tests.fixtures.dataset.segmentation_dataset","title":"<code>segmentation_dataset(tmp_path, dataset_arguments)</code>","text":"<p>Fixture to dinamically generate a segmentation dataset. By default generated images are 224x224 pixels     and associated masks contains a 50x50 pixels square with the corresponding image class, so at the current stage     is not possible to have images with multiple annotations. Split files are saved as train.txt,     val.txt and test.txt.</p> <p>Parameters:</p> <ul> <li> tmp_path             (<code>Path</code>)         \u2013          <p>path to temporary directory</p> </li> <li> dataset_arguments             (<code>SegmentationDatasetArguments</code>)         \u2013          <p>dataset arguments</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>tuple[str, SegmentationDatasetArguments, dict[str, int]]</code>         \u2013          <p>Tuple containing path to dataset, dataset arguments and class to index mapping</p> </li> </ul> Source code in <code>quadra/utils/tests/fixtures/dataset/segmentation.py</code> <pre><code>@pytest.fixture\ndef segmentation_dataset(\n    tmp_path: Path, dataset_arguments: SegmentationDatasetArguments\n) -&gt; tuple[str, SegmentationDatasetArguments, dict[str, int]]:\n\"\"\"Fixture to dinamically generate a segmentation dataset. By default generated images are 224x224 pixels\n        and associated masks contains a 50x50 pixels square with the corresponding image class, so at the current stage\n        is not possible to have images with multiple annotations. Split files are saved as train.txt,\n        val.txt and test.txt.\n\n    Args:\n        tmp_path: path to temporary directory\n        dataset_arguments: dataset arguments\n\n    Yields:\n        Tuple containing path to dataset, dataset arguments and class to index mapping\n    \"\"\"\n    yield _build_segmentation_dataset(tmp_path, dataset_arguments)\n    if tmp_path.exists():\n        shutil.rmtree(tmp_path)\n</code></pre>"},{"location":"reference/quadra/utils/tests/fixtures/dataset/anomaly.html","title":"anomaly","text":""},{"location":"reference/quadra/utils/tests/fixtures/dataset/anomaly.html#quadra.utils.tests.fixtures.dataset.anomaly.AnomalyDatasetArguments","title":"<code>AnomalyDatasetArguments</code>  <code>dataclass</code>","text":"<p>Anomaly dataset arguments.</p> <p>Parameters:</p> <ul> <li> train_samples             (<code>int</code>)         \u2013          <p>number of train samples</p> </li> <li> val_samples             (<code>tuple[int, int]</code>)         \u2013          <p>number of validation samples (good, bad)</p> </li> <li> test_samples             (<code>tuple[int, int]</code>)         \u2013          <p>number of test samples (good, bad)</p> </li> </ul>"},{"location":"reference/quadra/utils/tests/fixtures/dataset/anomaly.html#quadra.utils.tests.fixtures.dataset.anomaly.anomaly_dataset","title":"<code>anomaly_dataset(tmp_path, dataset_arguments)</code>","text":"<p>Fixture used to dinamically generate anomaly dataset. By default images are random grayscales with size 10x10.</p> <p>Parameters:</p> <ul> <li> tmp_path             (<code>Path</code>)         \u2013          <p>path to temporary directory</p> </li> <li> dataset_arguments             (<code>AnomalyDatasetArguments</code>)         \u2013          <p>dataset arguments</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[str, AnomalyDatasetArguments]</code>         \u2013          <p>path to anomaly dataset</p> </li> </ul> Source code in <code>quadra/utils/tests/fixtures/dataset/anomaly.py</code> <pre><code>@pytest.fixture\ndef anomaly_dataset(tmp_path: Path, dataset_arguments: AnomalyDatasetArguments) -&gt; tuple[str, AnomalyDatasetArguments]:\n\"\"\"Fixture used to dinamically generate anomaly dataset. By default images are random grayscales with size 10x10.\n\n    Args:\n        tmp_path: path to temporary directory\n        dataset_arguments: dataset arguments\n\n    Returns:\n        path to anomaly dataset\n    \"\"\"\n    yield _build_anomaly_dataset(tmp_path, dataset_arguments)\n    if tmp_path.exists():\n        shutil.rmtree(tmp_path)\n</code></pre>"},{"location":"reference/quadra/utils/tests/fixtures/dataset/anomaly.html#quadra.utils.tests.fixtures.dataset.anomaly.base_anomaly_dataset","title":"<code>base_anomaly_dataset(tmp_path, request)</code>","text":"Generate base anomaly dataset with the following parameters <ul> <li>train_samples: 10</li> <li>val_samples: (10, 10)</li> <li>test_samples: (10, 10).</li> </ul> <p>Parameters:</p> <ul> <li> tmp_path             (<code>Path</code>)         \u2013          <p>Path to temporary directory</p> </li> <li> request             (<code>Any</code>)         \u2013          <p>Pytest SubRequest object</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>tuple[str, AnomalyDatasetArguments]</code>         \u2013          <p>Path to anomaly dataset and dataset arguments</p> </li> </ul> Source code in <code>quadra/utils/tests/fixtures/dataset/anomaly.py</code> <pre><code>@pytest.fixture(\n    params=[AnomalyDatasetArguments(**{\"train_samples\": 10, \"val_samples\": (1, 1), \"test_samples\": (1, 1)})]\n)\ndef base_anomaly_dataset(tmp_path: Path, request: Any) -&gt; tuple[str, AnomalyDatasetArguments]:\n\"\"\"Generate base anomaly dataset with the following parameters:\n        - train_samples: 10\n        - val_samples: (10, 10)\n        - test_samples: (10, 10).\n\n    Args:\n        tmp_path: Path to temporary directory\n        request: Pytest SubRequest object\n\n    Yields:\n        Path to anomaly dataset and dataset arguments\n    \"\"\"\n    yield _build_anomaly_dataset(tmp_path, request.param)\n    if tmp_path.exists():\n        shutil.rmtree(tmp_path)\n</code></pre>"},{"location":"reference/quadra/utils/tests/fixtures/dataset/classification.html","title":"classification","text":""},{"location":"reference/quadra/utils/tests/fixtures/dataset/classification.html#quadra.utils.tests.fixtures.dataset.classification.ClassificationDatasetArguments","title":"<code>ClassificationDatasetArguments</code>  <code>dataclass</code>","text":"<p>Classification dataset arguments.</p> <p>Parameters:</p> <ul> <li> samples             (<code>list[int]</code>)         \u2013          <p>number of samples per class</p> </li> <li> classes             (<code>list[str] | None</code>, default:                 <code>None</code> )         \u2013          <p>class names, if set it must be the same length as samples</p> </li> <li> val_size             (<code>float | None</code>, default:                 <code>None</code> )         \u2013          <p>validation set size</p> </li> <li> test_size             (<code>float | None</code>, default:                 <code>None</code> )         \u2013          <p>test set size</p> </li> </ul>"},{"location":"reference/quadra/utils/tests/fixtures/dataset/classification.html#quadra.utils.tests.fixtures.dataset.classification.ClassificationMultilabelDatasetArguments","title":"<code>ClassificationMultilabelDatasetArguments</code>  <code>dataclass</code>","text":"<p>Classification dataset arguments.</p> <p>Parameters:</p> <ul> <li> samples             (<code>list[int]</code>)         \u2013          <p>number of samples per class</p> </li> <li> classes             (<code>list[str] | None</code>, default:                 <code>None</code> )         \u2013          <p>class names, if set it must be the same length as samples</p> </li> <li> val_size             (<code>float | None</code>, default:                 <code>None</code> )         \u2013          <p>validation set size</p> </li> <li> test_size             (<code>float | None</code>, default:                 <code>None</code> )         \u2013          <p>test set size</p> </li> <li> percentage_other_classes             (<code>float | None</code>, default:                 <code>0.0</code> )         \u2013          <p>probability of adding other classes to the labels of each sample</p> </li> </ul>"},{"location":"reference/quadra/utils/tests/fixtures/dataset/classification.html#quadra.utils.tests.fixtures.dataset.classification.ClassificationPatchDatasetArguments","title":"<code>ClassificationPatchDatasetArguments</code>  <code>dataclass</code>","text":"<p>Classification patch dataset arguments.</p> <p>Parameters:</p> <ul> <li> samples             (<code>list[int]</code>)         \u2013          <p>number of samples per class</p> </li> <li> overlap             (<code>float</code>)         \u2013          <p>overlap between patches</p> </li> <li> patch_size             (<code>tuple[int, int] | None</code>, default:                 <code>None</code> )         \u2013          <p>patch size</p> </li> <li> patch_number             (<code>tuple[int, int] | None</code>, default:                 <code>None</code> )         \u2013          <p>number of patches</p> </li> <li> classes             (<code>list[str] | None</code>, default:                 <code>None</code> )         \u2013          <p>class names, if set it must be the same length as samples</p> </li> <li> val_size             (<code>float | None</code>, default:                 <code>0.0</code> )         \u2013          <p>validation set size</p> </li> <li> test_size             (<code>float | None</code>, default:                 <code>0.0</code> )         \u2013          <p>test set size</p> </li> <li> annotated_good             (<code>list[str] | None</code>, default:                 <code>None</code> )         \u2013          <p>list of class names that are considered as good annotations (E.g. [\"good\"])</p> </li> </ul>"},{"location":"reference/quadra/utils/tests/fixtures/dataset/classification.html#quadra.utils.tests.fixtures.dataset.classification.base_classification_dataset","title":"<code>base_classification_dataset(tmp_path, request)</code>","text":"Generate base classification dataset with the following parameters <ul> <li>10 samples per class</li> <li>2 classes (class_1 and class_2) By default generated images are grayscale and 10x10 pixels.</li> </ul> <p>Parameters:</p> <ul> <li> tmp_path             (<code>Path</code>)         \u2013          <p>path to temporary directory</p> </li> <li> request             (<code>Any</code>)         \u2013          <p>pytest request</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>tuple[str, ClassificationDatasetArguments]</code>         \u2013          <p>Tuple containing path to created dataset and dataset arguments</p> </li> </ul> Source code in <code>quadra/utils/tests/fixtures/dataset/classification.py</code> <pre><code>@pytest.fixture(\n    params=[\n        ClassificationDatasetArguments(\n            **{\"samples\": [10, 10], \"classes\": [\"class_1\", \"class_2\"], \"val_size\": 0.1, \"test_size\": 0.1}\n        )\n    ]\n)\ndef base_classification_dataset(tmp_path: Path, request: Any) -&gt; tuple[str, ClassificationDatasetArguments]:\n\"\"\"Generate base classification dataset with the following parameters:\n        - 10 samples per class\n        - 2 classes (class_1 and class_2)\n        By default generated images are grayscale and 10x10 pixels.\n\n    Args:\n        tmp_path: path to temporary directory\n        request: pytest request\n\n    Yields:\n        Tuple containing path to created dataset and dataset arguments\n    \"\"\"\n    yield _build_classification_dataset(tmp_path, request.param)\n    if tmp_path.exists():\n        shutil.rmtree(tmp_path)\n</code></pre>"},{"location":"reference/quadra/utils/tests/fixtures/dataset/classification.html#quadra.utils.tests.fixtures.dataset.classification.base_multilabel_classification_dataset","title":"<code>base_multilabel_classification_dataset(tmp_path, request)</code>","text":"Fixture to generate base multilabel classification dataset with the following parameters <ul> <li>10 samples per class</li> <li>3 classes (class_1, class_2 and class_3)</li> <li>10% of samples in validation set</li> <li>10% of samples in test set</li> <li>30% of possibility to add each other class to the sample By default generated images are grayscale and 10x10 pixels.</li> </ul> <p>Parameters:</p> <ul> <li> tmp_path             (<code>Path</code>)         \u2013          <p>path to temporary directory</p> </li> <li> request             (<code>Any</code>)         \u2013          <p>pytest request</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>tuple[str, ClassificationMultilabelDatasetArguments]</code>         \u2013          <p>Tuple containing path to created dataset and dataset arguments</p> </li> </ul> Source code in <code>quadra/utils/tests/fixtures/dataset/classification.py</code> <pre><code>@pytest.fixture(\n    params=[\n        ClassificationMultilabelDatasetArguments(\n            **{\n                \"samples\": [10, 10, 10],\n                \"classes\": [\"class_1\", \"class_2\", \"class_3\"],\n                \"val_size\": 0.1,\n                \"test_size\": 0.1,\n                \"percentage_other_classes\": 0.3,\n            }\n        )\n    ]\n)\ndef base_multilabel_classification_dataset(\n    tmp_path: Path, request: Any\n) -&gt; tuple[str, ClassificationMultilabelDatasetArguments]:\n\"\"\"Fixture to generate base multilabel classification dataset with the following parameters:\n        - 10 samples per class\n        - 3 classes (class_1, class_2 and class_3)\n        - 10% of samples in validation set\n        - 10% of samples in test set\n        - 30% of possibility to add each other class to the sample\n        By default generated images are grayscale and 10x10 pixels.\n\n    Args:\n        tmp_path: path to temporary directory\n        request: pytest request\n\n    Yields:\n        Tuple containing path to created dataset and dataset arguments\n    \"\"\"\n    yield _build_multilabel_classification_dataset(tmp_path, request.param)\n    if tmp_path.exists():\n        shutil.rmtree(tmp_path)\n</code></pre>"},{"location":"reference/quadra/utils/tests/fixtures/dataset/classification.html#quadra.utils.tests.fixtures.dataset.classification.base_patch_classification_dataset","title":"<code>base_patch_classification_dataset(tmp_path, request)</code>","text":"Generate a classification patch dataset with the following parameters <ul> <li>3 classes named bg, a and b</li> <li>5, 5 and 5 samples for each class</li> <li>2 horizontal patches and 2 vertical patches</li> <li>0% overlap</li> <li>10% validation set</li> <li>10% test set.</li> </ul> <p>Parameters:</p> <ul> <li> tmp_path             (<code>Path</code>)         \u2013          <p>path to temporary directory</p> </li> <li> request             (<code>Any</code>)         \u2013          <p>pytest SubRequest object</p> </li> </ul> Source code in <code>quadra/utils/tests/fixtures/dataset/classification.py</code> <pre><code>@pytest.fixture(\n    params=[\n        ClassificationPatchDatasetArguments(\n            **{\n                \"samples\": [5, 5, 5],\n                \"classes\": [\"bg\", \"a\", \"b\"],\n                \"patch_number\": [2, 2],\n                \"overlap\": 0,\n                \"val_size\": 0.1,\n                \"test_size\": 0.1,\n            }\n        )\n    ]\n)\ndef base_patch_classification_dataset(\n    tmp_path: Path, request: Any\n) -&gt; tuple[str, ClassificationDatasetArguments, dict[str, int]]:\n\"\"\"Generate a classification patch dataset with the following parameters:\n        - 3 classes named bg, a and b\n        - 5, 5 and 5 samples for each class\n        - 2 horizontal patches and 2 vertical patches\n        - 0% overlap\n        - 10% validation set\n        - 10% test set.\n\n    Args:\n        tmp_path: path to temporary directory\n        request: pytest SubRequest object\n    \"\"\"\n    yield _build_classification_patch_dataset(tmp_path, request.param)\n    if tmp_path.exists():\n        shutil.rmtree(tmp_path)\n</code></pre>"},{"location":"reference/quadra/utils/tests/fixtures/dataset/classification.html#quadra.utils.tests.fixtures.dataset.classification.classification_dataset","title":"<code>classification_dataset(tmp_path, dataset_arguments)</code>","text":"<p>Generate classification dataset. If val_size or test_size are set, it will generate a train.txt, val.txt and     test.txt file in the dataset directory. By default generated images are 10x10 pixels.</p> <p>Parameters:</p> <ul> <li> tmp_path             (<code>Path</code>)         \u2013          <p>path to temporary directory</p> </li> <li> dataset_arguments             (<code>ClassificationDatasetArguments</code>)         \u2013          <p>dataset arguments</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>tuple[str, ClassificationDatasetArguments]</code>         \u2013          <p>Tuple containing path to created dataset and dataset arguments</p> </li> </ul> Source code in <code>quadra/utils/tests/fixtures/dataset/classification.py</code> <pre><code>@pytest.fixture\ndef classification_dataset(\n    tmp_path: Path, dataset_arguments: ClassificationDatasetArguments\n) -&gt; tuple[str, ClassificationDatasetArguments]:\n\"\"\"Generate classification dataset. If val_size or test_size are set, it will generate a train.txt, val.txt and\n        test.txt file in the dataset directory. By default generated images are 10x10 pixels.\n\n    Args:\n        tmp_path: path to temporary directory\n        dataset_arguments: dataset arguments\n\n    Yields:\n        Tuple containing path to created dataset and dataset arguments\n    \"\"\"\n    yield _build_classification_dataset(tmp_path, dataset_arguments)\n    if tmp_path.exists():\n        shutil.rmtree(tmp_path)\n</code></pre>"},{"location":"reference/quadra/utils/tests/fixtures/dataset/classification.html#quadra.utils.tests.fixtures.dataset.classification.classification_patch_dataset","title":"<code>classification_patch_dataset(tmp_path, dataset_arguments)</code>","text":"<p>Fixture to dinamically generate a classification patch dataset.</p> <pre><code>By default generated images are 224x224 pixels\nand associated masks contains a 50x50 pixels square with the corresponding image class, so at the current stage\nis not possible to have images with multiple annotations. The patch dataset will be generated using the standard\nparameters of generate_patch_dataset function.\n</code></pre> <p>Parameters:</p> <ul> <li> tmp_path             (<code>Path</code>)         \u2013          <p>path to temporary directory</p> </li> <li> dataset_arguments             (<code>ClassificationDatasetArguments</code>)         \u2013          <p>dataset arguments</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>tuple[str, ClassificationDatasetArguments, dict[str, int]]</code>         \u2013          <p>Tuple containing path to created dataset, dataset arguments and class to index mapping</p> </li> </ul> Source code in <code>quadra/utils/tests/fixtures/dataset/classification.py</code> <pre><code>@pytest.fixture\ndef classification_patch_dataset(\n    tmp_path: Path, dataset_arguments: ClassificationDatasetArguments\n) -&gt; tuple[str, ClassificationDatasetArguments, dict[str, int]]:\n\"\"\"Fixture to dinamically generate a classification patch dataset.\n\n        By default generated images are 224x224 pixels\n        and associated masks contains a 50x50 pixels square with the corresponding image class, so at the current stage\n        is not possible to have images with multiple annotations. The patch dataset will be generated using the standard\n        parameters of generate_patch_dataset function.\n\n    Args:\n        tmp_path: path to temporary directory\n        dataset_arguments: dataset arguments\n\n    Yields:\n        Tuple containing path to created dataset, dataset arguments and class to index mapping\n    \"\"\"\n    yield _build_classification_patch_dataset(tmp_path, dataset_arguments)\n    if tmp_path.exists():\n        shutil.rmtree(tmp_path)\n</code></pre>"},{"location":"reference/quadra/utils/tests/fixtures/dataset/classification.html#quadra.utils.tests.fixtures.dataset.classification.multilabel_classification_dataset","title":"<code>multilabel_classification_dataset(tmp_path, dataset_arguments)</code>","text":"<p>Fixture to dinamically generate a multilabel classification dataset.     Generates a samples.txt file in the dataset directory containing the path to the image and the corresponding     classes. If val_size or test_size are set, it will generate a train.txt, val.txt and test.txt file in the     dataset directory. By default generated images are 10x10 pixels.</p> <p>Parameters:</p> <ul> <li> tmp_path             (<code>Path</code>)         \u2013          <p>path to temporary directory</p> </li> <li> dataset_arguments             (<code>ClassificationMultilabelDatasetArguments</code>)         \u2013          <p>dataset arguments</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[str, ClassificationMultilabelDatasetArguments]</code>         \u2013          <p>Tuple containing path to created dataset and dataset arguments</p> </li> </ul> Source code in <code>quadra/utils/tests/fixtures/dataset/classification.py</code> <pre><code>@pytest.fixture\ndef multilabel_classification_dataset(\n    tmp_path: Path, dataset_arguments: ClassificationMultilabelDatasetArguments\n) -&gt; tuple[str, ClassificationMultilabelDatasetArguments]:\n\"\"\"Fixture to dinamically generate a multilabel classification dataset.\n        Generates a samples.txt file in the dataset directory containing the path to the image and the corresponding\n        classes. If val_size or test_size are set, it will generate a train.txt, val.txt and test.txt file in the\n        dataset directory. By default generated images are 10x10 pixels.\n\n    Args:\n        tmp_path: path to temporary directory\n        dataset_arguments: dataset arguments\n\n    Returns:\n        Tuple containing path to created dataset and dataset arguments\n    \"\"\"\n    yield _build_multilabel_classification_dataset(tmp_path, dataset_arguments)\n    if tmp_path.exists():\n        shutil.rmtree(tmp_path)\n</code></pre>"},{"location":"reference/quadra/utils/tests/fixtures/dataset/imagenette.html","title":"imagenette","text":""},{"location":"reference/quadra/utils/tests/fixtures/dataset/imagenette.html#quadra.utils.tests.fixtures.dataset.imagenette.imagenette_dataset","title":"<code>imagenette_dataset(tmp_path)</code>","text":"<p>Generate a mock imagenette dataset to test efficient_ad model.</p> <p>Parameters:</p> <ul> <li> tmp_path             (<code>Path</code>)         \u2013          <p>Path to temporary directory</p> </li> <li> request         \u2013          <p>Pytest SubRequest object</p> </li> </ul> Source code in <code>quadra/utils/tests/fixtures/dataset/imagenette.py</code> <pre><code>@pytest.fixture\ndef imagenette_dataset(tmp_path: Path) -&gt; str:\n\"\"\"Generate a mock imagenette dataset to test efficient_ad model.\n\n    Args:\n        tmp_path: Path to temporary directory\n        request: Pytest SubRequest object\n    Yields:\n        Path to imagenette dataset folder\n    \"\"\"\n    yield _build_imagenette_dataset(tmp_path, classes=3, class_samples=3)\n\n    if tmp_path.exists():\n        shutil.rmtree(tmp_path)\n</code></pre>"},{"location":"reference/quadra/utils/tests/fixtures/dataset/segmentation.html","title":"segmentation","text":""},{"location":"reference/quadra/utils/tests/fixtures/dataset/segmentation.html#quadra.utils.tests.fixtures.dataset.segmentation.SegmentationDatasetArguments","title":"<code>SegmentationDatasetArguments</code>  <code>dataclass</code>","text":"<p>Segmentation dataset arguments.</p> <p>Parameters:</p> <ul> <li> train_samples             (<code>list[int]</code>)         \u2013          <p>List of samples per class in train set, element at index 0 are good samples</p> </li> <li> val_samples             (<code>list[int] | None</code>, default:                 <code>None</code> )         \u2013          <p>List of samples per class in validation set, same as above.</p> </li> <li> test_samples             (<code>list[int] | None</code>, default:                 <code>None</code> )         \u2013          <p>List of samples per class in test set, same as above.</p> </li> <li> classes             (<code>list[str] | None</code>, default:                 <code>None</code> )         \u2013          <p>Optional list of class names, must be equal to len(train_samples) - 1</p> </li> </ul>"},{"location":"reference/quadra/utils/tests/fixtures/dataset/segmentation.html#quadra.utils.tests.fixtures.dataset.segmentation.base_binary_segmentation_dataset","title":"<code>base_binary_segmentation_dataset(tmp_path, request)</code>","text":"Generate a base binary segmentation dataset with the following structure <ul> <li>3 good and 2 bad samples in train set</li> <li>2 good and 2 bad samples in validation set</li> <li>11 good and 1 bad sample in test set</li> <li>2 classes: good and bad.</li> </ul> <p>Parameters:</p> <ul> <li> tmp_path             (<code>Path</code>)         \u2013          <p>path to temporary directory</p> </li> <li> request             (<code>Any</code>)         \u2013          <p>pytest request</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>tuple[str, SegmentationDatasetArguments, dict[str, int]]</code>         \u2013          <p>Tuple containing path to dataset, dataset arguments and class to index mapping</p> </li> </ul> Source code in <code>quadra/utils/tests/fixtures/dataset/segmentation.py</code> <pre><code>@pytest.fixture(\n    params=[\n        SegmentationDatasetArguments(\n            **{\"train_samples\": [3, 2], \"val_samples\": [2, 2], \"test_samples\": [1, 1], \"classes\": [\"bad\"]}\n        )\n    ]\n)\ndef base_binary_segmentation_dataset(\n    tmp_path: Path, request: Any\n) -&gt; tuple[str, SegmentationDatasetArguments, dict[str, int]]:\n\"\"\"Generate a base binary segmentation dataset with the following structure:\n        - 3 good and 2 bad samples in train set\n        - 2 good and 2 bad samples in validation set\n        - 11 good and 1 bad sample in test set\n        - 2 classes: good and bad.\n\n    Args:\n        tmp_path: path to temporary directory\n        request: pytest request\n\n    Yields:\n        Tuple containing path to dataset, dataset arguments and class to index mapping\n    \"\"\"\n    yield _build_segmentation_dataset(tmp_path, request.param)\n    if tmp_path.exists():\n        shutil.rmtree(tmp_path)\n</code></pre>"},{"location":"reference/quadra/utils/tests/fixtures/dataset/segmentation.html#quadra.utils.tests.fixtures.dataset.segmentation.base_multiclass_segmentation_dataset","title":"<code>base_multiclass_segmentation_dataset(tmp_path, request)</code>","text":"Generate a base binary segmentation dataset with the following structure <ul> <li>2 good, 2 defect_1 and 2 defect_2 samples in train set</li> <li>2 good, 2 defect_1 and 2 defect_2 samples in validation set</li> <li>1 good, 1 defect_1 and 1 defect_2 sample in test set</li> <li>3 classes: good, defect_1 and defect_2.</li> </ul> <p>Parameters:</p> <ul> <li> tmp_path             (<code>Path</code>)         \u2013          <p>path to temporary directory</p> </li> <li> request             (<code>Any</code>)         \u2013          <p>pytest request</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>tuple[str, SegmentationDatasetArguments, dict[str, int]]</code>         \u2013          <p>Tuple containing path to dataset, dataset arguments and class to index mapping</p> </li> </ul> Source code in <code>quadra/utils/tests/fixtures/dataset/segmentation.py</code> <pre><code>@pytest.fixture(\n    params=[\n        SegmentationDatasetArguments(\n            **{\n                \"train_samples\": [2, 2, 2],\n                \"val_samples\": [2, 2, 2],\n                \"test_samples\": [1, 1, 1],\n                \"classes\": [\"defect_1\", \"defect_2\"],\n            }\n        )\n    ]\n)\ndef base_multiclass_segmentation_dataset(\n    tmp_path: Path, request: Any\n) -&gt; tuple[str, SegmentationDatasetArguments, dict[str, int]]:\n\"\"\"Generate a base binary segmentation dataset with the following structure:\n        - 2 good, 2 defect_1 and 2 defect_2 samples in train set\n        - 2 good, 2 defect_1 and 2 defect_2 samples in validation set\n        - 1 good, 1 defect_1 and 1 defect_2 sample in test set\n        - 3 classes: good, defect_1 and defect_2.\n\n    Args:\n        tmp_path: path to temporary directory\n        request: pytest request\n\n    Yields:\n        Tuple containing path to dataset, dataset arguments and class to index mapping\n    \"\"\"\n    yield _build_segmentation_dataset(tmp_path, request.param)\n    if tmp_path.exists():\n        shutil.rmtree(tmp_path)\n</code></pre>"},{"location":"reference/quadra/utils/tests/fixtures/dataset/segmentation.html#quadra.utils.tests.fixtures.dataset.segmentation.segmentation_dataset","title":"<code>segmentation_dataset(tmp_path, dataset_arguments)</code>","text":"<p>Fixture to dinamically generate a segmentation dataset. By default generated images are 224x224 pixels     and associated masks contains a 50x50 pixels square with the corresponding image class, so at the current stage     is not possible to have images with multiple annotations. Split files are saved as train.txt,     val.txt and test.txt.</p> <p>Parameters:</p> <ul> <li> tmp_path             (<code>Path</code>)         \u2013          <p>path to temporary directory</p> </li> <li> dataset_arguments             (<code>SegmentationDatasetArguments</code>)         \u2013          <p>dataset arguments</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>tuple[str, SegmentationDatasetArguments, dict[str, int]]</code>         \u2013          <p>Tuple containing path to dataset, dataset arguments and class to index mapping</p> </li> </ul> Source code in <code>quadra/utils/tests/fixtures/dataset/segmentation.py</code> <pre><code>@pytest.fixture\ndef segmentation_dataset(\n    tmp_path: Path, dataset_arguments: SegmentationDatasetArguments\n) -&gt; tuple[str, SegmentationDatasetArguments, dict[str, int]]:\n\"\"\"Fixture to dinamically generate a segmentation dataset. By default generated images are 224x224 pixels\n        and associated masks contains a 50x50 pixels square with the corresponding image class, so at the current stage\n        is not possible to have images with multiple annotations. Split files are saved as train.txt,\n        val.txt and test.txt.\n\n    Args:\n        tmp_path: path to temporary directory\n        dataset_arguments: dataset arguments\n\n    Yields:\n        Tuple containing path to dataset, dataset arguments and class to index mapping\n    \"\"\"\n    yield _build_segmentation_dataset(tmp_path, dataset_arguments)\n    if tmp_path.exists():\n        shutil.rmtree(tmp_path)\n</code></pre>"},{"location":"reference/quadra/utils/tests/fixtures/models/index.html","title":"models","text":""},{"location":"reference/quadra/utils/tests/fixtures/models/index.html#python-files","title":"Python Files","text":"<ul> <li>anomaly.py</li> <li>segmentation.py</li> <li>classification.py </li> </ul>"},{"location":"reference/quadra/utils/tests/fixtures/models/anomaly.html","title":"anomaly","text":""},{"location":"reference/quadra/utils/tests/fixtures/models/anomaly.html#quadra.utils.tests.fixtures.models.anomaly.draem","title":"<code>draem()</code>","text":"<p>Yield a draem model.</p> Source code in <code>quadra/utils/tests/fixtures/models/anomaly.py</code> <pre><code>@pytest.fixture\ndef draem():\n\"\"\"Yield a draem model.\"\"\"\n    yield DraemModel()\n</code></pre>"},{"location":"reference/quadra/utils/tests/fixtures/models/anomaly.html#quadra.utils.tests.fixtures.models.anomaly.efficient_ad_small","title":"<code>efficient_ad_small()</code>","text":"<p>Yield a draem model.</p> Source code in <code>quadra/utils/tests/fixtures/models/anomaly.py</code> <pre><code>@pytest.fixture\ndef efficient_ad_small():\n\"\"\"Yield a draem model.\"\"\"\n\n    class EfficientAdForwardWrapper(EfficientAdModel):\n\"\"\"Wrap the forward method to avoid passing optional parameters.\"\"\"\n\n        def forward(self, x):\n            return super().forward(x, None)\n\n    model = EfficientAdForwardWrapper(\n        teacher_out_channels=384,\n        input_size=[256, 256],  # TODO: This is hardcoded may be not a good idea\n        pretrained_teacher_type=\"nelson\",\n    )\n\n    yield model\n</code></pre>"},{"location":"reference/quadra/utils/tests/fixtures/models/anomaly.html#quadra.utils.tests.fixtures.models.anomaly.padim_resnet18","title":"<code>padim_resnet18()</code>","text":"<p>Yield a padim model with resnet18 encoder.</p> Source code in <code>quadra/utils/tests/fixtures/models/anomaly.py</code> <pre><code>@pytest.fixture\ndef padim_resnet18():\n\"\"\"Yield a padim model with resnet18 encoder.\"\"\"\n    yield PadimModel(\n        input_size=[224, 224],  # TODO: This is hardcoded may be not a good idea\n        backbone=\"resnet18\",\n        layers=[\"layer1\", \"layer2\", \"layer3\"],\n        pretrained_weights=None,\n        tied_covariance=False,\n        pre_trained=False,\n    )\n</code></pre>"},{"location":"reference/quadra/utils/tests/fixtures/models/anomaly.html#quadra.utils.tests.fixtures.models.anomaly.patchcore_resnet18","title":"<code>patchcore_resnet18()</code>","text":"<p>Yield a patchcore model with resnet18 encoder.</p> Source code in <code>quadra/utils/tests/fixtures/models/anomaly.py</code> <pre><code>@pytest.fixture\ndef patchcore_resnet18():\n\"\"\"Yield a patchcore model with resnet18 encoder.\"\"\"\n    model = PatchcoreModel(\n        input_size=[224, 224],  # TODO: This is hardcoded may be not a good idea\n        backbone=\"resnet18\",\n        layers=[\"layer2\", \"layer3\"],\n        pre_trained=False,\n    )\n\n    yield _initialize_patchcore_model(model)\n</code></pre>"},{"location":"reference/quadra/utils/tests/fixtures/models/classification.html","title":"classification","text":""},{"location":"reference/quadra/utils/tests/fixtures/models/classification.html#quadra.utils.tests.fixtures.models.classification.dino_vitb8","title":"<code>dino_vitb8()</code>","text":"<p>Yield a dino_vitb8 model.</p> Source code in <code>quadra/utils/tests/fixtures/models/classification.py</code> <pre><code>@pytest.fixture\ndef dino_vitb8():\n\"\"\"Yield a dino_vitb8 model.\"\"\"\n    yield TorchHubNetworkBuilder(\n        repo_or_dir=\"facebookresearch/dino:main\",\n        model_name=\"dino_vitb8\",\n        pretrained=False,\n        freeze=True,\n        exportable=True,\n    )\n</code></pre>"},{"location":"reference/quadra/utils/tests/fixtures/models/classification.html#quadra.utils.tests.fixtures.models.classification.dino_vits8","title":"<code>dino_vits8()</code>","text":"<p>Yield a dino_vits8 model.</p> Source code in <code>quadra/utils/tests/fixtures/models/classification.py</code> <pre><code>@pytest.fixture\ndef dino_vits8():\n\"\"\"Yield a dino_vits8 model.\"\"\"\n    yield TorchHubNetworkBuilder(\n        repo_or_dir=\"facebookresearch/dino:main\",\n        model_name=\"dino_vits8\",\n        pretrained=False,\n        freeze=True,\n        exportable=True,\n    )\n</code></pre>"},{"location":"reference/quadra/utils/tests/fixtures/models/classification.html#quadra.utils.tests.fixtures.models.classification.resnet18","title":"<code>resnet18()</code>","text":"<p>Yield a resnet18 model.</p> Source code in <code>quadra/utils/tests/fixtures/models/classification.py</code> <pre><code>@pytest.fixture\ndef resnet18():\n\"\"\"Yield a resnet18 model.\"\"\"\n    yield TimmNetworkBuilder(\"resnet18\", pretrained=False, freeze=True, exportable=True)\n</code></pre>"},{"location":"reference/quadra/utils/tests/fixtures/models/classification.html#quadra.utils.tests.fixtures.models.classification.resnet50","title":"<code>resnet50()</code>","text":"<p>Yield a resnet50 model.</p> Source code in <code>quadra/utils/tests/fixtures/models/classification.py</code> <pre><code>@pytest.fixture\ndef resnet50():\n\"\"\"Yield a resnet50 model.\"\"\"\n    yield TimmNetworkBuilder(\"resnet50\", pretrained=False, freeze=True, exportable=True)\n</code></pre>"},{"location":"reference/quadra/utils/tests/fixtures/models/classification.html#quadra.utils.tests.fixtures.models.classification.vit_tiny_patch16_224","title":"<code>vit_tiny_patch16_224()</code>","text":"<p>Yield a vit_tiny_patch16_224 model.</p> Source code in <code>quadra/utils/tests/fixtures/models/classification.py</code> <pre><code>@pytest.fixture\ndef vit_tiny_patch16_224():\n\"\"\"Yield a vit_tiny_patch16_224 model.\"\"\"\n    yield TimmNetworkBuilder(\"vit_tiny_patch16_224\", pretrained=False, freeze=True, exportable=True)\n</code></pre>"},{"location":"reference/quadra/utils/tests/fixtures/models/segmentation.html","title":"segmentation","text":""},{"location":"reference/quadra/utils/tests/fixtures/models/segmentation.html#quadra.utils.tests.fixtures.models.segmentation.smp_resnet18_unet","title":"<code>smp_resnet18_unet()</code>","text":"<p>Yield a unet with resnet18 encoder.</p> Source code in <code>quadra/utils/tests/fixtures/models/segmentation.py</code> <pre><code>@pytest.fixture\ndef smp_resnet18_unet():\n\"\"\"Yield a unet with resnet18 encoder.\"\"\"\n    yield create_smp_backbone(\n        arch=\"unet\",\n        encoder_name=\"resnet18\",\n        encoder_weights=None,\n        encoder_depth=5,\n        freeze_encoder=True,\n        in_channels=3,\n        num_classes=1,\n        activation=None,\n    )\n</code></pre>"},{"location":"reference/quadra/utils/tests/fixtures/models/segmentation.html#quadra.utils.tests.fixtures.models.segmentation.smp_resnet18_unetplusplus","title":"<code>smp_resnet18_unetplusplus()</code>","text":"<p>Yield a unetplusplus with resnet18 encoder.</p> Source code in <code>quadra/utils/tests/fixtures/models/segmentation.py</code> <pre><code>@pytest.fixture\ndef smp_resnet18_unetplusplus():\n\"\"\"Yield a unetplusplus with resnet18 encoder.\"\"\"\n    yield create_smp_backbone(\n        arch=\"unetplusplus\",\n        encoder_name=\"resnet18\",\n        encoder_weights=None,\n        encoder_depth=5,\n        freeze_encoder=True,\n        in_channels=3,\n        num_classes=1,\n        activation=None,\n    )\n</code></pre>"},{"location":"tutorials/configurations.html","title":"Configuration","text":"<p>This section explains how the configuration files and folders are structured. It will help you understand how to use and add new configuration files, and where to add them.</p> <p>Warning</p> <p>Configuration files heavily depend on the <code>hydra</code> library. If you are not familiar with <code>hydra</code>, you are strongly advised to read their documentation before using this library.</p>"},{"location":"tutorials/configurations.html#parent-folder-structure","title":"Parent Folder Structure","text":"<p>Quadra configurations are divided into macro-categories. These categories can be found under the <code>configs</code> folder. The structure of the <code>configs</code> folder is the following:</p> <pre><code>configs/\n\u251c\u2500\u2500 backbone\n\u251c\u2500\u2500 callbacks\n\u251c\u2500\u2500 core\n\u251c\u2500\u2500 datamodule\n\u251c\u2500\u2500 experiment\n\u251c\u2500\u2500 hydra\n\u251c\u2500\u2500 logger\n\u251c\u2500\u2500 loss\n\u251c\u2500\u2500 model\n\u251c\u2500\u2500 optimizer\n\u251c\u2500\u2500 scheduler\n\u251c\u2500\u2500 task\n\u251c\u2500\u2500 trainer\n\u2514\u2500\u2500 transforms\n</code></pre>"},{"location":"tutorials/configurations.html#config-folders","title":"Config Folders","text":"<p>In this section, we will explain the structure of the config folders. Each folder contains a set of config files or subfolders.</p>"},{"location":"tutorials/configurations.html#backbone","title":"Backbone","text":"<p>Backbones are the <code>torch.nn.Module</code> objects that are used in experiments, generally for feature extraction. Lets Have a look at one example:</p> <pre><code>#configs/backbone/dino_vitb8.yaml\nmodel:\n_target_: quadra.models.classification.TorchHubNetworkBuilder\nrepo_or_dir: facebookresearch/dino:main\nmodel_name: dino_vitb8\npretrained: true\nfreeze: false\nhyperspherical: false\nmetadata:\ninput_size: 224\noutput_dim: 768\npatch_size: 8\nnb_heads: 12\n</code></pre> <ul> <li>model: This is the object to instantiate. In this case, it will load a <code>VitB-8</code> pretrained model from the torch hub which will be wrapped in a <code>TorchHubNetworkBuilder</code> adapter class to make it compatible with our framework.</li> <li>metadata: This is the object where we store all the parameters related to model. It generally contains useful information for the model, such as input size, output dimension, patch size (for transformers), etc.</li> </ul> <p>Under backbones we may have other kind of models which may not necessary be related to Pytorch, but so far all the implemented tasks are based on Pytorch.</p>"},{"location":"tutorials/configurations.html#callbacks","title":"Callbacks","text":"<p>In this folder we will store all the callbacks that are used in experiments. These callbacks are passed to pytorch-lightning as <code>trainer.callbacks</code>.</p>"},{"location":"tutorials/configurations.html#core","title":"Core","text":"<p>Core files contain some global settings for experiments. The default config is the following:</p> <p><pre><code>version:\n_target_: quadra.get_version\nseed: 42\ntag: null\nname: null\ncv2_num_threads: 1\ncommand: \"python \"\nexperiment_path: null\nupload_artifacts: False\nlog_level: info\n</code></pre> For example we can set the seed, decide for a run name, or set the log level.</p>"},{"location":"tutorials/configurations.html#datamodule","title":"Datamodule","text":"<p>Datamodule setting files are used to configure the datamodule which manages the datasets and dataloaders used in experiment. For a detailed explanation on how to implement <code>DataModule</code> classes, please refer to the datamodule documentation.</p> <p>Here is the structure of the folder:</p> <pre><code>datamodule/\n\u251c\u2500\u2500 base\n\u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 generic\n    \u2514\u2500\u2500 ...\n</code></pre> <p>Right now we provide two types of configurations:</p> <ul> <li>base: These are the default configurations for all the experiments. Standard experiments are using this configuration to initialize the datamodules.</li> <li>generic: These configurations are used to define the datamodules for the generic tasks, which are tasks used to provide examples of how to use the framework.</li> </ul> <p>Let's have a look at one example (<code>datamodule/base/anomaly.yaml</code>) configuring the datamodule for anomaly detection:</p> <pre><code># DataModule class to instantiate\n_target_: quadra.datamodules.AnomalyDataModule\n# When we find ??? it means that the value must be provided by the user\ndata_path: ???\ncategory:\nnum_workers: 8\ntrain_batch_size: 32\ntest_batch_size: 32\n# We can use hydra interpolation to define the value of a variable based on another variable\nseed: ${core.seed}\ntrain_transform: ${transforms.train_transform}\ntest_transform: ${transforms.test_transform}\nval_transform: ${transforms.val_transform}\nphase: train\nvalid_area_mask:\ncrop_area:\n</code></pre> <p>In this case we have all the parameters to instantiate an <code>AnomalyDataModule</code> class. Most of the time setting the <code>data_path</code> is enough to instantiate the datamodule.</p> <p>Question</p> <p>How can I add a new dataset/datamodule?</p> <ol> <li>Check if your task is suitable for already defined Datamodule classes defined here.<ul> <li>If it is not suitable, you can create a new Datamodule class <code>&lt;your-new-task&gt;.py</code> under <code>quadra.datamodules</code>.</li> <li>If it is suitable, you don't need to add a new Datamodule class.</li> </ul> </li> <li>Add new configuration file. For example, if the task is <code>video_recognition</code>, we can create a configuration <code>video_recognition.yaml</code> in the <code>datamodule/base</code> folder. </li> </ol>"},{"location":"tutorials/configurations.html#experiment","title":"Experiment","text":"<p>The experiment files is the entry-point of the configuration. In here, we combine the different building block configurations and then we add eventual final updates or changes to them. The folder structure is as follows:</p> <pre><code>experiment/\n\u251c\u2500\u2500 base\n\u2502   \u251c\u2500\u2500 anomaly\n|   |   \u2514\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 classification\n|   |   \u2514\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 segmentation\n|   |   \u2514\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500 ssl\n|       \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 generic\n    \u2514\u2500\u2500 ...\n</code></pre> <p>Again we have a set of base experiments providing standard configuration for all the tasks and generic ones providing examples of how to use the framework. In this case experiments are divided by the task type. Let's see an example taking the base experiment for pytorch classification (<code>experiment/base/classification/classification.yaml</code>):</p> <pre><code># @package _global_\ndefaults:\n- override /backbone: resnet18\n- override /datamodule: base/classification\n- override /loss: cross_entropy\n- override /model: classification\n- override /optimizer: adam\n- override /task: classification\n- override /scheduler: rop\n- override /transforms: default_resize\n\nexport:\ntypes: [torchscript]\n\ndatamodule:\nnum_workers: 8\nbatch_size: 32\ndata_path: ???\n\nprint_config: true\n\nmodel:\nnum_classes: ???\nmodule:\nlr_scheduler_interval: \"epoch\"\n\ntask:\nlr_multiplier: 0.0\nrun_test: True\nreport: True\noutput:\nexample: True\n\ncore:\ntag: \"run\"\nupload_artifacts: true\nname: classification_base_${trainer.max_epochs}\n\nlogger:\nmlflow:\nexperiment_name: classification_base\nrun_name: ${core.name}\n\nbackbone:\nmodel:\npretrained: True\nfreeze: False\ndrop_rate: 0.1\nfreeze_parameters_name:\n- conv1\n- bn1\n- layer1\n- layer2\n\ntrainer:\nprecision: 32\nmax_epochs: 200\ncheck_val_every_n_epoch: 1\nlog_every_n_steps: 1\ndevices: [0]\n\nscheduler:\npatience: 20\nfactor: 0.9\nverbose: False\nthreshold: 0.01\n\ncallbacks:\nearly_stopping:\n_target_: pytorch_lightning.callbacks.EarlyStopping\nmonitor: val_loss_epoch\nmin_delta: 0.01\nmode: min\npatience: 35\nverbose: false\nstopping_threshold: 0\nmodel_checkpoint:\nmonitor: val_loss_epoch\n</code></pre> <p>In the experiment configuration we aggregate the various building blocks of the framework using the <code>defaults</code> key. In this case we are using the <code>classification</code> datamodule, a <code>resnet18</code> backbone, the <code>cross_entropy</code> loss, the <code>classification</code> model (Lightning Module), <code>adam</code> as optimizer, the <code>classification</code> task, the reduce on plateau (<code>rop</code>) scheduler and the <code>default_resize</code> transform.</p> <p>We can also see that we are overriding some of the parameters of the different modules. For example, we are overriding the <code>lr_scheduler_interval</code> of the backbone to be <code>step</code> instead of <code>epoch</code>. We are also overriding the <code>max_epochs</code> of the trainer to be 200 instead of the default value.</p> <p>The experiment is the most important configuration file as it is the one actually telling the framework what to do!</p> <p>Question</p> <p>How can I create a new experiment extending the default experiment configuration?</p> <p>For example, if you want to create a new pytorch classification experiment starting from the configuration above. You can create a new configuration file <code>my_custom_experiment.yaml</code> containing the following lines:</p> <pre><code># @package _global_\ndefaults:\n- base/classification/classification # extend from base classsification\n- override /datamodule: my_custom_datamodule # use custom datamodule\n- override /backbone: vit16_tiny # use a different backbone\n- override /trainer: lighting_multigpu # use a different trainer\n- _self_ # apply the rest of the configuration as final change\n\n# change the default experiment name\ncore:\nname: \"my_custom_experiment\"\n\n# use different trainer settings\ntrainer:\ndevices: [0, 1] # use 2 gpus\nmax_epochs: 1000\nnum_sanity_val_steps: 0\nprecision: 16\ncheck_val_every_n_epoch: 10\nsync_batchnorm: true\n# use other customizations\n</code></pre>"},{"location":"tutorials/configurations.html#hydra","title":"Hydra","text":"<p>These configuration files manage where and how to create folders or subfolders for experiments and other hydra related configurations.</p>"},{"location":"tutorials/configurations.html#logger","title":"Logger","text":"<p>Here we define logger classes for saving the experiment data. Most of the configurations are based on <code>pytorch_lightning.loggers</code>. Right now we support the following loggers:</p> <ul> <li>MLFlowLogger: This is the default logger. It will save the experiment data to an MLFlow server.</li> <li>TensorBoardLogger: This is a logger that will save the experiment data to a TensorBoard server.</li> <li>CSVLogger: This is a logger that will simply save the experiment data to a CSV file.</li> </ul>"},{"location":"tutorials/configurations.html#mlflow-credentials","title":"Mlflow credentials","text":"<p>The default logging backend for most of the experiments is https://mlflow.org/. To use <code>mlflow</code> you need to create a <code>.env</code> file in the main folder of your project containing the following variables:</p> <pre><code>MLFLOW_TRACKING_URI=&lt;url&gt;\nMLFLOW_S3_ENDPOINT_URL=&lt;url&gt;\nAWS_ACCESS_KEY_ID=&lt;str&gt; # Optional for artifact storage\nAWS_SECRET_ACCESS_KEY=&lt;str&gt; # Optional for artifact storage\n</code></pre> <p>Artifact storage for files such as images or models are using AWS backend at the moment. Other types of third-party <code>AWS S3</code> storage providers are also supported. This part is left to user for setting up the infrastructure.</p>"},{"location":"tutorials/configurations.html#loss","title":"Loss","text":"<p>The loss functions configurations are defined in this folder.</p>"},{"location":"tutorials/configurations.html#model","title":"Model","text":"<p>The model configurations are the settings we use to instantiate <code>Lightning Modules</code>. An example of a model configuration is the one describing a classification module (<code>model/classification.yaml</code>):</p> <pre><code>model: ${backbone.model}\nnum_classes: ???\npre_classifier: null\nclassifier:\n_target_: torch.nn.Linear\nin_features: ${backbone.metadata.output_dim}\nout_features: ${model.num_classes}\nmodule:\n_target_: quadra.modules.classification.ClassificationModule\nlr_scheduler_interval: \"step\"\ncriterion: ${loss}\ngradcam: true\n</code></pre> <p>We have four main sections in this model configuration:</p> <ul> <li><code>model</code>: This is the backbone model we use to extract features from the input data.</li> <li><code>pre_classifier</code>: This is an optional module that we can add before the classifier layer. It is useful for example to add some MLP before the actual classifier.</li> <li><code>classifier</code>: This is the classifier layer that we use to predict the label of the input data.</li> <li><code>module</code>: This is the actual <code>Lightning Module</code> that we use to train the model. It contains the loss function, the optimizer and the scheduler which will be instantiated using the configurations defined in the <code>optimizer</code>, <code>scheduler</code> and <code>loss</code> sections.</li> </ul>"},{"location":"tutorials/configurations.html#optimizer","title":"Optimizer","text":"<p>Each optimizer file defines how we initialize the training optimizers with their parameters.</p>"},{"location":"tutorials/configurations.html#scheduler","title":"Scheduler","text":"<p>Each scheduler file defines how we initialize the learning rate schedulers with their parameters.</p>"},{"location":"tutorials/configurations.html#task","title":"Task","text":"<p>The tasks are the building blocks containing the actual training and evaluation logic. They are discussed in more details in the tasks section.</p>"},{"location":"tutorials/configurations.html#trainer","title":"Trainer","text":"<p>In this folder we define the configurations of the classes used to train models, right now we support Lightning based trainers and Sklearn based trainers.</p>"},{"location":"tutorials/configurations.html#transforms","title":"Transforms","text":"<p>These configurations specify how we apply data augmentation to the datasets.</p> <p>Note</p> <p>Usually each transformation configuration creates three data processing pipeline:</p> <ul> <li>train_transform</li> <li>val_transform</li> <li>test_transform</li> </ul> <p>The most used transform is the <code>default_resize</code> (<code>transforms/default_resize</code>) which is used to resize the input images to the size expected by the backbone model and normalize using imagenet mean and std. For example, the <code>resnet18</code> backbone expects images of size <code>[224, 224]</code> so we use the <code>default_resize</code> transform to resize the input images to this size.</p> <pre><code>defaults:\n- default\n- _self_\n\ninput_height: 224\ninput_width: 224\n\nstandard_transform:\n_target_: albumentations.Compose\ntransforms:\n- _target_: albumentations.Resize\nheight: ${transforms.input_height}\nwidth: ${transforms.input_width}\ninterpolation: 2\nalways_apply: True\n- ${transforms.normalize}\n\ntrain_transform: ${transforms.standard_transform}\nval_transform: ${transforms.standard_transform}\ntest_transform: ${transforms.standard_transform}\n\nname: default_resize\n</code></pre> <p>Basically all the transforms have an <code>input_height</code> and <code>input_width</code> parameter which are used to resize the input images to the size expected by the backbone model. Right now we support only Albumentations based transforms.</p> <p>We make large use of hydra variable interpolation to make the configuration files more readable and avoid repeating the same parameters over and over.</p>"},{"location":"tutorials/contribution.html","title":"Contribution","text":"<p>In this guide, we'll cover the steps you should take to ensure your code meets the following standards:</p> <ul> <li>Pre-commit hooks for formatting code and checking for common errors.</li> <li>Google-style docstrings for documenting source code.</li> <li>Type annotations for functions and variables using <code>mypy</code>.</li> <li>Unit tests using <code>pytest</code> to ensure code correctness.</li> </ul>"},{"location":"tutorials/contribution.html#setting-up-the-environment","title":"Setting up the Environment","text":"<p>Before contributing to the repository, you'll need to set up your development environment. Please check the Getting Started Guide for instructions on how to set up your environment.</p> <p>After setting up your environment you can install <code>Quadra</code> Library in different ways:</p> <p>Info</p> <ul> <li><code>poetry install --with dev</code> (for development) </li> <li><code>poetry install --with docs</code> (for documentation)</li> <li><code>poetry install --with test</code> (for testing)</li> </ul>"},{"location":"tutorials/contribution.html#pre-commit-hooks","title":"Pre-commit Hooks","text":"<p>Pre-commit hooks are scripts that run before a commit is made to the repository. They can be used to check for common errors, enforce code formatting standards, and perform other tasks. The repository should have a pre-commit configuration file already set up. To use it, run the following command:</p> <pre><code>pre-commit install\n</code></pre> <p>This will install the pre-commit hooks and they will run automatically before each commit. The pre-commit hooks will check the code for formatting errors using many built-in library defined under <code>.pre-commit-config.yaml</code> file. If any errors are found, the commit will fail and you will need to fix the errors before you can commit your changes.</p>"},{"location":"tutorials/contribution.html#google-style-docstrings","title":"Google-style Docstrings","text":"<p>This library is using <code>Google-style</code> docstrings. They provide a consistent format for documenting functions and classes, making it easier for other developers to understand how to use the code. Here's an example of a Google-style docstring:</p> <pre><code>def my_function(arg1, arg2):\n\"\"\"Summary line.\n\n    Extended description of function.\n\n    Args:\n        arg1: Description of arg1.\n        arg2: Description of arg2.\n\n    Returns:\n        Description of return value.\n    \"\"\"\n</code></pre> <p>To ensure that all functions and classes in the repository are documented with Google-style docstrings, we are using ruff as a pre-commit hook that checks for missing or incorrect docstrings.</p>"},{"location":"tutorials/contribution.html#type-annotations-with-mypy","title":"Type Annotations with Mypy","text":"<p>Type annotations are a way to specify the expected types of function arguments and return values. This makes it easier to understand how functions should be used and can catch type-related errors early. To enforce type annotations, we use <code>mypy</code>, a static type checker for Python. It is installed as a pre-commit hook, so it will run automatically before each commit. We don't enforce every rule in <code>mypy</code>, but we do require that all functions and variables have type annotations. Please check the mypy documentation for more information on how to use type annotations. If you are interested in which rules we enforce, you can check the <code>pyproject.toml</code> file in the root of the repository.</p>"},{"location":"tutorials/contribution.html#unit-tests-with-pytest","title":"Unit Tests with Pytest","text":"<p>Unit tests are a critical part of ensuring code correctness. They should be written to test each function and class in the repository, verifying that they behave as expected under different conditions. We use <code>pytest</code> to run the unit tests. We have a <code>tests</code> folder in the root of the repository that contains all the unit tests. Under this folder, there is a <code>conftest.py</code> file that contains the fixtures used by the unit tests. The fixtures are used to set up the environment for each test, and they are automatically run before each test. Here is the test folder structure:</p> <pre><code>tests/\n\u251c\u2500\u2500 configurations\n\u251c\u2500\u2500 conftest.py\n\u251c\u2500\u2500 datamodules\n\u251c\u2500\u2500 datasets\n\u251c\u2500\u2500 models\n\u2514\u2500\u2500 tasks\n</code></pre>"},{"location":"tutorials/contribution.html#adding-something-new","title":"Adding Something New","text":"<p>Here are the usual steps you should take when adding something new to the repository:</p> <ol> <li>Create a new branch for your changes.</li> <li>Add task, model, datamodule or other type of component under the package folder.</li> <li>Add relevant configuration files under the <code>configs</code> folder.</li> <li>Add unit tests under the <code>tests</code> folder.</li> <li>Add documentation under the <code>docs</code> folder.</li> <li>Run the pre-commit hooks to check for errors.</li> <li>Open a pull request to merge your changes into the main or development branch.</li> <li>Assign maintainers to review your pull request.</li> </ol>"},{"location":"tutorials/datamodules.html","title":"DataModules","text":"<p>DataModule classes handle the data loading and preprocessing. They are consuming dataset classes defined under <code>quadra.datasets</code>. The generic datasets that will be used for different projects have to be implemented in <code>quadra</code> library.</p>"},{"location":"tutorials/datamodules.html#datamodule-hierarchy","title":"DataModule Hierarchy","text":"<pre><code>classDiagram\n  BaseDataModule &lt;|-- ClassificationDataModule\n  BaseDataModule &lt;|-- SklearnClassificationDataModule\n  BaseDataModule &lt;|-- MultilabelClassificationDataModule\n  BaseDataModule &lt;|-- AnomalyDataModule\n  BaseDataModule &lt;|-- PatchSklearnClassificationDataModule\n  BaseDataModule &lt;|-- SegmentationDataModule\n  ClassificationDataModule &lt;|-- SSLDataModule\n\n  class BaseDataModule{\n    +pd.DataFrame data\n    +save_checkpoint()\n    +prepare_data()\n    #_prepare_data()\n    +restore_checkpoint()\n    +setup()\n    +train_dataloader()\n    +test_dataloader()\n    +val_dataloader()\n    +predict_dataloader()\n  }\n  class ClassificationDataModule{\n    #_prepare_data()\n  }\n  class MultilabelClassificationDataModule{\n    #_prepare_data()\n  }\n  class SklearnClassificationDataModule{\n    #_prepare_data()\n    +full_dataloader()\n  }\n  class PatchSklearnClassificationDataModule{\n    #_prepare_data()\n  }\n  class SSLDataModule{\n    #prepare_data()\n    +classifier_train_dataloader()\n  }\n  class SegmentationDataModule{\n    #_prepare_data()\n  }\n  class AnomalyDataModule{\n    #_prepare_data()\n  }</code></pre> <p>Quadra DataModules are an extension of <code>pytorch_lightning</code> DataModules. They are inheriting from <code>BaseDataModule</code> class and must reimplement the <code>_prepare_data</code> function. </p> <p>This function will be called from the <code>prepare_data</code> function of the base class and it's designed in such a way that data preparation (download, extract, load etc.) and the split creation will be done only once even in a distributed environment.</p> <p>The <code>prepare_data</code> function of the base class will be called from <code>pytorch_lightning</code> when a fit, test or prediction operation is performed.Data location, labelling and division will be saved inside a pandas dataframe and the whole DataModule will be saved on disk as a pickle object which will be loaded when the <code>setup</code> function is called (again from lightning).</p> <p>Warning</p> <p>Since we are saving the DataModule on disk all its attributes must be serializable.</p> <p>If lightning is not used, the <code>prepare_data</code> and <code>setup</code> functions must be called manually to prepare the data and setup the DataModule.</p>"},{"location":"tutorials/datamodules.html#organizing-the-data","title":"Organizing the Data","text":"<p>A detailed overview of data organization is given inside the tutorial of each kind of task.</p>"},{"location":"tutorials/datamodules.html#classification","title":"Classification","text":"<p>Classification datasets are divided with folder names without having any subfolders. If there are many subfolders, the leaf folder name will be used as the label.</p> <p><pre><code>project_root_folder/\n\u251c\u2500\u2500 class_0\n\u2502   \u2514\u2500\u2500 xyz.png\n\u251c\u2500\u2500 class_N\n\u2502   \u2514\u2500\u2500 xyz.png\n\u251c\u2500\u2500 test.txt (Optional)\n\u251c\u2500\u2500 train.txt (Optional)\n\u2514\u2500\u2500 val.txt (Optional)\n</code></pre> Each split file should contain relative path to dataset root.</p> <pre><code>class_0/xyz.png\n...\n</code></pre>"},{"location":"tutorials/datamodules.html#self-supervised","title":"Self-supervised","text":"<p>This tasks follows the same data structure of the classification tasks.</p>"},{"location":"tutorials/datamodules.html#segmentation","title":"Segmentation","text":"<p>If you are using the base DataModule for segmentation tasks, organizing your images and masks with a given optional split files is enough to load the data.</p> <pre><code>project_root_folder/\n\u251c\u2500\u2500 images\n\u2502   \u2514\u2500\u2500 xyz.png\n\u251c\u2500\u2500 masks\n\u2502   \u2514\u2500\u2500 xyz.png\n\u251c\u2500\u2500 test.txt (Optional)\n\u251c\u2500\u2500 train.txt (Optional)\n\u2514\u2500\u2500 val.txt (Optional)\n</code></pre> <p>Each split file should contain relative path to dataset root. <pre><code>images/xyz.png\n...\n</code></pre></p>"},{"location":"tutorials/datamodules.html#anomaly-detection","title":"Anomaly Detection","text":"<p>Anomaly detection tasks expects only good (images without anomalies) for the training set and both good and (optionally) anomaly images for the test set.</p> <pre><code>project_root_folder/\n\u251c\u2500\u2500 train \n\u2502   \u2514\u2500\u2500 good\n\u2502       \u2514\u2500\u2500 xyz.png\n\u251c\u2500\u2500 val \n\u2502   \u251c\u2500\u2500 good\n\u2502   \u2502   \u2514\u2500\u2500 xyz.png\n\u2502   \u251c\u2500\u2500 defect_type_1\n\u2502   \u2502   \u2514\u2500\u2500 xyz.png\n\u2502   \u2514\u2500\u2500 defect_type_N\n\u2502       \u2514\u2500\u2500 xyz.png\n\u251c\u2500\u2500 test\n\u2502   \u251c\u2500\u2500 good\n\u2502   \u2502   \u2514\u2500\u2500 xyz.png\n\u2502   \u251c\u2500\u2500 defect_type_1\n\u2502   \u2502   \u2514\u2500\u2500 xyz.png\n\u2502   \u2514\u2500\u2500 defect_type_N\n\u2502       \u2514\u2500\u2500 xyz.png\n\u2514\u2500\u2500 ground_truth\n    \u251c\u2500\u2500 good\n    \u2502   \u2502 # good images do not require ground truth masks\n    \u2502   \u2514\u2500\u2500 (optional)xyz.png\n    \u251c\u2500\u2500 defect_type_1\n    \u2502   \u2502 # masks for defects can have an optional suffix such as `xyz_mask.png`\n    \u2502   \u2514\u2500\u2500 xyz_&lt;suffix&gt;.png\n    \u2514\u2500\u2500 defect_type_N\n        \u2514\u2500\u2500 xyz_&lt;suffix&gt;.png\n</code></pre>"},{"location":"tutorials/datamodules.html#extending-base-datamodules","title":"Extending base DataModules","text":"<p>To extend the base datamodule is necessary to implement the <code>_prepare_data</code> function. This function should do all the necessary operations to prepare your data and split it into train, test and validation sets. The data should be saved inside the <code>data</code> attribute of the class. This attribute is a pandas dataframe with the following columns:</p> <ul> <li><code>samples</code>: paths to the image files</li> <li><code>targets</code>: label of the image (type of the label is defined by the task)</li> <li><code>split</code>: split of the image (train, test or val)</li> </ul> <p>These are generally the required fields, different tasks may require additional fields. For example, in the case of segmentation tasks, the <code>masks</code> field is required.</p>"},{"location":"tutorials/datamodules.html#data-hashing","title":"Data hashing","text":"<p>During <code>prepare_data</code> call of each datamodule we apply hashing algorithm for each sample of the dataset. This information helps developer to track not only the data path used for the experiment but also to track the data content. This is useful when the data is stored in a remote location and the developer wants to check if the data is the same as the one used for the experiment. BaseDataModule class has following arguments to control the hashing process:</p> <ul> <li><code>enable_hashing</code>: If <code>True</code> the data will be hashed.</li> <li><code>hash_size</code>: Size of the hash. Must be one of [32, 64, 128]. Defaults to 64.</li> <li><code>hash_type</code>: Type of hash to use, if content hash is used, the hash is computed on the file content, otherwise the hash is computed on the file size (<code>hash_type=size</code>) which is faster but less safe. Defaults to <code>content</code>.</li> </ul> <p>After the training is completed. The hash value of each sample used from given dataset will be saved under <code>hash</code> column inside <code>&lt;experiment_folder&gt;/data/dataset.csv</code> file.</p> <p>Info</p> <p>If the user wants to disable hashing from command line, it is possible to pass <code>datamodule.enable_hashing=False</code> as override argument.</p>"},{"location":"tutorials/devices_setup.html","title":"Set up devices","text":"<p>In this section we will see how to set up the devices for training and inference.</p>"},{"location":"tutorials/devices_setup.html#lightning-based-tasks","title":"Lightning based tasks","text":"<p>If you are using a Lightning based task, you can select the appropriate <code>trainer</code> configuration based on your needs. By default experiments will use the <code>lightning_gpu</code> configuration which is designed specifically for single gpu training. The configuration file is located at <code>configs/trainer/lightning_gpu.yaml</code> and it is shown below:</p> <pre><code>_target_: pytorch_lightning.Trainer\ndevices: [0]\naccelerator: gpu\nmin_epochs: 1\nmax_epochs: 10\nlog_every_n_steps: 10\n</code></pre> <p>We provide also configurations for multi-gpu training (<code>lightning_multigpu.yaml</code>) and cpu training (<code>lightning_cpu.yaml</code>). You can also create your own configuration to use different accelerators.</p> <p>It's important that in the final experiment configuration you set the <code>trainer</code> key to the name of the configuration you want to use. For example:</p> <pre><code>defaults:\n- base/classification/classification\n- override /trainer: lightning_multigpu\n\ntrainer:\ndevices: [0, 1] # Use two gpus\n</code></pre>"},{"location":"tutorials/devices_setup.html#sklearn-based-tasks","title":"Sklearn based tasks","text":"<p>For Sklearn based tasks there's generally a <code>device</code> field in the configuration file. For example, in the <code>configs/task/sklearn_classification.yaml</code> file we have:</p> <pre><code>_target_: quadra.tasks.SklearnClassification\ndevice: \"cuda:0\"\noutput:\nfolder: \"classification_experiment\"\nreport: true\nexample: true\ntest_full_data: true\n</code></pre> <p>You can change the device to <code>cpu</code> or a different cuda device depending on your needs.</p>"},{"location":"tutorials/documentation.html","title":"Build documentation","text":""},{"location":"tutorials/documentation.html#creating-the-website-with-updated-content","title":"Creating the Website with updated content.","text":"<p>there are multiple ways to create the website.</p> <ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build -d &lt;path&gt;</code> - Build the documentation site to output path.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul> <p>Warning</p> <p>The website generator creates the pages for each code reference (python file) automatically. The user does not need to create the pages manually. However, other pages such as tutorials or explanations should be created manually.</p>"},{"location":"tutorials/documentation.html#contributing","title":"Contributing","text":"<p>If you want to create a new page, follow the given steps:</p> <ol> <li>Under the docs folder create or use related folder for each markdown file.</li> <li>Open <code>mkdocs.yml</code> and add the new page under the <code>nav</code> tag with a relative path to <code>docs</code> folder.</li> </ol> <pre><code>nav:\n- index.md\n# defer to gen-files + literate-nav\n- Code Reference: reference/\n- Tutorials:\n- Getting Started: tutorials/install.md\n# You can add new pages here\n# ...\n</code></pre>"},{"location":"tutorials/documentation.html#developing-landing-page","title":"Developing Landing Page","text":"<p>Landing page extends the <code>home.html</code> from <code>mkdocs-material</code> theme. It uses <code>tailwindcss</code> for styling. The developer has to run the following command to update the <code>tailwindcss</code> styles.</p> <pre><code>npx tailwindcss -i ./docs/template/landing.css -o ./docs/css/landing.css --watch\n</code></pre>"},{"location":"tutorials/documentation.html#features","title":"Features","text":"<p>Most of the available features are defined here.</p>"},{"location":"tutorials/export.html","title":"Export models for inference","text":"<p>In this section we will see how quadra allows you to export your trained models for inference. We will see how to export models for both Lightning based tasks and Sklearn based tasks. By default the standard export format is Torchscript, but you can also export models to ONNX or plain Pytorch (this is done for particular operations like gradcam).</p>"},{"location":"tutorials/export.html#standard-export-configuration","title":"Standard export configuration","text":"<p>The standard configuration for exporting models is located at <code>configs/export/default.yaml</code> and it is shown below:</p> <pre><code>types: [torchscript]\ninput_shapes: # Redefine the input shape if not automatically inferred\nonnx:\n# torch.onnx.export options\ninput_names: # If null automatically inferred\noutput_names: # If null automatically inferred\ndynamic_axes: # If null automatically inferred\nexport_params: true\nopset_version: 16\ndo_constant_folding: true\n# Custom options\nfixed_batch_size: # If not null export with fixed batch size (ignore dynamic axes)\nsimplify: true\n</code></pre> <p><code>types</code> is a list of the export types that you want to perform. The available types are <code>torchscript</code>, <code>onnx</code> and <code>pytorch</code>. By default the models will be saved under the <code>deployment_model</code> folder of the experiment with extension <code>.pt</code>, <code>.onnx</code> and <code>.pth</code> respectively. Pytorch models will be saved alongside the yaml configuration for the model itself so that you can easily load them back in python.</p> <p><code>input_shapes</code> is a parameter that will be <code>None</code> most of the time, quadra features a model wrapper that is capable of inferring the input shape of the trained model based on its forward function. It supports a large variety of custom forward functions where parameters are combinations of lists, tuples or dicts. However, if the model wrapper is not able to infer the input shape you can specify it here, the format is a list of tuples/lists/dicts where each element represents a single input shape without batch size. For example if your model has an input of shape (1, 3, 224, 224) you can specify it as:</p> <pre><code>input_shapes:\n- [3, 224, 224]\n</code></pre> <p>Onnx support a set of extra options passed as kwargs to the <code>torch.onnx.export</code>, once again we will try to automatically infer most of them but if you need to specify them you can do it under the <code>onnx</code> section of the configuration file. The two custom options are:</p> <ul> <li><code>fixed_batch_size</code>: It can be used to export the model with a fixed batch size, this is useful if you want to use the model in a context where you know the batch size in advance. If you specify this option the model will be exported with a fixed batch size and the dynamic axes will be ignored.</li> <li><code>simplify</code>: If true the model will be simplified using the onnx-simplifier package, the resulting model is called <code>model_simplified.onnx</code> and it is saved alongside the original model.</li> </ul>"},{"location":"tutorials/export.html#lightning-based-tasks","title":"Lightning based tasks","text":"<p>Currently quadra supports exporting models for the following tasks:</p> <ul> <li>Image classification</li> <li>Image segmentation</li> <li>Anomaly detection (certain models may not be supported)</li> <li>SSL training</li> </ul>"},{"location":"tutorials/export.html#sklearn-based-tasks","title":"Sklearn based tasks","text":"<p>Currently quadra supports exporting models for the following tasks:</p> <ul> <li>Image classification</li> <li>Image classification with patches</li> </ul> <p>When working with sklearn based tasks alongside the exported backbone in the <code>deployment_model</code> folder you will also find a <code>classifier.joblib</code> containing the exported sklearn model.</p>"},{"location":"tutorials/export.html#importing-models-for-quadra-evaluation","title":"Importing models for quadra evaluation","text":"<p>Quadra exported models are fully compatible with quadra evaluation tasks, this is possible because quadra uses a model wrapper emulating the standard pytorch interface for all the exported models. </p>"},{"location":"tutorials/export.html#standard-inference-configuration","title":"Standard inference configuration","text":"<p>Evaluation models are regulated by a configuration file located at <code>configs/inference/default.yaml</code> shown below:</p> <pre><code>onnx:\nsession_options:\ninter_op_num_threads: 8\nintra_op_num_threads: 8\ngraph_optimization_level:\n_target_: onnxruntime.GraphOptimizationLevel\nvalue: 99 # ORT_ENABLE_ALL\nenable_mem_pattern: true\nenable_cpu_mem_arena: true\nenable_profiling: false\nenable_mem_reuse: true\nexecution_mode:\n_target_: onnxruntime.ExecutionMode\nvalue: 0 # ORT_SEQUENTIAL\nexecution_order:\n_target_: onnxruntime.ExecutionOrder\nvalue: 0 # DEFAULT\nlog_severity_level: 2\nlog_verbosity_level: 0\nlogid: \"\"\noptimized_model_filepath: \"\"\nuse_deterministic_compute: false\nprofile_file_prefix: onnxruntime_profile_\n\npytorch:\ntorchscript:\n</code></pre> <p>Right now we support custom option only for ONNX runtime, but we plan to add more inference configuration options in the future.</p>"},{"location":"tutorials/integration.html","title":"Integration with external projects","text":"<p>This library is designed to be used as a starting point for machine learning projects. Imagine you want to train a classification model on your datasets, but you don't want to spend time rewriting the code. If quadra is installed, you have access to all the configuration defined in the repository and calling</p> <pre><code>quadra {overrides}\n</code></pre> <p>Will run the experiment with the given overrides.</p> <p>If you want to add more hydra configurations on your own and make them available to quadra, it's necessary to create a <code>.env</code> file in the folder you are working on and specify the path or the package where to find the configuration as <code>QUADRA_SEARCH_PATH</code> variable.</p> <p>Config specification must follow the schema defined by hydra config search path. Multiple configs can be specified by separating them with a semicolon.</p> <p>For example:</p> <pre><code>QUADRA_SEARCH_PATH=file://configs;pkg://mypackage.configs\n</code></pre> <p>Warning</p> <p>Be careful that the configs share the same \"space\" of the quadra one, so it's required to avoid name collisions to avoid errors. One easy way to do so is to wrap the configs under a subfolder with the name of the project. E.g. <code>configs/datamodule/myproject/myconfig.yaml</code></p> <p>Warning</p> <p>If you have installed your configs inside a package you need to make sure that the package actually contains them, by default yaml files are not packaged! One clean way to solve this issue is to create a MANIFEST.in file in your repository containing a line like this one: <pre><code>recursive-include your_package_name *.yaml\n</code></pre> And set the <code>include_package_data</code> flag to <code>True</code> in your <code>setup.py</code> file.</p>"},{"location":"tutorials/integration.html#debugging-with-vscode","title":"Debugging with VSCode","text":"<p>If you are developing an external project which uses <code>quadra</code> as experiment manager, you can setup debugger configurations to attach debugger to the training process.</p> <p>In VSCode, you can create a <code>launch.json</code> file in the <code>.vscode</code> folder of your project. The following is an example of a configuration that can be used to debug the training process of a segmentation model.</p> <pre><code>{\n\"version\": \"0.2.0\",\n\"configurations\": [\n{\n\"name\": \"segmentation\",\n\"type\": \"python\",\n\"request\": \"launch\",\n\"module\": \"quadra.main\",\n\"console\": \"integratedTerminal\",\n\"justMyCode\": false,\n\"env\": {\n\"HYDRA_FULL_ERROR\": \"1\",\n},\n\"args\": [ \"experiment=generic/oxford_pet/segmentation/smp.yaml\",\n\"trainer.max_epochs=1\",\n\"trainer.devices=1\",\n\"trainer.accelerator=cpu\"\n]\n},\n]\n}\n</code></pre>"},{"location":"tutorials/integration.html#debugging-with-pycharm","title":"Debugging with PyCharm","text":"<p>Debugging with PyCharm is very similar to vscode, we can make use of the GUI to pick <code>quadra.main</code> as the entry point and pass the arguments as a string.</p> <p> Example of a valid debug configuration </p>"},{"location":"tutorials/model_management.html","title":"Model Management","text":""},{"location":"tutorials/model_management.html#overview","title":"Overview","text":"<p>Model Management is the process of maintaining and orchestrating the lifecycle of machine learning models. It involves steps such as model creation, training, evaluation, deployment, and monitoring, among other steps. Model Versioning, on the other hand, allows data scientists to track and manage different versions of models during the model's lifecycle. It's crucial for reproducibility, collaboration, and consistent model performance. Quadra, out-of-the-box, supports managing models and their versions through <code>MlflowModelManager</code> wrapper class with the help of Mlflow library. </p> <p><code>MlflowModelManager</code> is an extension of <code>AbstractModelManager</code> which serves as a blueprint for model managers and specifies the required methods for managing models. It includes methods such as:</p> <ul> <li>Register the model</li> <li>Retrieve latest version</li> <li>Transition the model to a new stage</li> <li>Delete the model </li> </ul> <p>By defining an abstract class, we establish a common interface that can be implemented by different model managers besides Mlflow.</p>"},{"location":"tutorials/model_management.html#example-usage","title":"Example Usage","text":"<p>In this section, we will create example project for segmentation task and use <code>MlflowModelManager</code> to manage the production model. <code>Quadra</code> provides a toy example where you can train a segmentation model for Oxford-IIIT Pet Dataset. </p> <p>Note</p> <p>You can find a detailed explanation for customizing the segmentation task under Segmentation Example section.</p> <p>First of all, we need to run <code>Mlflow</code> server with artifact store. You can find the instructions for running <code>Mlflow</code> server here. Let's open a new terminal and run the following command:</p> <pre><code>mlflow server \\\n--backend-store-uri sqlite:///mlflow.db \\\n--default-artifact-root file:///tmp/mlflow \\\n--host 0.0.0.0\n</code></pre> <p>Then, we can start training from different terminal window while <code>Mlflow</code> server is running:</p> <pre><code>MLFLOW_TRACKING_URI=\"http://localhost:5000\" \\\nquadra experiment=generic/oxford_pet/segmentation/smp \\\ntrainer.max_epochs=5 \\\ncore.name=cats_vs_dogs \\\nbackbone.model.arch=unet,unetplusplus \\\nbackbone.model.encoder_name=resnet18,resnet50 \\\n--multirun\n</code></pre> <p>This command will train a segmentation model for 5 epochs and save experiments run under <code>cats_vs_dogs</code> experiment tab. It will run the same experiment with different backbones or segmentation model architectures. After all trainings are completed, we open up the <code>Mlflow</code> UI and see experiments run under <code>cats_vs_dogs</code> directory. Under the artifacts section of each run, we can find <code>Mlflow</code> model artifact under the <code>deployment_model</code> directory. Other folders contains metadata or reports about the experiment run.</p>"},{"location":"tutorials/model_management.html#registering-the-model","title":"Registering the Model","text":"<p>After all runs are completed, we can register the best model with the following command:</p> <pre><code>import os\nfrom quadra.utils.model_manager import MlflowModelManager\nos.environ[\"MLFLOW_TRACKING_URI\"] = \"http://localhost:5000\"\n\nmanager = MlflowModelManager()\nmanager.register_best_model(experiment_name=\"cats_vs_dogs\",\n                            # metric we want to use for selecting the best model\n                            metric=\"val_loss_epoch\", \n                            # model path under the artifact store\n                            model_path=\"deployment_model\", \n                            # the name of the model to be registered\n                            model_name=\"cvsd_model\", \n                            # optional tags\n                            tags={\"type\":\"segmentation\"}, \n                            # metric sorting order for selecting the best model\n                            mode=\"min\")  \n</code></pre> <p> Registered Model </p>"},{"location":"tutorials/model_management.html#staging-the-model","title":"Staging the Model","text":"<p>After registering the model, we can transition the model to a new stage. We can transition the model to <code>staging</code> stage with the following command:</p> <pre><code>manager.transition_model(model_name=\"cvsd_model\",\n                        version=1,\n                        stage=\"staging\",\n                        description=\"Staging Model for demo\")\n</code></pre> <p> Staged Model </p>"},{"location":"tutorials/model_management.html#update-the-model","title":"Update the Model","text":"<p>Let's say we want to update the model with better performance. Let's train the one of the configuration with more epochs and register the model again:</p> <pre><code>MLFLOW_TRACKING_URI=\"http://localhost:5000\" \\\nquadra experiment=generic/oxford_pet/segmentation/smp \\\ntrainer.max_epochs=20 \\\ncore.name=cats_vs_dogs \\\nbackbone.model.arch=unetplusplus \\\nbackbone.model.encoder_name=resnet50 </code></pre> <p>With this new model, we have improved <code>val_loss_epoch</code> metric compared to the previous model.</p> <p> Comparison of Runs </p> <p>Let's register the new model with the following command:</p> <pre><code>manager.register_model(model_location=\"runs:/&lt;run-id&gt;/deployment_model/model.pt\",\n                       model_name=\"cvsd_model\",\n                       tags={\"type\":\"smp\"},\n                       description=\"Better model\"\n                       )\n</code></pre> <p>and finally transition the model to <code>production</code> stage:</p> <pre><code>manager.transition_model(model_name=\"cvsd_model\",\n                        version=2,\n                        stage=\"production\",\n                        description=\"Production Model for demo\")\n</code></pre> <p>Note</p> <p>When we visit the model page from Mlflow UI, we can see the model version and stage information under the <code>Versions</code> tab. Moreover, we can see the model history automatically stored under the <code>Description</code> field.</p> <p><p> Model Transition History </p></p>"},{"location":"tutorials/reproducibility.html","title":"Reproducibility and Logging","text":"<p>One of the most important aspect of the experiment management is reproducibility which is ensured by some framework utilities.</p> <p>In this page, we will introduce how experiments and configuration are stored and how they can be used in other projects. </p> <p>First of all, the configuration files contain a fixed seed under <code>core.seed</code> that is set before running any line of code. This seed is also used by <code>datamodule</code> classes.</p> <p>Each experiment that has been initiated has its own folder. If the user is using the default settings. The folder will be saved under <code>logs/(multi)&lt;runs&gt;/&lt;core.name&gt;/YYYY-MM-DD_HH-mm-SS</code> that is relative to the folder where <code>quadra</code> command is running.</p> <p>Typical example of the folder structure is:</p> <pre><code>logs/.../experiment_name/\n\u251c\u2500\u2500 checkpoints\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 config_resolved.yaml\n\u251c\u2500\u2500 config_tree.txt\n\u251c\u2500\u2500 data\n\u2502   \u2514\u2500\u2500 datamodule.pkl\n\u251c\u2500\u2500 deployment_model\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 main.log\n\u251c\u2500\u2500 .hydra\n\u2502   \u251c\u2500\u2500 config.yaml\n\u2502   \u2514\u2500\u2500 overrides.yaml\n\u2514\u2500\u2500 task_specific_outputs...\n</code></pre> <ul> <li>checkpoints: This folder contains the checkpoints of the model (if any was saved).</li> <li>config_resolved.yaml: This file contains the resolved configuration after hydra parses it. It is useful when you want to know what is the final configuration after resolving the overrides.</li> <li>config_tree.txt: Human readable version of the configuration.</li> <li>datamodule.pkl: A serialized version of the datamodule that is used in the experiment, it contains the train, val and split datasets alongside all the other parameters.</li> <li>deployment_model: Contains the eventual model in deployment format.</li> <li>main.log: This file contains the system logs of the experiment.</li> <li>.hydra: This folder contains the hydra configuration file created by hydra.</li> <li>task_specific_outputs: These are auxiliary files that are used to store the results of the experiment, generally they are treated as artifacts so specific loggers can handle them</li> </ul> <p>Note</p> <pre><code>If the user specifies `core.upload_artifacts=True` in the config file, the artifacts will be uploaded to the logger artifact storage if possible.\n</code></pre> <p>Since all final configurations are written into <code>config_resolved.yaml</code> file, it is the single file enough to compare or replicate the experiment. However, we need to sync the codebase as the same it has been used to train the model. To do so, the quadra library also saves the git commit hash of the codebase as hyperparameter under <code>git/commit</code> tag. Using this hash, user can <code>git checkout &lt;commit-hash&gt;</code> the codebase and have the identical repository setup to run the same experiment.</p>"},{"location":"tutorials/tasks.html","title":"Tasks","text":"<p>Tasks are the main classes to interact with the experiments running using the <code>quadra</code> library. They implement functions as a pipeline of operations to be executed in a specific order. Following sections are explaining already implemented task pipelines.</p>"},{"location":"tutorials/tasks.html#task-flow","title":"Task Flow","text":"<p>Each task has a set of functions to design the flow of each experiment. The <code>execute</code> function calls the given function below in a sequential way.</p> <pre><code>graph LR;\n    P(Prepare)--&gt;TR(Train);\n    TR--&gt;TE(Test);\n    TE-.-&gt;EM(Export model);\n    EM-.-&gt;GR(Generate Report);\n    GR--&gt;F(Finalize);</code></pre> <p>Note</p> <p>You can extend or modify <code>execute</code> function or add new functions to change or modify the order of functions calling.</p> <p>Note</p> <p>The <code>metadata</code> field of the task is used to store the results of each step in the flow. This field is suitable for using output of previous steps as input of the next step.</p> <p>Tasks are also handling class instantiation and configuration from hydra configuration files.</p>"},{"location":"tutorials/tasks.html#class-inheritance","title":"Class Inheritance","text":"<pre><code>graph LR;\n    T(Task)--&gt;L(LightningTask);\n    T--&gt;E(Evaluation);\n    L--&gt;SG(Segmentation);\n    L--&gt;CL(Classification);\n    T--&gt;SCL(SklearnClassification);\n    T--&gt;PSCL(PatchSklearnClassification);\n    L--&gt;SSL(SSL);\n    L--&gt;AD(AnomalibDetection)</code></pre> <ul> <li><code>Task</code>: Instantiate the DataModule, pretty much everything else must be implemented in the child classes.</li> <li><code>Lightning Task</code>: Adds trainer, callbacks, logger and GPU parsing to the base task class as default. During the train and test phases it will use the <code>pytorch-lightning</code> library to train/test the model.</li> <li><code>Evaluation Task</code>: Base task to load a model in the deployment format and run inference with it on new data, should be re-implemented for each task.</li> <li><code>Segmentation Task</code>: It has the same functionality as the lightning task but it will also generate segmentation reports on demand.</li> <li><code>Classification Task</code>: This task is designed to train from scratch or finetune a classification model using the <code>pytorch-lightning</code> library.</li> <li><code>SklearnClassification Task</code>: This task is designed to train an <code>sklearn</code> classifier on top of a torch feature extractor.</li> <li><code>PatchSklearnClassification Task</code>: This task is designed to train an <code>sklearn</code> patch classifier on top of a torch feature extractor.</li> <li><code>Anomalib Detection Task</code>: This task is designed to train an anomaly detection model using the <code>anomalib</code> library.</li> <li><code>SSL (Self Supervised Learning) Task</code>: This task is designed to train a torch module with a given SSL algorithm.</li> </ul> <p>Most of these tasks have an associated evaluation task used for inference.</p>"},{"location":"tutorials/tasks.html#adding-new-tasks","title":"Adding new tasks","text":"<p>If you require the <code>pytorch-lightning</code> library, you can add a new task by extending the <code>LightningTask</code> class. Otherwise, you can simply start implementing a new task by extending the <code>Task</code> class.</p>"},{"location":"tutorials/examples/anomaly_detection.html","title":"Anomaly detection example","text":"<p>In this page, we will show you how to run anomaly detections experiment exploiting the functionality of the anomalib library.</p>"},{"location":"tutorials/examples/anomaly_detection.html#training","title":"Training","text":"<p>This example will demonstrate how to create custom experiments starting from default settings.</p>"},{"location":"tutorials/examples/anomaly_detection.html#dataset","title":"Dataset","text":"<p>Let's start with the dataset that we are going to use. Since we are using the base anomaly datamodule, images and masks must be arranged in a folder structure that follows the anomaly datamodule guidelines defined in the  anomaly datamodule documentation.  For this example, we will use the <code>mnist</code> dataset (using 9 as good and all the other number as anomalies), the dataset will be automatically downloaded by the generic experiment described next.</p> <pre><code>MNIST/\n\u251c\u2500\u2500 train \n\u2502   \u2514\u2500\u2500 good\n\u2502       \u2514\u2500\u2500 xyz.png\n\u2514\u2500\u2500 test\n  \u251c\u2500\u2500 good\n  \u2502   \u2514\u2500\u2500 xyz.png\n  \u251c\u2500\u2500 0\n  \u2502   \u2514\u2500\u2500 xyz.png\n  \u251c\u2500\u2500 1\n  \u2502   \u2514\u2500\u2500 xyz.png\n  \u2514\u2500\u2500 ...\n      \u2514\u2500\u2500 xyz.png\n</code></pre> <p>MNIST doesn't have ground truth masks for defects, by default we will use empty masks for good images and full white masks for anomalies.</p> <p>The standard datamodule configuration for anomaly is found under <code>datamodule/base/anomaly.yaml</code>.</p> <pre><code>_target_: quadra.datamodules.AnomalyDataModule\ndata_path: ???\ncategory:\nnum_workers: 8\ntrain_batch_size: 32\ntest_batch_size: 32\nseed: ${core.seed}\ntrain_transform: ${transforms.train_transform}\ntest_transform: ${transforms.test_transform}\nval_transform: ${transforms.val_transform}\nphase: train\nmask_suffix:\nvalid_area_mask:\ncrop_area:\n</code></pre> <p>But for the <code>mnist</code> example we will use the generic datamodule configuration under <code>datamodule/generic/mnist/anomaly/base.yaml</code>.</p> <pre><code>_target_: quadra.generic.mnist.MNISTAnomalyDataModule\ndata_path: ${oc.env:HOME}/.quadra/datasets/MNIST\ngood_number: 9\nnum_workers: 8\nlimit_data: 100\ntrain_batch_size: 32\ntest_batch_size: 32\nseed: ${core.seed}\ntrain_transform: ${transforms.train_transform}\ntest_transform: ${transforms.test_transform}\nval_transform: ${transforms.val_transform}\nphase: train\nvalid_area_mask:\ncrop_area:\n</code></pre> <p>The MNISTDataModule will automatically download the dataset and create the folder structure described above under the <code>data_path</code> directory.</p>"},{"location":"tutorials/examples/anomaly_detection.html#anomaly-detection-techniques","title":"Anomaly detection techniques","text":"<p>At the current stage, six methods taken from the anomalib library are available for anomaly detection (descriptions are taken or readaptaded from the anomalib documentation):</p> <ul> <li>PADIM: Padim extends the concepts of DFM fitting gaussian distributions on a lot of feature vectors extracted from intermediate layer of the network, thus retaining spatial information and allowing outputting anomaly maps that can be used to segment the images. From the conducted experiments is a strong model that fits well on multiple datasets.</li> <li>Patchcore: Similarly to PADIM feature extraction is done on intermediate layers, that are then pooled to  furtherly extend the receptive field of the model. Features from the training images are used to create a so called \"memory bank\" that is used as base dataset to perform KNN in the inference stage. This technique should be even stronger than PADIM but it is a bit slower, generally I would use it if PADIM fails.</li> <li>CFLOW: While the previous techniques generally performs a single forward pass to extract features and fit some kind of model, CFLOW is more similar to the standard neural network training procedure, where the model is trained for  multiple epochs. The model is fairly complex and is based on the concept of \"Normalizing flow\" which main idea is to transform the model complex distribution into a simpler one using Invertible Neural Networks. The concept of  normalizing flow looks very promising and many recent papers are achieving good results with it. But so far conducted experiments have shown that the previous models are better and faster to train.</li> <li>Fastflow: FastFlow is a two-dimensional normalizing flow-based probability distribution estimator. It can be used as a plug-in module with any deep feature extractor, such as ResNet and vision transformer, for unsupervised anomaly detection and localisation. In the training phase, FastFlow learns to transform the input visual feature into a tractable distribution, and in the inference phase, it assesses the likelihood of identifying anomalies.</li> <li>DRAEM: Is a reconstruction based algorithm that consists of a reconstructive subnetwork and a discriminative subnetwork. DRAEM is trained on simulated anomaly images, generated by augmenting normal input images from the training set with a random Perlin noise mask extracted from an unrelated source of image data. The reconstructive subnetwork is an autoencoder architecture that is trained to reconstruct the original input images from the augmented images. The reconstructive submodel is trained using a combination of L2 loss and Structural Similarity loss. The input of the discriminative subnetwork consists of the channel-wise concatenation of the (augmented) input image and the output of the reconstructive subnetwork. The output of the discriminative subnetwork is an anomaly map that contains the predicted anomaly scores for each pixel location. The discriminative subnetwork is trained using Focal Loss</li> <li>CS-FLOW: The central idea of the paper is to handle fine-grained representations by incorporating global and local image context. This is done by taking multiple scales when extracting features and using a fully-convolutional normalizing flow to process the scales jointly.</li> <li>EfficientAd Fast anomaly segmentation algorithm that consists of a distilled pre-trained teacher model, a student model and an autoencoder. It detects local anomalies via the teacher-student discrepany and global anomalies via the student-autoencoder discrepancy.</li> </ul> <p>For a detailed description of the models and their parameters please refer to the anomalib documentation.</p> <p>For each one of these techniques there's a different config file defining the experiment foundations that can be found  under <code>experiment/base/anomaly</code>.</p>"},{"location":"tutorials/examples/anomaly_detection.html#customizing-the-basic-configuration","title":"Customizing the basic configuration","text":"<p>The base setup will delete the trained model at the end of the experiment to save space so be careful.</p> <p>What can be useful to customize are the default callbacks: <pre><code>callbacks:\n# Anomalib specific callbacks\nscore_normalization:\n_target_: quadra.utils.anomaly.ThresholdNormalizationCallback\nthreshold_type: image\npost_processing_configuration:\n_target_: anomalib.utils.callbacks.post_processing_configuration.PostProcessingConfigurationCallback\nthreshold_method: ${model.metrics.threshold.method}\nmanual_image_threshold: ${model.metrics.threshold.manual_image}\nmanual_pixel_threshold: ${model.metrics.threshold.manual_pixel}\nmetrics:\n_target_: anomalib.utils.callbacks.metrics_configuration.MetricsConfigurationCallback\ntask: ${model.dataset.task}\nimage_metrics: ${model.metrics.image}\npixel_metrics: ${model.metrics.pixel}\nvisualizer:\n_target_: quadra.callbacks.anomalib.VisualizerCallback\ninputs_are_normalized: true\noutput_path: anomaly_output\nthreshold_type: ${callbacks.score_normalization.threshold_type}\ndisable: true\nplot_only_wrong: false\nplot_raw_outputs: false\nbatch_size_finder:\n_target_: quadra.callbacks.lightning.BatchSizeFinder\nmode: power\nsteps_per_trial: 3\ninit_val: 2\nmax_trials: 5 # Max 64\nbatch_arg_name: train_batch_size\ndisable: true\n</code></pre></p> <p>Warning</p> <p>By default lightning batch_size_finder callback is disabled. This callback will automatically try to infer the maximum batch size that can be used for training without running out of memory. We've experimented runtime errors with this callback on some machines due to a Pytorch/CUDNN incompatibility so be careful when using it.</p> <p>The score_normalization callback is used to normalize the anomaly maps to the range [0, 1000] such that the threshold will become 100.</p> <p>The threshold_type can be either \"image\" or \"pixel\" and it indicates which threshold to use to normalize the pixel level threshold, if no masks are available for segmentation this should always be \"image\", otherwise the normalization will use the threshold computed without masks which would result in wrong segmentations.</p> <p>The post processing configuration allow to specify the method used to compute the threshold, methods and manual metrics are generally specified in the model configuration and should not be changed here.</p> <p>The visualizer callback is used to produce a visualization of the results on the test data, when the score_normalization callback is used the input_are_normalized flag must be set to true and the threshold_type should match the one used for normalization. By default it is disabled as it may take a while to compute, to enable just set <code>disable: false</code>.</p> <p>In the context where many images are supplied to our model, we may be more interested in restricting the output images that are generated to only the cases where the result is not correct. By default it is disabled, to enable just set <code>plot_only_wrong: true</code>.</p> <p>The display of the outputs of a model can be done in a preset format. However, this option may not be as desired, or may be affecting the resolution of the images. In order to give more flexibility to the generation of reports, the heatmap and segmentation ouput files can be generated independently and with the same resolution of the original image. By default it is disabled, to enable just set <code>plot_raw_outputs: true</code>.</p>"},{"location":"tutorials/examples/anomaly_detection.html#anomalib-configuration","title":"Anomalib configuration","text":"<p>Anomalib library doesn't use hydra but still uses yaml configurations that are found under <code>model/anomalib</code>. This for example is the configuration used for PADIM. <pre><code>dataset:\ntask: segmentation\ntiling:\napply: false\ntile_size: null\nstride: null\nremove_border_count: 0\nuse_random_tiling: False\nrandom_tile_count: 16\n\nmodel:\ninput_size: [224, 224]\nbackbone: resnet18.tv_in1k\nlayers:\n- layer1\n- layer2\n- layer3\npre_trained: true\nn_features: null\n\nmetrics:\nimage:\n- F1Score\n- AUROC\npixel:\n- F1Score\n- AUROC\nthreshold:\nmethod: adaptive # options: [adaptive, manual]\nmanual_image: null\nmanual_pixel: null\n</code></pre> What we are mostly interested about is the <code>model</code> section. In this section we can specify the backbone of the model (mainly resnet18.tv_in1k and wide_resnet50_2.tv_in1k), which layers are used for feature extraction and the number of features used for dimensionality reduction (there are some default values for resnet18 and wide_resnet50_2). <pre><code>Notice: \".tv_in1k\" is an extension for timm backbones' model_name which refers to torchvision pretrained weights.\n</code></pre> Generally we always compute an adaptive threshold based on the validation data, but it is possible to specify a manual threshold for both image and pixel as we may want a different tradeoff between false  positives and false negatives. The threshold specified must be the unnormalized one.</p> <p>As already mentioned anomaly detection requires just good images for training, to compute the threshold used for separating good and anomalous examples it's required to have a validation set generally containing both good and anomalous examples. If the validation set is not provided the threshold will be computed on the test set.</p>"},{"location":"tutorials/examples/anomaly_detection.html#experiment","title":"Experiment","text":"<p>Suppose that we want to run the experiment on the given dataset using the PADIM technique. We can take the generic padim config for mnist as an example found under <code>experiment/generic/mnist/anomaly/padim.yaml</code>.</p> <pre><code># @package _global_\ndefaults:\n- base/anomaly/padim\n- override /datamodule: generic/mnist/anomaly/base\n\nexport:\ntypes: [torchscript]\n\nmodel:\nmodel:\ninput_size: [224, 224]\nbackbone: resnet18\n\ndatamodule:\nnum_workers: 12\ntrain_batch_size: 32\ntask: ${model.dataset.task}\ngood_number: 9\n\ncallbacks:\nscore_normalization:\nthreshold_type: \"image\"\n\nprint_config: false\n\ncore:\ntag: \"run\"\ntest_after_training: true\nupload_artifacts: true\nname: padim_${datamodule.good_number}_${model.model.backbone}_${trainer.max_epochs}\n\nlogger:\nmlflow:\nexperiment_name: mnist-anomaly\nrun_name: ${core.name}\n\ntrainer:\ndevices: [0]\nmax_epochs: 1\ncheck_val_every_n_epoch: ${trainer.max_epochs}\n</code></pre> <p>We start from the base configuration for PADIM, then we override the datamodule to use the generic mnist datamodule. Using this configuration we specify that we want to use PADIM, extracting features using the resnet18 backbone with image size 224x224, the dataset is <code>mnist</code>, we specify that the task is taken from the anomalib configuration which specify it to be segmentation. One very important thing to watch out is the <code>check_val_every_n_epoch</code> parameter. This parameter should match the number of epochs for <code>PADIM</code> and <code>Patchcore</code>, the reason is that in the validation phase the model will be fitted and we want the fit to be done only once and on all the data, increasing the max_epoch is useful when we apply data augmentation, otherwise it doesn't make a lot of sense as we would fit the model on the same, replicated data. The model will be exported at the end of the training phase, as we have specified the <code>export.types</code> parameter to <code>torchscript</code> the model will be exported only in torchscript format.</p>"},{"location":"tutorials/examples/anomaly_detection.html#run","title":"Run","text":"<p>Assuming that you have created a virtual environment and installed the <code>quadra</code> library, you can run the experiment by running the following command:</p> <pre><code>quadra experiment=generic/mnist/anomaly/padim\n</code></pre>"},{"location":"tutorials/examples/anomaly_detection.html#evaluation","title":"Evaluation","text":"<p>The same datamodule specified before can be used for inference by setting the <code>phase</code> parameter to <code>test</code>. The dataset structure is the same as to the one used for training, but only the test folder is required. <pre><code>MNIST/\n\u251c\u2500\u2500 train \n\u2502   \u2514\u2500\u2500 good\n\u2502       \u2514\u2500\u2500 xyz.png\n\u2514\u2500\u2500 test\n  \u251c\u2500\u2500 good\n  \u2502   \u2514\u2500\u2500 xyz.png\n  \u251c\u2500\u2500 0\n  \u2502   \u2514\u2500\u2500 xyz.png\n  \u251c\u2500\u2500 1\n  \u2502   \u2514\u2500\u2500 xyz.png\n  \u251c\u2500\u2500 unknown\n  \u2502   \u2514\u2500\u2500 xyz.png\n  \u2514\u2500\u2500 ...\n      \u2514\u2500\u2500 xyz.png\n</code></pre></p> <p>It's possible to define a folder named <code>unknown</code> in the test folder to define images for which we don't know the label, but we still want to perform inference on them.</p>"},{"location":"tutorials/examples/anomaly_detection.html#experiment_1","title":"Experiment","text":"<p>The default experiment config is found under <code>configs/experiment/base/anomaly/inference</code>.</p> <pre><code># @package _global_\n\ndefaults:\n- override /datamodule: base/anomaly\n- override /model: null\n- override /optimizer: null\n- override /scheduler: null\n- override /transforms: default_resize\n- override /loss: null\n- override /task: anomalib/inference\n- override /backbone: null\n- override /trainer: null\n- _self_\n\ndatamodule:\nphase: test\n\ntask:\nmodel_path: ???\nuse_training_threshold: false\ntraining_threshold_type: image\n</code></pre> <p>By default, the inference will recompute the threshold based on test data to maximize the F1-score, if you want to use the threshold from the training phase you can set the <code>use_training_threshold</code> parameter to true. The <code>training_threshold_type</code> can be used to specify which training threshold to use, it can be either <code>image</code> or <code>pixel</code>, if not specified the <code>image</code> threshold will be used.</p> <p>The model path is the path to an exported model, at the moment <code>torchscript</code> and <code>onnx</code> models are supported (exported automatically after a training experiment). Right now only the <code>CFLOW</code> model is not supported for inference as it's not compatible with botyh torchscript and onnx.</p> <p>An inference configuration using the mnist dataset is found under <code>configs/experiment/generic/mnist/anomaly/inference.yaml</code>.</p>"},{"location":"tutorials/examples/anomaly_detection.html#run_1","title":"Run","text":"<p>Same as above, assuming that you have created a virtual environment and installed the <code>quadra</code> can run the experiment by running the following command:</p> <pre><code>quadra experiment=generic/mnist/anomaly/inference task.model_path={path to the exported trained model}\n</code></pre> <p>Generally for inference is enough to use the base experiment providing both model_path and data_path like this:</p> <pre><code>quadra experiment=base/anomaly/inference task.model_path={path to the exported trained model} datamodule.data_path={path to the dataset}\n</code></pre>"},{"location":"tutorials/examples/classification.html","title":"Pytorch classification example","text":"<p>In this page, we will show you how to run a classification experiment exploiting Pytorch and Pytorch Lightning to finetune or train from scratch a model on a custom dataset.</p> <p>This example will demonstrate how to create a custom experiment starting from default settings.</p>"},{"location":"tutorials/examples/classification.html#training","title":"Training","text":""},{"location":"tutorials/examples/classification.html#dataset","title":"Dataset","text":"<p>Let's start with the dataset that we are going to use. Since we are using the classification datamodule, images must be arranged in a folder structure that reflects the classes' partition. </p> <p>Suppose we have a dataset with the following structure: <pre><code>dataset/\n\u251c\u2500\u2500 class_1\n\u2502   \u251c\u2500\u2500 abc.xyz\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 class_2\n\u2502   \u251c\u2500\u2500 abc.xyz\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 class_3 \n\u2502   \u251c\u2500\u2500 abc.xyz\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 test.txt # optional\n\u251c\u2500\u2500 train.txt # optional\n\u2514\u2500\u2500 val.txt # optional\n</code></pre></p> <p>The standard datamodule configuration for classification is found under <code>configs/datamodule/base/classification.yaml</code>.</p> <pre><code>_target_: quadra.datamodules.classification.ClassificationDataModule\ndata_path: ???\nexclude_filter: [\".ipynb_checkpoints\"]\ninclude_filter:\nseed: ${core.seed}\nnum_workers: 8\nbatch_size: 16\ntest_size: 0.2\nval_size: 0.2\ntrain_transform: ${transforms.train_transform}\ntest_transform: ${transforms.test_transform}\nval_transform: ${transforms.val_transform}\ntrain_split_file:\ntest_split_file:\nval_split_file:\nlabel_map:\nclass_to_idx:\nname:\ndataset:\n_target_: hydra.utils.get_method\npath: quadra.datasets.classification.ClassificationDataset\n</code></pre> <p>We give a small description for each tweakable parameter inside the base datamodule config:</p> <ul> <li><code>data_path</code>: Path to root_folder. \"???\" denotes mandatory parameters.</li> <li><code>exclude_filter</code>: If an image path contains one of the strings in this list, it will be ignored.</li> <li><code>include_filter</code>: If an image path does not contain one of the strings in this list, it will be ignored.</li> <li><code>seed</code>: Seed for experiment reproducibility (if training is on gpu, complete reproducibility can not be ensured).</li> <li><code>num_workers</code>: Number of workers used by the dataloaders (same for train/val/test).</li> <li><code>bath_size</code>: Batch size for the dataloaders (same for train/val/test).</li> <li><code>test_size</code>: If no test_split_file is provided, test_size * len(training_set) will be put in test set.</li> <li><code>val_size</code>: If no val_split_file is provided, val_size * len(remaining_training_set) will be put in validation set.</li> <li><code>label_map</code>: You can map classes to other ones in order to group more sub-classes into one macro-class.</li> <li><code>class_to_idx</code>: Map classes to indexes, if you need to ensure that one specific mapping is respected (otherwise a ordered class_to_idx is built)</li> </ul> <p>If you want to choose which files to put in training/val/test sets, provide the split files' paths in the respective fields in the datamodule config (train_split_file, val_split_file, test_split_file). You can leave val_split_file empty and the training set will be automatically split in train/val.</p> <p>The content of the files must be formatted in the same way for <code>train.txt</code>/<code>val.txt</code>/<code>test.txt</code>:</p> <pre><code>images/abc.xyz,class_1,class_2\nimages/abc_2.xyz,class_3\n</code></pre> <p>The first column is the path to the image, while the other columns are the labels associated with that image. The labels must be separated by a comma.</p>"},{"location":"tutorials/examples/classification.html#experiment","title":"Experiment","text":"<p>Suppose that we want to run the experiment on the given dataset, we can define a config starting from the base config (found under <code>configs/experiment/base/classification/classification.yaml</code>).</p> <pre><code># @package _global_\ndefaults:\n- override /backbone: resnet18\n- override /datamodule: base/classification\n- override /loss: cross_entropy\n- override /model: classification\n- override /optimizer: adam\n- override /task: classification\n- override /scheduler: rop\n- override /transforms: default_resize\n\ndatamodule:\nnum_workers: 8\nbatch_size: 32\ndata_path: ???\n\nprint_config: true\n\nmodel:\nmodule:\nlr_scheduler_interval: \"epoch\"\n\ntask:\nlr_multiplier: 0.0\ngradcam: true\nrun_test: True\nreport: True\noutput:\nexample: True\n\n\ncore:\ntag: \"run\"\nupload_artifacts: true\nname: classification_base_${trainer.max_epochs}\n\nlogger:\nmlflow:\nexperiment_name: classification_base\nrun_name: ${core.name}\n\nbackbone:\nmodel:\npretrained: True\nfreeze: False\nfreeze_parameters_name:\n\ntrainer:\nprecision: 32\nmax_epochs: 200\ncheck_val_every_n_epoch: 1\nlog_every_n_steps: 1\ndevices: [0]\n\nscheduler:\npatience: 20\nfactor: 0.9\nverbose: False\nthreshold: 0.01\n\ncallbacks:\nearly_stopping:\n_target_: pytorch_lightning.callbacks.EarlyStopping\nmonitor: val_loss_epoch\nmin_delta: 0.01\nmode: min\npatience: 35\nverbose: false\nstopping_threshold: 0\nmodel_checkpoint:\nmonitor: val_loss_epoch\n</code></pre> <p>The base experiment will train a resnet18 (by default pretrained on Imagenet) for 200 epochs using Adam as optimizer and reducing the learning rate on plateaus.</p> <p>We can define a custom experiment starting from the base one, and override the parameters that we want to change. Suppose we create a yaml configuration under <code>configs/experiment/custom_experiment/torch_classification.yaml</code> with the following content:</p> <pre><code># @package _global_\ndefaults:\n- base/classification/classification\n- override /backbone: vit16_tiny\n- _self_\n\nexport:\ntypes: [onnx, torchscript]\n\ndatamodule:\nnum_workers: 12\nbatch_size: 32\ndata_path: path/to/experiment/dataset\nclass_to_idx:\nclass_1: 0\nclass_2: 1\nclass_3: 2\n\ntask:\ngradcam: True # Enable gradcam computation during evaluation\nrun_test: True # Perform test evaluation at the end of training\nreport: True output:\nexample: True # Generate an example of concordants and discordants predictions for each class\n\n\nmodel:\nmodule:\nlr_scheduler_interval: \"epoch\"\n\nbackbone:\nmodel:\npretrained: True\nfreeze: False\nfreeze_parameters_name:\n- conv1\n- bn1\n- layer1\n- layer2\n\ncore:\ntag: \"run\"\nname: \"train_core_name\"\n\nlogger:\nmlflow:\nexperiment_name: name_of_the_experiment\nrun_name: ${core.name}\n</code></pre> <p>Warning</p> <p>Remember to set the mandatory parameter \"data_path\".</p>"},{"location":"tutorials/examples/classification.html#run","title":"Run","text":"<p>Assuming that you have created a virtual environment and installed the <code>quadra</code> library, you can run the experiment by running the following command:</p> <pre><code>quadra experiment=custom_experiment/torch_classification\n</code></pre> <p>This should produce the following output files:</p> <pre><code>checkpoints           config_tree.txt  deployment_model  test\nconfig_resolved.yaml  data             main.log\n</code></pre> <p>Where <code>checkpoints</code> contains the pytorch lightning checkpoints of the model, <code>data</code> contains the joblib dump of the datamodule with its parameters and dataset split, <code>deployment_model</code> contains the model in exported format (in this case onnx and torchscript, but by default is only torchscript), <code>test</code> contains the test artifacts.</p>"},{"location":"tutorials/examples/classification.html#evaluation","title":"Evaluation","text":""},{"location":"tutorials/examples/classification.html#experiment_1","title":"Experiment","text":"<p>The same datamodule specified before can be used for inference. There are different modalities to define the test-set. The simplest one is setting test_size=1.0 (remember the .0) and data_path=path/to/another_root_folder, where \"another_root_folder\" has the same structure as the root_folder described at the start of this document, but it contains only images you want to use for tests. Another possibility is to pass a test_split_file to the datamodule config:</p> <pre><code>test_split_file: path/to/test_split_file.txt\n</code></pre> <p>Where test_split_file is a simple .txt file structured in this way: <pre><code>class_1/image1.png\nclass_1/image2.png\n...\nclass_2/image1.png\nclass_2/image2.png\n...\n</code></pre></p> <p>Where each line contains the relative path to the image from the data_path folder.</p> <p>The default experiment configuration can be found at <code>configs/experiment/base/classification/classification_evaluation.yaml</code>:</p> <pre><code># @package _global_\ndefaults:\n- override /datamodule: base/classification\n- override /transforms: default_resize\n\ndatamodule:\nnum_workers: 6\nbatch_size: 32\n\ncore:\ntag: \"run\"\nupload_artifacts: true\nname: classification_evalutation_base\n\nlogger:\nmlflow:\nexperiment_name: name_of_the_experiment\nrun_name: ${core.name}\n\ntask:\n_target_: quadra.tasks.ClassificationEvaluation\ngradcam: true\noutput:\nexample: true\nmodel_path: ???\n</code></pre> <p>Given that we don't have to set all the training-related parameters, the evaluation experiment .yaml file will be much simpler, suppose it is saved under <code>configs/experiment/custom_experiment/torch_classification_evaluation.yaml</code>:</p> <pre><code># @package _global_\ndefaults:\n- base/classification/classification_evaluation\n- _self_\n\ndatamodule:\nnum_workers: 6\nbatch_size: 32\ndata_path: path/to/test/dataset\ntest_size: 1.0\nclass_to_idx:\nclass_1: 0\nclass_2: 1\nclass_3: 2\n\ncore:\ntag: \"run\"\nupload_artifacts: true\nname: eval_core_name\n\ntask:\noutput:\nexample: true\nmodel_path: path/to/model.pth\n</code></pre> <p>Notice that we must provide the path to a deployment model file that will be used to perform inferences. In this case class_to_idx is mandatory (we can not infer it from a test-set). We suggest to be careful to set the same class_to_idx that has been used to train the model.</p>"},{"location":"tutorials/examples/classification.html#run_1","title":"Run","text":"<p>Just as before, assuming that you have created a virtual environment and installed the <code>quadra</code> library, you can run the experiment by running the following command:</p> <pre><code>quadra experiment=custom_experiment/torch_classification_evaluation\n</code></pre> <p>This will compute the metrics on the test-set and since <code>example</code> is set to <code>true</code> it will generate an example of concordants and discordants predictions for each class. </p>"},{"location":"tutorials/examples/multilabel_classification.html","title":"Pytorch multilabel classification example","text":"<p>In this page, we will show you how to run a multilabel classification experiment exploiting Pytorch and Pytorch Lightning to finetune or train from scratch a model on a custom dataset.</p> <p>This example will demonstrate how to create a custom experiment starting from default settings.</p>"},{"location":"tutorials/examples/multilabel_classification.html#training","title":"Training","text":""},{"location":"tutorials/examples/multilabel_classification.html#dataset","title":"Dataset","text":"<p>Let's start with the dataset that we are going to use. Since we are using the multilabel classification datamodule we must follow a precise structure for the dataset.</p> <pre><code>dataset/\n\u251c\u2500\u2500 images\n\u2502   \u251c\u2500\u2500 abc.xyz\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 samples.txt # optional\n\u251c\u2500\u2500 test.txt # optional\n\u251c\u2500\u2500 train.txt # optional\n\u2514\u2500\u2500 val.txt # optional\n</code></pre> <p>Either <code>samples.txt</code> or both <code>train.txt</code> and <code>test.txt</code> must be provided. If <code>samples.txt</code> is provided, it will be used to split the dataset into train/val/test based on the datamodule parameters. Otherwise, <code>train.txt</code> and <code>val.txt</code> will be used to split the dataset into train/val and <code>test.txt</code> will be used to create the test set.</p> <p>The content of the files is the same for both <code>samples.txt</code> and <code>train.txt</code>/<code>val.txt</code>/<code>test.txt</code>:</p> <pre><code>images/abc.xyz,class_1,class_2\nimages/abc_2.xyz,class_3\n</code></pre> <p>So the first column is the path to the image, while the other columns are the labels associated with that image. The labels must be separated by a comma.</p> <p>The standard datamodule configuration for classification is found under <code>configs/datamodule/base/multilabel_classification.yaml</code>.</p> <pre><code>_target_: quadra.datamodules.MultilabelClassificationDataModule\ndata_path: ???\nimages_and_labels_file: null\ntrain_split_file: null\ntest_split_file: null\nval_split_file: null\ndataset:\n_target_: hydra.utils.get_method\npath: quadra.datasets.classification.MultilabelClassificationDataset\nnum_classes: null\nnum_workers: 8\nbatch_size: 64\ntest_batch_size: 64\nseed: ${core.seed}\nval_size: 0.2\ntest_size: 0.2\ntrain_transform: ${transforms.train_transform}\ntest_transform: ${transforms.test_transform}\nval_transform: ${transforms.val_transform}\nclass_to_idx: null\n</code></pre> <p>The most important parameters are: - <code>data_path</code>: the path to the dataset folder - <code>images_and_labels_file</code>: the path to the <code>samples.txt</code> file - <code>train_split_file</code>: the path to the <code>train.txt</code> file - <code>test_split_file</code>: the path to the <code>test.txt</code> file - <code>val_split_file</code>: the path to the <code>val.txt</code> file</p>"},{"location":"tutorials/examples/multilabel_classification.html#experiment","title":"Experiment","text":"<p>Suppose that we want to run the experiment on the given dataset, we can define a config starting from the base config (found under <code>configs/experiment/base/classification/multilabel_classification.yaml</code>).</p> <pre><code># @package _global_\ndefaults:\n- override /backbone: resnet18\n- override /datamodule: base/multilabel_classification\n- override /loss: asl\n- override /model: multilabel_classification\n- override /optimizer: adam\n- override /task: classification\n- override /scheduler: rop\n- override /transforms: default_resize\n\ndatamodule:\nnum_workers: 8\nbatch_size: 32\ntest_batch_size: 32\n\nprint_config: true\n\ncore:\ntag: \"run\"\ntest_after_training: true\nupload_artifacts: true\nname: multilabel-classification\n\nlogger:\nmlflow:\nexperiment_name: ${core.name}\nrun_name: ${core.name}\n\nbackbone:\nmodel:\npretrained: True\nfreeze: False\n\ntrainer:\ndevices: [0]\nmax_epochs: 300\ncheck_val_every_n_epoch: 1\n</code></pre> <p>The base experiment will train a resnet18 (by default pretrained on Imagenet) for 300 epochs using Adam as optimizer and reducing the learning rate on plateaus. In we give a look inside the model configuration we will find that on top of the backbone there is a simple Linear layer mapping the output of the backbone to the number of classes.</p> <p>We can define a custom experiment starting from the base one, and override the parameters that we want to change. Suppose we create a yaml configuration under <code>configs/experiment/custom_experiment/torch_multilabel_classification.yaml</code> with the following content:</p> <pre><code># @package _global_\ndefaults:\n- base/classification/multilabel_classification\n- override /backbone: vit16_tiny\n- _self_\n\ndatamodule:\nnum_workers: 12\nbatch_size: 32\ndata_path: path/to/experiment/dataset\nimages_and_labels_file: ${datamodule.data_path}/samples.txt # We make use of hydra variable interpolation\nclass_to_idx:\nclass_1: 0\nclass_2: 1\nclass_3: 2\n\nmodel:\nclassifier:\nout_features: 3 # This is very important as it defines the number of classes\n\ntask:\nrun_test: True # Perform test evaluation at the end of training\nreport: False output:\nexample: False logger:\nmlflow:\nexperiment_name: name_of_the_experiment\nrun_name: ${core.name}\n</code></pre> <p>Warning</p> <p>At the current time the report generation is not supported for multilabel classification.</p>"},{"location":"tutorials/examples/multilabel_classification.html#run","title":"Run","text":"<p>Assuming that you have created a virtual environment and installed the <code>quadra</code> library, you can run the experiment by running the following command:</p> <pre><code>quadra experiment=custom_experiment/torch_multilabel_classification\n</code></pre> <p>This should produce the following output files:</p> <pre><code>checkpoints  config_resolved.yaml  config_tree.txt  data  deployment_model  main.log\n</code></pre> <p>Where <code>checkpoints</code> contains the pytorch lightning checkpoints of the model, <code>data</code> contains the joblib dump of the datamodule with its parameters and dataset split, <code>deployment_model</code> contains the model in exported format (default is torchscript).</p>"},{"location":"tutorials/examples/segmentation.html","title":"Segmentation Example","text":"<p>In this page, we will show you how to run a segmentation experiment (either binary or multiclass) step by step.</p>"},{"location":"tutorials/examples/segmentation.html#training","title":"Training","text":"<p>This example will demonstrate how to create a custom experiment starting from default settings.</p>"},{"location":"tutorials/examples/segmentation.html#dataset","title":"Dataset","text":"<p>Let's start with the dataset we are going to use. Since we are using base segmentation datamodule, we must arrange our images and masks in a folder structure that follows the segmentation datamodule guideline defined in the segmentation datamodule documentation. Imagine that we have a dataset with the following structure:</p> <pre><code>dataset/\n\u251c\u2500\u2500 images\n\u2502   \u2514\u2500\u2500 1.png\n\u251c\u2500\u2500 masks\n\u2502   \u2514\u2500\u2500 1.png\n\u251c\u2500\u2500 test.txt\n\u251c\u2500\u2500 train.txt\n\u2514\u2500\u2500 val.txt\n</code></pre> <p>The main difference between multi class segmentation and binary segmentation is that masks contain multiple values.  For example, if we have a project that uses the following classes:</p> <ul> <li><code>0</code>: background</li> <li><code>1</code>: apple</li> <li><code>2</code>: orange</li> <li><code>3</code>: banana</li> </ul> <p>Suppose you have a 4x4 image, a possible mask for it may be the following:</p> \\[ \\begin{bmatrix} 1 &amp; 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 &amp; 2 \\\\ 0 &amp; 1 &amp; 0 &amp; 2 \\\\ 0 &amp; 0 &amp; 3 &amp; 3 \\\\ \\end{bmatrix} \\] <p>The base datamodule configuration file <code>datamodule/base/segmentation.yaml</code> is defined as follows:</p> <pre><code>_target_: quadra.datamodules.SegmentationMulticlassDataModule\ndata_path: ???\nidx_to_class:\ntest_size: 0.3\nval_size: 0.3\nseed: 42\nbatch_size: 32\nnum_workers: 6\ntrain_transform: ${transforms.train_transform}\ntest_transform: ${transforms.test_transform}\nval_transform: ${transforms.val_transform}\ntrain_split_file:\ntest_split_file:\nval_split_file:\nexclude_good: false\nnum_data_train:\none_hot_encoding:\n</code></pre> <p>All the parameters with value <code>???</code> must be provided by the user, particularly the <code>data_path</code> parameter represents the path to the dataset, since we also have split files we need to provide them as well. We will do it in the experiment configuration file.</p>"},{"location":"tutorials/examples/segmentation.html#backbone","title":"Backbone","text":"<p>In this example, we will compare models from <code>segmentation_models.pytorch</code> library. There is already a function implemented to load a model from this library acting as a bridge between that library and ours. It is defined as follows under <code>quadra.modules.backbone</code>:</p> <pre><code>def create_smp_backbone(\n    arch: str,\n    encoder_name: str,\n    freeze_encoder: bool = False,\n    in_channels: int = 3,\n    num_classes: int = 0,\n    **kwargs: Any,\n):\n\"\"\"Create Segmentation.models.pytorch model backbone\n    Args:\n        arch: architecture name\n        encoder_name: architecture name\n        freeze_encoder: freeze encoder or not\n        in_channels: number of input channels\n        num_classes: number of classes\n        **kwargs: extra arguments for model (for example classification head).\n    \"\"\"\n    model = smp.create_model(\n        arch=arch, encoder_name=encoder_name, in_channels=in_channels, classes=num_classes, **kwargs\n    )\n    if freeze_encoder:\n        for child in model.encoder.children():\n            for param in child.parameters():\n                param.requires_grad = False\n    return model\n</code></pre> <p>The default configuration can be found under <code>configs/backbone/smp.yaml</code>:</p> <pre><code>model:\n_target_: quadra.modules.backbone.create_smp_backbone\narch: unet\nencoder_name: resnet18\nencoder_weights: imagenet\nfreeze_encoder: True\nin_channels: 3\nnum_classes: 1\nactivation: null\n</code></pre> <p>This will create a <code>Unet</code> model with <code>resnet18</code> encoder and pretrained <code>imagenet</code> weights. The encoder will be frozen and the number of input channels will be 3. The number of classes will be changed according to the dataset. The activation function is set to be <code>null</code> (converted to <code>None</code> in Python) which means that the model will output logits.</p> <p>Note</p> <p>These settings are provided by the <code>segmentation_models.pytorch</code> library. You can check the other settings from their documentation.</p> <p>Question</p> <p>Why do we need to create a backbone function?</p> <p>The <code>segmentation_models.pytorch</code> library provides a function to load the model from the <code>segmentation_models.pytorch</code> library. But we are limited by their customization. If we want to add/try new settings such as freezing the encoder, we need to create a new function to load the model. </p>"},{"location":"tutorials/examples/segmentation.html#experiment","title":"Experiment","text":"<p>For this example, we will extend base experiment configuration file located in <code>configs/experiment/base/segmentation/smp_multiclass.yaml</code> as follows:</p> <pre><code># @package _global_\n\ndefaults:\n- override /datamodule: base/segmentation_multiclass\n- override /model: smp_multiclass\n- override /optimizer: adam\n- override /scheduler: rop\n- override /transforms: default_resize\n- override /loss: smp_dice_multiclass\n- override /task: segmentation\n- override /backbone: smp\n- override /trainer: lightning_gpu\n\ncore:\ntag: \"run\"\nname: \"quadra_default\"\nupload_artifacts: True\n\ntrainer:\ndevices: [0]\nmax_epochs: 100\nnum_sanity_val_steps: 0\n\ndatamodule:\nnum_workers: 8\nbatch_size: 32\n</code></pre> <p>Let's assume that we would like to change some settings in the default configuration file and add the datamodule parameters. We can extend configuration file as follows creating a custom one in <code>configs/experiment/segmentation/custom_experiment/smp_multiclass.yaml</code>:</p> <pre><code># @package _global_\ndefaults:\n- base/segmentation/smp_multiclass  # use smp file as default\n- _self_ # use this file as final config\n\nexport:\ntypes: [onnx, torchscript]\n\nbackbone:\nmodel:\nnum_classes: 4 # The total number of classes (background + foreground)\n\ntask:\nrun_test: true # run test after training is completed\nreport: false # allows to generate reports\nevaluate: # custom evaluation toggles\nanalysis: false # Perform in depth analysis\n\ndatamodule:\ndata_path: /path/to/the/dataset # change the path to the dataset\nbatch_size: 64 # update batch size from 32 to 64\ntrain_split_file: ${datamodule.data_path}/train.txt # Use hydra variable interpolation to create path to the train split file\ntest_split_file: ${datamodule.data_path}/test.txt val_split_file: ${datamodule.data_path}/val.txt idx_to_class: # Contains the mapping of all classes without background\n1: \"apple\"\n2: \"orange\"\n3: \"banana\"\n\ntrainer:\ndevices: [3] # change gpu from 0 to 3\n\ncore:\nupload_artifacts: True # upload artifacts after training is completed (if supported by the current logger)\nname: \"custom_segmentation_experiment\" # change the name of experiment\n</code></pre> <p>Warning</p> <p>When defining the <code>idx_to_class</code> dictionary, the keys should be the class index and the values should be the class name. The class index starts from 1.</p> <p>In the final configuration experiment we have specified the path to the dataset, batch size, split files, GPU device, experiment name and toggled some evaluation options, moreover we have specified that we want to export the model to <code>onnx</code> and <code>torchscript</code> formats.</p> <p>By default data will be logged to <code>Mlflow</code>. If <code>Mlflow</code> is not available it's possible to configure a simple csv logger by adding an override to the file above:</p> <pre><code># @package _global_\ndefaults:\n- base/segmentation/smp_multiclass  # use smp file as default\n- override /logger: csv # override logger to csv\n- _self_ # use this file as final config\n\n... # The rest of the configuration\n</code></pre>"},{"location":"tutorials/examples/segmentation.html#run","title":"Run","text":"<p>Assuming that you have created a virtual environment and installed the <code>quadra</code> library, you can run the experiment by running the following command:</p> <pre><code>quadra experiment=segmentation/custom_experiment/smp_multiclass\n</code></pre>"},{"location":"tutorials/examples/segmentation.html#run-advanced","title":"Run (Advanced)","text":"<p>Lets assume that you would like to run the experiment with different models and with/without freezing the encoder part of the model, thanks to <code>hydra</code> multi-run option we can run as follows:</p> <pre><code>quadra experiment=segmentation/custom_experiment/smp_multiclass \\\nbackbone.model.arch=unet,unetplusplus \\\nbackbone.model.encoder_name=resnet18,resnet50 \\\nbackbone.model.freeze_encoder=False,True \\\n--multirun\n</code></pre> <p>Note</p> <ul> <li>Each comma seperated value will be treated as a separate run. In this example will have a total of 8 experiments running. After running this command you may grab a \u2615 and wait for the results. </li> <li><code>--multirun</code> option allows hydra to parse the command-line arguments and create multiple runs.</li> </ul>"},{"location":"tutorials/examples/sklearn_classification.html","title":"Sklearn classification example","text":"<p>In this page, we will show you how to train an Sklearn classifier using a Pytorch feature extractor.</p> <p>This example will demonstrate how to create custom experiments starting from default settings.</p>"},{"location":"tutorials/examples/sklearn_classification.html#training","title":"Training","text":""},{"location":"tutorials/examples/sklearn_classification.html#dataset","title":"Dataset","text":"<p>Let's start with the dataset that we are going to use. Since we are using the base datamodule, we need to organize the data in a specific way. The base datamodule expects the following structure:</p> <pre><code>dataset/\n\u251c\u2500\u2500 class_0 \n\u2502   \u251c\u2500\u2500 abc.xyz\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 class_1\n\u2502   \u251c\u2500\u2500 abc.xyz\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 class_N \n\u2502   \u251c\u2500\u2500 abc.xyz\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 None \n\u2502   \u251c\u2500\u2500 abc.xyz\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 train.txt # optional\n\u2514\u2500\u2500 test.txt # optional\n</code></pre> <p>Note</p> <p>Only for inference a None class representing unknown data can be used. If a folder with the same name during training it will be interpreted as a class.</p> <p>The <code>train.txt</code> and <code>test.txt</code> files are optional and can be used to specify the list of images to use for training (validation) and testing. If not specified, the datamodule will base the split using a different parameter. The files should contain the relative path to the image from the dataset root folder. For example, if the dataset is organized as above, the <code>train.txt</code> file could be:</p> <pre><code>class_0/abc.xyz\n...\nclass_1/abc.xyz\n...\nclass_N/abc.xyz\n...\n</code></pre> <p>The standard datamodule configuration for this kind of task is found under <code>datamodule/base/sklearn_classification.yaml</code>.</p> <pre><code>_target_: quadra.datamodules.SklearnClassificationDataModule\ndata_path: ???\nexclude_filter:\ninclude_filter:\nval_size: 0.3\nclass_to_idx:\nlabel_map:\nseed: ${core.seed}\nbatch_size: 32\nnum_workers: 8\ntrain_transform: ${transforms.train_transform}\nval_transform: ${transforms.val_transform}\ntest_transform: ${transforms.test_transform}\nroi:\nn_splits: 1\nphase:\ncache: false\nlimit_training_data:\ntrain_split_file:\ntest_split_file:\n</code></pre> <p>The only required parameter is <code>data_path</code> which should point to the dataset root folder. The other parameters can be used to customize the datamodule behavior, the most important parameters are:</p> <ul> <li><code>val_size</code>: The percentage of the dataset to use for validation (if test_split_file is not specified)</li> <li><code>class_to_idx</code>: A dictionary mapping class names to class indexes</li> <li><code>label_map</code>: A dictionary mapping groups of classes to a single class (E.g. \"good\": [\"class_1\", \"class_2\"]), it may be useful for testing different scenarios or simplify the classification task</li> <li><code>roi</code>: Optional region of intereset in the following format (x_upper_left, y_upper_left, x_bottom_right, y_bottom_right)</li> <li><code>n_splits</code>: The number of splits to use to partition the dataset, if 1 the dataset will be split in train and test, if &gt; 1 cross-validation will be used</li> <li><code>cache</code>: If cross validation is used is it possible to cache the features extracted from the backbone to speed up the process</li> <li><code>limit_training_data</code>: If specified, the datamodule will use only a subset of the training data (useful for debugging)</li> <li><code>train_split_file</code>: If specified, the datamodule will use the given file to create the train dataset (which will be the base for validation)</li> <li><code>test_split_file</code>: If specified, the datamodule will use the given file to create the test dataset</li> </ul> <p>No matter if cross validation or standard train/test split is used, the final model will be trained on the whole training dataset, the splits are only used to validate the model.</p>"},{"location":"tutorials/examples/sklearn_classification.html#experiment","title":"Experiment","text":"<p>Suppose that we want to run the experiment on the given dataset, we can define a config starting from the base config: <pre><code># @package _global_\n\ndefaults:\n- override /model: logistic_regression\n- override /transforms: default_resize\n- override /task: sklearn_classification\n- override /backbone: dino_vitb8\n- override /trainer: sklearn_classification\n- override /datamodule: base/sklearn_classification\n\nexport:\ntypes: [pytorch, torchscript]\n\nbackbone:\nmodel:\npretrained: true\nfreeze: true\n\ncore:\ntag: \"run\"\nname: \"sklearn-classification\"\n\ndatamodule:\nnum_workers: 8\nbatch_size: 32\nphase: train\nn_splits: 1\n</code></pre></p> <p>By default the experiment will use dino_vitb8 as backbone, resizing the images to 224x224 and training a logistic regression classifier. Setting the <code>n_splits</code> parameter to 1 will use a standard 70/30 train/validation split (given the parameters specified in the base datamodule) instead of cross validation. It will also export the model in two formats, \"torchscript\" and \"pytorch\".</p> <p>An actual configuration file based on the above could be this one (suppose it's saved under <code>configs/experiment/custom_experiment/sklearn_classification.yaml</code>):</p> <pre><code># @package _global_\n\ndefaults:\n- base/classification/sklearn_classification\n- override /backbone: resnet18\n- _self_\n\ncore:\nname: experiment-name\n\nexport:\ntypes: [pytorch, torchscript]\n\ndatamodule:\ndata_path: path_to_dataset\nbatch_size: 64\nclass_to_idx:\nclass_0: 0\nclass_1: 1\nclass_2: 2\nn_splits: 5\ntrain_split_file: ${datamodule.data_path}/train.txt\ntest_split_file: ${datamodule.data_path}/test.txt\n\n\ntask:\ndevice: cuda:0\nhalf_precision: false\ngradcam: false\nautomatic_batch_size:\nstarting_batch_size: 1024\ndisable: true\nsave_model_summary: false\noutput:\nfolder: classification_experiment\nreport: true\nexample: true\ntest_full_data: true\n</code></pre> <p>This will train a logistic regression classifier using a resnet18 backbone, resizing the images to 224x224 and using a 5-fold cross validation. The <code>class_to_idx</code> parameter is used to map the class names to indexes, the indexes will be used to train the classifier. The <code>output</code> parameter is used to specify the output folder and the type of output to save. The <code>export.types</code> parameter can be used to export the model in different formats, at the moment <code>torchscript</code>, <code>onnx</code> and <code>pytorch</code> are supported. The backbone (in torchscript and pytorch format) will be saved along with the classifier. <code>test_full_data</code> is used to specify if a final test should be performed on all the data (after training on the training and validation datasets). It's possible to enable half precision training by setting <code>half_precision</code> to <code>true</code> and export gradcam results by setting <code>gradcam</code> to <code>true</code>.</p> <p>Optionally it's possible to enable the automatic batch size finder by setting <code>automatic_batch_size.disable</code> to <code>false</code>. This will try to find the maximum batch size that can be used on the given device without running out of memory. The <code>starting_batch_size</code> parameter is used to specify the starting batch size to use for the search, the algorithm will start from this value and will try to divide it by two until it doesn't run out of memory. Finally, the <code>save_model_summary</code> parameter can be used to save the backbone information in a text file called <code>model_summary.txt</code> located in the root of the output folder.</p>"},{"location":"tutorials/examples/sklearn_classification.html#run","title":"Run","text":"<p>Assuming that you have created a virtual environment and installed the <code>quadra</code> library, you can run the experiment by running the following command:</p> <pre><code>quadra experiment=custom_experiment/sklearn_classification\n</code></pre> <p>This will run the experiment training a classifier and saving metrics and reports under the <code>classification_experiment</code> folder.</p> <p>The output folder should contain the following entries: <pre><code>classification_experiment    classification_experiment_3  data\nclassification_experiment_0  classification_experiment_4  deployment_model\nclassification_experiment_1  config_resolved.yaml         main.log\nclassification_experiment_2  config_tree.txt              test\n</code></pre></p> <p>Each <code>classification_experiment_X</code> folder contains the metrics for the corresponding fold while the <code>classification_experiment</code> folder contains the metrics computed aggregating the results of all the folds.</p> <p>The <code>data</code> folder contains a joblib version of the datamodule containing parameters and splits for reproducibility. The <code>deployment_model</code> folder contains the backbone exported in torchscript and pytorch format alongside the joblib version of trained classifier. The <code>test</code> folder contains the metrics for the final test on all the data after the model has been trained on both train and validation.</p>"},{"location":"tutorials/examples/sklearn_classification.html#evaluation","title":"Evaluation","text":"<p>The same datamodule specified before can be used for inference by setting the <code>phase</code> parameter to <code>test</code>. </p>"},{"location":"tutorials/examples/sklearn_classification.html#experiment_1","title":"Experiment","text":"<p>The default experiment config is found under <code>configs/experiment/base/classification/sklearn_classification_test.yaml</code>.</p> <pre><code># @package _global_\n\ndefaults:\n- override /transforms: default_resize\n- override /task: sklearn_classification_test\n- override /trainer: sklearn_classification\n- override /datamodule: base/sklearn_classification\n\ncore:\ntag: run\nname: sklearn-classification-test\n\ndatamodule:\nnum_workers: 8\nbatch_size: 32\nphase: test\n</code></pre> <p>An actual configuration file based on the above could be this one (suppose it's saved under <code>configs/experiment/custom_experiment/sklearn_classification_test.yaml</code>):</p> <pre><code># @package _global_\ndefaults:\n- base/classification/sklearn_classification_test\n- _self_\n\ncore:\nname: experiment-test-name\n\ndatamodule:\ndata_path: path_to_test_dataset\nbatch_size: 64\n\ntask:\ndevice: cuda:0\ngradcam: true\noutput:\nfolder: classification_test_experiment\nreport: true\nexample: true\nmodel_path: ???\n</code></pre> <p>This will test the model trained in the given experiment on the given dataset. The experiment results will be saved under the <code>classification_test_experiment</code> folder. If gradcam is set to True, original and gradcam results will be saved during the generate_report(). Model_path must point to a model file. It could either be a '.pt'/'.pth' or a backbone_config '.yaml' file.</p>"},{"location":"tutorials/examples/sklearn_classification.html#run_1","title":"Run","text":"<p>Same as above, assuming that you have created a virtual environment and installed the <code>quadra</code> library, you can run the experiment by running the following command:</p> <pre><code>quadra experiment=custom_experiment/sklearn_classification_test\n</code></pre>"},{"location":"tutorials/examples/sklearn_patch_classification.html","title":"Sklearn patch classification example","text":"<p>In this page, we will show you how to train an Sklearn classifier using a Pytorch feature extractor in a patch based fashion.</p> <p>This example will demonstrate how to create custom experiments starting from default settings.</p>"},{"location":"tutorials/examples/sklearn_patch_classification.html#training","title":"Training","text":""},{"location":"tutorials/examples/sklearn_patch_classification.html#dataset","title":"Dataset","text":"<p>Let's start with the dataset that we are going to use; since we are using the base patch datamodule, we need to organize the data in a specific way. The base patch datamodule expects the following structure:</p> <pre><code>patch_dataset/\n\u251c\u2500\u2500 info.json\n\u251c\u2500\u2500 original \n\u2502   \u251c\u2500\u2500 images\n\u2502   |   \u2514\u2500\u2500 abc.xyz\n\u2502   \u251c\u2500\u2500 masks\n|   |   \u2514\u2500\u2500 abc.png\n|   \u2514\u2500\u2500 labelled_masks\n|       \u2514\u2500\u2500 abc.png\n\u251c\u2500\u2500 train\n\u2502   \u251c\u2500\u2500 abc_0.h5\n\u2502   \u251c\u2500\u2500 abc_N.h5\n\u2502   \u2514\u2500\u2500 dataset.txt\n\u251c\u2500\u2500 val\n\u2502   \u251c\u2500\u2500 class_0\n\u2502   |   \u2514\u2500\u2500 abc_X.xyz\n\u2502   \u251c\u2500\u2500 class_1\n|   |   \u2514\u2500\u2500 abc_Y.xyz\n|   \u2514\u2500\u2500 class_N\n|       \u2514\u2500\u2500 abc_Z.xyz\n\u2514\u2500\u2500 test\n    \u251c\u2500\u2500 class_0\n    |   \u2514\u2500\u2500 abc_X.xyz\n    \u251c\u2500\u2500 class_1\n    |   \u2514\u2500\u2500 abc_Y.xyz\n    \u2514\u2500\u2500 class_N\n        \u2514\u2500\u2500 abc_Z.xyz\n</code></pre> <p>To achieve this structure we need to use two different functions taken from <code>quadra</code> utilities.</p> <ul> <li><code>quadra.utils.patch.get_image_mask_association</code>: This function will create a list of dictionaries mapping images and masks (if available).</li> <li><code>quadra.utils.patch.generate_patch_dataset</code>: This function will generate the dataset following the structure above.</li> </ul> <p>For example imagine we have a standard dataset with the following structure:</p> <pre><code>dataset/\n\u251c\u2500\u2500 images\n|   \u251c\u2500\u2500 abc.xyz\n\u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 masks\n    \u251c\u2500\u2500 abc.png\n    \u2514\u2500\u2500 ...\n</code></pre> <p>By calling <code>get_image_mask_association</code> passing the right data_folder and mask_folder, we will get the following dictionary:</p> <pre><code>[\n{\n\"base_name\": \"abc.png\",\n\"path\": \"dataset/images/abc.png\",\n\"mask\": \"dataset/masks/abc.png\"\n}, ...\n]\n</code></pre> <p>The function allows also to specify an extension name for masks (E.g. _mask) for the mapping, for more details see the function documentation. This is just an helper, you can create the dictionary in any way you want, but the dictionary must follow this structure so that the <code>generate_patch_dataset</code> function can work.</p> <p>Note</p> <p>Masks contain the labels of the images in index format. For example, if the image has 3 classes, the mask will contain values between 0 and 2. The <code>class_to_idx</code> parameter of the <code>generate_patch_dataset</code> function will be used to map the index to the class name. 0 will be considered as the background class, as such it is generally ignored when making predictions. </p> <p>Now that we have the dictionary, we can call <code>generate_patch_dataset</code> to generate the dataset. Here it's an example:</p> <pre><code>generate_patch_dataset(\n    data_dictionary: my_dictionary,\n    class_to_idx: {\"background\": 0, \"class_1\": 1, \"class_2\": 2},\n    val_size: 0.3,\n    test_size: 0.1,\n    seed: 42,\n    patch_number: [16, 16],\n    patch_size: None, # Either patch_size or patch_number must be specified\n    overlap: 0.0,\n    save_original_images_and_masks: bool = True,\n    output_folder: \"patch_dataset\",\n    area_threshold: 0.45,\n    area_defect_threshold: 0.2,\n    mask_extension: \"_mask\",\n    mask_output_folder: None,\n    save_mask: False,\n    clear_output_folder: False,\n    mask_preprocessing: None,\n    train_filename: \"dataset.txt\",\n    repeat_good_images: 1,\n    balance_defects: True,\n    annotated_good: None,\n    num_workers: 1,\n) \n</code></pre> <p>This function will generate the dataset in the specified output folder. The function has a lot of parameters, but the most important are:</p> <ul> <li><code>data_dictionary</code>: The dictionary containing the mapping between images and masks.</li> <li><code>class_to_idx</code>: A dictionary mapping the class names to the corresponding index.</li> <li><code>patch_number</code>: Specify the number of [vertical, horizontal] patches to extract from each image. If the image is not divisible by the patch number the patch size and overlap will be adjusted to fit the image as much as possible, if some part of the image is not covered properly on the edges that part will be taken from the border of the image and the reconstruction will be done accordingly.</li> <li><code>patch_size</code>: Specify the size [h, w] of the patches to extract from each image. If this is specified, the <code>patch_number</code> parameter will be ignored.</li> <li><code>overlap</code>: The overlap between patches. This is a float between 0 and 1.</li> <li><code>area_threshold</code>: The minimum percentage area of the patch that must be covered by an annotation to be considered of that class. For example if we have 224x224 patches and an area_threhsold of 0.2, the patch will be considered as a specific class if the area of an annotation overlapping that patch it at least (224 * 224 * 0.2) pixels, otherwise it will be considered as background.</li> <li><code>area_defect_threshold</code>: The minimum area of an annotation to consider the patch as a defect. This is useful when you have small annotations that will always be considered as good patches by the <code>area_threshold</code> parameter. For example if we have 224x224 patches and an area_defect_threshold of 0.2, even if the area overlapping is smaller than the one specified by the <code>area_threshold</code> parameter, the patch will be considered as a defect if the area of the annotation itself is at least the 20% of the totoal area of the annotation.</li> <li><code>repeat_good_images</code>: The number of times good h5 will be repeated in the train txt.</li> <li><code>balance_defects</code>: If True, the number of good and bad patches will be balanced in the train txt.</li> <li><code>annotated_good</code>: Specify the class of the good patches. If None, all the patches with an annotation will be considered as defected patches and only the index 0 will be considered as good.</li> <li><code>save_original_images_and_masks</code>: By default images and masks will be copied in the new dataset folder so that they can be used in the future even if the original dataset is deleted or moved. If this is set to False, the original images and masks will not be copied and the h5 files for training will contain a reference to the original images and masks.</li> </ul> <p>The mask related parameters are used to save on disk also masks for potential patch based segmentation, but they are not used for this part. Eventually it's possible to specify a <code>mask_preprocessing</code> function that will be applied to the masks before extracting patches (for example to convert the masks to a binary format).</p> <p>This function will generate multiple subfolders in the output folder:</p> <ul> <li><code>original</code>: This folder will contain the original images and masks (if available) if the <code>save_original_images_and_masks</code> parameter is set to True.</li> <li><code>train</code>: This folder will contain the train h5 files and the train txt file.</li> <li><code>val</code>: This folder will contain the validation images in the standard classification split (based on label).</li> <li><code>test</code>: This folder will contain the test images in the standard classification split (based on label).</li> </ul> <p>To create the training dataset each single annotation is converted into a polygon, the polygon is divided into triangles using polygon triangulation, this allow for sampling uniformly center points for patches extraction (see this example), triangles and their sample probability are saved inside h5 files alongside information about the original image and the annotation. The h5 files are then used to sample patches during training. A txt file (generally named <code>dataset.txt</code>) is also generated and contains the list of h5 files to use for training. By regulating the replication of good images and the balancing of good and bad patches, it's possible to control the ratio of good and bad patches in the training dataset.</p> <p>Validation and test datasets are generated by simply splitting the original images into patches of the specified size and saving them in the corresponding folder based on the label. </p> <p>During validation, test and inference a prediction is made for each patch and the final prediction is obtained by aggregating the predictions of the patches. There are currently two ways to aggregate the predictions:</p> <ul> <li><code>major_voting</code>: The pixel prediction will be the class with the highest number of votes in all the patches that overlap that pixel, in case of tie the order is established by the class index.</li> <li><code>priority</code>: The pixel prediction is determined by the class of the patch with the highest priority. The priority is determined by the order of the classes in the <code>class_to_idx</code> from high to low.</li> </ul> <p>Finally a <code>info.json</code> file is generated and saved in the output folder. This file contains information about the dataset, like the number of patches, which annotated_good classes were used, and the list of basenames for train, val and test. This file is loaded by the datamodule and used for different task operations.</p> <p>The standard datamodule configuration for patch training is found under <code>datamodule/base/sklearn_classification_patch.yaml</code>.</p> <pre><code>_target_: quadra.datamodules.PatchSklearnClassificationDataModule\ndata_path: path_to_patch_dataset\ntrain_filename: dataset.txt\nexclude_filter:\ninclude_filter:\nclass_to_idx: this should be the same as the one used to generate the dataset\nseed: 42\nbatch_size: 32\nnum_workers: 8\ntrain_transform: ${transforms.train_transform}\ntest_transform: ${transforms.test_transform}\nval_transform: ${transforms.val_transform}\nbalance_classes: false\nclass_to_skip_training:\n</code></pre> <p>If <code>balance_classes</code> is set to true, the classes will be balanced by randomly replicating samples for the less frequent classes. This is useful when the dataset is imbalanced. <code>class_to_skip_training</code> can be used to specify a list of classes that will be excluded from training. This classes will be also ignored during validation. This is particularly useful to skip the background class (generally index 0) when the image is just partially annotated and the background may contain actual defects.</p>"},{"location":"tutorials/examples/sklearn_patch_classification.html#experiment","title":"Experiment","text":"<p>Suppose that we want to run the experiment on the given dataset, we can define a config starting from the base config: <pre><code># @package _global_\n\ndefaults:\n- override /model: logistic_regression\n- override /transforms: default_resize\n- override /task: sklearn_classification_patch\n- override /backbone: dino_vitb8\n- override /trainer: sklearn_classification\n- override /datamodule: base/sklearn_classification_patch\n\nbackbone:\nmodel:\npretrained: true\nfreeze: true\n\ncore:\ntag: \"run\"\nname: \"sklearn-classification-patch\"\n\ntrainer:\niteration_over_training: 20 # Regulate how many patches are extracted\n\ndatamodule:\nnum_workers: 8\nbatch_size: 32\n</code></pre></p> <p>By default the experiment will use dino_vitb8 as backbone, resizing the images to 224x224 and training a logistic regression classifier. Patches are extracted iterating 20 times over the training dataset (since sampling is random) to get more information.</p> <p>An actual configuration file based on the above could be this one (suppose it's saved under <code>configs/experiment/custom_experiment/sklearn_classification_patch.yaml</code>): <pre><code># @package _global_\n\ndefaults:\n- base/classification/sklearn_classification_patch\n- override /backbone: resnet18\n- _self_\n\nexport:\ntypes: [torchscript]\n\ncore:\nname: experiment-name\n\ndatamodule:\ndata_path: path_to_patch_dataset\nbatch_size: 256\nclass_to_idx:\nbackground: 0\nclass_1: 1\nclass_2: 2\nclass_to_skip_training:\n- background\n\ntask:\ndevice: cuda:2\nhalf_precision: false\nautomatic_batch_size:\nstarting_batch_size: 1024\ndisable: true\noutput:\nfolder: classification_patch_experiment\nreport: true\nexample: true\nreconstruction_method: major_voting\n</code></pre></p> <p>This will train a resnet18 model on the given dataset, using 256 as batch size and skipping the background class during training. The experiment results will be saved under the <code>classification_patch_experiment</code> folder. The deployment model will be generated but only the classifier will be saved (in joblib format), to reconstruct patches for evaluation the <code>major_voting</code> method will be used. It is possible to extract features in half precision by setting <code>half_precision</code> to true. The <code>automatic_batch_size</code> parameter can be used to automatically adjust the batch size to fit the memory of the device. The <code>starting_batch_size</code> parameter is used to specify the starting batch size, the algorithm will try to decrease the batch size until it can fit the batch into memory. The <code>disable</code> parameter can be used to disable the automatic batch size adjustment.</p>"},{"location":"tutorials/examples/sklearn_patch_classification.html#run","title":"Run","text":"<p>Assuming that you have created a virtual environment and installed the <code>quadra</code> library, you can run the experiment by running the following command:</p> <pre><code>quadra experiment=custom_experiment/sklearn_classification_patch\n</code></pre> <p>This will run the experiment training a classifier and validating on the validation dataset. Patch based and reconstruction based metrics will be computed and saved under the task output folder. The output folder should contain the following files:</p> <pre><code>classification_patch_experiment  data              reconstruction_results.json\nconfig_resolved.yaml             deployment_model\nconfig_tree.txt                  main.log\n</code></pre> <p>Inside the <code>classification_patch_experiment</code> folder you should find some report utilities computed over the validation dataset, like the confusion matrix. The <code>reconstruction_results.json</code> file contains the reconstruction metrics computed over the validation dataset in terms of covered defects, it will also contain the coordinates of the polygons extracted over predicted areas of the image with the same label.</p> <p>The <code>data</code> folder contains a joblib version of the datamodule containing parameters and splits for reproducibility. The <code>deployment_model</code> folder contains the backbone exported in torchscript format alongside the joblib version of trained classifier.</p>"},{"location":"tutorials/examples/sklearn_patch_classification.html#evaluation","title":"Evaluation","text":"<p>The same datamodule specified before can be used for inference. </p>"},{"location":"tutorials/examples/sklearn_patch_classification.html#experiment_1","title":"Experiment","text":"<p>The default experiment config is found under <code>configs/experiment/base/classification/sklearn_classification_patch_test.yaml</code>.</p> <pre><code># @package _global_\n\ndefaults:\n- override /transforms: default_resize\n- override /task: sklearn_classification_patch_test\n- override /datamodule: base/sklearn_classification_patch\n- override /trainer: sklearn_classification\n\ncore:\ntag: \"run\"\nname: \"sklearn-classification-patch-test\"\n\ndatamodule:\nnum_workers: 8\nbatch_size: 32\n</code></pre> <p>Here we specify again the backbone as it's required if the runtime model was generated without saving it in a deployment format.</p> <p>An actual configuration file based on the above could be this one (suppose it's saved under <code>configs/experiment/custom_experiment/sklearn_classification_patch_test.yaml</code>): <pre><code># @package _global_\ndefaults:\n- base/classification/sklearn_classification_patch_test\n- override /backbone: resnet18\n- _self_\n\ncore:\nname: experiment-test-name\n\ndatamodule:\ndata_path: path_to_patch_dataset\nbatch_size: 256\n\ntask:\ndevice: cuda:2 # Specify the device to use\noutput:\nfolder: classification_patch_test\nreport: true\nexample: true\nreconstruction_method: major_voting\nmodel_path: ???\n</code></pre></p> <p>This will test the model trained in the given experiment on the given dataset. The experiment results will be saved under the <code>classification_patch_test</code> folder. Patch reconstruction will be performed using the <code>major_voting</code> method (can be this or <code>priority</code>). The <code>model_path</code> parameter is required to specify the path to the trained model. It could either be a '.pt'/'.pth' or a backbone_config '.yaml' file. In this case is not necessary to specify <code>class_to_idx</code> and <code>class_to_skip_training</code> as they will be loaded from the training experiment.</p>"},{"location":"tutorials/examples/sklearn_patch_classification.html#run_1","title":"Run","text":"<p>Same as above, assuming that you have created a virtual environment and installed the  <code>quadra</code> library, you can run the experiment by running the following command:</p> <pre><code>quadra experiment=custom_experiment/sklearn_classification_patch_test\n</code></pre>"},{"location":"tutorials/examples/ssl.html","title":"Self Supervised Learning example","text":"<p>In this tutorial we will explain how to train a self-supervised learning model using <code>Quadra</code>. Particularly we will focus on the <code>Bootstrap your own latent</code>(BYOL) algorithm.</p>"},{"location":"tutorials/examples/ssl.html#training","title":"Training","text":""},{"location":"tutorials/examples/ssl.html#dataset","title":"Dataset","text":"<p>For self-supervised learning tasks, we will use the same classification dataset structure defined for the <code>ClassificationDataModule</code>. In fact the <code>SSLDataModule</code> is a subclass of <code>ClassificationDataModule</code> and it shares the same API and implementation so it's fairly easy to move from a classification task to a self-supervised learning task.</p> <pre><code>dataset/\n\u251c\u2500\u2500 class_0 \n\u2502   \u251c\u2500\u2500 abc.xyz\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 class_1\n\u2502   \u251c\u2500\u2500 abc.xyz\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 class_N \n\u2502   \u251c\u2500\u2500 abc.xyz\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 test.txt # optional\n\u251c\u2500\u2500 train.txt # optional\n\u2514\u2500\u2500 val.txt # optional\n</code></pre> <p>The <code>train.txt</code>, <code>val.txt</code> and <code>test.txt</code> files are optional and can be used to specify the list of images to use for training, validation and test. If not specified, the datamodule will base the split using a different parameter. The files should contain the relative path to the image from the dataset root folder. For example, if the dataset is organized as above, the <code>train.txt</code> file could be:</p> <pre><code>class_0/abc.xyz\n...\nclass_1/abc.xyz\n...\nclass_N/abc.xyz\n...\n</code></pre> <p>Validation is not required but it may be useful to evaluate the embeddings learned by the model during training for example using a linear classifier. The test set will be used to evaluate the model performance at the end of the training.</p> <p>The default datamodule configuration is found under <code>configs/datamodule/base/ssl.yaml</code> and it's defined as follows:</p> <pre><code>_target_: quadra.datamodules.SSLDataModule\ndata_path: ???\nexclude_filter:\ninclude_filter:\nseed: ${core.seed}\nnum_workers: 8\nbatch_size: 16\naugmentation_dataset: null\ntrain_transform: null\ntest_transform: ${transforms.test_transform}\nval_transform: ${transforms.val_transform}\ntrain_split_file:\nval_split_file:\ntest_split_file:\nval_size: 0.3\ntest_size: 0.1\nsplit_validation: true\nclass_to_idx:\n</code></pre> <p>We will make some changes to the datamodule in the experiment configuration file.</p>"},{"location":"tutorials/examples/ssl.html#experiment","title":"Experiment","text":"<p>First, let's check how base experiment configuration file is defined for BYOL algorithm located in <code>configs/experiment/base/ssl/byol.yaml</code>.</p> <pre><code># @package _global_\n\ndefaults:\n- override /datamodule: base/ssl\n- override /backbone: resnet18\n- override /model: byol\n- override /optimizer: lars\n- override /scheduler: warmup\n- override /transforms: byol\n- override /loss: byol\n- override /task: ssl\n- override /trainer: lightning_gpu_fp16\ncore:\ntag: \"run\"\nname: \"byol_ssl\"\ntask:\n_target_: quadra.tasks.ssl.BYOL\n\ncallbacks:\nmodel_checkpoint:\n_target_: pytorch_lightning.callbacks.ModelCheckpoint\nmonitor: \"val_acc\"\nmode: \"max\"\n\ntrainer:\ndevices: [0]\nmax_epochs: 500\nnum_sanity_val_steps: 0\ncheck_val_every_n_epoch: 10\n\ndatamodule:\nnum_workers: 12\nbatch_size: 256\naugmentation_dataset:\n_target_: quadra.datasets.TwoAugmentationDataset\ntransform:\n- ${transforms.augmentation1}\n- ${transforms.augmentation2}\ndataset: null\n\nscheduler:\ninit_lr:\n- 0.4\n</code></pre> <p>The default configuration can be used to train a <code>resnet18</code> model using <code>lars</code> as optimizer and a <code>cosine annealing</code> scheduler with warmup. Every 10 epoch we will perform a step of validation using a <code>KNN</code> classifier with 20 neighbors (check the model definition for more details). The model will be saved every time the validation accuracy improves and at the end of training.</p> <p>We will make use of automatic mixed precision to speed up the training process.</p> <p>Since we are going to use custom dataset for this task we can add a custom experiment configurations under <code>configs/experiment/custom_experiment/byol.yaml</code> file.</p> <pre><code># @package _global_\ndefaults:\n- base/ssl/byol\n- override /backbone: vit16_tiny # let's use different backbone instead of the resnet\n- _self_\n\ntrainer:\ndevices: [2] # we may need to use different gpu(s)\nmax_epochs: 1000 # let's assume we would like to train for 1000 epochs\n\ndatamodule:\ndata_path: /path/to/the/dataset\n</code></pre> <p>Note</p> <p>If you possess a GPU with bf16 support you can use the <code>lightning_gpu_bf16</code> trainer configuration instead of <code>lightning_gpu_fp16</code> by overriding the <code>trainer</code> section in the experiment configuration file.</p> <pre><code># @package _global_\ndefaults:\n- base/ssl/byol\n- override /backbone: vit16_tiny # let's use different backbone instead of the resnet\n- override /trainer: lightning_gpu_bf16\n- _self_\n... # rest of the configuration\n</code></pre>"},{"location":"tutorials/examples/ssl.html#run","title":"Run","text":"<p>Now we are ready to run our experiment with following command:</p> <pre><code>quadra experiment=custom_experiment/byol\n</code></pre> <p>The output folder should contain the following entries: <pre><code>checkpoints  config_resolved.yaml  config_tree.txt  data  deployment_model  main.log\n</code></pre></p> <p>The <code>checkpoints</code> folder contains the saved <code>pytorch</code> lightning checkpoints. The <code>data</code> folder contains a joblib version of the datamodule containing all parameters and dataset spits. The <code>deployment_model</code> folder contains the model ready for production in the format specified in the <code>export.types</code> parameter (default <code>torchscript</code>). </p>"},{"location":"tutorials/examples/ssl.html#run-advanced-changing-transformations","title":"Run (Advanced) - Changing transformations","text":"<p>In previous example, we have used default transformations defined in the original paper. However, these settings may not be suitable for our dataset. For example, <code>Gaussian Blur</code> may destroy important details. In this case, we can extend the experiment configuration file and add our custom transformations.</p> <pre><code># @package _global_\ndefaults:\n- base/ssl/byol\n- override /backbone: vit16_tiny # let's use different backbone instead of the resnet\n- _self_\n\ntrainer:\ndevices: [2] # we may need to use different gpu(s)\nmax_epochs: 1000 # let's assume we would like to train for 1000 epochs\n\ndatamodule:\ndata_path: /path/to/the/dataset\n\n# check configs/transforms/byol.yaml for more details\ntransforms:\naugmentation1:\n_target_: albumentations.Compose\ntransforms:\n- _target_: albumentations.RandomResizedCrop\nheight: ${transforms.input_height}\nwidth: ${transforms.input_width}\nscale: [0.08, 1.0]\n- ${transforms.flip_and_jitter}\n# remove gaussian blur\n# - _target_: albumentations.GaussianBlur\n#   blur_limit: 23\n#   sigma_limit: [0.1, 2]\n#   p: 1.0\n- ${transforms.normalize}\naugmentation2:\n_target_: albumentations.Compose\ntransforms:\n- _target_: albumentations.RandomResizedCrop\nheight: ${transforms.input_height}\nwidth: ${transforms.input_width}\nscale: [0.08, 1.0]\n- ${transforms.flip_and_jitter}\n# remove gaussian blur\n# - _target_: albumentations.GaussianBlur\n#   blur_limit: 23\n#   sigma_limit: [0.1, 2]\n#   p: 0.1\n- _target_: albumentations.Solarize\np: 0.2\n- ${transforms.normalize}\n</code></pre> <p>During training two different augmentations of the same image will be sampled based on the given parameter and the algorithm will try to match the representations of the two augmentations so picking the right set of transformations is important.</p>"}]}
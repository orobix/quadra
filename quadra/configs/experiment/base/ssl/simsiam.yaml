# @package _global_

defaults:
  - override /datamodule: base/ssl
  - override /backbone: resnet18
  - override /model: simsiam
  - override /optimizer: sgd
  - override /scheduler: warmup
  - override /transforms: byol
  - override /loss: simsiam
  - override /task: ssl
  - override /trainer: lightning_gpu_fp16
core:
  tag: "run"
  name: "simsiam_ssl"
task:
  _target_: quadra.tasks.ssl.Simsiam

trainer:
  devices: [0]
  max_epochs: 300
  num_sanity_val_steps: 0
  gradient_clip_val: 2.0
  check_val_every_n_epoch: 10
  gradient_clip_algorithm: "value"
  sync_batchnorm: true

datamodule:
  num_workers: 16
  batch_size: 128
  augmentation_dataset:
    _target_: quadra.datasets.TwoAugmentationDataset
    transform:
      - ${transforms.augmentation1}
      - ${transforms.augmentation2}
    dataset: null
callbacks:
  model_checkpoint:
    _target_: pytorch_lightning.callbacks.ModelCheckpoint
    monitor: "val_acc"
    mode: "max"

optimizer:
  lr: 0.0

scheduler:
  init_lr:
    - 0.001
  linear_warmup_epochs: 10

logger:
  mlflow:
    experiment_name: ${core.name}
